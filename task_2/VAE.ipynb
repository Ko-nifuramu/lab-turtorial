{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1676627634455,"user":{"displayName":"猪狩高","userId":"00742343053661794311"},"user_tz":-540},"id":"Mv-yPolM98su"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]},{"data":{"text/plain":["<torch._C.Generator at 0x14d50a586798>"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","import matplotlib.pyplot as plt\n","import math\n","import os\n","#from torchsummary import summary\n","from sklearn.decomposition import PCA\n","\n","seed =2\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(device)\n","gpu_id = 0\n","device = torch.device(device, gpu_id)\n","\n","if device =='cuda : 0':\n","    torch.cuda.manual_seed(seed)\n","\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":597,"status":"ok","timestamp":1676627652570,"user":{"displayName":"猪狩高","userId":"00742343053661794311"},"user_tz":-540},"id":"dAqam4Rq98sy","outputId":"028b27ae-ee27-420f-c8d6-001f6f988cf4"},"outputs":[{"name":"stdout","output_type":"stream","text":["original_data.shape = (100, 160, 24, 32, 3)\n","input_data.shape = (16000, 3, 24, 32)\n","1.0\n","0.0\n"]}],"source":["#データの取り込み\n","data_narray = np.load(\"image_states.npy\")\n","print(\"original_data.shape = {}\".format(data_narray.shape))\n","\n","#cnn入力用にreshapeする\n","data_input = np.reshape(data_narray, (-1, 3, 24, 32))\n","print(\"input_data.shape = {}\".format(data_input.shape))\n","print(np.max(data_input))\n","print(np.min(data_input))"]},{"cell_type":"code","execution_count":3,"metadata":{"cellView":"form","executionInfo":{"elapsed":211,"status":"ok","timestamp":1676627654460,"user":{"displayName":"猪狩高","userId":"00742343053661794311"},"user_tz":-540},"id":"kjfvC46H98sz"},"outputs":[],"source":["#@title データ定義\n","class MyDataset(torch.utils.data.Dataset):\n","    def __init__(self, input_data):\n","        self.input_data = torch.from_numpy(input_data).float()\n","\n","    def __len__(self):\n","        return self.input_data.shape[0]\n","\n","    def __getitem__(self, idx):\n","        input_datum = self.input_data[idx]#+torch.normal(mean=0, std = 1, size=self.input_data[idx].shape)<-入力0-1の外の値も入力してしまう\n","        return input_datum\n","\n","def create_dataloader(batch_size, input_data):\n","\n","    indeces = [int(input_data.shape[0] * n) for n in [0.4, 0.4+0.1, 0.9]]\n","    train_data1, val_data1, train_data2, val_data2 = np.split(input_data, indeces, axis=0)\n","    train_data = np.concatenate([train_data1, train_data2], axis=0)\n","    val_data = np.concatenate([val_data1, val_data2], axis=0)\n","\n","    train_dataset = MyDataset(train_data)\n","    val_dataset = MyDataset(val_data)\n","    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n","    \n","    return train_dataloader, val_dataloader\n","\n","def visualize_loss(epochs, dict_eachLoss, fig_name):\n","    fig = plt.figure()\n","    ax = fig.add_subplot()\n","    ax.plot(dict_eachLoss['train_recon'], linestyle=\"solid\")\n","    ax.plot(dict_eachLoss['train_kl'], linestyle=\"dashed\")\n","    ax.plot(dict_eachLoss['train_loss'], linestyle = \"dotted\")\n","    ax.plot(dict_eachLoss['val_recon'], linestyle=\"solid\")\n","    ax.plot(dict_eachLoss['val_kl'], linestyle=\"dashed\")\n","    ax.plot(dict_eachLoss['val_loss'], linestyle = \"dotted\")\n","    ax.set_yscale('log')\n","    ax.legend(['train_recon_loss * weight_lamda', 'train_kld', 'train_loss', 'val_recon * weight_lamda', 'val_kld', 'val_loss'],\\\n","              loc='center left', bbox_to_anchor=(1., .5))\n","    \n","\n","    ax.set_xlim(0, epochs)\n","    ax.set_title('each_loss')\n","    fig.savefig(fig_name + \".png\")\n","\n","\n","    {'train_loss' : [], 'train_recon' : [], 'train_kl' : [], 'val_loss' : [], 'val_recon' : [], 'val_kl' : [],}"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["class EarlyStopping:\n","    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n","    def __init__(self, patience=100, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n","        \"\"\"\n","        Args:\n","            patience (int): How long to wait after last time validation loss improved.\n","                            Default: 7\n","            verbose (bool): If True, prints a message for each validation loss improvement. \n","                            Default: False\n","            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n","                            Default: 0\n","            path (str): Path for the checkpoint to be saved to.\n","                            Default: 'checkpoint.pt'\n","            trace_func (function): trace print function.\n","                            Default: print            \n","        \"\"\"\n","        self.patience = patience\n","        self.verbose = verbose\n","        self.counter = 0\n","        self.best_score = None\n","        self.early_stop = False\n","        self.val_loss_min = np.Inf\n","        self.delta = delta\n","        self.path = path\n","        self.trace_func = trace_func\n","    def __call__(self, val_loss, model):\n","\n","        score = -val_loss\n","\n","        if self.best_score is None:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","        elif score < self.best_score + self.delta:\n","            self.counter += 1\n","            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","        else:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","            self.counter = 0\n","\n","    def save_checkpoint(self, val_loss, model):\n","        '''Saves model when validation loss decrease.'''\n","        if self.verbose:\n","            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n","        torch.save(model.state_dict(), self.path)\n","        self.val_loss_min = val_loss\n"]},{"cell_type":"code","execution_count":5,"metadata":{"cellView":"form","executionInfo":{"elapsed":226,"status":"ok","timestamp":1676627655744,"user":{"displayName":"猪狩高","userId":"00742343053661794311"},"user_tz":-540},"id":"T2Jw0XJ498s0"},"outputs":[],"source":["#@title モデル定義\n","# x -> (encoder) -> z_latent -> (decoder) -> y\n","#method : forward, reparameterize, cal_loss\n","class VAE_BCE(nn.Module):\n","    def __init__(self, z_dim,device):\n","        super(VAE_BCE, self).__init__()\n","        self.device = device\n","        self.encoder = VAE_Encoder(z_dim)\n","        self.decoder = VAE_Decoder(z_dim)\n","        self.z_dim = z_dim\n","\n","    def forward(self, x):\n","        mean, log_var = self.encoder(x)\n","        z = self.reparameterize(mean, log_var)\n","        y = self.decoder(z)\n","        return y, z\n","\n","    def reparameterize(self, mean, var):\n","        z = (mean + torch.mul(torch.sqrt(torch.exp(var)), torch.normal(mean = 0, std=1, size=mean.shape).to(self.device)))\n","        return z\n","\n","    def cal_loss(self, x, criterion = nn.BCELoss()):\n","        \n","        mean, log_var = self.encoder.forward(x)\n","        #print(\"var{}\".format(var))\n","        z = self.reparameterize(mean, log_var)\n","        y = self.decoder.forward(z)\n","        x = x.view(x.size(0), -1)\n","        y = y.view(y.size(0), -1)\n","\n","        #変分下限Lの最大化　-> -Lの最小化\n","        reconstruction = criterion(y , x)\n","        kl = -torch.sum(1+log_var- mean**2 - torch.exp(log_var))/2\n","        #print(\"reconstruction : {}\".format(reconstruction.shape))\n","        #print(\"KL : {}\".format(kl.shape))\n","        loss = reconstruction + kl\n","        return loss, reconstruction, kl \n","\n","class VAE_MSE(nn.Module):\n","    def __init__(self, z_dim,device):\n","        super(VAE_MSE, self).__init__()\n","        self.device = device\n","        self.encoder = VAE_Encoder(z_dim)\n","        self.decoder = VAE_Decoder(z_dim)\n","        self.z_dim = z_dim\n","        self.beta = 1\n","        self.var = 1e-2\n","\n","    def forward(self, x):\n","        mean, log_var = self.encoder(x)\n","        z = self.reparameterize(mean, log_var)\n","        y = self.decoder(z)\n","        return z, y\n","\n","    def reparameterize(self, mean, var):\n","        z = (mean + torch.mul(torch.sqrt(torch.exp(var)), torch.normal(mean = 0, std=1, size=mean.shape).to(self.device)))\n","        return z\n","\n","    def cal_loss(self, x, criterion = nn.MSELoss()):\n","        scale_adjust = x.shape[1] * x.shape[2] *x.shape[3]/ self.z_dim\n","        mean, log_var = self.encoder.forward(x)\n","        #print(\"var{}\".format(var))\n","        z = self.reparameterize(mean, log_var)\n","        y = self.decoder.forward(z)\n","        x = x.view(x.size(0), -1)\n","        y = y.view(y.size(0), -1)\n","\n","        #変分下限Lの最大化　-> -Lの最小化\n","        reconstruction = criterion(y , x) * scale_adjust/(self.var*2)\n","        kl = -torch.mean(1+log_var- mean**2 - torch.exp(log_var))/2#beta_vae\n","        #print(\"reconstruction : {}\".format(reconstruction.shape))\n","        #print(\"KL : {}\".format(kl.shape))\n","        loss =  reconstruction + kl\n","        return loss, reconstruction, kl \n","\n","\n","class VAE_Encoder(nn.Module):\n","    def __init__(self, z_dim):\n","        super(VAE_Encoder, self).__init__()\n","        self.cnn_layer1 = nn.Sequential(\n","            nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(num_features = 8),\n","            nn.Mish(),\n","            \n","        )#->(8, 24, 32)\n","\n","        self.cnn_layer2 = nn.Sequential(\n","            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=2, padding=1),\n","            nn.BatchNorm2d(num_features=16),\n","            nn.Mish(),\n","            \n","        )#->(8, 12, 16)\n","\n","        self.cnn_layer3 = nn.Sequential(\n","            nn.Conv2d(in_channels =16, out_channels=32, kernel_size=3, stride=2,),\n","            nn.BatchNorm2d(num_features=32),\n","            nn.Mish(),\n","            \n","        )#->(16, 5, 7)\n","        \n","        self.mean_layer = nn.Sequential(\n","            nn.Linear(32*5*7, 200),\n","            nn.Mish(),\n","            nn.BatchNorm1d(num_features=200),\n","            nn.Linear(200, z_dim)    \n","        )   \n","        self.log_var_layer = nn.Sequential(\n","            nn.Linear(32*5*7, 150),\n","            nn.Mish(),\n","            nn.BatchNorm1d(num_features=150),\n","            nn.Linear(150, z_dim)\n","            \n","        )   \n","\n","    def forward(self, x):\n","        out = self.cnn_layer1(x)\n","        out = self.cnn_layer2(out)\n","        out = self.cnn_layer3(out)\n","        out = out.view(out.size(0), -1)\n","\n","        mean = self.mean_layer(out)\n","        log_var = self.log_var_layer(out)\n","\n","        return mean, log_var\n","\n","class VAE_Decoder(nn.Module):\n","    def __init__(self, z_dim):\n","        super(VAE_Decoder, self).__init__()\n","        self.fc1 = nn.Linear(z_dim, 100)\n","        self.fc2 = nn.Linear(100, 32*3*4)\n","\n","        self.cnn_layer1 = nn.Sequential(\n","            nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=4, stride=2, padding=1),\n","            nn.BatchNorm2d(num_features = 16),\n","            nn.Mish(),\n","            \n","        )#->(8, 7, 9)\n","\n","        self.cnn_layer2 = nn.Sequential(\n","            nn.ConvTranspose2d(in_channels=16, out_channels=8, kernel_size=4, stride=2,padding=1),\n","            nn.BatchNorm2d(num_features = 8),\n","            nn.Mish(),\n","            \n","        )#->(8, 13, 17)\n","\n","        self.cnn_layer3 = nn.Sequential(\n","            nn.ConvTranspose2d(in_channels=8, out_channels=3, kernel_size=4, stride=2, padding=1),\n","        )#->(3, 24, 32)\n","\n","\n","    def forward(self, z):\n","        out = self.fc1(z)\n","        out = torch.relu(out)\n","        #out = nn.BatchNorm1d(num_features=40)(out)\n","        out = self.fc2(out)\n","        out = torch.relu(out)\n","        out = out.view(-1, 32, 3, 4)\n","        out = self.cnn_layer1(out)\n","        out = self.cnn_layer2(out)\n","        out = self.cnn_layer3(out)\n","        out = torch.sigmoid(out)\n","        \n","        return out\n","        \n","\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{"cellView":"form","executionInfo":{"elapsed":2,"status":"ok","timestamp":1676627656673,"user":{"displayName":"猪狩高","userId":"00742343053661794311"},"user_tz":-540},"id":"RmSMlWHF98s1"},"outputs":[],"source":["# earlyStopping、datalaoder\n","def train(model, optimizer, epochs, batch_size, lr, data_input, loss_fig_name):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    train_dataloader, val_dataloader = create_dataloader(batch_size, data_input)\n","    torch.manual_seed(seed=seed)\n","    optimizer = optimizer(model.parameters(), lr=lr)\n","    early_stopping = EarlyStopping()\n","\n","    #勾配クリッピング\n","    #grad_clip = 1\n","\n","    loss_dict = {'train_loss' : [], 'train_recon' : [], 'train_kl' : [], 'val_loss' : [], 'val_recon' : [], 'val_kl' : [],}\n","\n","    for epoch in range(epochs):\n","        train_loss = 0\n","        train_reconstruction = 0\n","        train_kld = 0\n","\n","        for inputs in train_dataloader:\n","\n","            model.train()\n","            inputs = inputs.to(device)\n","            loss, _, _ = model.cal_loss(inputs)\n","            #print(\"loss.shpe {}\".format(loss.shape))\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            #nn.utils.clip_grad_value_(parameters=model.parameters(), clip_value=grad_clip)\n","            optimizer.step()    \n","\n","            model.eval()\n","            with torch.no_grad():\n","                each_train_loss, train_recon, train_kl = model.cal_loss(inputs)\n","                train_loss += each_train_loss.item()\n","                train_reconstruction += train_recon.item()\n","                train_kld += train_kl.item()\n","\n","        model.eval()\n","        with torch.no_grad():\n","            val_loss = 0\n","            val_reconstruction = 0\n","            val_kld = 0\n","            for inputs in val_dataloader:\n","                inputs = inputs.to(device)\n","                each_val_loss, val_recon, val_kl = model.cal_loss(inputs)\n","                val_loss += each_val_loss.item()\n","                val_reconstruction += val_recon.item()\n","                val_kld += val_kl.item()\n","\n","\n","        print(\"Epoch: {}/{} \".format(epoch + 1, epochs),\n","            \"Traning Loss: {} \".format(train_loss/len(train_dataloader)),\n","            \"Train_Reconstruction: {} \".format(train_reconstruction/len(train_dataloader)),\n","            \"Train_KL: {} \".format(train_kld/len(train_dataloader)),\n","            \"Validation Loss : {}\".format(val_loss/len(val_dataloader)),\n","            \"Val_Reconstruction : {}\".format(val_reconstruction/len(val_dataloader)),\n","            \"Val_KL : {}\".format(val_kld/len(val_dataloader)))\n","\n","        loss_dict[\"train_loss\"].append(train_loss/len(train_dataloader))\n","        loss_dict[\"train_recon\"].append(train_reconstruction/len(train_dataloader))\n","        loss_dict[\"train_kl\"].append(train_kld/len(train_dataloader))\n","        loss_dict[\"val_loss\"].append(val_loss/len(val_dataloader))\n","        loss_dict[\"val_recon\"].append(val_reconstruction/len(val_dataloader))\n","        loss_dict[\"val_kl\"].append(val_kld/len(val_dataloader))\n","\n","        if math.isnan(val_loss) and epoch>0:\n","            break\n","        \n","        if epoch > 7000:\n","            early_stopping(val_loss/len(val_dataloader), model) # 最良モデルならモデルパラメータ保存\n","            if early_stopping.early_stop: \n","                        # 一定epochだけval_lossが最低値を更新しなかった場合、ここに入り学習を終了\n","                break   \n","\n","    visualize_loss( epoch,loss_dict,\"image/\"+loss_fig_name+\".png\")\n","    return epoch\n","    "]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["#x -> (encoder) -> z_latent -> (decoder) -> y\n","#16000のデータ化から、ランダムにデータxを１０個生成してx,yを比較\n","def generate_xy_Image(model, data_input, fig_name):\n","    device = torch.device('cpu')\n","    data_input = torch.from_numpy(data_input)\n","    data_input = data_input.to(device)\n","\n","    inputs, outputs = genrate_random_inputAndReconst(model, data_input)\n","\n","    #入力xの表示(10個分)\n","    input_images =inputs.to(device).detach().numpy().copy()\n","    print(\"input_image\")\n","    print(input_images.shape)\n","    print(np.min(input_images))\n","    print(np.max(input_images))\n","    for i, image in enumerate(input_images):\n","        plt.title(\"original\")\n","        plt.subplot(2, 5, i+1)\n","        plt.imshow(image.reshape(24, 32, 3))\n","        plt.axis(\"off\")\n","\n","    plt.tight_layout()\n","    plt.savefig(\"image/inputs\"+fig_name+\".png\")\n","\n","    #出力yの表示(10個)\n","    output_images = (outputs).to(device).detach().numpy().copy()\n","    print(\"renconst_image\")\n","    print(output_images.shape)\n","    print(np.min(output_images))\n","    print(np.max(output_images))\n","    for i, image in enumerate(output_images):\n","        plt.title(\"reconst\")\n","        plt.subplot(2, 5, i+1)\n","        image = image.reshape(24, 32, 3)\n","        plt.imshow(image)\n","        plt.axis(\"off\")\n","\n","    plt.tight_layout()\n","    plt.savefig(\"image/recon\" + fig_name + \".png\")     \n","\n","def genrate_random_inputAndReconst(model, data_input):\n","    device = torch.device(\"cpu\")\n","\n","    #入力データを2で割って学習させたので、画像の生成時には2をかけて元に戻す\n","    data_input_tensor = data_input.clone()\n","    inputs = torch.zeros((10, 3, 24, 32))\n","    outputs = torch.zeros((10, 3, 24, 32))\n","\n","    #xをランダムに生成\n","    torch.manual_seed(seed=2)\n","    random_index = torch.randint(low=0, high=16000, size=(10,))\n","    for i in range(10):\n","        input = data_input_tensor[random_index[i]]\n","        inputs[i] = input\n","    \n","    model = model.to(device)\n","    #yを求める    \n","    model.eval()\n","    with torch.no_grad():\n","        _, outputs = model.forward(inputs.to(device))\n","\n","    return inputs, outputs    "]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":389},"executionInfo":{"elapsed":351,"status":"error","timestamp":1676627658430,"user":{"displayName":"猪狩高","userId":"00742343053661794311"},"user_tz":-540},"id":"mml6Gkoq98s2","outputId":"52175812-3a16-4e0e-ffa9-43056a397bd1"},"outputs":[{"name":"stdout","output_type":"stream","text":["(16000, 3, 24, 32)\n","Epoch: 1/8000  Traning Loss: 681.4173202514648  Train_Reconstruction: 681.4141540527344  Train_KL: 0.003167698174365796  Validation Loss : 666.9275207519531 Val_Reconstruction : 666.9202880859375 Val_KL : 0.007240108912810683\n","Epoch: 2/8000  Traning Loss: 676.0977783203125  Train_Reconstruction: 676.0792083740234  Train_KL: 0.018571735825389624  Validation Loss : 659.9606323242188 Val_Reconstruction : 659.9297180175781 Val_KL : 0.030919977463781834\n","Epoch: 3/8000  Traning Loss: 663.7845458984375  Train_Reconstruction: 663.7104873657227  Train_KL: 0.07404496613889933  Validation Loss : 642.483154296875 Val_Reconstruction : 642.3598327636719 Val_KL : 0.12333983927965164\n","Epoch: 4/8000  Traning Loss: 633.7071762084961  Train_Reconstruction: 633.4302444458008  Train_KL: 0.27692614682018757  Validation Loss : 601.4993286132812 Val_Reconstruction : 601.0511474609375 Val_KL : 0.44817767292261124\n","Epoch: 5/8000  Traning Loss: 578.6428527832031  Train_Reconstruction: 577.7457427978516  Train_KL: 0.8971119299530983  Validation Loss : 542.3468322753906 Val_Reconstruction : 541.1015167236328 Val_KL : 1.2453137040138245\n","Epoch: 6/8000  Traning Loss: 522.540283203125  Train_Reconstruction: 520.5812492370605  Train_KL: 1.9590382725000381  Validation Loss : 494.95484924316406 Val_Reconstruction : 492.63427734375 Val_KL : 2.320572078227997\n","Epoch: 7/8000  Traning Loss: 483.02468490600586  Train_Reconstruction: 479.97867584228516  Train_KL: 3.0460080802440643  Validation Loss : 463.2416534423828 Val_Reconstruction : 460.0527038574219 Val_KL : 3.188958525657654\n","Epoch: 8/8000  Traning Loss: 456.4653968811035  Train_Reconstruction: 452.8878288269043  Train_KL: 3.5775619447231293  Validation Loss : 440.0546112060547 Val_Reconstruction : 436.6253204345703 Val_KL : 3.4292874336242676\n","Epoch: 9/8000  Traning Loss: 435.0969657897949  Train_Reconstruction: 431.38029861450195  Train_KL: 3.7166676819324493  Validation Loss : 420.3016662597656 Val_Reconstruction : 416.75926208496094 Val_KL : 3.54240083694458\n","Epoch: 10/8000  Traning Loss: 416.69530868530273  Train_Reconstruction: 412.8378219604492  Train_KL: 3.8574829399585724  Validation Loss : 402.80320739746094 Val_Reconstruction : 399.1288757324219 Val_KL : 3.674332857131958\n","Epoch: 11/8000  Traning Loss: 400.41807556152344  Train_Reconstruction: 396.4267463684082  Train_KL: 3.9913337230682373  Validation Loss : 387.8666687011719 Val_Reconstruction : 384.05897521972656 Val_KL : 3.8076976537704468\n","Epoch: 12/8000  Traning Loss: 385.9557304382324  Train_Reconstruction: 381.7837600708008  Train_KL: 4.17196524143219  Validation Loss : 374.65789794921875 Val_Reconstruction : 370.66943359375 Val_KL : 3.9884636402130127\n","Epoch: 13/8000  Traning Loss: 373.9189147949219  Train_Reconstruction: 369.58837127685547  Train_KL: 4.3305453062057495  Validation Loss : 363.0526123046875 Val_Reconstruction : 358.90504455566406 Val_KL : 4.14756715297699\n","Epoch: 14/8000  Traning Loss: 362.8037796020508  Train_Reconstruction: 358.3115921020508  Train_KL: 4.492188036441803  Validation Loss : 352.9356689453125 Val_Reconstruction : 348.69361877441406 Val_KL : 4.242054462432861\n","Epoch: 15/8000  Traning Loss: 352.9580993652344  Train_Reconstruction: 348.3955993652344  Train_KL: 4.562497496604919  Validation Loss : 343.3231506347656 Val_Reconstruction : 338.9918975830078 Val_KL : 4.3312461376190186\n","Epoch: 16/8000  Traning Loss: 343.5393943786621  Train_Reconstruction: 338.89219665527344  Train_KL: 4.647197127342224  Validation Loss : 334.2504577636719 Val_Reconstruction : 329.86463928222656 Val_KL : 4.385819435119629\n","Epoch: 17/8000  Traning Loss: 334.1728630065918  Train_Reconstruction: 329.3995018005371  Train_KL: 4.773361504077911  Validation Loss : 324.9353942871094 Val_Reconstruction : 320.49339294433594 Val_KL : 4.442007541656494\n","Epoch: 18/8000  Traning Loss: 324.9981002807617  Train_Reconstruction: 320.1203918457031  Train_KL: 4.877712726593018  Validation Loss : 316.06333923339844 Val_Reconstruction : 311.4801788330078 Val_KL : 4.5831618309021\n","Epoch: 19/8000  Traning Loss: 316.2308464050293  Train_Reconstruction: 311.28712463378906  Train_KL: 4.943723976612091  Validation Loss : 308.1041259765625 Val_Reconstruction : 303.4900665283203 Val_KL : 4.614065408706665\n","Epoch: 20/8000  Traning Loss: 308.03799057006836  Train_Reconstruction: 303.0641174316406  Train_KL: 4.973875820636749  Validation Loss : 300.0424041748047 Val_Reconstruction : 295.35457611083984 Val_KL : 4.687835454940796\n","Epoch: 21/8000  Traning Loss: 300.07362365722656  Train_Reconstruction: 295.01049041748047  Train_KL: 5.063131749629974  Validation Loss : 292.1705551147461 Val_Reconstruction : 287.4266357421875 Val_KL : 4.7439258098602295\n","Epoch: 22/8000  Traning Loss: 292.4732475280762  Train_Reconstruction: 287.3174133300781  Train_KL: 5.15583336353302  Validation Loss : 284.7323455810547 Val_Reconstruction : 279.8346710205078 Val_KL : 4.897676944732666\n","Epoch: 23/8000  Traning Loss: 284.84033966064453  Train_Reconstruction: 279.60322189331055  Train_KL: 5.237123072147369  Validation Loss : 277.5835876464844 Val_Reconstruction : 272.6736373901367 Val_KL : 4.909947156906128\n","Epoch: 24/8000  Traning Loss: 278.14440536499023  Train_Reconstruction: 272.89524841308594  Train_KL: 5.249154269695282  Validation Loss : 270.8399124145508 Val_Reconstruction : 265.90621185302734 Val_KL : 4.9337077140808105\n","Epoch: 25/8000  Traning Loss: 271.8202247619629  Train_Reconstruction: 266.57103729248047  Train_KL: 5.249184012413025  Validation Loss : 264.8678970336914 Val_Reconstruction : 259.9131317138672 Val_KL : 4.954772472381592\n","Epoch: 26/8000  Traning Loss: 265.7746238708496  Train_Reconstruction: 260.4918575286865  Train_KL: 5.282765209674835  Validation Loss : 259.12232971191406 Val_Reconstruction : 254.15919494628906 Val_KL : 4.963136196136475\n","Epoch: 27/8000  Traning Loss: 260.2144966125488  Train_Reconstruction: 254.92771530151367  Train_KL: 5.286777555942535  Validation Loss : 253.4090576171875 Val_Reconstruction : 248.37742614746094 Val_KL : 5.03163743019104\n","Epoch: 28/8000  Traning Loss: 254.53405380249023  Train_Reconstruction: 249.2132396697998  Train_KL: 5.320809006690979  Validation Loss : 247.92860412597656 Val_Reconstruction : 242.86528778076172 Val_KL : 5.063317775726318\n","Epoch: 29/8000  Traning Loss: 249.53101348876953  Train_Reconstruction: 244.21070289611816  Train_KL: 5.320310533046722  Validation Loss : 243.26265716552734 Val_Reconstruction : 238.20645141601562 Val_KL : 5.056198358535767\n","Epoch: 30/8000  Traning Loss: 244.85261917114258  Train_Reconstruction: 239.5595817565918  Train_KL: 5.293038785457611  Validation Loss : 238.79999542236328 Val_Reconstruction : 233.7696762084961 Val_KL : 5.030316591262817\n","Epoch: 31/8000  Traning Loss: 240.45819854736328  Train_Reconstruction: 235.10539436340332  Train_KL: 5.352804660797119  Validation Loss : 234.1324234008789 Val_Reconstruction : 228.99100494384766 Val_KL : 5.141421556472778\n","Epoch: 32/8000  Traning Loss: 236.21184539794922  Train_Reconstruction: 230.86566734313965  Train_KL: 5.346179306507111  Validation Loss : 230.41342163085938 Val_Reconstruction : 225.3280792236328 Val_KL : 5.085349798202515\n","Epoch: 33/8000  Traning Loss: 232.24818801879883  Train_Reconstruction: 226.96974563598633  Train_KL: 5.2784425020217896  Validation Loss : 226.295166015625 Val_Reconstruction : 221.2253875732422 Val_KL : 5.069775819778442\n","Epoch: 34/8000  Traning Loss: 228.75823402404785  Train_Reconstruction: 223.50125312805176  Train_KL: 5.256981313228607  Validation Loss : 222.82953643798828 Val_Reconstruction : 217.7949981689453 Val_KL : 5.034532070159912\n","Epoch: 35/8000  Traning Loss: 225.25323486328125  Train_Reconstruction: 220.03748893737793  Train_KL: 5.215744435787201  Validation Loss : 219.2340545654297 Val_Reconstruction : 214.28207397460938 Val_KL : 4.951975345611572\n","Epoch: 36/8000  Traning Loss: 221.90247535705566  Train_Reconstruction: 216.77593803405762  Train_KL: 5.126536548137665  Validation Loss : 216.15558624267578 Val_Reconstruction : 211.21876525878906 Val_KL : 4.936816692352295\n","Epoch: 37/8000  Traning Loss: 218.78225708007812  Train_Reconstruction: 213.65044975280762  Train_KL: 5.131809175014496  Validation Loss : 212.99693298339844 Val_Reconstruction : 208.1036834716797 Val_KL : 4.893249750137329\n","Epoch: 38/8000  Traning Loss: 215.66944885253906  Train_Reconstruction: 210.64861869812012  Train_KL: 5.0208282470703125  Validation Loss : 209.98190307617188 Val_Reconstruction : 205.21150970458984 Val_KL : 4.770393371582031\n","Epoch: 39/8000  Traning Loss: 212.85460472106934  Train_Reconstruction: 207.90105628967285  Train_KL: 4.953549385070801  Validation Loss : 207.37672424316406 Val_Reconstruction : 202.60751342773438 Val_KL : 4.76921272277832\n","Epoch: 40/8000  Traning Loss: 209.99029350280762  Train_Reconstruction: 205.0807590484619  Train_KL: 4.909535586833954  Validation Loss : 204.73292541503906 Val_Reconstruction : 199.98904418945312 Val_KL : 4.743885517120361\n","Epoch: 41/8000  Traning Loss: 207.62848281860352  Train_Reconstruction: 202.76370811462402  Train_KL: 4.864777088165283  Validation Loss : 202.18730926513672 Val_Reconstruction : 197.5673599243164 Val_KL : 4.619950771331787\n","Epoch: 42/8000  Traning Loss: 205.1292896270752  Train_Reconstruction: 200.39249420166016  Train_KL: 4.736794948577881  Validation Loss : 199.8121337890625 Val_Reconstruction : 195.28416442871094 Val_KL : 4.527965068817139\n","Epoch: 43/8000  Traning Loss: 202.7381820678711  Train_Reconstruction: 198.04994010925293  Train_KL: 4.688244462013245  Validation Loss : 197.3399200439453 Val_Reconstruction : 192.89806365966797 Val_KL : 4.44185996055603\n","Epoch: 44/8000  Traning Loss: 200.46423721313477  Train_Reconstruction: 195.8391284942627  Train_KL: 4.625106632709503  Validation Loss : 195.03659057617188 Val_Reconstruction : 190.6234588623047 Val_KL : 4.4131317138671875\n","Epoch: 45/8000  Traning Loss: 198.26959609985352  Train_Reconstruction: 193.7371063232422  Train_KL: 4.532489001750946  Validation Loss : 192.9223175048828 Val_Reconstruction : 188.6352996826172 Val_KL : 4.287022352218628\n","Epoch: 46/8000  Traning Loss: 196.11445808410645  Train_Reconstruction: 191.67125701904297  Train_KL: 4.443200469017029  Validation Loss : 190.74373626708984 Val_Reconstruction : 186.50027465820312 Val_KL : 4.243466734886169\n","Epoch: 47/8000  Traning Loss: 194.08888816833496  Train_Reconstruction: 189.70896911621094  Train_KL: 4.379920542240143  Validation Loss : 188.83226013183594 Val_Reconstruction : 184.6605987548828 Val_KL : 4.17165732383728\n","Epoch: 48/8000  Traning Loss: 192.1231689453125  Train_Reconstruction: 187.7976131439209  Train_KL: 4.325555622577667  Validation Loss : 186.7736587524414 Val_Reconstruction : 182.66586303710938 Val_KL : 4.10779881477356\n","Epoch: 49/8000  Traning Loss: 190.24062538146973  Train_Reconstruction: 185.9901828765869  Train_KL: 4.2504425048828125  Validation Loss : 184.87144470214844 Val_Reconstruction : 180.8349609375 Val_KL : 4.036485075950623\n","Epoch: 50/8000  Traning Loss: 188.50580596923828  Train_Reconstruction: 184.33394050598145  Train_KL: 4.171863555908203  Validation Loss : 183.02458953857422 Val_Reconstruction : 179.03562927246094 Val_KL : 3.988960027694702\n","Epoch: 51/8000  Traning Loss: 186.57457542419434  Train_Reconstruction: 182.46133041381836  Train_KL: 4.113245248794556  Validation Loss : 181.34922790527344 Val_Reconstruction : 177.42919921875 Val_KL : 3.9200310707092285\n","Epoch: 52/8000  Traning Loss: 184.99441719055176  Train_Reconstruction: 180.95559120178223  Train_KL: 4.038822889328003  Validation Loss : 179.61341857910156 Val_Reconstruction : 175.7504653930664 Val_KL : 3.862952470779419\n","Epoch: 53/8000  Traning Loss: 183.36942100524902  Train_Reconstruction: 179.3953094482422  Train_KL: 3.974111884832382  Validation Loss : 178.1484146118164 Val_Reconstruction : 174.35662078857422 Val_KL : 3.791791319847107\n","Epoch: 54/8000  Traning Loss: 181.76767539978027  Train_Reconstruction: 177.8427963256836  Train_KL: 3.9248769283294678  Validation Loss : 176.2808837890625 Val_Reconstruction : 172.53768920898438 Val_KL : 3.74319589138031\n","Epoch: 55/8000  Traning Loss: 180.2724094390869  Train_Reconstruction: 176.39289093017578  Train_KL: 3.8795194029808044  Validation Loss : 174.67901611328125 Val_Reconstruction : 170.9484100341797 Val_KL : 3.730603814125061\n","Epoch: 56/8000  Traning Loss: 178.7784881591797  Train_Reconstruction: 174.93181037902832  Train_KL: 3.8466783463954926  Validation Loss : 173.62714385986328 Val_Reconstruction : 169.94788360595703 Val_KL : 3.679264545440674\n","Epoch: 57/8000  Traning Loss: 177.30900955200195  Train_Reconstruction: 173.50329971313477  Train_KL: 3.8057113885879517  Validation Loss : 171.94489288330078 Val_Reconstruction : 168.28492736816406 Val_KL : 3.659969925880432\n","Epoch: 58/8000  Traning Loss: 175.8657283782959  Train_Reconstruction: 172.08490753173828  Train_KL: 3.780822455883026  Validation Loss : 170.48218536376953 Val_Reconstruction : 166.8586196899414 Val_KL : 3.623562216758728\n","Epoch: 59/8000  Traning Loss: 174.49040412902832  Train_Reconstruction: 170.75827980041504  Train_KL: 3.7321247160434723  Validation Loss : 168.98639678955078 Val_Reconstruction : 165.4062728881836 Val_KL : 3.5801174640655518\n","Epoch: 60/8000  Traning Loss: 173.2787265777588  Train_Reconstruction: 169.58431816101074  Train_KL: 3.694407045841217  Validation Loss : 168.15167999267578 Val_Reconstruction : 164.59909057617188 Val_KL : 3.5525888204574585\n","Epoch: 61/8000  Traning Loss: 172.1919002532959  Train_Reconstruction: 168.54818725585938  Train_KL: 3.643711596727371  Validation Loss : 166.77132415771484 Val_Reconstruction : 163.2467498779297 Val_KL : 3.524576783180237\n","Epoch: 62/8000  Traning Loss: 170.9266529083252  Train_Reconstruction: 167.2938938140869  Train_KL: 3.6327605545520782  Validation Loss : 165.69034576416016 Val_Reconstruction : 162.22169494628906 Val_KL : 3.468648910522461\n","Epoch: 63/8000  Traning Loss: 169.648530960083  Train_Reconstruction: 166.04838752746582  Train_KL: 3.6001452207565308  Validation Loss : 164.4903106689453 Val_Reconstruction : 161.03819274902344 Val_KL : 3.452115774154663\n","Epoch: 64/8000  Traning Loss: 168.41755867004395  Train_Reconstruction: 164.81772994995117  Train_KL: 3.5998290479183197  Validation Loss : 163.15721893310547 Val_Reconstruction : 159.72900390625 Val_KL : 3.4282113313674927\n","Epoch: 65/8000  Traning Loss: 167.39465522766113  Train_Reconstruction: 163.8554801940918  Train_KL: 3.539178431034088  Validation Loss : 161.97837829589844 Val_Reconstruction : 158.57105255126953 Val_KL : 3.40733003616333\n","Epoch: 66/8000  Traning Loss: 166.30219268798828  Train_Reconstruction: 162.78441429138184  Train_KL: 3.5177784264087677  Validation Loss : 161.10616302490234 Val_Reconstruction : 157.72866821289062 Val_KL : 3.3774917125701904\n","Epoch: 67/8000  Traning Loss: 165.37429809570312  Train_Reconstruction: 161.8654613494873  Train_KL: 3.508837938308716  Validation Loss : 159.79813385009766 Val_Reconstruction : 156.43248748779297 Val_KL : 3.3656439781188965\n","Epoch: 68/8000  Traning Loss: 164.34314918518066  Train_Reconstruction: 160.87878799438477  Train_KL: 3.4643613398075104  Validation Loss : 158.9373550415039 Val_Reconstruction : 155.60382843017578 Val_KL : 3.3335264921188354\n","Epoch: 69/8000  Traning Loss: 163.3398895263672  Train_Reconstruction: 159.88441467285156  Train_KL: 3.4554734230041504  Validation Loss : 157.93170166015625 Val_Reconstruction : 154.58546447753906 Val_KL : 3.346237063407898\n","Epoch: 70/8000  Traning Loss: 162.40480995178223  Train_Reconstruction: 158.96394729614258  Train_KL: 3.4408625960350037  Validation Loss : 157.22374725341797 Val_Reconstruction : 153.9122772216797 Val_KL : 3.311470627784729\n","Epoch: 71/8000  Traning Loss: 161.5237979888916  Train_Reconstruction: 158.09708976745605  Train_KL: 3.4267061948776245  Validation Loss : 155.9956283569336 Val_Reconstruction : 152.6847152709961 Val_KL : 3.3109130859375\n","Epoch: 72/8000  Traning Loss: 160.51827239990234  Train_Reconstruction: 157.1010513305664  Train_KL: 3.4172223806381226  Validation Loss : 155.1845932006836 Val_Reconstruction : 151.91053771972656 Val_KL : 3.2740575075149536\n","Epoch: 73/8000  Traning Loss: 159.65855026245117  Train_Reconstruction: 156.28667068481445  Train_KL: 3.3718790411949158  Validation Loss : 154.34015655517578 Val_Reconstruction : 151.05921173095703 Val_KL : 3.2809455394744873\n","Epoch: 74/8000  Traning Loss: 158.75048065185547  Train_Reconstruction: 155.38881301879883  Train_KL: 3.3616689443588257  Validation Loss : 153.4752197265625 Val_Reconstruction : 150.23340606689453 Val_KL : 3.241814374923706\n","Epoch: 75/8000  Traning Loss: 157.98754501342773  Train_Reconstruction: 154.64766693115234  Train_KL: 3.3398779928684235  Validation Loss : 152.80978393554688 Val_Reconstruction : 149.549560546875 Val_KL : 3.260224461555481\n","Epoch: 76/8000  Traning Loss: 157.21918296813965  Train_Reconstruction: 153.8621597290039  Train_KL: 3.357021749019623  Validation Loss : 151.93212890625 Val_Reconstruction : 148.6874008178711 Val_KL : 3.2447301149368286\n","Epoch: 77/8000  Traning Loss: 156.2850799560547  Train_Reconstruction: 152.93638801574707  Train_KL: 3.348691374063492  Validation Loss : 150.91650390625 Val_Reconstruction : 147.6781997680664 Val_KL : 3.238307476043701\n","Epoch: 78/8000  Traning Loss: 155.59064483642578  Train_Reconstruction: 152.25450134277344  Train_KL: 3.3361416161060333  Validation Loss : 150.35698699951172 Val_Reconstruction : 147.13583374023438 Val_KL : 3.221148729324341\n","Epoch: 79/8000  Traning Loss: 155.0072956085205  Train_Reconstruction: 151.69397163391113  Train_KL: 3.313324987888336  Validation Loss : 149.68785095214844 Val_Reconstruction : 146.4635467529297 Val_KL : 3.224303126335144\n","Epoch: 80/8000  Traning Loss: 154.23867988586426  Train_Reconstruction: 150.9218692779541  Train_KL: 3.316810041666031  Validation Loss : 149.0317153930664 Val_Reconstruction : 145.82559967041016 Val_KL : 3.206122398376465\n","Epoch: 81/8000  Traning Loss: 153.60187911987305  Train_Reconstruction: 150.2916259765625  Train_KL: 3.3102549612522125  Validation Loss : 148.36817932128906 Val_Reconstruction : 145.17652130126953 Val_KL : 3.1916630268096924\n","Epoch: 82/8000  Traning Loss: 152.9700584411621  Train_Reconstruction: 149.68471908569336  Train_KL: 3.2853377163410187  Validation Loss : 147.66175079345703 Val_Reconstruction : 144.47734832763672 Val_KL : 3.1843972206115723\n","Epoch: 83/8000  Traning Loss: 152.258882522583  Train_Reconstruction: 148.95802688598633  Train_KL: 3.300855189561844  Validation Loss : 147.10076141357422 Val_Reconstruction : 143.92654418945312 Val_KL : 3.174214005470276\n","Epoch: 84/8000  Traning Loss: 151.70524787902832  Train_Reconstruction: 148.40359687805176  Train_KL: 3.3016510009765625  Validation Loss : 146.50797271728516 Val_Reconstruction : 143.29940032958984 Val_KL : 3.208571434020996\n","Epoch: 85/8000  Traning Loss: 151.2596263885498  Train_Reconstruction: 147.9752082824707  Train_KL: 3.2844162583351135  Validation Loss : 146.08828735351562 Val_Reconstruction : 142.89608001708984 Val_KL : 3.192208766937256\n","Epoch: 86/8000  Traning Loss: 150.6540927886963  Train_Reconstruction: 147.349515914917  Train_KL: 3.3045777678489685  Validation Loss : 145.3175811767578 Val_Reconstruction : 142.12548065185547 Val_KL : 3.1920955181121826\n","Epoch: 87/8000  Traning Loss: 150.06402206420898  Train_Reconstruction: 146.78342056274414  Train_KL: 3.280601680278778  Validation Loss : 144.70536041259766 Val_Reconstruction : 141.52666854858398 Val_KL : 3.1786900758743286\n","Epoch: 88/8000  Traning Loss: 149.48760604858398  Train_Reconstruction: 146.19011306762695  Train_KL: 3.297493040561676  Validation Loss : 144.07952880859375 Val_Reconstruction : 140.87734603881836 Val_KL : 3.20218288898468\n","Epoch: 89/8000  Traning Loss: 148.90510749816895  Train_Reconstruction: 145.6108226776123  Train_KL: 3.2942855656147003  Validation Loss : 143.47145080566406 Val_Reconstruction : 140.27894973754883 Val_KL : 3.192498803138733\n","Epoch: 90/8000  Traning Loss: 148.42455863952637  Train_Reconstruction: 145.13339805603027  Train_KL: 3.2911583483219147  Validation Loss : 143.0726089477539 Val_Reconstruction : 139.89459609985352 Val_KL : 3.1780147552490234\n","Epoch: 91/8000  Traning Loss: 147.76042366027832  Train_Reconstruction: 144.47916221618652  Train_KL: 3.281264513731003  Validation Loss : 142.411376953125 Val_Reconstruction : 139.24223709106445 Val_KL : 3.169138789176941\n","Epoch: 92/8000  Traning Loss: 147.19071197509766  Train_Reconstruction: 143.93209838867188  Train_KL: 3.2586103677749634  Validation Loss : 141.8788299560547 Val_Reconstruction : 138.71158599853516 Val_KL : 3.1672401428222656\n","Epoch: 93/8000  Traning Loss: 146.73837661743164  Train_Reconstruction: 143.48319816589355  Train_KL: 3.2551808655261993  Validation Loss : 141.31873321533203 Val_Reconstruction : 138.1653594970703 Val_KL : 3.1533743143081665\n","Epoch: 94/8000  Traning Loss: 146.1737766265869  Train_Reconstruction: 142.9182243347168  Train_KL: 3.255552440881729  Validation Loss : 140.86172485351562 Val_Reconstruction : 137.69336700439453 Val_KL : 3.1683560609817505\n","Epoch: 95/8000  Traning Loss: 145.6662940979004  Train_Reconstruction: 142.43067741394043  Train_KL: 3.2356168627738953  Validation Loss : 140.51295471191406 Val_Reconstruction : 137.39239883422852 Val_KL : 3.1205536127090454\n","Epoch: 96/8000  Traning Loss: 145.40041542053223  Train_Reconstruction: 142.16987228393555  Train_KL: 3.2305468916893005  Validation Loss : 140.18801498413086 Val_Reconstruction : 137.03094482421875 Val_KL : 3.157072424888611\n","Epoch: 97/8000  Traning Loss: 144.9298267364502  Train_Reconstruction: 141.67830276489258  Train_KL: 3.251524478197098  Validation Loss : 139.7003936767578 Val_Reconstruction : 136.5453338623047 Val_KL : 3.1550638675689697\n","Epoch: 98/8000  Traning Loss: 144.33428192138672  Train_Reconstruction: 141.0967140197754  Train_KL: 3.237566441297531  Validation Loss : 139.1431884765625 Val_Reconstruction : 136.01171875 Val_KL : 3.1314719915390015\n","Epoch: 99/8000  Traning Loss: 143.83968925476074  Train_Reconstruction: 140.59855461120605  Train_KL: 3.241135686635971  Validation Loss : 138.53052139282227 Val_Reconstruction : 135.38953399658203 Val_KL : 3.14098858833313\n","Epoch: 100/8000  Traning Loss: 143.4403533935547  Train_Reconstruction: 140.19979667663574  Train_KL: 3.2405554354190826  Validation Loss : 138.22997665405273 Val_Reconstruction : 135.07225799560547 Val_KL : 3.157719850540161\n","Epoch: 101/8000  Traning Loss: 143.06015014648438  Train_Reconstruction: 139.8069190979004  Train_KL: 3.2532317638397217  Validation Loss : 137.81006622314453 Val_Reconstruction : 134.64733123779297 Val_KL : 3.1627341508865356\n","Epoch: 102/8000  Traning Loss: 142.69146347045898  Train_Reconstruction: 139.44814109802246  Train_KL: 3.243321657180786  Validation Loss : 137.48374938964844 Val_Reconstruction : 134.32744598388672 Val_KL : 3.156308174133301\n","Epoch: 103/8000  Traning Loss: 142.42858695983887  Train_Reconstruction: 139.1951904296875  Train_KL: 3.2333956956863403  Validation Loss : 137.42780303955078 Val_Reconstruction : 134.27313232421875 Val_KL : 3.1546709537506104\n","Epoch: 104/8000  Traning Loss: 142.17473602294922  Train_Reconstruction: 138.94909286499023  Train_KL: 3.225642591714859  Validation Loss : 137.06539154052734 Val_Reconstruction : 133.91813278198242 Val_KL : 3.1472610235214233\n","Epoch: 105/8000  Traning Loss: 141.88545036315918  Train_Reconstruction: 138.63973999023438  Train_KL: 3.245711326599121  Validation Loss : 136.82720565795898 Val_Reconstruction : 133.66091918945312 Val_KL : 3.166290760040283\n","Epoch: 106/8000  Traning Loss: 141.36868858337402  Train_Reconstruction: 138.10597801208496  Train_KL: 3.2627090513706207  Validation Loss : 136.20966339111328 Val_Reconstruction : 133.05239868164062 Val_KL : 3.157265543937683\n","Epoch: 107/8000  Traning Loss: 140.9194736480713  Train_Reconstruction: 137.66747093200684  Train_KL: 3.25200417637825  Validation Loss : 135.68978118896484 Val_Reconstruction : 132.53545379638672 Val_KL : 3.154329776763916\n","Epoch: 108/8000  Traning Loss: 140.62505722045898  Train_Reconstruction: 137.3861083984375  Train_KL: 3.2389508485794067  Validation Loss : 135.50239944458008 Val_Reconstruction : 132.36055755615234 Val_KL : 3.1418412923812866\n","Epoch: 109/8000  Traning Loss: 140.5486125946045  Train_Reconstruction: 137.31039428710938  Train_KL: 3.2382159531116486  Validation Loss : 135.2956199645996 Val_Reconstruction : 132.14286422729492 Val_KL : 3.152756929397583\n","Epoch: 110/8000  Traning Loss: 139.96044921875  Train_Reconstruction: 136.7228832244873  Train_KL: 3.237566739320755  Validation Loss : 134.5116844177246 Val_Reconstruction : 131.36578369140625 Val_KL : 3.145902633666992\n","Epoch: 111/8000  Traning Loss: 139.46495056152344  Train_Reconstruction: 136.22821235656738  Train_KL: 3.2367368936538696  Validation Loss : 134.3458480834961 Val_Reconstruction : 131.20072555541992 Val_KL : 3.1451209783554077\n","Epoch: 112/8000  Traning Loss: 139.11822700500488  Train_Reconstruction: 135.88107872009277  Train_KL: 3.2371488511562347  Validation Loss : 133.69562149047852 Val_Reconstruction : 130.52852630615234 Val_KL : 3.167095899581909\n","Epoch: 113/8000  Traning Loss: 138.66238021850586  Train_Reconstruction: 135.41057205200195  Train_KL: 3.2518069744110107  Validation Loss : 133.55155563354492 Val_Reconstruction : 130.39082717895508 Val_KL : 3.1607248783111572\n","Epoch: 114/8000  Traning Loss: 138.60395622253418  Train_Reconstruction: 135.35502815246582  Train_KL: 3.248929589986801  Validation Loss : 133.44688415527344 Val_Reconstruction : 130.2785987854004 Val_KL : 3.1682873964309692\n","Epoch: 115/8000  Traning Loss: 138.21589469909668  Train_Reconstruction: 134.98858070373535  Train_KL: 3.2273162603378296  Validation Loss : 133.13970184326172 Val_Reconstruction : 130.00328063964844 Val_KL : 3.1364229917526245\n","Epoch: 116/8000  Traning Loss: 137.8184986114502  Train_Reconstruction: 134.58378791809082  Train_KL: 3.2347094118595123  Validation Loss : 132.5854835510254 Val_Reconstruction : 129.43365097045898 Val_KL : 3.1518378257751465\n","Epoch: 117/8000  Traning Loss: 137.8057098388672  Train_Reconstruction: 134.58178901672363  Train_KL: 3.223921477794647  Validation Loss : 132.71704864501953 Val_Reconstruction : 129.55895233154297 Val_KL : 3.1580944061279297\n","Epoch: 118/8000  Traning Loss: 137.48886680603027  Train_Reconstruction: 134.25280380249023  Train_KL: 3.2360634803771973  Validation Loss : 132.17623138427734 Val_Reconstruction : 129.01491928100586 Val_KL : 3.1613168716430664\n","Epoch: 119/8000  Traning Loss: 137.2622013092041  Train_Reconstruction: 134.02774047851562  Train_KL: 3.23446062207222  Validation Loss : 131.95333099365234 Val_Reconstruction : 128.8083724975586 Val_KL : 3.144955039024353\n","Epoch: 120/8000  Traning Loss: 136.66351318359375  Train_Reconstruction: 133.42461013793945  Train_KL: 3.238905131816864  Validation Loss : 131.43001556396484 Val_Reconstruction : 128.2723388671875 Val_KL : 3.1576751470565796\n","Epoch: 121/8000  Traning Loss: 136.45502853393555  Train_Reconstruction: 133.2229404449463  Train_KL: 3.232088267803192  Validation Loss : 131.2762451171875 Val_Reconstruction : 128.10145950317383 Val_KL : 3.174787163734436\n","Epoch: 122/8000  Traning Loss: 136.10805130004883  Train_Reconstruction: 132.86985969543457  Train_KL: 3.238188713788986  Validation Loss : 130.99705123901367 Val_Reconstruction : 127.84321975708008 Val_KL : 3.1538299322128296\n","Epoch: 123/8000  Traning Loss: 135.9385929107666  Train_Reconstruction: 132.7002468109131  Train_KL: 3.2383439540863037  Validation Loss : 130.87033462524414 Val_Reconstruction : 127.70931243896484 Val_KL : 3.1610230207443237\n","Epoch: 124/8000  Traning Loss: 135.690092086792  Train_Reconstruction: 132.46042251586914  Train_KL: 3.229669690132141  Validation Loss : 130.73530197143555 Val_Reconstruction : 127.58043670654297 Val_KL : 3.1548619270324707\n","Epoch: 125/8000  Traning Loss: 135.36905670166016  Train_Reconstruction: 132.13947105407715  Train_KL: 3.2295871675014496  Validation Loss : 130.27623748779297 Val_Reconstruction : 127.11153793334961 Val_KL : 3.1646991968154907\n","Epoch: 126/8000  Traning Loss: 135.1005344390869  Train_Reconstruction: 131.86052703857422  Train_KL: 3.240007668733597  Validation Loss : 130.08778762817383 Val_Reconstruction : 126.92596054077148 Val_KL : 3.1618236303329468\n","Epoch: 127/8000  Traning Loss: 134.8219699859619  Train_Reconstruction: 131.59003067016602  Train_KL: 3.2319397926330566  Validation Loss : 129.80165100097656 Val_Reconstruction : 126.64094161987305 Val_KL : 3.1607093811035156\n","Epoch: 128/8000  Traning Loss: 134.61459732055664  Train_Reconstruction: 131.38701820373535  Train_KL: 3.227578967809677  Validation Loss : 129.58273696899414 Val_Reconstruction : 126.41241073608398 Val_KL : 3.1703264713287354\n","Epoch: 129/8000  Traning Loss: 134.30321502685547  Train_Reconstruction: 131.06981563568115  Train_KL: 3.2334007024765015  Validation Loss : 129.6142349243164 Val_Reconstruction : 126.44085693359375 Val_KL : 3.173379898071289\n","Epoch: 130/8000  Traning Loss: 134.06601905822754  Train_Reconstruction: 130.82904434204102  Train_KL: 3.2369751036167145  Validation Loss : 128.95422744750977 Val_Reconstruction : 125.78402328491211 Val_KL : 3.1702046394348145\n","Epoch: 131/8000  Traning Loss: 133.88020706176758  Train_Reconstruction: 130.6512680053711  Train_KL: 3.228938966989517  Validation Loss : 128.97325134277344 Val_Reconstruction : 125.81762313842773 Val_KL : 3.155631184577942\n","Epoch: 132/8000  Traning Loss: 133.8120822906494  Train_Reconstruction: 130.59253597259521  Train_KL: 3.219546765089035  Validation Loss : 128.54530715942383 Val_Reconstruction : 125.3993034362793 Val_KL : 3.1460081338882446\n","Epoch: 133/8000  Traning Loss: 133.56894874572754  Train_Reconstruction: 130.3285675048828  Train_KL: 3.2403823733329773  Validation Loss : 128.8285903930664 Val_Reconstruction : 125.65229034423828 Val_KL : 3.17630136013031\n","Epoch: 134/8000  Traning Loss: 134.00530624389648  Train_Reconstruction: 130.76559829711914  Train_KL: 3.2397083938121796  Validation Loss : 129.26100540161133 Val_Reconstruction : 126.08366775512695 Val_KL : 3.1773364543914795\n","Epoch: 135/8000  Traning Loss: 133.55926513671875  Train_Reconstruction: 130.3251190185547  Train_KL: 3.234147608280182  Validation Loss : 128.7839126586914 Val_Reconstruction : 125.60891342163086 Val_KL : 3.174997925758362\n","Epoch: 136/8000  Traning Loss: 133.0333652496338  Train_Reconstruction: 129.78912734985352  Train_KL: 3.2442378103733063  Validation Loss : 127.83996963500977 Val_Reconstruction : 124.67349243164062 Val_KL : 3.166479468345642\n","Epoch: 137/8000  Traning Loss: 132.700590133667  Train_Reconstruction: 129.46820640563965  Train_KL: 3.23238468170166  Validation Loss : 127.72861862182617 Val_Reconstruction : 124.55257797241211 Val_KL : 3.176040291786194\n","Epoch: 138/8000  Traning Loss: 132.58581352233887  Train_Reconstruction: 129.34740924835205  Train_KL: 3.238405793905258  Validation Loss : 127.75666809082031 Val_Reconstruction : 124.59663009643555 Val_KL : 3.1600414514541626\n","Epoch: 139/8000  Traning Loss: 132.43328380584717  Train_Reconstruction: 129.21014213562012  Train_KL: 3.223142296075821  Validation Loss : 127.36407089233398 Val_Reconstruction : 124.1971549987793 Val_KL : 3.1669118404388428\n","Epoch: 140/8000  Traning Loss: 132.11622047424316  Train_Reconstruction: 128.87510776519775  Train_KL: 3.2411127984523773  Validation Loss : 127.2784538269043 Val_Reconstruction : 124.08446502685547 Val_KL : 3.1939868927001953\n","Epoch: 141/8000  Traning Loss: 132.01067924499512  Train_Reconstruction: 128.76327228546143  Train_KL: 3.247409909963608  Validation Loss : 127.33081817626953 Val_Reconstruction : 124.14323425292969 Val_KL : 3.1875860691070557\n","Epoch: 142/8000  Traning Loss: 131.67774772644043  Train_Reconstruction: 128.438627243042  Train_KL: 3.2391213476657867  Validation Loss : 126.6603889465332 Val_Reconstruction : 123.47122955322266 Val_KL : 3.1891616582870483\n","Epoch: 143/8000  Traning Loss: 131.47538661956787  Train_Reconstruction: 128.2253656387329  Train_KL: 3.2500184178352356  Validation Loss : 126.68019104003906 Val_Reconstruction : 123.49163818359375 Val_KL : 3.188547730445862\n","Epoch: 144/8000  Traning Loss: 131.44940567016602  Train_Reconstruction: 128.21760082244873  Train_KL: 3.2318071126937866  Validation Loss : 126.64120864868164 Val_Reconstruction : 123.45424270629883 Val_KL : 3.186964988708496\n","Epoch: 145/8000  Traning Loss: 131.07905387878418  Train_Reconstruction: 127.82520198822021  Train_KL: 3.253852128982544  Validation Loss : 126.12753677368164 Val_Reconstruction : 122.93133544921875 Val_KL : 3.196202278137207\n","Epoch: 146/8000  Traning Loss: 130.9495096206665  Train_Reconstruction: 127.703537940979  Train_KL: 3.2459693253040314  Validation Loss : 126.05866241455078 Val_Reconstruction : 122.87860488891602 Val_KL : 3.1800575256347656\n","Epoch: 147/8000  Traning Loss: 130.7783784866333  Train_Reconstruction: 127.52678871154785  Train_KL: 3.2515898048877716  Validation Loss : 125.94797897338867 Val_Reconstruction : 122.75993728637695 Val_KL : 3.188037872314453\n","Epoch: 148/8000  Traning Loss: 130.55218315124512  Train_Reconstruction: 127.29988765716553  Train_KL: 3.252295047044754  Validation Loss : 125.65904998779297 Val_Reconstruction : 122.46099472045898 Val_KL : 3.198056936264038\n","Epoch: 149/8000  Traning Loss: 130.32051753997803  Train_Reconstruction: 127.08654022216797  Train_KL: 3.2339771687984467  Validation Loss : 125.49374389648438 Val_Reconstruction : 122.32757949829102 Val_KL : 3.166160821914673\n","Epoch: 150/8000  Traning Loss: 130.26919078826904  Train_Reconstruction: 127.02295875549316  Train_KL: 3.246232658624649  Validation Loss : 125.73719787597656 Val_Reconstruction : 122.54262924194336 Val_KL : 3.194568634033203\n","Epoch: 151/8000  Traning Loss: 130.4274778366089  Train_Reconstruction: 127.18023014068604  Train_KL: 3.247249335050583  Validation Loss : 125.5174789428711 Val_Reconstruction : 122.3354377746582 Val_KL : 3.1820424795150757\n","Epoch: 152/8000  Traning Loss: 130.0859670639038  Train_Reconstruction: 126.84207057952881  Train_KL: 3.2438939809799194  Validation Loss : 125.34736251831055 Val_Reconstruction : 122.15131378173828 Val_KL : 3.1960498094558716\n","Epoch: 153/8000  Traning Loss: 129.9862823486328  Train_Reconstruction: 126.73935794830322  Train_KL: 3.2469217777252197  Validation Loss : 125.39567565917969 Val_Reconstruction : 122.2069091796875 Val_KL : 3.1887675523757935\n","Epoch: 154/8000  Traning Loss: 129.6812286376953  Train_Reconstruction: 126.44397640228271  Train_KL: 3.2372515201568604  Validation Loss : 124.87217712402344 Val_Reconstruction : 121.68408203125 Val_KL : 3.188091278076172\n","Epoch: 155/8000  Traning Loss: 129.38645458221436  Train_Reconstruction: 126.13457679748535  Train_KL: 3.251879572868347  Validation Loss : 124.74648666381836 Val_Reconstruction : 121.5458869934082 Val_KL : 3.2005958557128906\n","Epoch: 156/8000  Traning Loss: 129.42697143554688  Train_Reconstruction: 126.17379188537598  Train_KL: 3.2531778812408447  Validation Loss : 124.59794616699219 Val_Reconstruction : 121.39247131347656 Val_KL : 3.205472946166992\n","Epoch: 157/8000  Traning Loss: 129.09387493133545  Train_Reconstruction: 125.85782623291016  Train_KL: 3.236048698425293  Validation Loss : 124.31177520751953 Val_Reconstruction : 121.13844680786133 Val_KL : 3.1733334064483643\n","Epoch: 158/8000  Traning Loss: 129.0041790008545  Train_Reconstruction: 125.76411533355713  Train_KL: 3.2400650680065155  Validation Loss : 124.31604766845703 Val_Reconstruction : 121.13660430908203 Val_KL : 3.17944073677063\n","Epoch: 159/8000  Traning Loss: 128.96327590942383  Train_Reconstruction: 125.72585582733154  Train_KL: 3.2374207675457  Validation Loss : 124.2100830078125 Val_Reconstruction : 121.01540756225586 Val_KL : 3.19467294216156\n","Epoch: 160/8000  Traning Loss: 128.88591861724854  Train_Reconstruction: 125.62228775024414  Train_KL: 3.2636298835277557  Validation Loss : 124.44111251831055 Val_Reconstruction : 121.2287826538086 Val_KL : 3.2123308181762695\n","Epoch: 161/8000  Traning Loss: 128.74133396148682  Train_Reconstruction: 125.49105739593506  Train_KL: 3.250279486179352  Validation Loss : 123.93080139160156 Val_Reconstruction : 120.742919921875 Val_KL : 3.187881588935852\n","Epoch: 162/8000  Traning Loss: 128.53681755065918  Train_Reconstruction: 125.28682804107666  Train_KL: 3.2499900460243225  Validation Loss : 123.82147598266602 Val_Reconstruction : 120.63439559936523 Val_KL : 3.1870830059051514\n","Epoch: 163/8000  Traning Loss: 128.60641384124756  Train_Reconstruction: 125.36471748352051  Train_KL: 3.241697281599045  Validation Loss : 124.14954376220703 Val_Reconstruction : 120.96567153930664 Val_KL : 3.183873772621155\n","Epoch: 164/8000  Traning Loss: 128.28501224517822  Train_Reconstruction: 125.02669715881348  Train_KL: 3.258314847946167  Validation Loss : 123.57926940917969 Val_Reconstruction : 120.37472152709961 Val_KL : 3.2045477628707886\n","Epoch: 165/8000  Traning Loss: 127.97039699554443  Train_Reconstruction: 124.73092365264893  Train_KL: 3.239474594593048  Validation Loss : 123.45825576782227 Val_Reconstruction : 120.27927780151367 Val_KL : 3.1789757013320923\n","Epoch: 166/8000  Traning Loss: 127.87119197845459  Train_Reconstruction: 124.6255111694336  Train_KL: 3.245678097009659  Validation Loss : 123.35503387451172 Val_Reconstruction : 120.16378784179688 Val_KL : 3.1912426948547363\n","Epoch: 167/8000  Traning Loss: 127.96983051300049  Train_Reconstruction: 124.72579669952393  Train_KL: 3.244033247232437  Validation Loss : 123.3648452758789 Val_Reconstruction : 120.17479705810547 Val_KL : 3.1900508403778076\n","Epoch: 168/8000  Traning Loss: 127.90830135345459  Train_Reconstruction: 124.65732383728027  Train_KL: 3.250977873802185  Validation Loss : 123.56543731689453 Val_Reconstruction : 120.36797714233398 Val_KL : 3.1974637508392334\n","Epoch: 169/8000  Traning Loss: 128.034574508667  Train_Reconstruction: 124.77864646911621  Train_KL: 3.2559282183647156  Validation Loss : 122.90084838867188 Val_Reconstruction : 119.6983528137207 Val_KL : 3.2024930715560913\n","Epoch: 170/8000  Traning Loss: 127.67703342437744  Train_Reconstruction: 124.40767097473145  Train_KL: 3.2693635523319244  Validation Loss : 123.06489181518555 Val_Reconstruction : 119.85008239746094 Val_KL : 3.214810609817505\n","Epoch: 171/8000  Traning Loss: 127.15596961975098  Train_Reconstruction: 123.89740657806396  Train_KL: 3.2585625648498535  Validation Loss : 122.57954025268555 Val_Reconstruction : 119.36483764648438 Val_KL : 3.2147058248519897\n","Epoch: 172/8000  Traning Loss: 127.05426025390625  Train_Reconstruction: 123.78158569335938  Train_KL: 3.272672951221466  Validation Loss : 122.51720809936523 Val_Reconstruction : 119.2972412109375 Val_KL : 3.2199641466140747\n","Epoch: 173/8000  Traning Loss: 126.93881797790527  Train_Reconstruction: 123.68643951416016  Train_KL: 3.2523794770240784  Validation Loss : 122.48331069946289 Val_Reconstruction : 119.29335021972656 Val_KL : 3.1899603605270386\n","Epoch: 174/8000  Traning Loss: 126.87676811218262  Train_Reconstruction: 123.6070556640625  Train_KL: 3.2697127759456635  Validation Loss : 122.24842071533203 Val_Reconstruction : 119.0256118774414 Val_KL : 3.2228078842163086\n","Epoch: 175/8000  Traning Loss: 126.73656749725342  Train_Reconstruction: 123.46798706054688  Train_KL: 3.2685804069042206  Validation Loss : 122.44520950317383 Val_Reconstruction : 119.23846817016602 Val_KL : 3.2067397832870483\n","Epoch: 176/8000  Traning Loss: 126.8416395187378  Train_Reconstruction: 123.5711030960083  Train_KL: 3.270537406206131  Validation Loss : 122.05753326416016 Val_Reconstruction : 118.8410873413086 Val_KL : 3.2164511680603027\n","Epoch: 177/8000  Traning Loss: 126.51572132110596  Train_Reconstruction: 123.25424671173096  Train_KL: 3.2614735662937164  Validation Loss : 121.81167984008789 Val_Reconstruction : 118.60975646972656 Val_KL : 3.201922655105591\n","Epoch: 178/8000  Traning Loss: 126.43699073791504  Train_Reconstruction: 123.18168830871582  Train_KL: 3.2553007900714874  Validation Loss : 122.18404388427734 Val_Reconstruction : 118.97025299072266 Val_KL : 3.2137906551361084\n","Epoch: 179/8000  Traning Loss: 126.40942287445068  Train_Reconstruction: 123.13201999664307  Train_KL: 3.2774038314819336  Validation Loss : 122.30084228515625 Val_Reconstruction : 119.07457733154297 Val_KL : 3.2262611389160156\n","Epoch: 180/8000  Traning Loss: 126.1801061630249  Train_Reconstruction: 122.9115858078003  Train_KL: 3.2685201466083527  Validation Loss : 121.50474166870117 Val_Reconstruction : 118.29620742797852 Val_KL : 3.2085307836532593\n","Epoch: 181/8000  Traning Loss: 125.85343551635742  Train_Reconstruction: 122.58424949645996  Train_KL: 3.2691866755485535  Validation Loss : 121.39266586303711 Val_Reconstruction : 118.16949081420898 Val_KL : 3.223176121711731\n","Epoch: 182/8000  Traning Loss: 125.759690284729  Train_Reconstruction: 122.50698566436768  Train_KL: 3.252703994512558  Validation Loss : 121.30940628051758 Val_Reconstruction : 118.11378860473633 Val_KL : 3.195621967315674\n","Epoch: 183/8000  Traning Loss: 125.51907634735107  Train_Reconstruction: 122.24892330169678  Train_KL: 3.2701528072357178  Validation Loss : 120.97954177856445 Val_Reconstruction : 117.75702667236328 Val_KL : 3.2225115299224854\n","Epoch: 184/8000  Traning Loss: 125.50653171539307  Train_Reconstruction: 122.24296569824219  Train_KL: 3.2635683119297028  Validation Loss : 121.19463348388672 Val_Reconstruction : 117.98078918457031 Val_KL : 3.213844895362854\n","Epoch: 185/8000  Traning Loss: 125.37426567077637  Train_Reconstruction: 122.10622596740723  Train_KL: 3.2680388689041138  Validation Loss : 120.99848175048828 Val_Reconstruction : 117.76522445678711 Val_KL : 3.2332597970962524\n","Epoch: 186/8000  Traning Loss: 125.4574842453003  Train_Reconstruction: 122.17852020263672  Train_KL: 3.2789643108844757  Validation Loss : 121.07209396362305 Val_Reconstruction : 117.84423828125 Val_KL : 3.227856159210205\n","Epoch: 187/8000  Traning Loss: 125.30801963806152  Train_Reconstruction: 122.0407829284668  Train_KL: 3.267236292362213  Validation Loss : 120.7087516784668 Val_Reconstruction : 117.48686218261719 Val_KL : 3.2218881845474243\n","Epoch: 188/8000  Traning Loss: 124.96457958221436  Train_Reconstruction: 121.68994140625  Train_KL: 3.2746376991271973  Validation Loss : 120.55183029174805 Val_Reconstruction : 117.31969451904297 Val_KL : 3.2321377992630005\n","Epoch: 189/8000  Traning Loss: 125.04234313964844  Train_Reconstruction: 121.77384853363037  Train_KL: 3.2684954702854156  Validation Loss : 120.58858871459961 Val_Reconstruction : 117.36220169067383 Val_KL : 3.226391911506653\n","Epoch: 190/8000  Traning Loss: 124.92315483093262  Train_Reconstruction: 121.64360618591309  Train_KL: 3.2795477509498596  Validation Loss : 120.69780731201172 Val_Reconstruction : 117.4702262878418 Val_KL : 3.227582335472107\n","Epoch: 191/8000  Traning Loss: 124.8967809677124  Train_Reconstruction: 121.6230640411377  Train_KL: 3.2737173438072205  Validation Loss : 120.89027786254883 Val_Reconstruction : 117.65467834472656 Val_KL : 3.2356009483337402\n","Epoch: 192/8000  Traning Loss: 124.71245670318604  Train_Reconstruction: 121.43209552764893  Train_KL: 3.2803617119789124  Validation Loss : 120.20597839355469 Val_Reconstruction : 116.96267700195312 Val_KL : 3.2433016300201416\n","Epoch: 193/8000  Traning Loss: 124.71032333374023  Train_Reconstruction: 121.4270429611206  Train_KL: 3.283280909061432  Validation Loss : 120.58173370361328 Val_Reconstruction : 117.35169219970703 Val_KL : 3.230041027069092\n","Epoch: 194/8000  Traning Loss: 124.66094493865967  Train_Reconstruction: 121.38228702545166  Train_KL: 3.2786574959754944  Validation Loss : 120.42715072631836 Val_Reconstruction : 117.18006134033203 Val_KL : 3.247087836265564\n","Epoch: 195/8000  Traning Loss: 124.46866798400879  Train_Reconstruction: 121.19926452636719  Train_KL: 3.2694026827812195  Validation Loss : 120.18268966674805 Val_Reconstruction : 116.96735000610352 Val_KL : 3.215338110923767\n","Epoch: 196/8000  Traning Loss: 124.53939628601074  Train_Reconstruction: 121.25013160705566  Train_KL: 3.2892647683620453  Validation Loss : 120.47147369384766 Val_Reconstruction : 117.2153434753418 Val_KL : 3.2561278343200684\n","Epoch: 197/8000  Traning Loss: 124.4783296585083  Train_Reconstruction: 121.21320915222168  Train_KL: 3.2651210725307465  Validation Loss : 119.98911666870117 Val_Reconstruction : 116.78244018554688 Val_KL : 3.2066785097122192\n","Epoch: 198/8000  Traning Loss: 124.28982925415039  Train_Reconstruction: 121.00462532043457  Train_KL: 3.285203993320465  Validation Loss : 120.1170425415039 Val_Reconstruction : 116.86479568481445 Val_KL : 3.252250552177429\n","Epoch: 199/8000  Traning Loss: 124.44164180755615  Train_Reconstruction: 121.15653705596924  Train_KL: 3.2851036489009857  Validation Loss : 120.51350402832031 Val_Reconstruction : 117.2788314819336 Val_KL : 3.234675168991089\n","Epoch: 200/8000  Traning Loss: 124.23714637756348  Train_Reconstruction: 120.95047092437744  Train_KL: 3.2866746187210083  Validation Loss : 119.85340881347656 Val_Reconstruction : 116.59402465820312 Val_KL : 3.259381413459778\n","Epoch: 201/8000  Traning Loss: 123.89610767364502  Train_Reconstruction: 120.6067008972168  Train_KL: 3.2894076108932495  Validation Loss : 119.45268249511719 Val_Reconstruction : 116.21834182739258 Val_KL : 3.2343406677246094\n","Epoch: 202/8000  Traning Loss: 123.75709629058838  Train_Reconstruction: 120.4660472869873  Train_KL: 3.291050434112549  Validation Loss : 119.38956451416016 Val_Reconstruction : 116.14458465576172 Val_KL : 3.2449758052825928\n","Epoch: 203/8000  Traning Loss: 123.67395973205566  Train_Reconstruction: 120.3920030593872  Train_KL: 3.2819564640522003  Validation Loss : 119.3785629272461 Val_Reconstruction : 116.14441299438477 Val_KL : 3.2341495752334595\n","Epoch: 204/8000  Traning Loss: 123.41728210449219  Train_Reconstruction: 120.14159488677979  Train_KL: 3.27568718791008  Validation Loss : 119.25383377075195 Val_Reconstruction : 116.01077270507812 Val_KL : 3.243062138557434\n","Epoch: 205/8000  Traning Loss: 123.3848295211792  Train_Reconstruction: 120.09782695770264  Train_KL: 3.287000983953476  Validation Loss : 119.19257736206055 Val_Reconstruction : 115.93542098999023 Val_KL : 3.2571544647216797\n","Epoch: 206/8000  Traning Loss: 123.3846492767334  Train_Reconstruction: 120.08934497833252  Train_KL: 3.295305222272873  Validation Loss : 119.13065338134766 Val_Reconstruction : 115.88142395019531 Val_KL : 3.249232053756714\n","Epoch: 207/8000  Traning Loss: 123.08774852752686  Train_Reconstruction: 119.78793621063232  Train_KL: 3.299813210964203  Validation Loss : 118.75966262817383 Val_Reconstruction : 115.4961051940918 Val_KL : 3.2635550498962402\n","Epoch: 208/8000  Traning Loss: 123.10361289978027  Train_Reconstruction: 119.81399631500244  Train_KL: 3.289616346359253  Validation Loss : 118.79558181762695 Val_Reconstruction : 115.5626449584961 Val_KL : 3.2329365015029907\n","Epoch: 209/8000  Traning Loss: 123.02045917510986  Train_Reconstruction: 119.73686027526855  Train_KL: 3.283599019050598  Validation Loss : 118.99502182006836 Val_Reconstruction : 115.76287078857422 Val_KL : 3.2321460247039795\n","Epoch: 210/8000  Traning Loss: 122.95504379272461  Train_Reconstruction: 119.68761825561523  Train_KL: 3.2674251198768616  Validation Loss : 118.95348358154297 Val_Reconstruction : 115.72016525268555 Val_KL : 3.2333147525787354\n","Epoch: 211/8000  Traning Loss: 122.80525016784668  Train_Reconstruction: 119.5213794708252  Train_KL: 3.283871114253998  Validation Loss : 118.58790588378906 Val_Reconstruction : 115.34120559692383 Val_KL : 3.2467029094696045\n","Epoch: 212/8000  Traning Loss: 122.57199668884277  Train_Reconstruction: 119.29126739501953  Train_KL: 3.2807293236255646  Validation Loss : 118.40751647949219 Val_Reconstruction : 115.18021774291992 Val_KL : 3.227295756340027\n","Epoch: 213/8000  Traning Loss: 122.62190532684326  Train_Reconstruction: 119.33947277069092  Train_KL: 3.282432496547699  Validation Loss : 118.5630111694336 Val_Reconstruction : 115.31045150756836 Val_KL : 3.252562403678894\n","Epoch: 214/8000  Traning Loss: 122.51059246063232  Train_Reconstruction: 119.21806335449219  Train_KL: 3.292529433965683  Validation Loss : 118.38160705566406 Val_Reconstruction : 115.14066314697266 Val_KL : 3.2409398555755615\n","Epoch: 215/8000  Traning Loss: 122.2804765701294  Train_Reconstruction: 118.99481773376465  Train_KL: 3.2856593132019043  Validation Loss : 118.2252426147461 Val_Reconstruction : 114.97575759887695 Val_KL : 3.2494882345199585\n","Epoch: 216/8000  Traning Loss: 122.19252014160156  Train_Reconstruction: 118.8982286453247  Train_KL: 3.2942923605442047  Validation Loss : 118.18417358398438 Val_Reconstruction : 114.92621994018555 Val_KL : 3.2579556703567505\n","Epoch: 217/8000  Traning Loss: 122.19626903533936  Train_Reconstruction: 118.90090274810791  Train_KL: 3.295365184545517  Validation Loss : 118.07260513305664 Val_Reconstruction : 114.82407760620117 Val_KL : 3.248528480529785\n","Epoch: 218/8000  Traning Loss: 122.08333110809326  Train_Reconstruction: 118.78878879547119  Train_KL: 3.2945430278778076  Validation Loss : 117.9028549194336 Val_Reconstruction : 114.63352966308594 Val_KL : 3.26932156085968\n","Epoch: 219/8000  Traning Loss: 121.98470878601074  Train_Reconstruction: 118.68943500518799  Train_KL: 3.295273244380951  Validation Loss : 117.77201080322266 Val_Reconstruction : 114.52387237548828 Val_KL : 3.2481398582458496\n","Epoch: 220/8000  Traning Loss: 121.97471237182617  Train_Reconstruction: 118.67677402496338  Train_KL: 3.2979385256767273  Validation Loss : 118.09153366088867 Val_Reconstruction : 114.82026672363281 Val_KL : 3.2712689638137817\n","Epoch: 221/8000  Traning Loss: 122.01241207122803  Train_Reconstruction: 118.71936225891113  Train_KL: 3.2930509448051453  Validation Loss : 117.96642684936523 Val_Reconstruction : 114.70870590209961 Val_KL : 3.2577195167541504\n","Epoch: 222/8000  Traning Loss: 121.99223327636719  Train_Reconstruction: 118.67529582977295  Train_KL: 3.3169370591640472  Validation Loss : 117.74838638305664 Val_Reconstruction : 114.46728897094727 Val_KL : 3.281098484992981\n","Epoch: 223/8000  Traning Loss: 121.76742267608643  Train_Reconstruction: 118.46251010894775  Train_KL: 3.3049133121967316  Validation Loss : 117.53215408325195 Val_Reconstruction : 114.27219772338867 Val_KL : 3.2599579095840454\n","Epoch: 224/8000  Traning Loss: 121.47078323364258  Train_Reconstruction: 118.15900135040283  Train_KL: 3.3117812275886536  Validation Loss : 117.46802139282227 Val_Reconstruction : 114.1962661743164 Val_KL : 3.2717549800872803\n","Epoch: 225/8000  Traning Loss: 121.43752765655518  Train_Reconstruction: 118.13868045806885  Train_KL: 3.2988468408584595  Validation Loss : 117.59124374389648 Val_Reconstruction : 114.3389778137207 Val_KL : 3.252264976501465\n","Epoch: 226/8000  Traning Loss: 121.48027420043945  Train_Reconstruction: 118.1741247177124  Train_KL: 3.3061487078666687  Validation Loss : 117.61195755004883 Val_Reconstruction : 114.35605239868164 Val_KL : 3.2559062242507935\n","Epoch: 227/8000  Traning Loss: 121.6694564819336  Train_Reconstruction: 118.37675857543945  Train_KL: 3.292699009180069  Validation Loss : 118.01920700073242 Val_Reconstruction : 114.75801849365234 Val_KL : 3.2611883878707886\n","Epoch: 228/8000  Traning Loss: 121.77734088897705  Train_Reconstruction: 118.46113967895508  Train_KL: 3.316202163696289  Validation Loss : 117.58377456665039 Val_Reconstruction : 114.3116569519043 Val_KL : 3.2721147537231445\n","Epoch: 229/8000  Traning Loss: 121.41155529022217  Train_Reconstruction: 118.10251712799072  Train_KL: 3.3090374767780304  Validation Loss : 117.24629974365234 Val_Reconstruction : 113.98563766479492 Val_KL : 3.2606630325317383\n","Epoch: 230/8000  Traning Loss: 121.48593235015869  Train_Reconstruction: 118.17899131774902  Train_KL: 3.3069406151771545  Validation Loss : 117.71928024291992 Val_Reconstruction : 114.4404296875 Val_KL : 3.2788476943969727\n","Epoch: 231/8000  Traning Loss: 121.57818126678467  Train_Reconstruction: 118.25722408294678  Train_KL: 3.3209575414657593  Validation Loss : 117.38504791259766 Val_Reconstruction : 114.10455703735352 Val_KL : 3.2804925441741943\n","Epoch: 232/8000  Traning Loss: 121.46320915222168  Train_Reconstruction: 118.13606452941895  Train_KL: 3.3271439373493195  Validation Loss : 117.2676010131836 Val_Reconstruction : 113.97718811035156 Val_KL : 3.2904152870178223\n","Epoch: 233/8000  Traning Loss: 121.12316989898682  Train_Reconstruction: 117.80419921875  Train_KL: 3.3189720511436462  Validation Loss : 116.91718673706055 Val_Reconstruction : 113.64693450927734 Val_KL : 3.2702492475509644\n","Epoch: 234/8000  Traning Loss: 121.02890491485596  Train_Reconstruction: 117.71677303314209  Train_KL: 3.312131404876709  Validation Loss : 116.97112274169922 Val_Reconstruction : 113.70155715942383 Val_KL : 3.269566297531128\n","Epoch: 235/8000  Traning Loss: 121.04187202453613  Train_Reconstruction: 117.7342119216919  Train_KL: 3.3076595962047577  Validation Loss : 117.11004638671875 Val_Reconstruction : 113.84946060180664 Val_KL : 3.2605843544006348\n","Epoch: 236/8000  Traning Loss: 120.89050769805908  Train_Reconstruction: 117.58089256286621  Train_KL: 3.3096143305301666  Validation Loss : 116.78137969970703 Val_Reconstruction : 113.49675369262695 Val_KL : 3.2846262454986572\n","Epoch: 237/8000  Traning Loss: 120.6407995223999  Train_Reconstruction: 117.31508445739746  Train_KL: 3.325715035200119  Validation Loss : 116.76579666137695 Val_Reconstruction : 113.48829650878906 Val_KL : 3.277503728866577\n","Epoch: 238/8000  Traning Loss: 120.66035079956055  Train_Reconstruction: 117.33580589294434  Train_KL: 3.3245449662208557  Validation Loss : 116.86175537109375 Val_Reconstruction : 113.57228088378906 Val_KL : 3.2894716262817383\n","Epoch: 239/8000  Traning Loss: 120.9204158782959  Train_Reconstruction: 117.6025972366333  Train_KL: 3.317818820476532  Validation Loss : 117.43167495727539 Val_Reconstruction : 114.16213607788086 Val_KL : 3.2695393562316895\n","Epoch: 240/8000  Traning Loss: 120.84196758270264  Train_Reconstruction: 117.5129280090332  Train_KL: 3.32903853058815  Validation Loss : 116.93278121948242 Val_Reconstruction : 113.63898086547852 Val_KL : 3.2937984466552734\n","Epoch: 241/8000  Traning Loss: 120.68521881103516  Train_Reconstruction: 117.35848140716553  Train_KL: 3.3267376124858856  Validation Loss : 116.6672592163086 Val_Reconstruction : 113.38016128540039 Val_KL : 3.2870967388153076\n","Epoch: 242/8000  Traning Loss: 120.44485378265381  Train_Reconstruction: 117.12257766723633  Train_KL: 3.3222757279872894  Validation Loss : 116.59732437133789 Val_Reconstruction : 113.31919860839844 Val_KL : 3.2781273126602173\n","Epoch: 243/8000  Traning Loss: 120.3577184677124  Train_Reconstruction: 117.03730773925781  Train_KL: 3.3204107582569122  Validation Loss : 116.43310165405273 Val_Reconstruction : 113.13902282714844 Val_KL : 3.2940791845321655\n","Epoch: 244/8000  Traning Loss: 120.12462902069092  Train_Reconstruction: 116.80138397216797  Train_KL: 3.323243886232376  Validation Loss : 116.21634292602539 Val_Reconstruction : 112.9468765258789 Val_KL : 3.2694637775421143\n","Epoch: 245/8000  Traning Loss: 119.90201473236084  Train_Reconstruction: 116.59217166900635  Train_KL: 3.309843510389328  Validation Loss : 115.9591064453125 Val_Reconstruction : 112.67819213867188 Val_KL : 3.2809141874313354\n","Epoch: 246/8000  Traning Loss: 119.88897323608398  Train_Reconstruction: 116.57579326629639  Train_KL: 3.3131808936595917  Validation Loss : 116.02311706542969 Val_Reconstruction : 112.74411392211914 Val_KL : 3.2790032625198364\n","Epoch: 247/8000  Traning Loss: 119.9576473236084  Train_Reconstruction: 116.64766693115234  Train_KL: 3.3099791407585144  Validation Loss : 116.21639251708984 Val_Reconstruction : 112.94424057006836 Val_KL : 3.2721500396728516\n","Epoch: 248/8000  Traning Loss: 119.93907260894775  Train_Reconstruction: 116.62674236297607  Train_KL: 3.3123309910297394  Validation Loss : 116.12892150878906 Val_Reconstruction : 112.85916900634766 Val_KL : 3.2697503566741943\n","Epoch: 249/8000  Traning Loss: 119.7449369430542  Train_Reconstruction: 116.44139575958252  Train_KL: 3.303542196750641  Validation Loss : 115.73332214355469 Val_Reconstruction : 112.44929122924805 Val_KL : 3.2840317487716675\n","Epoch: 250/8000  Traning Loss: 119.60884952545166  Train_Reconstruction: 116.2881965637207  Train_KL: 3.320653587579727  Validation Loss : 115.72917938232422 Val_Reconstruction : 112.46308898925781 Val_KL : 3.2660900354385376\n","Epoch: 251/8000  Traning Loss: 119.56223773956299  Train_Reconstruction: 116.25009632110596  Train_KL: 3.312140256166458  Validation Loss : 115.61079025268555 Val_Reconstruction : 112.32865524291992 Val_KL : 3.282134175300598\n","Epoch: 252/8000  Traning Loss: 119.60266876220703  Train_Reconstruction: 116.26925086975098  Train_KL: 3.333418548107147  Validation Loss : 115.86866760253906 Val_Reconstruction : 112.58318328857422 Val_KL : 3.2854855060577393\n","Epoch: 253/8000  Traning Loss: 119.7808141708374  Train_Reconstruction: 116.47479343414307  Train_KL: 3.30602166056633  Validation Loss : 115.69750595092773 Val_Reconstruction : 112.43338012695312 Val_KL : 3.2641260623931885\n","Epoch: 254/8000  Traning Loss: 119.46403694152832  Train_Reconstruction: 116.15196895599365  Train_KL: 3.312067300081253  Validation Loss : 115.62835311889648 Val_Reconstruction : 112.34911727905273 Val_KL : 3.2792333364486694\n","Epoch: 255/8000  Traning Loss: 119.47725009918213  Train_Reconstruction: 116.1601333618164  Train_KL: 3.3171172440052032  Validation Loss : 115.55554580688477 Val_Reconstruction : 112.27375411987305 Val_KL : 3.2817909717559814\n","Epoch: 256/8000  Traning Loss: 119.47428894042969  Train_Reconstruction: 116.15078544616699  Train_KL: 3.323503077030182  Validation Loss : 115.78042984008789 Val_Reconstruction : 112.48191833496094 Val_KL : 3.2985124588012695\n","Epoch: 257/8000  Traning Loss: 119.54802799224854  Train_Reconstruction: 116.2206506729126  Train_KL: 3.3273784816265106  Validation Loss : 115.24090576171875 Val_Reconstruction : 111.96171188354492 Val_KL : 3.279193878173828\n","Epoch: 258/8000  Traning Loss: 119.3038444519043  Train_Reconstruction: 115.99475860595703  Train_KL: 3.309086799621582  Validation Loss : 115.15142822265625 Val_Reconstruction : 111.8682632446289 Val_KL : 3.2831658124923706\n","Epoch: 259/8000  Traning Loss: 119.11275291442871  Train_Reconstruction: 115.78743648529053  Train_KL: 3.325316399335861  Validation Loss : 115.33208084106445 Val_Reconstruction : 112.05022430419922 Val_KL : 3.2818546295166016\n","Epoch: 260/8000  Traning Loss: 119.02966499328613  Train_Reconstruction: 115.70623683929443  Train_KL: 3.3234293460845947  Validation Loss : 115.14734268188477 Val_Reconstruction : 111.85005950927734 Val_KL : 3.297285556793213\n","Epoch: 261/8000  Traning Loss: 118.9157133102417  Train_Reconstruction: 115.58787250518799  Train_KL: 3.32784104347229  Validation Loss : 115.14439010620117 Val_Reconstruction : 111.85173034667969 Val_KL : 3.292659282684326\n","Epoch: 262/8000  Traning Loss: 118.91693305969238  Train_Reconstruction: 115.59035015106201  Train_KL: 3.3265827000141144  Validation Loss : 115.01761245727539 Val_Reconstruction : 111.71097564697266 Val_KL : 3.306639552116394\n","Epoch: 263/8000  Traning Loss: 118.91601181030273  Train_Reconstruction: 115.58867835998535  Train_KL: 3.327334016561508  Validation Loss : 115.22562789916992 Val_Reconstruction : 111.9339485168457 Val_KL : 3.291680335998535\n","Epoch: 264/8000  Traning Loss: 118.71249389648438  Train_Reconstruction: 115.37025737762451  Train_KL: 3.342237412929535  Validation Loss : 114.81874465942383 Val_Reconstruction : 111.49860382080078 Val_KL : 3.320141553878784\n","Epoch: 265/8000  Traning Loss: 118.68543148040771  Train_Reconstruction: 115.35445308685303  Train_KL: 3.3309775292873383  Validation Loss : 114.83345031738281 Val_Reconstruction : 111.5441665649414 Val_KL : 3.2892844676971436\n","Epoch: 266/8000  Traning Loss: 118.7171630859375  Train_Reconstruction: 115.37275123596191  Train_KL: 3.344412386417389  Validation Loss : 114.74101638793945 Val_Reconstruction : 111.41268920898438 Val_KL : 3.3283263444900513\n","Epoch: 267/8000  Traning Loss: 118.70394611358643  Train_Reconstruction: 115.37873649597168  Train_KL: 3.3252090215682983  Validation Loss : 115.16658401489258 Val_Reconstruction : 111.87839126586914 Val_KL : 3.288193702697754\n","Epoch: 268/8000  Traning Loss: 118.63980770111084  Train_Reconstruction: 115.30446243286133  Train_KL: 3.335345208644867  Validation Loss : 115.16880416870117 Val_Reconstruction : 111.8551025390625 Val_KL : 3.3137036561965942\n","Epoch: 269/8000  Traning Loss: 118.62595176696777  Train_Reconstruction: 115.29013633728027  Train_KL: 3.3358143866062164  Validation Loss : 114.6550407409668 Val_Reconstruction : 111.34431076049805 Val_KL : 3.310729742050171\n","Epoch: 270/8000  Traning Loss: 118.4428300857544  Train_Reconstruction: 115.08309555053711  Train_KL: 3.3597348034381866  Validation Loss : 114.61626434326172 Val_Reconstruction : 111.3012580871582 Val_KL : 3.315006971359253\n","Epoch: 271/8000  Traning Loss: 118.47781372070312  Train_Reconstruction: 115.14897155761719  Train_KL: 3.328842282295227  Validation Loss : 114.78572082519531 Val_Reconstruction : 111.49381637573242 Val_KL : 3.291905164718628\n","Epoch: 272/8000  Traning Loss: 118.53381252288818  Train_Reconstruction: 115.18954658508301  Train_KL: 3.3442660868167877  Validation Loss : 114.73050689697266 Val_Reconstruction : 111.43102645874023 Val_KL : 3.299478769302368\n","Epoch: 273/8000  Traning Loss: 118.42860984802246  Train_Reconstruction: 115.09621334075928  Train_KL: 3.332396686077118  Validation Loss : 114.6912841796875 Val_Reconstruction : 111.38433456420898 Val_KL : 3.306948184967041\n","Epoch: 274/8000  Traning Loss: 118.43391990661621  Train_Reconstruction: 115.09403038024902  Train_KL: 3.339889019727707  Validation Loss : 114.68387222290039 Val_Reconstruction : 111.38526916503906 Val_KL : 3.2986022233963013\n","Epoch: 275/8000  Traning Loss: 118.5686845779419  Train_Reconstruction: 115.2403736114502  Train_KL: 3.32831147313118  Validation Loss : 115.23225402832031 Val_Reconstruction : 111.92621612548828 Val_KL : 3.3060383796691895\n","Epoch: 276/8000  Traning Loss: 118.2997407913208  Train_Reconstruction: 114.96570873260498  Train_KL: 3.334032505750656  Validation Loss : 114.49015045166016 Val_Reconstruction : 111.19329452514648 Val_KL : 3.2968560457229614\n","Epoch: 277/8000  Traning Loss: 118.10118865966797  Train_Reconstruction: 114.7606201171875  Train_KL: 3.3405690491199493  Validation Loss : 114.5721206665039 Val_Reconstruction : 111.27405166625977 Val_KL : 3.2980700731277466\n","Epoch: 278/8000  Traning Loss: 118.36083507537842  Train_Reconstruction: 115.03212261199951  Train_KL: 3.328713208436966  Validation Loss : 114.76570892333984 Val_Reconstruction : 111.46998596191406 Val_KL : 3.2957218885421753\n","Epoch: 279/8000  Traning Loss: 118.17943477630615  Train_Reconstruction: 114.83324241638184  Train_KL: 3.3461929857730865  Validation Loss : 114.31214904785156 Val_Reconstruction : 110.9878158569336 Val_KL : 3.324334979057312\n","Epoch: 280/8000  Traning Loss: 117.89213848114014  Train_Reconstruction: 114.54609298706055  Train_KL: 3.3460470736026764  Validation Loss : 114.23359298706055 Val_Reconstruction : 110.9313735961914 Val_KL : 3.302219271659851\n","Epoch: 281/8000  Traning Loss: 117.94814395904541  Train_Reconstruction: 114.61693572998047  Train_KL: 3.3312079310417175  Validation Loss : 114.29961013793945 Val_Reconstruction : 110.9997444152832 Val_KL : 3.2998653650283813\n","Epoch: 282/8000  Traning Loss: 117.79448986053467  Train_Reconstruction: 114.45348834991455  Train_KL: 3.3410013616085052  Validation Loss : 114.04376602172852 Val_Reconstruction : 110.74212646484375 Val_KL : 3.301638126373291\n","Epoch: 283/8000  Traning Loss: 117.83500957489014  Train_Reconstruction: 114.48156929016113  Train_KL: 3.3534405529499054  Validation Loss : 114.16040802001953 Val_Reconstruction : 110.8221549987793 Val_KL : 3.3382526636123657\n","Epoch: 284/8000  Traning Loss: 117.78822135925293  Train_Reconstruction: 114.43007755279541  Train_KL: 3.358142465353012  Validation Loss : 113.90260696411133 Val_Reconstruction : 110.58544540405273 Val_KL : 3.3171591758728027\n","Epoch: 285/8000  Traning Loss: 117.68059349060059  Train_Reconstruction: 114.33222389221191  Train_KL: 3.348368853330612  Validation Loss : 114.07482147216797 Val_Reconstruction : 110.75707244873047 Val_KL : 3.317749261856079\n","Epoch: 286/8000  Traning Loss: 117.62356472015381  Train_Reconstruction: 114.27995681762695  Train_KL: 3.3436078131198883  Validation Loss : 113.72138214111328 Val_Reconstruction : 110.41447067260742 Val_KL : 3.3069112300872803\n","Epoch: 287/8000  Traning Loss: 117.44352626800537  Train_Reconstruction: 114.09925079345703  Train_KL: 3.344275027513504  Validation Loss : 113.63021850585938 Val_Reconstruction : 110.32502746582031 Val_KL : 3.3051928281784058\n","Epoch: 288/8000  Traning Loss: 117.64147663116455  Train_Reconstruction: 114.30692672729492  Train_KL: 3.334549903869629  Validation Loss : 114.43136596679688 Val_Reconstruction : 111.13786315917969 Val_KL : 3.2935034036636353\n","Epoch: 289/8000  Traning Loss: 117.92250537872314  Train_Reconstruction: 114.58001518249512  Train_KL: 3.3424908220767975  Validation Loss : 114.37843322753906 Val_Reconstruction : 111.0605239868164 Val_KL : 3.317909002304077\n","Epoch: 290/8000  Traning Loss: 117.99281311035156  Train_Reconstruction: 114.64650344848633  Train_KL: 3.346308469772339  Validation Loss : 114.87841796875 Val_Reconstruction : 111.58883666992188 Val_KL : 3.2895809412002563\n","Epoch: 291/8000  Traning Loss: 118.0983304977417  Train_Reconstruction: 114.75857257843018  Train_KL: 3.3397576212882996  Validation Loss : 114.22770690917969 Val_Reconstruction : 110.91396713256836 Val_KL : 3.3137404918670654\n","Epoch: 292/8000  Traning Loss: 117.63091087341309  Train_Reconstruction: 114.27593326568604  Train_KL: 3.3549766540527344  Validation Loss : 114.11240005493164 Val_Reconstruction : 110.78596496582031 Val_KL : 3.326435089111328\n","Epoch: 293/8000  Traning Loss: 117.4847297668457  Train_Reconstruction: 114.13173389434814  Train_KL: 3.3529953062534332  Validation Loss : 113.50244903564453 Val_Reconstruction : 110.19036102294922 Val_KL : 3.3120867013931274\n","Epoch: 294/8000  Traning Loss: 117.49541187286377  Train_Reconstruction: 114.15023899078369  Train_KL: 3.3451731204986572  Validation Loss : 113.83973693847656 Val_Reconstruction : 110.51655197143555 Val_KL : 3.3231842517852783\n","Epoch: 295/8000  Traning Loss: 117.33095073699951  Train_Reconstruction: 113.96016120910645  Train_KL: 3.370789557695389  Validation Loss : 113.76791000366211 Val_Reconstruction : 110.43449401855469 Val_KL : 3.3334134817123413\n","Epoch: 296/8000  Traning Loss: 117.14912509918213  Train_Reconstruction: 113.79588222503662  Train_KL: 3.3532440960407257  Validation Loss : 113.5114517211914 Val_Reconstruction : 110.1865234375 Val_KL : 3.3249289989471436\n","Epoch: 297/8000  Traning Loss: 116.9871940612793  Train_Reconstruction: 113.62728500366211  Train_KL: 3.3599089682102203  Validation Loss : 113.28337860107422 Val_Reconstruction : 109.97694778442383 Val_KL : 3.3064318895339966\n","Epoch: 298/8000  Traning Loss: 116.97670364379883  Train_Reconstruction: 113.63906764984131  Train_KL: 3.337636500597  Validation Loss : 113.39484405517578 Val_Reconstruction : 110.071044921875 Val_KL : 3.323800563812256\n","Epoch: 299/8000  Traning Loss: 116.95147609710693  Train_Reconstruction: 113.5976619720459  Train_KL: 3.353814721107483  Validation Loss : 113.30609130859375 Val_Reconstruction : 109.99820709228516 Val_KL : 3.307884693145752\n","Epoch: 300/8000  Traning Loss: 116.83772945404053  Train_Reconstruction: 113.48624229431152  Train_KL: 3.3514872789382935  Validation Loss : 113.01454544067383 Val_Reconstruction : 109.68807983398438 Val_KL : 3.326466917991638\n","Epoch: 301/8000  Traning Loss: 116.87009716033936  Train_Reconstruction: 113.51756954193115  Train_KL: 3.352528154850006  Validation Loss : 113.23215103149414 Val_Reconstruction : 109.90874862670898 Val_KL : 3.323403000831604\n","Epoch: 302/8000  Traning Loss: 116.89752864837646  Train_Reconstruction: 113.54194831848145  Train_KL: 3.3555811047554016  Validation Loss : 113.33397674560547 Val_Reconstruction : 110.01789474487305 Val_KL : 3.3160836696624756\n","Epoch: 303/8000  Traning Loss: 117.13340187072754  Train_Reconstruction: 113.78064918518066  Train_KL: 3.352754831314087  Validation Loss : 113.59840774536133 Val_Reconstruction : 110.27249908447266 Val_KL : 3.3259068727493286\n","Epoch: 304/8000  Traning Loss: 116.95760726928711  Train_Reconstruction: 113.60372734069824  Train_KL: 3.35387921333313  Validation Loss : 113.47810363769531 Val_Reconstruction : 110.16633605957031 Val_KL : 3.3117690086364746\n","Epoch: 305/8000  Traning Loss: 116.99574375152588  Train_Reconstruction: 113.65791893005371  Train_KL: 3.3378253877162933  Validation Loss : 113.45791244506836 Val_Reconstruction : 110.14633178710938 Val_KL : 3.311581611633301\n","Epoch: 306/8000  Traning Loss: 116.7703685760498  Train_Reconstruction: 113.41963481903076  Train_KL: 3.350733309984207  Validation Loss : 113.06219100952148 Val_Reconstruction : 109.7337760925293 Val_KL : 3.3284140825271606\n","Epoch: 307/8000  Traning Loss: 116.72783374786377  Train_Reconstruction: 113.35943698883057  Train_KL: 3.3683952391147614  Validation Loss : 113.17141342163086 Val_Reconstruction : 109.84413528442383 Val_KL : 3.327278971672058\n","Epoch: 308/8000  Traning Loss: 116.68167018890381  Train_Reconstruction: 113.32849597930908  Train_KL: 3.3531738817691803  Validation Loss : 112.99641418457031 Val_Reconstruction : 109.66390991210938 Val_KL : 3.332502007484436\n","Epoch: 309/8000  Traning Loss: 116.71448707580566  Train_Reconstruction: 113.34834480285645  Train_KL: 3.3661422729492188  Validation Loss : 113.30972671508789 Val_Reconstruction : 109.9849853515625 Val_KL : 3.3247416019439697\n","Epoch: 310/8000  Traning Loss: 116.91611957550049  Train_Reconstruction: 113.55396938323975  Train_KL: 3.3621497452259064  Validation Loss : 113.20036315917969 Val_Reconstruction : 109.87477111816406 Val_KL : 3.3255945444107056\n","Epoch: 311/8000  Traning Loss: 116.56264019012451  Train_Reconstruction: 113.20699977874756  Train_KL: 3.355639934539795  Validation Loss : 112.88282012939453 Val_Reconstruction : 109.55465698242188 Val_KL : 3.3281617164611816\n","Epoch: 312/8000  Traning Loss: 116.47410678863525  Train_Reconstruction: 113.11146640777588  Train_KL: 3.362640827894211  Validation Loss : 113.15396118164062 Val_Reconstruction : 109.83166122436523 Val_KL : 3.322302460670471\n","Epoch: 313/8000  Traning Loss: 116.21695327758789  Train_Reconstruction: 112.87198829650879  Train_KL: 3.3449656665325165  Validation Loss : 112.62911605834961 Val_Reconstruction : 109.31029891967773 Val_KL : 3.318814992904663\n","Epoch: 314/8000  Traning Loss: 116.13010120391846  Train_Reconstruction: 112.76958847045898  Train_KL: 3.360511988401413  Validation Loss : 112.7540512084961 Val_Reconstruction : 109.43413543701172 Val_KL : 3.319918155670166\n","Epoch: 315/8000  Traning Loss: 116.29237937927246  Train_Reconstruction: 112.94141006469727  Train_KL: 3.3509702682495117  Validation Loss : 112.52458953857422 Val_Reconstruction : 109.20675277709961 Val_KL : 3.3178387880325317\n","Epoch: 316/8000  Traning Loss: 116.25694561004639  Train_Reconstruction: 112.90010738372803  Train_KL: 3.356838673353195  Validation Loss : 112.90592193603516 Val_Reconstruction : 109.58265686035156 Val_KL : 3.3232656717300415\n","Epoch: 317/8000  Traning Loss: 116.11056327819824  Train_Reconstruction: 112.76955318450928  Train_KL: 3.341011166572571  Validation Loss : 112.2612533569336 Val_Reconstruction : 108.95162200927734 Val_KL : 3.3096330165863037\n","Epoch: 318/8000  Traning Loss: 115.90653228759766  Train_Reconstruction: 112.54863739013672  Train_KL: 3.357895165681839  Validation Loss : 112.3891372680664 Val_Reconstruction : 109.05838775634766 Val_KL : 3.3307504653930664\n","Epoch: 319/8000  Traning Loss: 115.81897258758545  Train_Reconstruction: 112.47671127319336  Train_KL: 3.3422612249851227  Validation Loss : 112.39739990234375 Val_Reconstruction : 109.08608627319336 Val_KL : 3.311313271522522\n","Epoch: 320/8000  Traning Loss: 115.9281644821167  Train_Reconstruction: 112.58135414123535  Train_KL: 3.346810907125473  Validation Loss : 112.84689712524414 Val_Reconstruction : 109.54441833496094 Val_KL : 3.3024791479110718\n","Epoch: 321/8000  Traning Loss: 116.09919357299805  Train_Reconstruction: 112.74060344696045  Train_KL: 3.3585900366306305  Validation Loss : 112.7824821472168 Val_Reconstruction : 109.4449348449707 Val_KL : 3.3375455141067505\n","Epoch: 322/8000  Traning Loss: 116.33132266998291  Train_Reconstruction: 112.96304893493652  Train_KL: 3.368272513151169  Validation Loss : 112.60116195678711 Val_Reconstruction : 109.24357604980469 Val_KL : 3.357588529586792\n","Epoch: 323/8000  Traning Loss: 115.88471984863281  Train_Reconstruction: 112.50917625427246  Train_KL: 3.3755425214767456  Validation Loss : 112.3039436340332 Val_Reconstruction : 108.96673202514648 Val_KL : 3.337212085723877\n","Epoch: 324/8000  Traning Loss: 115.94242668151855  Train_Reconstruction: 112.58530616760254  Train_KL: 3.357121467590332  Validation Loss : 112.31349563598633 Val_Reconstruction : 108.9881706237793 Val_KL : 3.3253283500671387\n","Epoch: 325/8000  Traning Loss: 116.03445339202881  Train_Reconstruction: 112.6769495010376  Train_KL: 3.3575050830841064  Validation Loss : 112.3650894165039 Val_Reconstruction : 109.03744506835938 Val_KL : 3.3276431560516357\n","Epoch: 326/8000  Traning Loss: 116.0423173904419  Train_Reconstruction: 112.6779727935791  Train_KL: 3.3643454015254974  Validation Loss : 112.68590545654297 Val_Reconstruction : 109.35365676879883 Val_KL : 3.3322482109069824\n","Epoch: 327/8000  Traning Loss: 116.09830570220947  Train_Reconstruction: 112.74091911315918  Train_KL: 3.3573868572711945  Validation Loss : 112.60420608520508 Val_Reconstruction : 109.2755012512207 Val_KL : 3.3287068605422974\n","Epoch: 328/8000  Traning Loss: 115.97265434265137  Train_Reconstruction: 112.59702491760254  Train_KL: 3.375629633665085  Validation Loss : 112.5146713256836 Val_Reconstruction : 109.14935302734375 Val_KL : 3.365316867828369\n","Epoch: 329/8000  Traning Loss: 115.88923358917236  Train_Reconstruction: 112.53013896942139  Train_KL: 3.359093964099884  Validation Loss : 112.31214141845703 Val_Reconstruction : 109.01401901245117 Val_KL : 3.2981244325637817\n","Epoch: 330/8000  Traning Loss: 115.72513198852539  Train_Reconstruction: 112.36844730377197  Train_KL: 3.3566854596138  Validation Loss : 112.19229125976562 Val_Reconstruction : 108.85580825805664 Val_KL : 3.3364826440811157\n","Epoch: 331/8000  Traning Loss: 115.67756271362305  Train_Reconstruction: 112.32834815979004  Train_KL: 3.349216103553772  Validation Loss : 112.21083450317383 Val_Reconstruction : 108.89368057250977 Val_KL : 3.317155361175537\n","Epoch: 332/8000  Traning Loss: 115.81339836120605  Train_Reconstruction: 112.45572471618652  Train_KL: 3.357674390077591  Validation Loss : 112.4278564453125 Val_Reconstruction : 109.09772491455078 Val_KL : 3.330133080482483\n","Epoch: 333/8000  Traning Loss: 115.60510540008545  Train_Reconstruction: 112.25225639343262  Train_KL: 3.352847635746002  Validation Loss : 111.85770797729492 Val_Reconstruction : 108.5260124206543 Val_KL : 3.3316951990127563\n","Epoch: 334/8000  Traning Loss: 115.47743892669678  Train_Reconstruction: 112.10874938964844  Train_KL: 3.3686889111995697  Validation Loss : 112.39735412597656 Val_Reconstruction : 109.06047821044922 Val_KL : 3.336877465248108\n","Epoch: 335/8000  Traning Loss: 115.77248001098633  Train_Reconstruction: 112.40810585021973  Train_KL: 3.3643733263015747  Validation Loss : 112.38161849975586 Val_Reconstruction : 109.04952621459961 Val_KL : 3.3320913314819336\n","Epoch: 336/8000  Traning Loss: 115.74118614196777  Train_Reconstruction: 112.36881828308105  Train_KL: 3.3723678588867188  Validation Loss : 112.11527633666992 Val_Reconstruction : 108.79231262207031 Val_KL : 3.322962999343872\n","Epoch: 337/8000  Traning Loss: 115.41121768951416  Train_Reconstruction: 112.04699897766113  Train_KL: 3.364219069480896  Validation Loss : 111.66922378540039 Val_Reconstruction : 108.33135223388672 Val_KL : 3.3378690481185913\n","Epoch: 338/8000  Traning Loss: 115.40872478485107  Train_Reconstruction: 112.04501724243164  Train_KL: 3.363707482814789  Validation Loss : 112.13661575317383 Val_Reconstruction : 108.82021713256836 Val_KL : 3.316395401954651\n","Epoch: 339/8000  Traning Loss: 115.40442657470703  Train_Reconstruction: 112.04785919189453  Train_KL: 3.3565674424171448  Validation Loss : 111.64549255371094 Val_Reconstruction : 108.30609893798828 Val_KL : 3.339393138885498\n","Epoch: 340/8000  Traning Loss: 115.37725639343262  Train_Reconstruction: 112.01301670074463  Train_KL: 3.3642402291297913  Validation Loss : 112.22069549560547 Val_Reconstruction : 108.89297485351562 Val_KL : 3.3277238607406616\n","Epoch: 341/8000  Traning Loss: 115.38424110412598  Train_Reconstruction: 112.01827049255371  Train_KL: 3.365971326828003  Validation Loss : 111.85086059570312 Val_Reconstruction : 108.5034294128418 Val_KL : 3.3474329710006714\n","Epoch: 342/8000  Traning Loss: 115.2320327758789  Train_Reconstruction: 111.85540103912354  Train_KL: 3.3766325414180756  Validation Loss : 111.85729217529297 Val_Reconstruction : 108.52608489990234 Val_KL : 3.331206202507019\n","Epoch: 343/8000  Traning Loss: 115.15394496917725  Train_Reconstruction: 111.7995777130127  Train_KL: 3.354367971420288  Validation Loss : 111.88431167602539 Val_Reconstruction : 108.5425910949707 Val_KL : 3.3417205810546875\n","Epoch: 344/8000  Traning Loss: 115.17289543151855  Train_Reconstruction: 111.814377784729  Train_KL: 3.358518064022064  Validation Loss : 112.03727722167969 Val_Reconstruction : 108.71912002563477 Val_KL : 3.3181586265563965\n","Epoch: 345/8000  Traning Loss: 115.26393508911133  Train_Reconstruction: 111.90697288513184  Train_KL: 3.356962710618973  Validation Loss : 111.3845329284668 Val_Reconstruction : 108.05443572998047 Val_KL : 3.3300974369049072\n","Epoch: 346/8000  Traning Loss: 114.99987125396729  Train_Reconstruction: 111.63739204406738  Train_KL: 3.362480044364929  Validation Loss : 111.55976104736328 Val_Reconstruction : 108.24051666259766 Val_KL : 3.3192473649978638\n","Epoch: 347/8000  Traning Loss: 114.85502624511719  Train_Reconstruction: 111.49535369873047  Train_KL: 3.3596725463867188  Validation Loss : 111.40747833251953 Val_Reconstruction : 108.0745964050293 Val_KL : 3.332878589630127\n","Epoch: 348/8000  Traning Loss: 114.77163028717041  Train_Reconstruction: 111.41041374206543  Train_KL: 3.3612165451049805  Validation Loss : 111.34926986694336 Val_Reconstruction : 108.01980209350586 Val_KL : 3.329466938972473\n","Epoch: 349/8000  Traning Loss: 114.78920078277588  Train_Reconstruction: 111.423508644104  Train_KL: 3.3656923174858093  Validation Loss : 111.3739242553711 Val_Reconstruction : 108.03691864013672 Val_KL : 3.3370085954666138\n","Epoch: 350/8000  Traning Loss: 114.58730697631836  Train_Reconstruction: 111.21952247619629  Train_KL: 3.367784470319748  Validation Loss : 111.149169921875 Val_Reconstruction : 107.81935119628906 Val_KL : 3.329818844795227\n","Epoch: 351/8000  Traning Loss: 114.51455974578857  Train_Reconstruction: 111.15120315551758  Train_KL: 3.363356500864029  Validation Loss : 111.24591827392578 Val_Reconstruction : 107.91128540039062 Val_KL : 3.3346303701400757\n","Epoch: 352/8000  Traning Loss: 115.00614166259766  Train_Reconstruction: 111.63342666625977  Train_KL: 3.3727159798145294  Validation Loss : 111.50615310668945 Val_Reconstruction : 108.1484489440918 Val_KL : 3.3577059507369995\n","Epoch: 353/8000  Traning Loss: 114.81678295135498  Train_Reconstruction: 111.4303207397461  Train_KL: 3.386461913585663  Validation Loss : 111.44094848632812 Val_Reconstruction : 108.09221267700195 Val_KL : 3.348737359046936\n","Epoch: 354/8000  Traning Loss: 114.93535709381104  Train_Reconstruction: 111.56732845306396  Train_KL: 3.368028402328491  Validation Loss : 112.24208450317383 Val_Reconstruction : 108.91587448120117 Val_KL : 3.326212763786316\n","Epoch: 355/8000  Traning Loss: 115.32170963287354  Train_Reconstruction: 111.95959758758545  Train_KL: 3.3621124029159546  Validation Loss : 111.95219039916992 Val_Reconstruction : 108.60878372192383 Val_KL : 3.3434046506881714\n","Epoch: 356/8000  Traning Loss: 115.3710527420044  Train_Reconstruction: 112.00264835357666  Train_KL: 3.368404507637024  Validation Loss : 113.07773971557617 Val_Reconstruction : 109.74238586425781 Val_KL : 3.3353540897369385\n","Epoch: 357/8000  Traning Loss: 115.29953098297119  Train_Reconstruction: 111.9185733795166  Train_KL: 3.3809582889080048  Validation Loss : 111.8245620727539 Val_Reconstruction : 108.47129821777344 Val_KL : 3.3532657623291016\n","Epoch: 358/8000  Traning Loss: 114.90511512756348  Train_Reconstruction: 111.52819633483887  Train_KL: 3.37691792845726  Validation Loss : 111.49412155151367 Val_Reconstruction : 108.1549186706543 Val_KL : 3.3392016887664795\n","Epoch: 359/8000  Traning Loss: 114.73375701904297  Train_Reconstruction: 111.36245346069336  Train_KL: 3.3713048100471497  Validation Loss : 111.52238845825195 Val_Reconstruction : 108.17829895019531 Val_KL : 3.344089984893799\n","Epoch: 360/8000  Traning Loss: 114.84495544433594  Train_Reconstruction: 111.468994140625  Train_KL: 3.375962197780609  Validation Loss : 111.540771484375 Val_Reconstruction : 108.20074081420898 Val_KL : 3.340034246444702\n","Epoch: 361/8000  Traning Loss: 114.74409484863281  Train_Reconstruction: 111.38590240478516  Train_KL: 3.3581908643245697  Validation Loss : 111.36244583129883 Val_Reconstruction : 108.03384399414062 Val_KL : 3.3286012411117554\n","Epoch: 362/8000  Traning Loss: 114.71185398101807  Train_Reconstruction: 111.3349199295044  Train_KL: 3.3769338130950928  Validation Loss : 111.44381713867188 Val_Reconstruction : 108.09431838989258 Val_KL : 3.349496841430664\n","Epoch: 363/8000  Traning Loss: 114.74714088439941  Train_Reconstruction: 111.37314510345459  Train_KL: 3.373995542526245  Validation Loss : 111.17572021484375 Val_Reconstruction : 107.8294448852539 Val_KL : 3.346277594566345\n","Epoch: 364/8000  Traning Loss: 114.39437961578369  Train_Reconstruction: 111.01107692718506  Train_KL: 3.383302181959152  Validation Loss : 110.76407623291016 Val_Reconstruction : 107.42887878417969 Val_KL : 3.335200071334839\n","Epoch: 365/8000  Traning Loss: 114.17219829559326  Train_Reconstruction: 110.80143928527832  Train_KL: 3.3707589507102966  Validation Loss : 110.95991516113281 Val_Reconstruction : 107.61745071411133 Val_KL : 3.3424625396728516\n","Epoch: 366/8000  Traning Loss: 114.24669647216797  Train_Reconstruction: 110.87141418457031  Train_KL: 3.3752812147140503  Validation Loss : 111.06642150878906 Val_Reconstruction : 107.74959564208984 Val_KL : 3.316825270652771\n","Epoch: 367/8000  Traning Loss: 114.51838970184326  Train_Reconstruction: 111.15495491027832  Train_KL: 3.3634348809719086  Validation Loss : 111.50964736938477 Val_Reconstruction : 108.1614990234375 Val_KL : 3.348151206970215\n","Epoch: 368/8000  Traning Loss: 114.39690971374512  Train_Reconstruction: 111.02468490600586  Train_KL: 3.3722246289253235  Validation Loss : 110.9541015625 Val_Reconstruction : 107.60713577270508 Val_KL : 3.3469661474227905\n","Epoch: 369/8000  Traning Loss: 114.24606418609619  Train_Reconstruction: 110.86791706085205  Train_KL: 3.3781469762325287  Validation Loss : 110.8013801574707 Val_Reconstruction : 107.45998001098633 Val_KL : 3.3413981199264526\n","Epoch: 370/8000  Traning Loss: 114.0107250213623  Train_Reconstruction: 110.64160346984863  Train_KL: 3.369121551513672  Validation Loss : 110.56303787231445 Val_Reconstruction : 107.22002410888672 Val_KL : 3.343012571334839\n","Epoch: 371/8000  Traning Loss: 114.30208015441895  Train_Reconstruction: 110.91596412658691  Train_KL: 3.38611701130867  Validation Loss : 110.98242568969727 Val_Reconstruction : 107.63416290283203 Val_KL : 3.348263144493103\n","Epoch: 372/8000  Traning Loss: 114.10957050323486  Train_Reconstruction: 110.72852516174316  Train_KL: 3.3810444474220276  Validation Loss : 110.82843399047852 Val_Reconstruction : 107.48591613769531 Val_KL : 3.342517614364624\n","Epoch: 373/8000  Traning Loss: 114.00945663452148  Train_Reconstruction: 110.620361328125  Train_KL: 3.3890944123268127  Validation Loss : 110.85224533081055 Val_Reconstruction : 107.47687911987305 Val_KL : 3.375365138053894\n","Epoch: 374/8000  Traning Loss: 113.88526248931885  Train_Reconstruction: 110.49112796783447  Train_KL: 3.39413383603096  Validation Loss : 110.47091293334961 Val_Reconstruction : 107.12274551391602 Val_KL : 3.348166584968567\n","Epoch: 375/8000  Traning Loss: 113.79480171203613  Train_Reconstruction: 110.42406940460205  Train_KL: 3.3707341849803925  Validation Loss : 110.53877639770508 Val_Reconstruction : 107.20015716552734 Val_KL : 3.338622212409973\n","Epoch: 376/8000  Traning Loss: 113.74978065490723  Train_Reconstruction: 110.38107585906982  Train_KL: 3.368704915046692  Validation Loss : 110.43400192260742 Val_Reconstruction : 107.10420608520508 Val_KL : 3.329796314239502\n","Epoch: 377/8000  Traning Loss: 113.74709129333496  Train_Reconstruction: 110.37259864807129  Train_KL: 3.374494105577469  Validation Loss : 110.27947235107422 Val_Reconstruction : 106.9276237487793 Val_KL : 3.3518489599227905\n","Epoch: 378/8000  Traning Loss: 113.57377624511719  Train_Reconstruction: 110.19881820678711  Train_KL: 3.3749565482139587  Validation Loss : 110.1610221862793 Val_Reconstruction : 106.82242584228516 Val_KL : 3.3385965824127197\n","Epoch: 379/8000  Traning Loss: 113.80512523651123  Train_Reconstruction: 110.41541576385498  Train_KL: 3.3897092640399933  Validation Loss : 110.84563064575195 Val_Reconstruction : 107.49534606933594 Val_KL : 3.350281238555908\n","Epoch: 380/8000  Traning Loss: 113.9010419845581  Train_Reconstruction: 110.52015209197998  Train_KL: 3.3808904886245728  Validation Loss : 110.63721466064453 Val_Reconstruction : 107.29037857055664 Val_KL : 3.346839189529419\n","Epoch: 381/8000  Traning Loss: 113.83483600616455  Train_Reconstruction: 110.46064376831055  Train_KL: 3.3741917312145233  Validation Loss : 110.70818328857422 Val_Reconstruction : 107.35734558105469 Val_KL : 3.3508381843566895\n","Epoch: 382/8000  Traning Loss: 113.94759941101074  Train_Reconstruction: 110.57469844818115  Train_KL: 3.3729007244110107  Validation Loss : 110.69901657104492 Val_Reconstruction : 107.36584854125977 Val_KL : 3.3331655263900757\n","Epoch: 383/8000  Traning Loss: 113.80616092681885  Train_Reconstruction: 110.4364070892334  Train_KL: 3.3697540760040283  Validation Loss : 110.29216384887695 Val_Reconstruction : 106.94718933105469 Val_KL : 3.34497606754303\n","Epoch: 384/8000  Traning Loss: 113.76306819915771  Train_Reconstruction: 110.3819351196289  Train_KL: 3.3811327517032623  Validation Loss : 110.67245101928711 Val_Reconstruction : 107.32837295532227 Val_KL : 3.3440773487091064\n","Epoch: 385/8000  Traning Loss: 113.86888027191162  Train_Reconstruction: 110.48685073852539  Train_KL: 3.3820309340953827  Validation Loss : 110.89640426635742 Val_Reconstruction : 107.53849029541016 Val_KL : 3.3579139709472656\n","Epoch: 386/8000  Traning Loss: 113.93251132965088  Train_Reconstruction: 110.54252243041992  Train_KL: 3.3899891078472137  Validation Loss : 110.86445236206055 Val_Reconstruction : 107.49959564208984 Val_KL : 3.364854574203491\n","Epoch: 387/8000  Traning Loss: 113.6127872467041  Train_Reconstruction: 110.22544288635254  Train_KL: 3.3873440623283386  Validation Loss : 110.67955780029297 Val_Reconstruction : 107.34232330322266 Val_KL : 3.337232828140259\n","Epoch: 388/8000  Traning Loss: 113.70683574676514  Train_Reconstruction: 110.3233003616333  Train_KL: 3.38353431224823  Validation Loss : 110.44766616821289 Val_Reconstruction : 107.09162902832031 Val_KL : 3.356037139892578\n","Epoch: 389/8000  Traning Loss: 113.6796293258667  Train_Reconstruction: 110.31038570404053  Train_KL: 3.3692437410354614  Validation Loss : 110.41212844848633 Val_Reconstruction : 107.08079528808594 Val_KL : 3.3313335180282593\n","Epoch: 390/8000  Traning Loss: 113.43262958526611  Train_Reconstruction: 110.05518531799316  Train_KL: 3.3774441480636597  Validation Loss : 110.19308853149414 Val_Reconstruction : 106.86070251464844 Val_KL : 3.332388401031494\n","Epoch: 391/8000  Traning Loss: 113.32751083374023  Train_Reconstruction: 109.96059989929199  Train_KL: 3.3669107258319855  Validation Loss : 110.07879638671875 Val_Reconstruction : 106.7337417602539 Val_KL : 3.3450547456741333\n","Epoch: 392/8000  Traning Loss: 113.37395286560059  Train_Reconstruction: 110.00292205810547  Train_KL: 3.3710319697856903  Validation Loss : 110.29940414428711 Val_Reconstruction : 106.95429229736328 Val_KL : 3.3451110124588013\n","Epoch: 393/8000  Traning Loss: 113.2829942703247  Train_Reconstruction: 109.92243766784668  Train_KL: 3.360557049512863  Validation Loss : 110.02480697631836 Val_Reconstruction : 106.69902801513672 Val_KL : 3.3257774114608765\n","Epoch: 394/8000  Traning Loss: 113.3155460357666  Train_Reconstruction: 109.94991874694824  Train_KL: 3.3656265437602997  Validation Loss : 110.0320053100586 Val_Reconstruction : 106.69422912597656 Val_KL : 3.3377768993377686\n","Epoch: 395/8000  Traning Loss: 113.50710010528564  Train_Reconstruction: 110.12468719482422  Train_KL: 3.3824119567871094  Validation Loss : 110.31779861450195 Val_Reconstruction : 106.95646286010742 Val_KL : 3.361332893371582\n","Epoch: 396/8000  Traning Loss: 113.3165283203125  Train_Reconstruction: 109.9388780593872  Train_KL: 3.377650022506714  Validation Loss : 110.02946090698242 Val_Reconstruction : 106.68542098999023 Val_KL : 3.3440405130386353\n","Epoch: 397/8000  Traning Loss: 113.09902858734131  Train_Reconstruction: 109.71991634368896  Train_KL: 3.3791113793849945  Validation Loss : 109.84683609008789 Val_Reconstruction : 106.50179290771484 Val_KL : 3.3450446128845215\n","Epoch: 398/8000  Traning Loss: 113.06898593902588  Train_Reconstruction: 109.69305992126465  Train_KL: 3.375926226377487  Validation Loss : 110.38697814941406 Val_Reconstruction : 107.04125213623047 Val_KL : 3.3457274436950684\n","Epoch: 399/8000  Traning Loss: 113.8156509399414  Train_Reconstruction: 110.44163131713867  Train_KL: 3.3740192353725433  Validation Loss : 110.64437484741211 Val_Reconstruction : 107.29788589477539 Val_KL : 3.3464921712875366\n","Epoch: 400/8000  Traning Loss: 113.6927843093872  Train_Reconstruction: 110.31725788116455  Train_KL: 3.375526487827301  Validation Loss : 110.51179504394531 Val_Reconstruction : 107.16968154907227 Val_KL : 3.342112898826599\n","Epoch: 401/8000  Traning Loss: 113.20460510253906  Train_Reconstruction: 109.82633399963379  Train_KL: 3.3782725036144257  Validation Loss : 109.99451446533203 Val_Reconstruction : 106.64433288574219 Val_KL : 3.3501827716827393\n","Epoch: 402/8000  Traning Loss: 113.1290397644043  Train_Reconstruction: 109.72997760772705  Train_KL: 3.3990617096424103  Validation Loss : 109.73063659667969 Val_Reconstruction : 106.35321807861328 Val_KL : 3.3774163722991943\n","Epoch: 403/8000  Traning Loss: 113.12768363952637  Train_Reconstruction: 109.73940944671631  Train_KL: 3.388273745775223  Validation Loss : 109.88699340820312 Val_Reconstruction : 106.54659652709961 Val_KL : 3.34039843082428\n","Epoch: 404/8000  Traning Loss: 112.9227819442749  Train_Reconstruction: 109.53825378417969  Train_KL: 3.384528338909149  Validation Loss : 109.63192367553711 Val_Reconstruction : 106.28257751464844 Val_KL : 3.3493443727493286\n","Epoch: 405/8000  Traning Loss: 112.82747173309326  Train_Reconstruction: 109.45737743377686  Train_KL: 3.370094269514084  Validation Loss : 109.62734603881836 Val_Reconstruction : 106.28372573852539 Val_KL : 3.3436226844787598\n","Epoch: 406/8000  Traning Loss: 112.91157150268555  Train_Reconstruction: 109.53512001037598  Train_KL: 3.3764520585536957  Validation Loss : 109.90168762207031 Val_Reconstruction : 106.55364990234375 Val_KL : 3.348037600517273\n","Epoch: 407/8000  Traning Loss: 113.2520751953125  Train_Reconstruction: 109.87522029876709  Train_KL: 3.376854807138443  Validation Loss : 110.1807975769043 Val_Reconstruction : 106.83394622802734 Val_KL : 3.3468517065048218\n","Epoch: 408/8000  Traning Loss: 113.187668800354  Train_Reconstruction: 109.80913162231445  Train_KL: 3.378536880016327  Validation Loss : 109.96844863891602 Val_Reconstruction : 106.61879348754883 Val_KL : 3.349655866622925\n","Epoch: 409/8000  Traning Loss: 112.96746444702148  Train_Reconstruction: 109.59079837799072  Train_KL: 3.3766658008098602  Validation Loss : 109.59295654296875 Val_Reconstruction : 106.2505989074707 Val_KL : 3.3423575162887573\n","Epoch: 410/8000  Traning Loss: 112.6602201461792  Train_Reconstruction: 109.27440452575684  Train_KL: 3.385815054178238  Validation Loss : 109.30620956420898 Val_Reconstruction : 105.94757461547852 Val_KL : 3.3586353063583374\n","Epoch: 411/8000  Traning Loss: 112.5357894897461  Train_Reconstruction: 109.1575984954834  Train_KL: 3.37818905711174  Validation Loss : 109.2462158203125 Val_Reconstruction : 105.90356826782227 Val_KL : 3.3426483869552612\n","Epoch: 412/8000  Traning Loss: 112.60625743865967  Train_Reconstruction: 109.23514747619629  Train_KL: 3.371109217405319  Validation Loss : 109.45470428466797 Val_Reconstruction : 106.11176681518555 Val_KL : 3.342936635017395\n","Epoch: 413/8000  Traning Loss: 112.73459720611572  Train_Reconstruction: 109.35257625579834  Train_KL: 3.382020503282547  Validation Loss : 109.5319595336914 Val_Reconstruction : 106.17842864990234 Val_KL : 3.3535315990448\n","Epoch: 414/8000  Traning Loss: 112.69298362731934  Train_Reconstruction: 109.31008434295654  Train_KL: 3.3828989565372467  Validation Loss : 109.36777877807617 Val_Reconstruction : 106.00564193725586 Val_KL : 3.3621333837509155\n","Epoch: 415/8000  Traning Loss: 112.51851749420166  Train_Reconstruction: 109.13389015197754  Train_KL: 3.3846273124217987  Validation Loss : 109.23763656616211 Val_Reconstruction : 105.88639831542969 Val_KL : 3.3512370586395264\n","Epoch: 416/8000  Traning Loss: 112.65739822387695  Train_Reconstruction: 109.27593803405762  Train_KL: 3.381460189819336  Validation Loss : 109.93971252441406 Val_Reconstruction : 106.59429550170898 Val_KL : 3.3454161882400513\n","Epoch: 417/8000  Traning Loss: 112.67996501922607  Train_Reconstruction: 109.30152702331543  Train_KL: 3.3784390091896057  Validation Loss : 109.59576034545898 Val_Reconstruction : 106.23207092285156 Val_KL : 3.3636908531188965\n","Epoch: 418/8000  Traning Loss: 112.8225622177124  Train_Reconstruction: 109.43655967712402  Train_KL: 3.386002391576767  Validation Loss : 109.87308502197266 Val_Reconstruction : 106.52804565429688 Val_KL : 3.3450398445129395\n","Epoch: 419/8000  Traning Loss: 112.87046432495117  Train_Reconstruction: 109.48645687103271  Train_KL: 3.3840073943138123  Validation Loss : 109.97443771362305 Val_Reconstruction : 106.6144905090332 Val_KL : 3.3599482774734497\n","Epoch: 420/8000  Traning Loss: 113.15275478363037  Train_Reconstruction: 109.76363468170166  Train_KL: 3.3891206085681915  Validation Loss : 110.14785766601562 Val_Reconstruction : 106.79331970214844 Val_KL : 3.3545374870300293\n","Epoch: 421/8000  Traning Loss: 112.88377571105957  Train_Reconstruction: 109.50083923339844  Train_KL: 3.382936030626297  Validation Loss : 109.62889099121094 Val_Reconstruction : 106.27471542358398 Val_KL : 3.3541743755340576\n","Epoch: 422/8000  Traning Loss: 112.65693187713623  Train_Reconstruction: 109.27056312561035  Train_KL: 3.3863686621189117  Validation Loss : 109.81267929077148 Val_Reconstruction : 106.4408950805664 Val_KL : 3.371782064437866\n","Epoch: 423/8000  Traning Loss: 112.53902339935303  Train_Reconstruction: 109.14198112487793  Train_KL: 3.3970428109169006  Validation Loss : 109.38676071166992 Val_Reconstruction : 106.04183959960938 Val_KL : 3.3449196815490723\n","Epoch: 424/8000  Traning Loss: 112.4577989578247  Train_Reconstruction: 109.07579040527344  Train_KL: 3.3820081055164337  Validation Loss : 109.39402770996094 Val_Reconstruction : 106.03755950927734 Val_KL : 3.3564668893814087\n","Epoch: 425/8000  Traning Loss: 112.22339916229248  Train_Reconstruction: 108.83576965332031  Train_KL: 3.3876305520534515  Validation Loss : 109.22204208374023 Val_Reconstruction : 105.85269546508789 Val_KL : 3.3693454265594482\n","Epoch: 426/8000  Traning Loss: 112.3600721359253  Train_Reconstruction: 108.96861457824707  Train_KL: 3.3914577662944794  Validation Loss : 109.35476684570312 Val_Reconstruction : 106.01825714111328 Val_KL : 3.336509346961975\n","Epoch: 427/8000  Traning Loss: 112.36870574951172  Train_Reconstruction: 108.99295616149902  Train_KL: 3.375748872756958  Validation Loss : 109.21221923828125 Val_Reconstruction : 105.83066940307617 Val_KL : 3.381551146507263\n","Epoch: 428/8000  Traning Loss: 112.25796890258789  Train_Reconstruction: 108.86958312988281  Train_KL: 3.388385236263275  Validation Loss : 109.20513534545898 Val_Reconstruction : 105.8737564086914 Val_KL : 3.33137845993042\n","Epoch: 429/8000  Traning Loss: 112.07751750946045  Train_Reconstruction: 108.69427490234375  Train_KL: 3.383243441581726  Validation Loss : 108.98572158813477 Val_Reconstruction : 105.61412048339844 Val_KL : 3.3716012239456177\n","Epoch: 430/8000  Traning Loss: 112.0917911529541  Train_Reconstruction: 108.6980333328247  Train_KL: 3.3937582075595856  Validation Loss : 108.97437286376953 Val_Reconstruction : 105.614501953125 Val_KL : 3.3598743677139282\n","Epoch: 431/8000  Traning Loss: 111.93431186676025  Train_Reconstruction: 108.5433292388916  Train_KL: 3.3909822702407837  Validation Loss : 108.7082633972168 Val_Reconstruction : 105.36071014404297 Val_KL : 3.3475533723831177\n","Epoch: 432/8000  Traning Loss: 112.29780197143555  Train_Reconstruction: 108.92249870300293  Train_KL: 3.375302255153656  Validation Loss : 109.31269836425781 Val_Reconstruction : 105.98633575439453 Val_KL : 3.3263646364212036\n","Epoch: 433/8000  Traning Loss: 112.21876621246338  Train_Reconstruction: 108.84250068664551  Train_KL: 3.3762654960155487  Validation Loss : 109.01070785522461 Val_Reconstruction : 105.66083908081055 Val_KL : 3.3498679399490356\n","Epoch: 434/8000  Traning Loss: 112.11453533172607  Train_Reconstruction: 108.73124599456787  Train_KL: 3.3832885324954987  Validation Loss : 109.19377136230469 Val_Reconstruction : 105.84207916259766 Val_KL : 3.3516910076141357\n","Epoch: 435/8000  Traning Loss: 112.38208675384521  Train_Reconstruction: 108.99710083007812  Train_KL: 3.3849876523017883  Validation Loss : 109.2556381225586 Val_Reconstruction : 105.90558624267578 Val_KL : 3.3500514030456543\n","Epoch: 436/8000  Traning Loss: 112.270751953125  Train_Reconstruction: 108.88682842254639  Train_KL: 3.383925050497055  Validation Loss : 109.23825073242188 Val_Reconstruction : 105.87524032592773 Val_KL : 3.3630118370056152\n","Epoch: 437/8000  Traning Loss: 112.2713041305542  Train_Reconstruction: 108.87455463409424  Train_KL: 3.3967497646808624  Validation Loss : 109.5680160522461 Val_Reconstruction : 106.19547271728516 Val_KL : 3.37254536151886\n","Epoch: 438/8000  Traning Loss: 112.4227991104126  Train_Reconstruction: 109.04642391204834  Train_KL: 3.376375734806061  Validation Loss : 109.10590362548828 Val_Reconstruction : 105.75392532348633 Val_KL : 3.3519798517227173\n","Epoch: 439/8000  Traning Loss: 112.10943794250488  Train_Reconstruction: 108.72399139404297  Train_KL: 3.3854465782642365  Validation Loss : 109.21468734741211 Val_Reconstruction : 105.86167907714844 Val_KL : 3.3530091047286987\n","Epoch: 440/8000  Traning Loss: 112.01057147979736  Train_Reconstruction: 108.61829853057861  Train_KL: 3.392273247241974  Validation Loss : 108.6451530456543 Val_Reconstruction : 105.28776550292969 Val_KL : 3.3573871850967407\n","Epoch: 441/8000  Traning Loss: 111.89326477050781  Train_Reconstruction: 108.48735904693604  Train_KL: 3.40590637922287  Validation Loss : 108.78158950805664 Val_Reconstruction : 105.39505004882812 Val_KL : 3.3865405321121216\n","Epoch: 442/8000  Traning Loss: 111.95016860961914  Train_Reconstruction: 108.53549671173096  Train_KL: 3.414670616388321  Validation Loss : 108.63566207885742 Val_Reconstruction : 105.2756233215332 Val_KL : 3.3600375652313232\n","Epoch: 443/8000  Traning Loss: 112.18284797668457  Train_Reconstruction: 108.77847194671631  Train_KL: 3.4043760299682617  Validation Loss : 109.27042007446289 Val_Reconstruction : 105.9129638671875 Val_KL : 3.3574551343917847\n","Epoch: 444/8000  Traning Loss: 112.08234977722168  Train_Reconstruction: 108.69349479675293  Train_KL: 3.3888550400733948  Validation Loss : 109.21996307373047 Val_Reconstruction : 105.86864852905273 Val_KL : 3.3513171672821045\n","Epoch: 445/8000  Traning Loss: 111.61789798736572  Train_Reconstruction: 108.22762870788574  Train_KL: 3.3902697265148163  Validation Loss : 108.51723861694336 Val_Reconstruction : 105.15848159790039 Val_KL : 3.3587591648101807\n","Epoch: 446/8000  Traning Loss: 111.60067653656006  Train_Reconstruction: 108.1989517211914  Train_KL: 3.401723474264145  Validation Loss : 108.72593688964844 Val_Reconstruction : 105.3705825805664 Val_KL : 3.3553555011749268\n","Epoch: 447/8000  Traning Loss: 111.64943885803223  Train_Reconstruction: 108.26081466674805  Train_KL: 3.3886238634586334  Validation Loss : 108.91819381713867 Val_Reconstruction : 105.56538391113281 Val_KL : 3.3528075218200684\n","Epoch: 448/8000  Traning Loss: 111.63456439971924  Train_Reconstruction: 108.24766540527344  Train_KL: 3.3868985176086426  Validation Loss : 108.66219711303711 Val_Reconstruction : 105.29903411865234 Val_KL : 3.3631643056869507\n","Epoch: 449/8000  Traning Loss: 111.6281967163086  Train_Reconstruction: 108.23234558105469  Train_KL: 3.395850956439972  Validation Loss : 109.06730651855469 Val_Reconstruction : 105.68417358398438 Val_KL : 3.3831342458724976\n","Epoch: 450/8000  Traning Loss: 111.63619136810303  Train_Reconstruction: 108.22848606109619  Train_KL: 3.4077053368091583  Validation Loss : 108.5746841430664 Val_Reconstruction : 105.2174186706543 Val_KL : 3.3572659492492676\n","Epoch: 451/8000  Traning Loss: 111.57937717437744  Train_Reconstruction: 108.18651390075684  Train_KL: 3.392864912748337  Validation Loss : 108.7958984375 Val_Reconstruction : 105.43322372436523 Val_KL : 3.3626768589019775\n","Epoch: 452/8000  Traning Loss: 111.55824184417725  Train_Reconstruction: 108.16575717926025  Train_KL: 3.3924844563007355  Validation Loss : 108.59903335571289 Val_Reconstruction : 105.24228286743164 Val_KL : 3.3567497730255127\n","Epoch: 453/8000  Traning Loss: 111.54856967926025  Train_Reconstruction: 108.14515018463135  Train_KL: 3.403417855501175  Validation Loss : 108.5468635559082 Val_Reconstruction : 105.1750717163086 Val_KL : 3.37178897857666\n","Epoch: 454/8000  Traning Loss: 111.47548294067383  Train_Reconstruction: 108.07308959960938  Train_KL: 3.4023938477039337  Validation Loss : 108.39690017700195 Val_Reconstruction : 105.04352951049805 Val_KL : 3.3533722162246704\n","Epoch: 455/8000  Traning Loss: 111.70424461364746  Train_Reconstruction: 108.31696701049805  Train_KL: 3.3872773945331573  Validation Loss : 108.84733963012695 Val_Reconstruction : 105.49017333984375 Val_KL : 3.357166051864624\n","Epoch: 456/8000  Traning Loss: 111.8290147781372  Train_Reconstruction: 108.44563961029053  Train_KL: 3.3833733797073364  Validation Loss : 108.9936752319336 Val_Reconstruction : 105.64353942871094 Val_KL : 3.350134015083313\n","Epoch: 457/8000  Traning Loss: 111.84598541259766  Train_Reconstruction: 108.45331287384033  Train_KL: 3.3926726579666138  Validation Loss : 108.53742218017578 Val_Reconstruction : 105.15649032592773 Val_KL : 3.3809293508529663\n","Epoch: 458/8000  Traning Loss: 111.63184642791748  Train_Reconstruction: 108.23353958129883  Train_KL: 3.3983077704906464  Validation Loss : 108.6916275024414 Val_Reconstruction : 105.32374954223633 Val_KL : 3.3678752183914185\n","Epoch: 459/8000  Traning Loss: 111.82798480987549  Train_Reconstruction: 108.41675853729248  Train_KL: 3.411225587129593  Validation Loss : 108.84189224243164 Val_Reconstruction : 105.46970748901367 Val_KL : 3.372181534767151\n","Epoch: 460/8000  Traning Loss: 111.55078506469727  Train_Reconstruction: 108.15129089355469  Train_KL: 3.399493008852005  Validation Loss : 108.34066009521484 Val_Reconstruction : 104.97254180908203 Val_KL : 3.3681195974349976\n","Epoch: 461/8000  Traning Loss: 111.32735443115234  Train_Reconstruction: 107.92791080474854  Train_KL: 3.399441719055176  Validation Loss : 108.33179473876953 Val_Reconstruction : 104.96396255493164 Val_KL : 3.3678343296051025\n","Epoch: 462/8000  Traning Loss: 111.16608333587646  Train_Reconstruction: 107.77199268341064  Train_KL: 3.3940908014774323  Validation Loss : 108.12039566040039 Val_Reconstruction : 104.77199172973633 Val_KL : 3.348403215408325\n","Epoch: 463/8000  Traning Loss: 111.21784210205078  Train_Reconstruction: 107.83795833587646  Train_KL: 3.3798826932907104  Validation Loss : 108.45190811157227 Val_Reconstruction : 105.08564376831055 Val_KL : 3.36626660823822\n","Epoch: 464/8000  Traning Loss: 111.13735294342041  Train_Reconstruction: 107.73201560974121  Train_KL: 3.4053370356559753  Validation Loss : 108.07431411743164 Val_Reconstruction : 104.69885635375977 Val_KL : 3.375456929206848\n","Epoch: 465/8000  Traning Loss: 111.15166091918945  Train_Reconstruction: 107.76883792877197  Train_KL: 3.3828230798244476  Validation Loss : 108.1702995300293 Val_Reconstruction : 104.81442260742188 Val_KL : 3.355875849723816\n","Epoch: 466/8000  Traning Loss: 111.12760925292969  Train_Reconstruction: 107.72711181640625  Train_KL: 3.4004979729652405  Validation Loss : 108.4390869140625 Val_Reconstruction : 105.07337188720703 Val_KL : 3.3657171726226807\n","Epoch: 467/8000  Traning Loss: 111.2749605178833  Train_Reconstruction: 107.88113212585449  Train_KL: 3.3938280045986176  Validation Loss : 108.01591110229492 Val_Reconstruction : 104.65523910522461 Val_KL : 3.3606728315353394\n","Epoch: 468/8000  Traning Loss: 111.0642204284668  Train_Reconstruction: 107.66956901550293  Train_KL: 3.3946505784988403  Validation Loss : 108.04085922241211 Val_Reconstruction : 104.69210052490234 Val_KL : 3.3487578630447388\n","Epoch: 469/8000  Traning Loss: 111.38869762420654  Train_Reconstruction: 107.99714374542236  Train_KL: 3.39155375957489  Validation Loss : 108.38330841064453 Val_Reconstruction : 105.01443862915039 Val_KL : 3.3688710927963257\n","Epoch: 470/8000  Traning Loss: 111.24396419525146  Train_Reconstruction: 107.85211658477783  Train_KL: 3.3918475806713104  Validation Loss : 108.13972473144531 Val_Reconstruction : 104.77267837524414 Val_KL : 3.36704683303833\n","Epoch: 471/8000  Traning Loss: 110.88767433166504  Train_Reconstruction: 107.48928546905518  Train_KL: 3.3983893394470215  Validation Loss : 107.78894805908203 Val_Reconstruction : 104.41888809204102 Val_KL : 3.3700573444366455\n","Epoch: 472/8000  Traning Loss: 110.77318382263184  Train_Reconstruction: 107.38737487792969  Train_KL: 3.3858084678649902  Validation Loss : 107.98148345947266 Val_Reconstruction : 104.63591766357422 Val_KL : 3.345566153526306\n","Epoch: 473/8000  Traning Loss: 110.9041919708252  Train_Reconstruction: 107.51368808746338  Train_KL: 3.3905043601989746  Validation Loss : 107.76216125488281 Val_Reconstruction : 104.3844985961914 Val_KL : 3.3776626586914062\n","Epoch: 474/8000  Traning Loss: 110.69110679626465  Train_Reconstruction: 107.28288650512695  Train_KL: 3.408220022916794  Validation Loss : 107.80398178100586 Val_Reconstruction : 104.42069625854492 Val_KL : 3.3832870721817017\n","Epoch: 475/8000  Traning Loss: 110.80607509613037  Train_Reconstruction: 107.40396308898926  Train_KL: 3.402112662792206  Validation Loss : 107.59779357910156 Val_Reconstruction : 104.22418975830078 Val_KL : 3.37360155582428\n","Epoch: 476/8000  Traning Loss: 110.72621059417725  Train_Reconstruction: 107.3288927078247  Train_KL: 3.397318035364151  Validation Loss : 107.78109741210938 Val_Reconstruction : 104.40886306762695 Val_KL : 3.3722323179244995\n","Epoch: 477/8000  Traning Loss: 110.89504718780518  Train_Reconstruction: 107.48531246185303  Train_KL: 3.4097348749637604  Validation Loss : 107.89866256713867 Val_Reconstruction : 104.51925277709961 Val_KL : 3.3794102668762207\n","Epoch: 478/8000  Traning Loss: 110.74773025512695  Train_Reconstruction: 107.33845806121826  Train_KL: 3.4092705249786377  Validation Loss : 107.87310028076172 Val_Reconstruction : 104.49892044067383 Val_KL : 3.3741811513900757\n","Epoch: 479/8000  Traning Loss: 110.78278732299805  Train_Reconstruction: 107.38243579864502  Train_KL: 3.400350511074066  Validation Loss : 107.89717102050781 Val_Reconstruction : 104.51523208618164 Val_KL : 3.3819398880004883\n","Epoch: 480/8000  Traning Loss: 110.6833553314209  Train_Reconstruction: 107.27702331542969  Train_KL: 3.406331419944763  Validation Loss : 107.64142990112305 Val_Reconstruction : 104.26620483398438 Val_KL : 3.3752228021621704\n","Epoch: 481/8000  Traning Loss: 110.58468723297119  Train_Reconstruction: 107.17873668670654  Train_KL: 3.4059507250785828  Validation Loss : 107.68650817871094 Val_Reconstruction : 104.30887603759766 Val_KL : 3.377632737159729\n","Epoch: 482/8000  Traning Loss: 110.602126121521  Train_Reconstruction: 107.20596504211426  Train_KL: 3.3961600363254547  Validation Loss : 107.58853530883789 Val_Reconstruction : 104.23238372802734 Val_KL : 3.356149911880493\n","Epoch: 483/8000  Traning Loss: 110.46956062316895  Train_Reconstruction: 107.06067180633545  Train_KL: 3.408889025449753  Validation Loss : 107.60948181152344 Val_Reconstruction : 104.21517944335938 Val_KL : 3.394302725791931\n","Epoch: 484/8000  Traning Loss: 110.61299896240234  Train_Reconstruction: 107.2081184387207  Train_KL: 3.4048805236816406  Validation Loss : 107.54130554199219 Val_Reconstruction : 104.18413162231445 Val_KL : 3.357173204421997\n","Epoch: 485/8000  Traning Loss: 110.80314540863037  Train_Reconstruction: 107.39304637908936  Train_KL: 3.4100987017154694  Validation Loss : 108.07107162475586 Val_Reconstruction : 104.67408752441406 Val_KL : 3.3969836235046387\n","Epoch: 486/8000  Traning Loss: 110.72519016265869  Train_Reconstruction: 107.31365203857422  Train_KL: 3.411537677049637  Validation Loss : 107.77960205078125 Val_Reconstruction : 104.40392303466797 Val_KL : 3.375676155090332\n","Epoch: 487/8000  Traning Loss: 110.71602535247803  Train_Reconstruction: 107.32243824005127  Train_KL: 3.393586277961731  Validation Loss : 107.83608627319336 Val_Reconstruction : 104.47612380981445 Val_KL : 3.359961748123169\n","Epoch: 488/8000  Traning Loss: 110.47657775878906  Train_Reconstruction: 107.07251071929932  Train_KL: 3.4040664434432983  Validation Loss : 107.20925521850586 Val_Reconstruction : 103.83230590820312 Val_KL : 3.3769524097442627\n","Epoch: 489/8000  Traning Loss: 110.29088020324707  Train_Reconstruction: 106.88903427124023  Train_KL: 3.4018466770648956  Validation Loss : 107.45396041870117 Val_Reconstruction : 104.09695816040039 Val_KL : 3.3570048809051514\n","Epoch: 490/8000  Traning Loss: 110.44689178466797  Train_Reconstruction: 107.03716850280762  Train_KL: 3.409723252058029  Validation Loss : 107.97143173217773 Val_Reconstruction : 104.5783576965332 Val_KL : 3.393073320388794\n","Epoch: 491/8000  Traning Loss: 110.69763851165771  Train_Reconstruction: 107.27854347229004  Train_KL: 3.4190942645072937  Validation Loss : 107.72455596923828 Val_Reconstruction : 104.3506965637207 Val_KL : 3.373860001564026\n","Epoch: 492/8000  Traning Loss: 110.80474090576172  Train_Reconstruction: 107.39507579803467  Train_KL: 3.409665197134018  Validation Loss : 107.70130920410156 Val_Reconstruction : 104.31130981445312 Val_KL : 3.390002131462097\n","Epoch: 493/8000  Traning Loss: 110.561598777771  Train_Reconstruction: 107.15286350250244  Train_KL: 3.4087360203266144  Validation Loss : 107.67177963256836 Val_Reconstruction : 104.2823715209961 Val_KL : 3.3894059658050537\n","Epoch: 494/8000  Traning Loss: 110.5620002746582  Train_Reconstruction: 107.15923023223877  Train_KL: 3.402769684791565  Validation Loss : 107.60789108276367 Val_Reconstruction : 104.24283218383789 Val_KL : 3.365058660507202\n","Epoch: 495/8000  Traning Loss: 110.54032230377197  Train_Reconstruction: 107.12459754943848  Train_KL: 3.4157238006591797  Validation Loss : 107.41280746459961 Val_Reconstruction : 104.03693389892578 Val_KL : 3.375875949859619\n","Epoch: 496/8000  Traning Loss: 110.49504566192627  Train_Reconstruction: 107.09731960296631  Train_KL: 3.3977264165878296  Validation Loss : 107.92076110839844 Val_Reconstruction : 104.56084823608398 Val_KL : 3.3599122762680054\n","Epoch: 497/8000  Traning Loss: 110.84204959869385  Train_Reconstruction: 107.44892883300781  Train_KL: 3.393121600151062  Validation Loss : 107.98809432983398 Val_Reconstruction : 104.62220001220703 Val_KL : 3.365891218185425\n","Epoch: 498/8000  Traning Loss: 111.14406204223633  Train_Reconstruction: 107.74024391174316  Train_KL: 3.4038175344467163  Validation Loss : 108.69549942016602 Val_Reconstruction : 105.3194580078125 Val_KL : 3.3760414123535156\n","Epoch: 499/8000  Traning Loss: 110.90794563293457  Train_Reconstruction: 107.4950942993164  Train_KL: 3.4128516018390656  Validation Loss : 107.40687561035156 Val_Reconstruction : 104.02671813964844 Val_KL : 3.380158543586731\n","Epoch: 500/8000  Traning Loss: 110.17790222167969  Train_Reconstruction: 106.77509021759033  Train_KL: 3.4028110802173615  Validation Loss : 107.21536636352539 Val_Reconstruction : 103.8539810180664 Val_KL : 3.3613864183425903\n","Epoch: 501/8000  Traning Loss: 110.13961124420166  Train_Reconstruction: 106.73943901062012  Train_KL: 3.4001727402210236  Validation Loss : 107.10727310180664 Val_Reconstruction : 103.74343490600586 Val_KL : 3.3638378381729126\n","Epoch: 502/8000  Traning Loss: 109.95036125183105  Train_Reconstruction: 106.54829788208008  Train_KL: 3.4020625948905945  Validation Loss : 106.87607955932617 Val_Reconstruction : 103.49893569946289 Val_KL : 3.377141833305359\n","Epoch: 503/8000  Traning Loss: 109.92395305633545  Train_Reconstruction: 106.52174186706543  Train_KL: 3.402210146188736  Validation Loss : 107.1849594116211 Val_Reconstruction : 103.8287467956543 Val_KL : 3.356213331222534\n","Epoch: 504/8000  Traning Loss: 110.05283737182617  Train_Reconstruction: 106.65231800079346  Train_KL: 3.400519758462906  Validation Loss : 107.51388168334961 Val_Reconstruction : 104.15617370605469 Val_KL : 3.3577083349227905\n","Epoch: 505/8000  Traning Loss: 110.09857845306396  Train_Reconstruction: 106.69853782653809  Train_KL: 3.400040477514267  Validation Loss : 107.55505752563477 Val_Reconstruction : 104.16994094848633 Val_KL : 3.3851155042648315\n","Epoch: 506/8000  Traning Loss: 110.18677043914795  Train_Reconstruction: 106.77068138122559  Train_KL: 3.416088730096817  Validation Loss : 107.17519760131836 Val_Reconstruction : 103.78669738769531 Val_KL : 3.388500690460205\n","Epoch: 507/8000  Traning Loss: 110.1022481918335  Train_Reconstruction: 106.67940330505371  Train_KL: 3.4228445291519165  Validation Loss : 107.2349739074707 Val_Reconstruction : 103.85606002807617 Val_KL : 3.3789154291152954\n","Epoch: 508/8000  Traning Loss: 109.98098468780518  Train_Reconstruction: 106.57349109649658  Train_KL: 3.407493472099304  Validation Loss : 107.16691207885742 Val_Reconstruction : 103.77757263183594 Val_KL : 3.3893399238586426\n","Epoch: 509/8000  Traning Loss: 110.58766841888428  Train_Reconstruction: 107.18064212799072  Train_KL: 3.4070248901844025  Validation Loss : 107.80077362060547 Val_Reconstruction : 104.43021392822266 Val_KL : 3.3705590963363647\n","Epoch: 510/8000  Traning Loss: 110.847336769104  Train_Reconstruction: 107.4384765625  Train_KL: 3.408860892057419  Validation Loss : 108.03635025024414 Val_Reconstruction : 104.6571159362793 Val_KL : 3.379234790802002\n","Epoch: 511/8000  Traning Loss: 110.875075340271  Train_Reconstruction: 107.47052097320557  Train_KL: 3.404555857181549  Validation Loss : 107.67131042480469 Val_Reconstruction : 104.30054092407227 Val_KL : 3.37076997756958\n","Epoch: 512/8000  Traning Loss: 110.74213600158691  Train_Reconstruction: 107.33419132232666  Train_KL: 3.4079448878765106  Validation Loss : 107.98433303833008 Val_Reconstruction : 104.61462783813477 Val_KL : 3.369704842567444\n","Epoch: 513/8000  Traning Loss: 110.5892972946167  Train_Reconstruction: 107.18978023529053  Train_KL: 3.3995182812213898  Validation Loss : 107.40630340576172 Val_Reconstruction : 104.02981567382812 Val_KL : 3.376489996910095\n","Epoch: 514/8000  Traning Loss: 110.25364398956299  Train_Reconstruction: 106.83781337738037  Train_KL: 3.415830612182617  Validation Loss : 107.19269561767578 Val_Reconstruction : 103.79788208007812 Val_KL : 3.394814372062683\n","Epoch: 515/8000  Traning Loss: 110.23232936859131  Train_Reconstruction: 106.81796169281006  Train_KL: 3.414367139339447  Validation Loss : 107.02214813232422 Val_Reconstruction : 103.65354919433594 Val_KL : 3.3685991764068604\n","Epoch: 516/8000  Traning Loss: 110.11501502990723  Train_Reconstruction: 106.70250034332275  Train_KL: 3.4125143587589264  Validation Loss : 107.08859634399414 Val_Reconstruction : 103.7190170288086 Val_KL : 3.369578242301941\n","Epoch: 517/8000  Traning Loss: 110.19170188903809  Train_Reconstruction: 106.78641605377197  Train_KL: 3.4052859246730804  Validation Loss : 107.08121109008789 Val_Reconstruction : 103.69288635253906 Val_KL : 3.3883262872695923\n","Epoch: 518/8000  Traning Loss: 109.93423461914062  Train_Reconstruction: 106.52663898468018  Train_KL: 3.4075970351696014  Validation Loss : 106.92981719970703 Val_Reconstruction : 103.56987762451172 Val_KL : 3.3599408864974976\n","Epoch: 519/8000  Traning Loss: 109.80269622802734  Train_Reconstruction: 106.41239547729492  Train_KL: 3.3903005719184875  Validation Loss : 107.14041137695312 Val_Reconstruction : 103.76720428466797 Val_KL : 3.373205065727234\n","Epoch: 520/8000  Traning Loss: 110.0641736984253  Train_Reconstruction: 106.65748119354248  Train_KL: 3.406692534685135  Validation Loss : 107.15426635742188 Val_Reconstruction : 103.77956008911133 Val_KL : 3.374706268310547\n","Epoch: 521/8000  Traning Loss: 109.9595193862915  Train_Reconstruction: 106.56146907806396  Train_KL: 3.3980489373207092  Validation Loss : 106.93593215942383 Val_Reconstruction : 103.5726089477539 Val_KL : 3.36332106590271\n","Epoch: 522/8000  Traning Loss: 109.82973194122314  Train_Reconstruction: 106.42880725860596  Train_KL: 3.400924652814865  Validation Loss : 106.9969253540039 Val_Reconstruction : 103.6124382019043 Val_KL : 3.3844860792160034\n","Epoch: 523/8000  Traning Loss: 109.57708072662354  Train_Reconstruction: 106.17109870910645  Train_KL: 3.4059809148311615  Validation Loss : 106.66001510620117 Val_Reconstruction : 103.30539321899414 Val_KL : 3.3546223640441895\n","Epoch: 524/8000  Traning Loss: 109.66060543060303  Train_Reconstruction: 106.26111602783203  Train_KL: 3.39948970079422  Validation Loss : 106.80056762695312 Val_Reconstruction : 103.41445541381836 Val_KL : 3.386111259460449\n","Epoch: 525/8000  Traning Loss: 109.7943925857544  Train_Reconstruction: 106.3847484588623  Train_KL: 3.409645587205887  Validation Loss : 106.94118118286133 Val_Reconstruction : 103.58440780639648 Val_KL : 3.356770873069763\n","Epoch: 526/8000  Traning Loss: 109.85426044464111  Train_Reconstruction: 106.45344066619873  Train_KL: 3.4008205235004425  Validation Loss : 107.03759002685547 Val_Reconstruction : 103.6507797241211 Val_KL : 3.3868073225021362\n","Epoch: 527/8000  Traning Loss: 109.82621574401855  Train_Reconstruction: 106.40978050231934  Train_KL: 3.416436195373535  Validation Loss : 106.99755096435547 Val_Reconstruction : 103.62828063964844 Val_KL : 3.369273543357849\n","Epoch: 528/8000  Traning Loss: 109.66617107391357  Train_Reconstruction: 106.26378059387207  Train_KL: 3.402390033006668  Validation Loss : 106.71887969970703 Val_Reconstruction : 103.32800674438477 Val_KL : 3.3908724784851074\n","Epoch: 529/8000  Traning Loss: 109.56466484069824  Train_Reconstruction: 106.14877128601074  Train_KL: 3.4158935546875  Validation Loss : 106.7491455078125 Val_Reconstruction : 103.38141632080078 Val_KL : 3.36772882938385\n","Epoch: 530/8000  Traning Loss: 109.44017028808594  Train_Reconstruction: 106.03240776062012  Train_KL: 3.4077629148960114  Validation Loss : 106.49561309814453 Val_Reconstruction : 103.1229019165039 Val_KL : 3.37271249294281\n","Epoch: 531/8000  Traning Loss: 109.40990161895752  Train_Reconstruction: 106.00397491455078  Train_KL: 3.40592497587204  Validation Loss : 106.65924072265625 Val_Reconstruction : 103.27362823486328 Val_KL : 3.3856148719787598\n","Epoch: 532/8000  Traning Loss: 109.41074657440186  Train_Reconstruction: 105.99340724945068  Train_KL: 3.41733917593956  Validation Loss : 106.46500778198242 Val_Reconstruction : 103.0903434753418 Val_KL : 3.374664545059204\n","Epoch: 533/8000  Traning Loss: 109.33343601226807  Train_Reconstruction: 105.91299438476562  Train_KL: 3.4204421937465668  Validation Loss : 106.46663284301758 Val_Reconstruction : 103.0673942565918 Val_KL : 3.399235963821411\n","Epoch: 534/8000  Traning Loss: 109.44298553466797  Train_Reconstruction: 106.03259468078613  Train_KL: 3.4103905856609344  Validation Loss : 106.55716323852539 Val_Reconstruction : 103.18598937988281 Val_KL : 3.3711735010147095\n","Epoch: 535/8000  Traning Loss: 109.39605140686035  Train_Reconstruction: 105.99065780639648  Train_KL: 3.405392199754715  Validation Loss : 106.75808715820312 Val_Reconstruction : 103.37472915649414 Val_KL : 3.3833558559417725\n","Epoch: 536/8000  Traning Loss: 109.58533954620361  Train_Reconstruction: 106.185866355896  Train_KL: 3.3994732797145844  Validation Loss : 106.64520263671875 Val_Reconstruction : 103.27337265014648 Val_KL : 3.3718291521072388\n","Epoch: 537/8000  Traning Loss: 109.38645076751709  Train_Reconstruction: 105.98817729949951  Train_KL: 3.3982737064361572  Validation Loss : 106.40532684326172 Val_Reconstruction : 103.06372833251953 Val_KL : 3.3415966033935547\n","Epoch: 538/8000  Traning Loss: 109.19947242736816  Train_Reconstruction: 105.80685520172119  Train_KL: 3.392615884542465  Validation Loss : 106.29958724975586 Val_Reconstruction : 102.93267440795898 Val_KL : 3.3669129610061646\n","Epoch: 539/8000  Traning Loss: 109.21412467956543  Train_Reconstruction: 105.8144702911377  Train_KL: 3.3996540009975433  Validation Loss : 106.36725616455078 Val_Reconstruction : 102.98664855957031 Val_KL : 3.3806047439575195\n","Epoch: 540/8000  Traning Loss: 109.35420036315918  Train_Reconstruction: 105.9326114654541  Train_KL: 3.4215887784957886  Validation Loss : 106.41565322875977 Val_Reconstruction : 103.03105926513672 Val_KL : 3.3845925331115723\n","Epoch: 541/8000  Traning Loss: 109.43158531188965  Train_Reconstruction: 106.01936626434326  Train_KL: 3.4122200310230255  Validation Loss : 106.5987777709961 Val_Reconstruction : 103.22105026245117 Val_KL : 3.3777257204055786\n","Epoch: 542/8000  Traning Loss: 109.9250898361206  Train_Reconstruction: 106.49528789520264  Train_KL: 3.429801642894745  Validation Loss : 107.0594367980957 Val_Reconstruction : 103.66634750366211 Val_KL : 3.3930904865264893\n","Epoch: 543/8000  Traning Loss: 109.81904411315918  Train_Reconstruction: 106.3978271484375  Train_KL: 3.4212154150009155  Validation Loss : 106.81725311279297 Val_Reconstruction : 103.42565536499023 Val_KL : 3.3915969133377075\n","Epoch: 544/8000  Traning Loss: 109.49279308319092  Train_Reconstruction: 106.07174301147461  Train_KL: 3.421049654483795  Validation Loss : 106.4606704711914 Val_Reconstruction : 103.07790756225586 Val_KL : 3.3827600479125977\n","Epoch: 545/8000  Traning Loss: 109.41955375671387  Train_Reconstruction: 105.9996509552002  Train_KL: 3.4199022352695465  Validation Loss : 106.54227447509766 Val_Reconstruction : 103.13813018798828 Val_KL : 3.4041441679000854\n","Epoch: 546/8000  Traning Loss: 109.0515546798706  Train_Reconstruction: 105.63714790344238  Train_KL: 3.4144065380096436  Validation Loss : 105.97001647949219 Val_Reconstruction : 102.58447647094727 Val_KL : 3.3855412006378174\n","Epoch: 547/8000  Traning Loss: 108.96130084991455  Train_Reconstruction: 105.53986644744873  Train_KL: 3.4214354157447815  Validation Loss : 106.25488662719727 Val_Reconstruction : 102.88351440429688 Val_KL : 3.371372699737549\n","Epoch: 548/8000  Traning Loss: 108.9295597076416  Train_Reconstruction: 105.52287769317627  Train_KL: 3.406682461500168  Validation Loss : 106.12573623657227 Val_Reconstruction : 102.75102233886719 Val_KL : 3.3747135400772095\n","Epoch: 549/8000  Traning Loss: 108.98598194122314  Train_Reconstruction: 105.57397365570068  Train_KL: 3.4120084047317505  Validation Loss : 106.34614562988281 Val_Reconstruction : 102.95576858520508 Val_KL : 3.3903756141662598\n","Epoch: 550/8000  Traning Loss: 109.1461591720581  Train_Reconstruction: 105.73863410949707  Train_KL: 3.4075238406658173  Validation Loss : 106.36281967163086 Val_Reconstruction : 102.99775314331055 Val_KL : 3.3650671243667603\n","Epoch: 551/8000  Traning Loss: 109.18722629547119  Train_Reconstruction: 105.7709550857544  Train_KL: 3.4162705838680267  Validation Loss : 106.42560577392578 Val_Reconstruction : 103.05692291259766 Val_KL : 3.368680953979492\n","Epoch: 552/8000  Traning Loss: 109.16555595397949  Train_Reconstruction: 105.77173328399658  Train_KL: 3.3938237130641937  Validation Loss : 106.23773956298828 Val_Reconstruction : 102.84922790527344 Val_KL : 3.3885085582733154\n","Epoch: 553/8000  Traning Loss: 108.91067790985107  Train_Reconstruction: 105.49333953857422  Train_KL: 3.417338341474533  Validation Loss : 106.18924331665039 Val_Reconstruction : 102.81439208984375 Val_KL : 3.3748507499694824\n","Epoch: 554/8000  Traning Loss: 109.18452644348145  Train_Reconstruction: 105.76442432403564  Train_KL: 3.4201017320156097  Validation Loss : 107.17523193359375 Val_Reconstruction : 103.7679443359375 Val_KL : 3.4072870016098022\n","Epoch: 555/8000  Traning Loss: 109.47691917419434  Train_Reconstruction: 106.05502796173096  Train_KL: 3.4218908846378326  Validation Loss : 106.50647354125977 Val_Reconstruction : 103.13509750366211 Val_KL : 3.371374726295471\n","Epoch: 556/8000  Traning Loss: 109.30967140197754  Train_Reconstruction: 105.8962049484253  Train_KL: 3.413468211889267  Validation Loss : 106.24815368652344 Val_Reconstruction : 102.86802673339844 Val_KL : 3.3801281452178955\n","Epoch: 557/8000  Traning Loss: 108.88994979858398  Train_Reconstruction: 105.47695541381836  Train_KL: 3.4129941165447235  Validation Loss : 105.7026252746582 Val_Reconstruction : 102.33354568481445 Val_KL : 3.369078755378723\n","Epoch: 558/8000  Traning Loss: 108.7682294845581  Train_Reconstruction: 105.3660364151001  Train_KL: 3.4021935164928436  Validation Loss : 105.94608306884766 Val_Reconstruction : 102.57303619384766 Val_KL : 3.373049259185791\n","Epoch: 559/8000  Traning Loss: 108.6632080078125  Train_Reconstruction: 105.24963760375977  Train_KL: 3.4135721027851105  Validation Loss : 105.8242301940918 Val_Reconstruction : 102.4488410949707 Val_KL : 3.3753894567489624\n","Epoch: 560/8000  Traning Loss: 108.54022121429443  Train_Reconstruction: 105.13979530334473  Train_KL: 3.4004261791706085  Validation Loss : 105.70992279052734 Val_Reconstruction : 102.3203125 Val_KL : 3.3896087408065796\n","Epoch: 561/8000  Traning Loss: 108.50591087341309  Train_Reconstruction: 105.0931339263916  Train_KL: 3.4127767980098724  Validation Loss : 105.88993072509766 Val_Reconstruction : 102.53572463989258 Val_KL : 3.354204773902893\n","Epoch: 562/8000  Traning Loss: 108.81847381591797  Train_Reconstruction: 105.41221141815186  Train_KL: 3.4062634110450745  Validation Loss : 106.43391799926758 Val_Reconstruction : 103.03966903686523 Val_KL : 3.3942453861236572\n","Epoch: 563/8000  Traning Loss: 109.03411102294922  Train_Reconstruction: 105.61865711212158  Train_KL: 3.415454864501953  Validation Loss : 106.1673698425293 Val_Reconstruction : 102.79054641723633 Val_KL : 3.3768229484558105\n","Epoch: 564/8000  Traning Loss: 109.1787338256836  Train_Reconstruction: 105.76246356964111  Train_KL: 3.4162705838680267  Validation Loss : 106.85667037963867 Val_Reconstruction : 103.45523834228516 Val_KL : 3.4014320373535156\n","Epoch: 565/8000  Traning Loss: 109.03752899169922  Train_Reconstruction: 105.61871719360352  Train_KL: 3.4188108444213867  Validation Loss : 106.25302505493164 Val_Reconstruction : 102.87691497802734 Val_KL : 3.376106858253479\n","Epoch: 566/8000  Traning Loss: 109.1277027130127  Train_Reconstruction: 105.70237827301025  Train_KL: 3.4253238141536713  Validation Loss : 106.45831298828125 Val_Reconstruction : 103.06727981567383 Val_KL : 3.391032099723816\n","Epoch: 567/8000  Traning Loss: 108.94064998626709  Train_Reconstruction: 105.52794933319092  Train_KL: 3.4127008616924286  Validation Loss : 106.38019943237305 Val_Reconstruction : 103.005615234375 Val_KL : 3.3745821714401245\n","Epoch: 568/8000  Traning Loss: 109.17568492889404  Train_Reconstruction: 105.76513481140137  Train_KL: 3.41054829955101  Validation Loss : 105.96833038330078 Val_Reconstruction : 102.57735061645508 Val_KL : 3.390977382659912\n","Epoch: 569/8000  Traning Loss: 108.9485387802124  Train_Reconstruction: 105.53962421417236  Train_KL: 3.4089148342609406  Validation Loss : 105.98834609985352 Val_Reconstruction : 102.61506271362305 Val_KL : 3.3732811212539673\n","Epoch: 570/8000  Traning Loss: 108.60772514343262  Train_Reconstruction: 105.19414329528809  Train_KL: 3.413582742214203  Validation Loss : 105.88043975830078 Val_Reconstruction : 102.4811019897461 Val_KL : 3.3993369340896606\n","Epoch: 571/8000  Traning Loss: 108.57048416137695  Train_Reconstruction: 105.15181255340576  Train_KL: 3.418672651052475  Validation Loss : 105.76155090332031 Val_Reconstruction : 102.39466094970703 Val_KL : 3.3668915033340454\n","Epoch: 572/8000  Traning Loss: 108.70286083221436  Train_Reconstruction: 105.28962230682373  Train_KL: 3.4132380187511444  Validation Loss : 105.85209655761719 Val_Reconstruction : 102.45152282714844 Val_KL : 3.4005762338638306\n","Epoch: 573/8000  Traning Loss: 108.4565896987915  Train_Reconstruction: 105.01781749725342  Train_KL: 3.4387719929218292  Validation Loss : 105.70293807983398 Val_Reconstruction : 102.31563949584961 Val_KL : 3.387299418449402\n","Epoch: 574/8000  Traning Loss: 108.47805595397949  Train_Reconstruction: 105.07213497161865  Train_KL: 3.4059211015701294  Validation Loss : 105.9078483581543 Val_Reconstruction : 102.52645492553711 Val_KL : 3.381393790245056\n","Epoch: 575/8000  Traning Loss: 108.52333545684814  Train_Reconstruction: 105.0980749130249  Train_KL: 3.4252611696720123  Validation Loss : 105.92814636230469 Val_Reconstruction : 102.52963638305664 Val_KL : 3.3985077142715454\n","Epoch: 576/8000  Traning Loss: 108.64438343048096  Train_Reconstruction: 105.21483135223389  Train_KL: 3.4295521080493927  Validation Loss : 105.63792037963867 Val_Reconstruction : 102.24159622192383 Val_KL : 3.3963239192962646\n","Epoch: 577/8000  Traning Loss: 108.49612426757812  Train_Reconstruction: 105.07571983337402  Train_KL: 3.4204052686691284  Validation Loss : 105.84953308105469 Val_Reconstruction : 102.46749877929688 Val_KL : 3.382036566734314\n","Epoch: 578/8000  Traning Loss: 108.6168737411499  Train_Reconstruction: 105.21158409118652  Train_KL: 3.4052892327308655  Validation Loss : 105.94695663452148 Val_Reconstruction : 102.591064453125 Val_KL : 3.3558921813964844\n","Epoch: 579/8000  Traning Loss: 108.63367462158203  Train_Reconstruction: 105.23401355743408  Train_KL: 3.3996609449386597  Validation Loss : 105.54368209838867 Val_Reconstruction : 102.17277526855469 Val_KL : 3.3709089756011963\n","Epoch: 580/8000  Traning Loss: 108.50118255615234  Train_Reconstruction: 105.08396244049072  Train_KL: 3.4172201454639435  Validation Loss : 105.48534393310547 Val_Reconstruction : 102.1115493774414 Val_KL : 3.373795509338379\n","Epoch: 581/8000  Traning Loss: 108.23611164093018  Train_Reconstruction: 104.82316780090332  Train_KL: 3.4129441380500793  Validation Loss : 105.34831237792969 Val_Reconstruction : 101.96712493896484 Val_KL : 3.3811898231506348\n","Epoch: 582/8000  Traning Loss: 108.12790012359619  Train_Reconstruction: 104.71013450622559  Train_KL: 3.4177669882774353  Validation Loss : 105.3333511352539 Val_Reconstruction : 101.94457244873047 Val_KL : 3.3887792825698853\n","Epoch: 583/8000  Traning Loss: 108.15428924560547  Train_Reconstruction: 104.74314022064209  Train_KL: 3.411149948835373  Validation Loss : 105.34517669677734 Val_Reconstruction : 101.96574401855469 Val_KL : 3.379433512687683\n","Epoch: 584/8000  Traning Loss: 108.4136323928833  Train_Reconstruction: 104.99816703796387  Train_KL: 3.41546568274498  Validation Loss : 105.55021286010742 Val_Reconstruction : 102.15380477905273 Val_KL : 3.3964074850082397\n","Epoch: 585/8000  Traning Loss: 108.06626605987549  Train_Reconstruction: 104.64803791046143  Train_KL: 3.418227940797806  Validation Loss : 105.24218368530273 Val_Reconstruction : 101.86739730834961 Val_KL : 3.3747862577438354\n","Epoch: 586/8000  Traning Loss: 107.91522979736328  Train_Reconstruction: 104.50663566589355  Train_KL: 3.4085948169231415  Validation Loss : 105.02718734741211 Val_Reconstruction : 101.62846374511719 Val_KL : 3.3987252712249756\n","Epoch: 587/8000  Traning Loss: 108.12170314788818  Train_Reconstruction: 104.68307495117188  Train_KL: 3.438628226518631  Validation Loss : 105.45598983764648 Val_Reconstruction : 102.07243347167969 Val_KL : 3.3835556507110596\n","Epoch: 588/8000  Traning Loss: 108.29590606689453  Train_Reconstruction: 104.87123775482178  Train_KL: 3.424668252468109  Validation Loss : 105.42368698120117 Val_Reconstruction : 102.01423645019531 Val_KL : 3.409451961517334\n","Epoch: 589/8000  Traning Loss: 108.49460697174072  Train_Reconstruction: 105.0657606124878  Train_KL: 3.4288450479507446  Validation Loss : 105.72926330566406 Val_Reconstruction : 102.33450698852539 Val_KL : 3.3947588205337524\n","Epoch: 590/8000  Traning Loss: 108.46301174163818  Train_Reconstruction: 105.03998184204102  Train_KL: 3.4230297207832336  Validation Loss : 105.83878707885742 Val_Reconstruction : 102.4513931274414 Val_KL : 3.387391686439514\n","Epoch: 591/8000  Traning Loss: 108.17704772949219  Train_Reconstruction: 104.76541900634766  Train_KL: 3.411629319190979  Validation Loss : 105.29122161865234 Val_Reconstruction : 101.89827346801758 Val_KL : 3.3929460048675537\n","Epoch: 592/8000  Traning Loss: 107.99727821350098  Train_Reconstruction: 104.56964206695557  Train_KL: 3.427635669708252  Validation Loss : 105.10830307006836 Val_Reconstruction : 101.7177734375 Val_KL : 3.390529155731201\n","Epoch: 593/8000  Traning Loss: 108.21253299713135  Train_Reconstruction: 104.81051445007324  Train_KL: 3.4020199179649353  Validation Loss : 105.37636947631836 Val_Reconstruction : 102.00509643554688 Val_KL : 3.371271252632141\n","Epoch: 594/8000  Traning Loss: 107.97899913787842  Train_Reconstruction: 104.55965614318848  Train_KL: 3.41934072971344  Validation Loss : 105.30260848999023 Val_Reconstruction : 101.91126251220703 Val_KL : 3.3913482427597046\n","Epoch: 595/8000  Traning Loss: 107.87217426300049  Train_Reconstruction: 104.44828796386719  Train_KL: 3.423886299133301  Validation Loss : 105.2912368774414 Val_Reconstruction : 101.90256118774414 Val_KL : 3.3886728286743164\n","Epoch: 596/8000  Traning Loss: 107.96001434326172  Train_Reconstruction: 104.53964233398438  Train_KL: 3.4203715324401855  Validation Loss : 105.05261993408203 Val_Reconstruction : 101.67337799072266 Val_KL : 3.379242777824402\n","Epoch: 597/8000  Traning Loss: 107.86835098266602  Train_Reconstruction: 104.4529161453247  Train_KL: 3.415435165166855  Validation Loss : 105.1464614868164 Val_Reconstruction : 101.76609420776367 Val_KL : 3.3803653717041016\n","Epoch: 598/8000  Traning Loss: 108.24909687042236  Train_Reconstruction: 104.83381652832031  Train_KL: 3.4152791798114777  Validation Loss : 105.59431457519531 Val_Reconstruction : 102.20125579833984 Val_KL : 3.393057942390442\n","Epoch: 599/8000  Traning Loss: 108.4052095413208  Train_Reconstruction: 104.97422885894775  Train_KL: 3.430980145931244  Validation Loss : 105.78067016601562 Val_Reconstruction : 102.38215255737305 Val_KL : 3.398518204689026\n","Epoch: 600/8000  Traning Loss: 108.56384372711182  Train_Reconstruction: 105.15007400512695  Train_KL: 3.4137702882289886  Validation Loss : 106.50039291381836 Val_Reconstruction : 103.1229362487793 Val_KL : 3.3774574995040894\n","Epoch: 601/8000  Traning Loss: 108.73452281951904  Train_Reconstruction: 105.30954170227051  Train_KL: 3.4249816238880157  Validation Loss : 106.43107604980469 Val_Reconstruction : 103.0188102722168 Val_KL : 3.4122642278671265\n","Epoch: 602/8000  Traning Loss: 108.84194374084473  Train_Reconstruction: 105.41990375518799  Train_KL: 3.422040283679962  Validation Loss : 106.34683227539062 Val_Reconstruction : 102.95497131347656 Val_KL : 3.391860842704773\n","Epoch: 603/8000  Traning Loss: 108.29752254486084  Train_Reconstruction: 104.86348056793213  Train_KL: 3.4340422451496124  Validation Loss : 105.2676887512207 Val_Reconstruction : 101.85108184814453 Val_KL : 3.4166043996810913\n","Epoch: 604/8000  Traning Loss: 107.9279317855835  Train_Reconstruction: 104.49014472961426  Train_KL: 3.4377869963645935  Validation Loss : 105.1983528137207 Val_Reconstruction : 101.80782318115234 Val_KL : 3.390528082847595\n","Epoch: 605/8000  Traning Loss: 107.74719715118408  Train_Reconstruction: 104.32860851287842  Train_KL: 3.4185881316661835  Validation Loss : 105.01755142211914 Val_Reconstruction : 101.62720489501953 Val_KL : 3.3903467655181885\n","Epoch: 606/8000  Traning Loss: 108.04694080352783  Train_Reconstruction: 104.62631320953369  Train_KL: 3.420627176761627  Validation Loss : 104.8860969543457 Val_Reconstruction : 101.50029754638672 Val_KL : 3.3857967853546143\n","Epoch: 607/8000  Traning Loss: 108.05626392364502  Train_Reconstruction: 104.6383171081543  Train_KL: 3.4179455041885376  Validation Loss : 105.04891586303711 Val_Reconstruction : 101.63422775268555 Val_KL : 3.4146881103515625\n","Epoch: 608/8000  Traning Loss: 108.19732475280762  Train_Reconstruction: 104.76508140563965  Train_KL: 3.432243764400482  Validation Loss : 105.20614624023438 Val_Reconstruction : 101.80495071411133 Val_KL : 3.401198625564575\n","Epoch: 609/8000  Traning Loss: 107.99274730682373  Train_Reconstruction: 104.56341552734375  Train_KL: 3.429331988096237  Validation Loss : 105.08790969848633 Val_Reconstruction : 101.70809555053711 Val_KL : 3.379814028739929\n","Epoch: 610/8000  Traning Loss: 107.70236206054688  Train_Reconstruction: 104.28531169891357  Train_KL: 3.4170511960983276  Validation Loss : 104.91231536865234 Val_Reconstruction : 101.50141525268555 Val_KL : 3.410900354385376\n","Epoch: 611/8000  Traning Loss: 107.66274929046631  Train_Reconstruction: 104.22526931762695  Train_KL: 3.4374797642230988  Validation Loss : 104.7885971069336 Val_Reconstruction : 101.38291931152344 Val_KL : 3.4056769609451294\n","Epoch: 612/8000  Traning Loss: 107.63252353668213  Train_Reconstruction: 104.20193195343018  Train_KL: 3.430591106414795  Validation Loss : 104.88801193237305 Val_Reconstruction : 101.50558853149414 Val_KL : 3.3824241161346436\n","Epoch: 613/8000  Traning Loss: 107.54698944091797  Train_Reconstruction: 104.13635158538818  Train_KL: 3.410638213157654  Validation Loss : 104.81655502319336 Val_Reconstruction : 101.43515014648438 Val_KL : 3.3814040422439575\n","Epoch: 614/8000  Traning Loss: 107.72452163696289  Train_Reconstruction: 104.31591033935547  Train_KL: 3.40861177444458  Validation Loss : 105.43301010131836 Val_Reconstruction : 102.0534782409668 Val_KL : 3.3795297145843506\n","Epoch: 615/8000  Traning Loss: 107.50407314300537  Train_Reconstruction: 104.07996273040771  Train_KL: 3.424111008644104  Validation Loss : 104.8016586303711 Val_Reconstruction : 101.39386749267578 Val_KL : 3.4077903032302856\n","Epoch: 616/8000  Traning Loss: 107.9628267288208  Train_Reconstruction: 104.54382801055908  Train_KL: 3.418999969959259  Validation Loss : 105.42726516723633 Val_Reconstruction : 102.05510711669922 Val_KL : 3.3721576929092407\n","Epoch: 617/8000  Traning Loss: 107.72843933105469  Train_Reconstruction: 104.31136989593506  Train_KL: 3.417068511247635  Validation Loss : 104.55762481689453 Val_Reconstruction : 101.15254592895508 Val_KL : 3.4050790071487427\n","Epoch: 618/8000  Traning Loss: 107.39563369750977  Train_Reconstruction: 103.95333671569824  Train_KL: 3.4422968924045563  Validation Loss : 104.59799575805664 Val_Reconstruction : 101.17418670654297 Val_KL : 3.4238089323043823\n","Epoch: 619/8000  Traning Loss: 107.40087032318115  Train_Reconstruction: 103.96936225891113  Train_KL: 3.4315085113048553  Validation Loss : 104.65175247192383 Val_Reconstruction : 101.26415252685547 Val_KL : 3.3876019716262817\n","Epoch: 620/8000  Traning Loss: 107.29146385192871  Train_Reconstruction: 103.8746452331543  Train_KL: 3.416818141937256  Validation Loss : 104.34195327758789 Val_Reconstruction : 100.94496536254883 Val_KL : 3.396988868713379\n","Epoch: 621/8000  Traning Loss: 107.60902309417725  Train_Reconstruction: 104.18310737609863  Train_KL: 3.425916016101837  Validation Loss : 105.04756164550781 Val_Reconstruction : 101.64765930175781 Val_KL : 3.399901032447815\n","Epoch: 622/8000  Traning Loss: 107.73467063903809  Train_Reconstruction: 104.30682373046875  Train_KL: 3.4278480410575867  Validation Loss : 104.9786376953125 Val_Reconstruction : 101.58154678344727 Val_KL : 3.3970913887023926\n","Epoch: 623/8000  Traning Loss: 107.56067657470703  Train_Reconstruction: 104.12245178222656  Train_KL: 3.438224583864212  Validation Loss : 104.66720581054688 Val_Reconstruction : 101.23885726928711 Val_KL : 3.42835009098053\n","Epoch: 624/8000  Traning Loss: 107.13799953460693  Train_Reconstruction: 103.7059211730957  Train_KL: 3.432078570127487  Validation Loss : 104.16991806030273 Val_Reconstruction : 100.78349685668945 Val_KL : 3.3864227533340454\n","Epoch: 625/8000  Traning Loss: 107.11360168457031  Train_Reconstruction: 103.68610858917236  Train_KL: 3.4274940192699432  Validation Loss : 104.57481002807617 Val_Reconstruction : 101.18599700927734 Val_KL : 3.3888134956359863\n","Epoch: 626/8000  Traning Loss: 107.44801044464111  Train_Reconstruction: 104.02698040008545  Train_KL: 3.421029359102249  Validation Loss : 104.47151947021484 Val_Reconstruction : 101.08440399169922 Val_KL : 3.3871142864227295\n","Epoch: 627/8000  Traning Loss: 107.26104354858398  Train_Reconstruction: 103.84286308288574  Train_KL: 3.4181808829307556  Validation Loss : 104.5315055847168 Val_Reconstruction : 101.12662124633789 Val_KL : 3.4048839807510376\n","Epoch: 628/8000  Traning Loss: 107.37439441680908  Train_Reconstruction: 103.93069744110107  Train_KL: 3.443697452545166  Validation Loss : 104.71892929077148 Val_Reconstruction : 101.31312561035156 Val_KL : 3.4058048725128174\n","Epoch: 629/8000  Traning Loss: 107.44134140014648  Train_Reconstruction: 104.01377010345459  Train_KL: 3.4275713860988617  Validation Loss : 105.14025497436523 Val_Reconstruction : 101.74526977539062 Val_KL : 3.394982933998108\n","Epoch: 630/8000  Traning Loss: 107.32889366149902  Train_Reconstruction: 103.90244197845459  Train_KL: 3.4264521300792694  Validation Loss : 104.47955703735352 Val_Reconstruction : 101.10079193115234 Val_KL : 3.378765344619751\n","Epoch: 631/8000  Traning Loss: 107.13848209381104  Train_Reconstruction: 103.71135997772217  Train_KL: 3.4271220564842224  Validation Loss : 104.38558959960938 Val_Reconstruction : 100.96138381958008 Val_KL : 3.4242074489593506\n","Epoch: 632/8000  Traning Loss: 106.96976089477539  Train_Reconstruction: 103.53415489196777  Train_KL: 3.43560591340065  Validation Loss : 104.22868347167969 Val_Reconstruction : 100.82666015625 Val_KL : 3.402022123336792\n","Epoch: 633/8000  Traning Loss: 107.05889415740967  Train_Reconstruction: 103.63408660888672  Train_KL: 3.4248081743717194  Validation Loss : 104.39563369750977 Val_Reconstruction : 100.98702239990234 Val_KL : 3.4086132049560547\n","Epoch: 634/8000  Traning Loss: 106.95423793792725  Train_Reconstruction: 103.52459144592285  Train_KL: 3.4296472370624542  Validation Loss : 104.16683578491211 Val_Reconstruction : 100.7895278930664 Val_KL : 3.3773083686828613\n","Epoch: 635/8000  Traning Loss: 107.24174118041992  Train_Reconstruction: 103.81933784484863  Train_KL: 3.4224029183387756  Validation Loss : 104.7791519165039 Val_Reconstruction : 101.36252975463867 Val_KL : 3.416621685028076\n","Epoch: 636/8000  Traning Loss: 107.30310821533203  Train_Reconstruction: 103.85886859893799  Train_KL: 3.444238394498825  Validation Loss : 104.44503021240234 Val_Reconstruction : 101.05455017089844 Val_KL : 3.390479326248169\n","Epoch: 637/8000  Traning Loss: 107.37156295776367  Train_Reconstruction: 103.92835235595703  Train_KL: 3.443210005760193  Validation Loss : 104.81695938110352 Val_Reconstruction : 101.3926010131836 Val_KL : 3.4243581295013428\n","Epoch: 638/8000  Traning Loss: 107.34568214416504  Train_Reconstruction: 103.91427516937256  Train_KL: 3.4314072728157043  Validation Loss : 104.55783081054688 Val_Reconstruction : 101.17681503295898 Val_KL : 3.3810133934020996\n","Epoch: 639/8000  Traning Loss: 107.50317192077637  Train_Reconstruction: 104.08210563659668  Train_KL: 3.421066492795944  Validation Loss : 104.99042892456055 Val_Reconstruction : 101.58857727050781 Val_KL : 3.4018523693084717\n","Epoch: 640/8000  Traning Loss: 107.32432651519775  Train_Reconstruction: 103.90778923034668  Train_KL: 3.416536569595337  Validation Loss : 104.55570602416992 Val_Reconstruction : 101.17955017089844 Val_KL : 3.376153826713562\n","Epoch: 641/8000  Traning Loss: 107.44764423370361  Train_Reconstruction: 104.03149604797363  Train_KL: 3.416147232055664  Validation Loss : 104.6816177368164 Val_Reconstruction : 101.2976188659668 Val_KL : 3.383998394012451\n","Epoch: 642/8000  Traning Loss: 107.35245895385742  Train_Reconstruction: 103.93700122833252  Train_KL: 3.415458232164383  Validation Loss : 104.39789199829102 Val_Reconstruction : 101.01162719726562 Val_KL : 3.3862626552581787\n","Epoch: 643/8000  Traning Loss: 107.0732946395874  Train_Reconstruction: 103.649827003479  Train_KL: 3.4234680235385895  Validation Loss : 104.20572662353516 Val_Reconstruction : 100.79878616333008 Val_KL : 3.406938672065735\n","Epoch: 644/8000  Traning Loss: 107.00775241851807  Train_Reconstruction: 103.57732391357422  Train_KL: 3.4304290115833282  Validation Loss : 104.0590591430664 Val_Reconstruction : 100.6510238647461 Val_KL : 3.408037543296814\n","Epoch: 645/8000  Traning Loss: 107.0466890335083  Train_Reconstruction: 103.59564113616943  Train_KL: 3.451048880815506  Validation Loss : 104.11371231079102 Val_Reconstruction : 100.6991195678711 Val_KL : 3.4145931005477905\n","Epoch: 646/8000  Traning Loss: 107.03328895568848  Train_Reconstruction: 103.59508323669434  Train_KL: 3.4382049441337585  Validation Loss : 104.3021125793457 Val_Reconstruction : 100.8874397277832 Val_KL : 3.414674758911133\n","Epoch: 647/8000  Traning Loss: 107.44892883300781  Train_Reconstruction: 104.00802898406982  Train_KL: 3.440898150205612  Validation Loss : 104.66427230834961 Val_Reconstruction : 101.25995254516602 Val_KL : 3.4043205976486206\n","Epoch: 648/8000  Traning Loss: 107.40918445587158  Train_Reconstruction: 103.97236061096191  Train_KL: 3.4368245601654053  Validation Loss : 104.27172088623047 Val_Reconstruction : 100.86204147338867 Val_KL : 3.409680724143982\n","Epoch: 649/8000  Traning Loss: 107.2737922668457  Train_Reconstruction: 103.8509874343872  Train_KL: 3.4228033423423767  Validation Loss : 104.60974502563477 Val_Reconstruction : 101.21549987792969 Val_KL : 3.3942466974258423\n","Epoch: 650/8000  Traning Loss: 107.11033630371094  Train_Reconstruction: 103.67900371551514  Train_KL: 3.4313334226608276  Validation Loss : 104.49019622802734 Val_Reconstruction : 101.08000183105469 Val_KL : 3.4101957082748413\n","Epoch: 651/8000  Traning Loss: 107.06595993041992  Train_Reconstruction: 103.6387996673584  Train_KL: 3.427160143852234  Validation Loss : 104.65384292602539 Val_Reconstruction : 101.25976181030273 Val_KL : 3.3940829038619995\n","Epoch: 652/8000  Traning Loss: 107.06081581115723  Train_Reconstruction: 103.63074111938477  Train_KL: 3.4300768971443176  Validation Loss : 104.69882583618164 Val_Reconstruction : 101.2957992553711 Val_KL : 3.4030277729034424\n","Epoch: 653/8000  Traning Loss: 107.06195545196533  Train_Reconstruction: 103.62833976745605  Train_KL: 3.433615952730179  Validation Loss : 104.42039489746094 Val_Reconstruction : 101.01146697998047 Val_KL : 3.408928394317627\n","Epoch: 654/8000  Traning Loss: 106.87600326538086  Train_Reconstruction: 103.4574089050293  Train_KL: 3.4185941219329834  Validation Loss : 104.03056335449219 Val_Reconstruction : 100.64616394042969 Val_KL : 3.3843984603881836\n","Epoch: 655/8000  Traning Loss: 106.83045101165771  Train_Reconstruction: 103.38786697387695  Train_KL: 3.4425844848155975  Validation Loss : 104.0037841796875 Val_Reconstruction : 100.59245681762695 Val_KL : 3.4113242626190186\n","Epoch: 656/8000  Traning Loss: 107.06438636779785  Train_Reconstruction: 103.63083076477051  Train_KL: 3.4335553348064423  Validation Loss : 104.7066650390625 Val_Reconstruction : 101.29819869995117 Val_KL : 3.4084653854370117\n","Epoch: 657/8000  Traning Loss: 107.03862380981445  Train_Reconstruction: 103.62411499023438  Train_KL: 3.414507269859314  Validation Loss : 104.61893844604492 Val_Reconstruction : 101.23925399780273 Val_KL : 3.3796839714050293\n","Epoch: 658/8000  Traning Loss: 106.8603572845459  Train_Reconstruction: 103.43010520935059  Train_KL: 3.430251181125641  Validation Loss : 104.04196166992188 Val_Reconstruction : 100.62442016601562 Val_KL : 3.4175411462783813\n","Epoch: 659/8000  Traning Loss: 106.72735023498535  Train_Reconstruction: 103.29736232757568  Train_KL: 3.429987221956253  Validation Loss : 104.05314636230469 Val_Reconstruction : 100.66074752807617 Val_KL : 3.3923972845077515\n","Epoch: 660/8000  Traning Loss: 106.64247417449951  Train_Reconstruction: 103.22366905212402  Train_KL: 3.418804317712784  Validation Loss : 103.9833869934082 Val_Reconstruction : 100.5720100402832 Val_KL : 3.4113787412643433\n","Epoch: 661/8000  Traning Loss: 106.70030307769775  Train_Reconstruction: 103.2601490020752  Train_KL: 3.440153181552887  Validation Loss : 104.13259506225586 Val_Reconstruction : 100.72648620605469 Val_KL : 3.406108021736145\n","Epoch: 662/8000  Traning Loss: 106.9470796585083  Train_Reconstruction: 103.51156902313232  Train_KL: 3.4355097711086273  Validation Loss : 104.15979766845703 Val_Reconstruction : 100.74944686889648 Val_KL : 3.4103504419326782\n","Epoch: 663/8000  Traning Loss: 106.62232303619385  Train_Reconstruction: 103.1863374710083  Train_KL: 3.4359840750694275  Validation Loss : 103.78326034545898 Val_Reconstruction : 100.39473342895508 Val_KL : 3.38852596282959\n","Epoch: 664/8000  Traning Loss: 106.2086534500122  Train_Reconstruction: 102.77823448181152  Train_KL: 3.430418759584427  Validation Loss : 103.48152160644531 Val_Reconstruction : 100.09359741210938 Val_KL : 3.3879268169403076\n","Epoch: 665/8000  Traning Loss: 106.27501773834229  Train_Reconstruction: 102.85198020935059  Train_KL: 3.4230376482009888  Validation Loss : 103.59171295166016 Val_Reconstruction : 100.19846725463867 Val_KL : 3.393248677253723\n","Epoch: 666/8000  Traning Loss: 106.32863426208496  Train_Reconstruction: 102.90710830688477  Train_KL: 3.4215255081653595  Validation Loss : 103.76592636108398 Val_Reconstruction : 100.36320114135742 Val_KL : 3.4027243852615356\n","Epoch: 667/8000  Traning Loss: 106.53926467895508  Train_Reconstruction: 103.12400150299072  Train_KL: 3.415261596441269  Validation Loss : 104.07786178588867 Val_Reconstruction : 100.70515823364258 Val_KL : 3.372704863548279\n","Epoch: 668/8000  Traning Loss: 106.6294059753418  Train_Reconstruction: 103.21098041534424  Train_KL: 3.4184249341487885  Validation Loss : 104.04193115234375 Val_Reconstruction : 100.63881301879883 Val_KL : 3.4031189680099487\n","Epoch: 669/8000  Traning Loss: 106.98090076446533  Train_Reconstruction: 103.54112529754639  Train_KL: 3.439775913953781  Validation Loss : 104.36204147338867 Val_Reconstruction : 100.93893432617188 Val_KL : 3.4231053590774536\n","Epoch: 670/8000  Traning Loss: 107.40768432617188  Train_Reconstruction: 103.95751762390137  Train_KL: 3.4501665234565735  Validation Loss : 104.9992904663086 Val_Reconstruction : 101.59566497802734 Val_KL : 3.40362548828125\n","Epoch: 671/8000  Traning Loss: 107.19561386108398  Train_Reconstruction: 103.77098560333252  Train_KL: 3.4246287047863007  Validation Loss : 104.59784317016602 Val_Reconstruction : 101.20392608642578 Val_KL : 3.393918037414551\n","Epoch: 672/8000  Traning Loss: 106.84485149383545  Train_Reconstruction: 103.41419410705566  Train_KL: 3.4306567907333374  Validation Loss : 103.93997955322266 Val_Reconstruction : 100.5389175415039 Val_KL : 3.4010626077651978\n","Epoch: 673/8000  Traning Loss: 106.69120025634766  Train_Reconstruction: 103.25677299499512  Train_KL: 3.4344264566898346  Validation Loss : 103.8743667602539 Val_Reconstruction : 100.45294189453125 Val_KL : 3.42142391204834\n","Epoch: 674/8000  Traning Loss: 106.50886058807373  Train_Reconstruction: 103.04615497589111  Train_KL: 3.4627053439617157  Validation Loss : 103.71238708496094 Val_Reconstruction : 100.29658508300781 Val_KL : 3.4158021211624146\n","Epoch: 675/8000  Traning Loss: 106.5142068862915  Train_Reconstruction: 103.0738935470581  Train_KL: 3.4403136372566223  Validation Loss : 104.00335693359375 Val_Reconstruction : 100.58394241333008 Val_KL : 3.4194151163101196\n","Epoch: 676/8000  Traning Loss: 106.84015560150146  Train_Reconstruction: 103.39516735076904  Train_KL: 3.444988399744034  Validation Loss : 104.06685256958008 Val_Reconstruction : 100.65889358520508 Val_KL : 3.40795636177063\n","Epoch: 677/8000  Traning Loss: 106.49256610870361  Train_Reconstruction: 103.05304145812988  Train_KL: 3.439523607492447  Validation Loss : 103.92828750610352 Val_Reconstruction : 100.53092956542969 Val_KL : 3.3973584175109863\n","Epoch: 678/8000  Traning Loss: 106.4909086227417  Train_Reconstruction: 103.04776191711426  Train_KL: 3.443147510290146  Validation Loss : 103.85151290893555 Val_Reconstruction : 100.44237518310547 Val_KL : 3.4091367721557617\n","Epoch: 679/8000  Traning Loss: 106.81696796417236  Train_Reconstruction: 103.4038200378418  Train_KL: 3.413147896528244  Validation Loss : 104.06605529785156 Val_Reconstruction : 100.67864990234375 Val_KL : 3.3874047994613647\n","Epoch: 680/8000  Traning Loss: 107.03634262084961  Train_Reconstruction: 103.61157035827637  Train_KL: 3.4247721135616302  Validation Loss : 104.5470085144043 Val_Reconstruction : 101.15155410766602 Val_KL : 3.3954555988311768\n","Epoch: 681/8000  Traning Loss: 106.67092514038086  Train_Reconstruction: 103.24811172485352  Train_KL: 3.4228126406669617  Validation Loss : 103.77927780151367 Val_Reconstruction : 100.40536880493164 Val_KL : 3.3739101886749268\n","Epoch: 682/8000  Traning Loss: 106.31249523162842  Train_Reconstruction: 102.89410781860352  Train_KL: 3.418387621641159  Validation Loss : 103.55379104614258 Val_Reconstruction : 100.16004943847656 Val_KL : 3.393739938735962\n","Epoch: 683/8000  Traning Loss: 106.30004215240479  Train_Reconstruction: 102.87167549133301  Train_KL: 3.4283666610717773  Validation Loss : 103.84692001342773 Val_Reconstruction : 100.41957092285156 Val_KL : 3.427348256111145\n","Epoch: 684/8000  Traning Loss: 106.53788661956787  Train_Reconstruction: 103.09665870666504  Train_KL: 3.4412269294261932  Validation Loss : 104.07064437866211 Val_Reconstruction : 100.67252349853516 Val_KL : 3.3981211185455322\n","Epoch: 685/8000  Traning Loss: 106.18365097045898  Train_Reconstruction: 102.74976539611816  Train_KL: 3.433885157108307  Validation Loss : 103.54831314086914 Val_Reconstruction : 100.11806106567383 Val_KL : 3.43025004863739\n","Epoch: 686/8000  Traning Loss: 106.31780242919922  Train_Reconstruction: 102.87822151184082  Train_KL: 3.4395810961723328  Validation Loss : 103.80550765991211 Val_Reconstruction : 100.41873931884766 Val_KL : 3.3867692947387695\n","Epoch: 687/8000  Traning Loss: 106.21329116821289  Train_Reconstruction: 102.77575206756592  Train_KL: 3.4375384151935577  Validation Loss : 103.32773208618164 Val_Reconstruction : 99.9168586730957 Val_KL : 3.4108738899230957\n","Epoch: 688/8000  Traning Loss: 106.14013004302979  Train_Reconstruction: 102.70597839355469  Train_KL: 3.4341526329517365  Validation Loss : 103.63840866088867 Val_Reconstruction : 100.22256469726562 Val_KL : 3.415843963623047\n","Epoch: 689/8000  Traning Loss: 106.23228931427002  Train_Reconstruction: 102.79306125640869  Train_KL: 3.439228653907776  Validation Loss : 103.4067153930664 Val_Reconstruction : 100.0124282836914 Val_KL : 3.394289493560791\n","Epoch: 690/8000  Traning Loss: 106.44822978973389  Train_Reconstruction: 103.01165866851807  Train_KL: 3.436571478843689  Validation Loss : 104.3411750793457 Val_Reconstruction : 100.9120979309082 Val_KL : 3.429075837135315\n","Epoch: 691/8000  Traning Loss: 106.3811902999878  Train_Reconstruction: 102.93113899230957  Train_KL: 3.450051575899124  Validation Loss : 103.6152114868164 Val_Reconstruction : 100.21856307983398 Val_KL : 3.396649718284607\n","Epoch: 692/8000  Traning Loss: 106.22619533538818  Train_Reconstruction: 102.79219818115234  Train_KL: 3.43399715423584  Validation Loss : 103.53681182861328 Val_Reconstruction : 100.11735534667969 Val_KL : 3.4194536209106445\n","Epoch: 693/8000  Traning Loss: 106.20166969299316  Train_Reconstruction: 102.7546443939209  Train_KL: 3.447026163339615  Validation Loss : 103.68127059936523 Val_Reconstruction : 100.28163146972656 Val_KL : 3.3996371030807495\n","Epoch: 694/8000  Traning Loss: 106.22853469848633  Train_Reconstruction: 102.80155849456787  Train_KL: 3.4269760251045227  Validation Loss : 103.77727890014648 Val_Reconstruction : 100.3691177368164 Val_KL : 3.4081610441207886\n","Epoch: 695/8000  Traning Loss: 106.12793064117432  Train_Reconstruction: 102.69983386993408  Train_KL: 3.4280968010425568  Validation Loss : 103.55131912231445 Val_Reconstruction : 100.17364883422852 Val_KL : 3.3776707649230957\n","Epoch: 696/8000  Traning Loss: 105.85564041137695  Train_Reconstruction: 102.436692237854  Train_KL: 3.4189476370811462  Validation Loss : 103.22552490234375 Val_Reconstruction : 99.82878112792969 Val_KL : 3.3967437744140625\n","Epoch: 697/8000  Traning Loss: 106.40668106079102  Train_Reconstruction: 102.96687316894531  Train_KL: 3.4398084580898285  Validation Loss : 103.79551696777344 Val_Reconstruction : 100.40521240234375 Val_KL : 3.390306830406189\n","Epoch: 698/8000  Traning Loss: 106.30401706695557  Train_Reconstruction: 102.89446449279785  Train_KL: 3.409551590681076  Validation Loss : 103.67543411254883 Val_Reconstruction : 100.29051971435547 Val_KL : 3.3849152326583862\n","Epoch: 699/8000  Traning Loss: 106.2566442489624  Train_Reconstruction: 102.81173133850098  Train_KL: 3.444912374019623  Validation Loss : 103.3563117980957 Val_Reconstruction : 99.94734954833984 Val_KL : 3.4089624881744385\n","Epoch: 700/8000  Traning Loss: 105.93553447723389  Train_Reconstruction: 102.50282573699951  Train_KL: 3.4327097833156586  Validation Loss : 103.33137512207031 Val_Reconstruction : 99.92669296264648 Val_KL : 3.404680848121643\n","Epoch: 701/8000  Traning Loss: 105.91131782531738  Train_Reconstruction: 102.47607898712158  Train_KL: 3.435237765312195  Validation Loss : 103.44429397583008 Val_Reconstruction : 100.05311584472656 Val_KL : 3.391179323196411\n","Epoch: 702/8000  Traning Loss: 106.00966167449951  Train_Reconstruction: 102.5753059387207  Train_KL: 3.4343558251857758  Validation Loss : 103.6396255493164 Val_Reconstruction : 100.23268508911133 Val_KL : 3.4069409370422363\n","Epoch: 703/8000  Traning Loss: 106.08948135375977  Train_Reconstruction: 102.63663101196289  Train_KL: 3.4528486728668213  Validation Loss : 103.33074569702148 Val_Reconstruction : 99.9210090637207 Val_KL : 3.409734845161438\n","Epoch: 704/8000  Traning Loss: 105.92905330657959  Train_Reconstruction: 102.49179935455322  Train_KL: 3.4372532069683075  Validation Loss : 103.3962631225586 Val_Reconstruction : 99.99408340454102 Val_KL : 3.402177333831787\n","Epoch: 705/8000  Traning Loss: 105.91896343231201  Train_Reconstruction: 102.47266864776611  Train_KL: 3.446295440196991  Validation Loss : 103.3897476196289 Val_Reconstruction : 99.97834777832031 Val_KL : 3.4113969802856445\n","Epoch: 706/8000  Traning Loss: 105.73046207427979  Train_Reconstruction: 102.29543018341064  Train_KL: 3.4350315034389496  Validation Loss : 103.06877517700195 Val_Reconstruction : 99.67327880859375 Val_KL : 3.3954964876174927\n","Epoch: 707/8000  Traning Loss: 105.79757499694824  Train_Reconstruction: 102.36265468597412  Train_KL: 3.434919238090515  Validation Loss : 103.27364730834961 Val_Reconstruction : 99.87871170043945 Val_KL : 3.394936203956604\n","Epoch: 708/8000  Traning Loss: 105.4721040725708  Train_Reconstruction: 102.03243446350098  Train_KL: 3.4396693110466003  Validation Loss : 103.1334342956543 Val_Reconstruction : 99.72359848022461 Val_KL : 3.409834146499634\n","Epoch: 709/8000  Traning Loss: 105.49994468688965  Train_Reconstruction: 102.06707572937012  Train_KL: 3.4328687489032745  Validation Loss : 102.95016860961914 Val_Reconstruction : 99.55293655395508 Val_KL : 3.397234559059143\n","Epoch: 710/8000  Traning Loss: 105.53246879577637  Train_Reconstruction: 102.09736633300781  Train_KL: 3.4351009726524353  Validation Loss : 103.29563903808594 Val_Reconstruction : 99.88655853271484 Val_KL : 3.4090808629989624\n","Epoch: 711/8000  Traning Loss: 105.78686046600342  Train_Reconstruction: 102.34500980377197  Train_KL: 3.4418494403362274  Validation Loss : 103.14978408813477 Val_Reconstruction : 99.72454833984375 Val_KL : 3.4252350330352783\n","Epoch: 712/8000  Traning Loss: 105.653151512146  Train_Reconstruction: 102.20711421966553  Train_KL: 3.4460368752479553  Validation Loss : 103.20096206665039 Val_Reconstruction : 99.79785919189453 Val_KL : 3.4031022787094116\n","Epoch: 713/8000  Traning Loss: 105.71742057800293  Train_Reconstruction: 102.2695140838623  Train_KL: 3.447906941175461  Validation Loss : 103.16997528076172 Val_Reconstruction : 99.75314712524414 Val_KL : 3.4168277978897095\n","Epoch: 714/8000  Traning Loss: 106.03229808807373  Train_Reconstruction: 102.58920288085938  Train_KL: 3.443096160888672  Validation Loss : 103.50852584838867 Val_Reconstruction : 100.10218811035156 Val_KL : 3.406336545944214\n","Epoch: 715/8000  Traning Loss: 105.93328666687012  Train_Reconstruction: 102.51269340515137  Train_KL: 3.4205923974514008  Validation Loss : 103.36549377441406 Val_Reconstruction : 99.98263168334961 Val_KL : 3.38286292552948\n","Epoch: 716/8000  Traning Loss: 106.01629066467285  Train_Reconstruction: 102.58602046966553  Train_KL: 3.430269807577133  Validation Loss : 103.2959213256836 Val_Reconstruction : 99.8858642578125 Val_KL : 3.410055637359619\n","Epoch: 717/8000  Traning Loss: 105.73898124694824  Train_Reconstruction: 102.31377506256104  Train_KL: 3.4252070784568787  Validation Loss : 103.10129928588867 Val_Reconstruction : 99.70820236206055 Val_KL : 3.3930968046188354\n","Epoch: 718/8000  Traning Loss: 105.92201614379883  Train_Reconstruction: 102.47898292541504  Train_KL: 3.4430330097675323  Validation Loss : 103.473876953125 Val_Reconstruction : 100.05935668945312 Val_KL : 3.414520263671875\n","Epoch: 719/8000  Traning Loss: 106.1645679473877  Train_Reconstruction: 102.71356773376465  Train_KL: 3.450999915599823  Validation Loss : 103.44462966918945 Val_Reconstruction : 100.04736709594727 Val_KL : 3.3972644805908203\n","Epoch: 720/8000  Traning Loss: 106.38996315002441  Train_Reconstruction: 102.95014381408691  Train_KL: 3.4398206174373627  Validation Loss : 103.57433700561523 Val_Reconstruction : 100.17217254638672 Val_KL : 3.402162194252014\n","Epoch: 721/8000  Traning Loss: 106.45207595825195  Train_Reconstruction: 103.01843166351318  Train_KL: 3.4336435198783875  Validation Loss : 103.49372100830078 Val_Reconstruction : 100.08894729614258 Val_KL : 3.404773235321045\n","Epoch: 722/8000  Traning Loss: 106.52288150787354  Train_Reconstruction: 103.07600116729736  Train_KL: 3.4468808472156525  Validation Loss : 104.01623153686523 Val_Reconstruction : 100.61084747314453 Val_KL : 3.4053844213485718\n","Epoch: 723/8000  Traning Loss: 106.29600429534912  Train_Reconstruction: 102.85636043548584  Train_KL: 3.43964347243309  Validation Loss : 103.9033088684082 Val_Reconstruction : 100.49565124511719 Val_KL : 3.4076595306396484\n","Epoch: 724/8000  Traning Loss: 106.32470512390137  Train_Reconstruction: 102.87817668914795  Train_KL: 3.446529746055603  Validation Loss : 104.62443542480469 Val_Reconstruction : 101.20622634887695 Val_KL : 3.418210744857788\n","Epoch: 725/8000  Traning Loss: 107.02874374389648  Train_Reconstruction: 103.59849643707275  Train_KL: 3.430247962474823  Validation Loss : 105.87967681884766 Val_Reconstruction : 102.45948791503906 Val_KL : 3.4201892614364624\n","Epoch: 726/8000  Traning Loss: 107.69705486297607  Train_Reconstruction: 104.23632717132568  Train_KL: 3.4607273042201996  Validation Loss : 104.71267700195312 Val_Reconstruction : 101.29877853393555 Val_KL : 3.4138978719711304\n","Epoch: 727/8000  Traning Loss: 105.93765640258789  Train_Reconstruction: 102.50271987915039  Train_KL: 3.4349375665187836  Validation Loss : 103.20414352416992 Val_Reconstruction : 99.80487823486328 Val_KL : 3.399266242980957\n","Epoch: 728/8000  Traning Loss: 105.89753913879395  Train_Reconstruction: 102.45124816894531  Train_KL: 3.446291506290436  Validation Loss : 103.35234069824219 Val_Reconstruction : 99.9437026977539 Val_KL : 3.4086354970932007\n","Epoch: 729/8000  Traning Loss: 105.92249870300293  Train_Reconstruction: 102.48410606384277  Train_KL: 3.4383922815322876  Validation Loss : 103.14836502075195 Val_Reconstruction : 99.73302459716797 Val_KL : 3.4153387546539307\n","Epoch: 730/8000  Traning Loss: 105.80685997009277  Train_Reconstruction: 102.35607051849365  Train_KL: 3.4507896900177  Validation Loss : 103.05088806152344 Val_Reconstruction : 99.64639282226562 Val_KL : 3.404495596885681\n","Epoch: 731/8000  Traning Loss: 105.65672397613525  Train_Reconstruction: 102.22608470916748  Train_KL: 3.4306392073631287  Validation Loss : 102.81482696533203 Val_Reconstruction : 99.4018669128418 Val_KL : 3.4129581451416016\n","Epoch: 732/8000  Traning Loss: 105.32195091247559  Train_Reconstruction: 101.88774585723877  Train_KL: 3.4342050552368164  Validation Loss : 102.90153884887695 Val_Reconstruction : 99.51681518554688 Val_KL : 3.3847259283065796\n","Epoch: 733/8000  Traning Loss: 105.45789909362793  Train_Reconstruction: 102.0367078781128  Train_KL: 3.4211916029453278  Validation Loss : 103.15997695922852 Val_Reconstruction : 99.77200317382812 Val_KL : 3.38797390460968\n","Epoch: 734/8000  Traning Loss: 105.25050830841064  Train_Reconstruction: 101.83447742462158  Train_KL: 3.4160298705101013  Validation Loss : 102.41616821289062 Val_Reconstruction : 99.02248764038086 Val_KL : 3.3936816453933716\n","Epoch: 735/8000  Traning Loss: 105.19229030609131  Train_Reconstruction: 101.73898410797119  Train_KL: 3.4533062875270844  Validation Loss : 102.50404739379883 Val_Reconstruction : 99.0788688659668 Val_KL : 3.4251785278320312\n","Epoch: 736/8000  Traning Loss: 105.20874500274658  Train_Reconstruction: 101.76644802093506  Train_KL: 3.442297399044037  Validation Loss : 102.71129608154297 Val_Reconstruction : 99.32907485961914 Val_KL : 3.382218360900879\n","Epoch: 737/8000  Traning Loss: 105.19849395751953  Train_Reconstruction: 101.75426578521729  Train_KL: 3.444227546453476  Validation Loss : 102.66935348510742 Val_Reconstruction : 99.23494720458984 Val_KL : 3.4344074726104736\n","Epoch: 738/8000  Traning Loss: 105.47687530517578  Train_Reconstruction: 102.04735279083252  Train_KL: 3.4295226633548737  Validation Loss : 102.66100311279297 Val_Reconstruction : 99.2760009765625 Val_KL : 3.384998917579651\n","Epoch: 739/8000  Traning Loss: 105.35612773895264  Train_Reconstruction: 101.91592788696289  Train_KL: 3.44019877910614  Validation Loss : 102.72312927246094 Val_Reconstruction : 99.3043212890625 Val_KL : 3.4188082218170166\n","Epoch: 740/8000  Traning Loss: 105.38289546966553  Train_Reconstruction: 101.94484043121338  Train_KL: 3.4380537569522858  Validation Loss : 102.79506301879883 Val_Reconstruction : 99.38813018798828 Val_KL : 3.406932830810547\n","Epoch: 741/8000  Traning Loss: 105.43192481994629  Train_Reconstruction: 101.97283744812012  Train_KL: 3.459086924791336  Validation Loss : 102.83940887451172 Val_Reconstruction : 99.41349411010742 Val_KL : 3.425916314125061\n","Epoch: 742/8000  Traning Loss: 105.32408714294434  Train_Reconstruction: 101.89292430877686  Train_KL: 3.4311631619930267  Validation Loss : 102.76941680908203 Val_Reconstruction : 99.37033462524414 Val_KL : 3.3990840911865234\n","Epoch: 743/8000  Traning Loss: 105.33525085449219  Train_Reconstruction: 101.90023040771484  Train_KL: 3.4350196719169617  Validation Loss : 102.53863906860352 Val_Reconstruction : 99.14341354370117 Val_KL : 3.395228147506714\n","Epoch: 744/8000  Traning Loss: 105.39382648468018  Train_Reconstruction: 101.95527362823486  Train_KL: 3.438552051782608  Validation Loss : 102.950927734375 Val_Reconstruction : 99.54388809204102 Val_KL : 3.4070385694503784\n","Epoch: 745/8000  Traning Loss: 105.21311473846436  Train_Reconstruction: 101.77649688720703  Train_KL: 3.4366180300712585  Validation Loss : 102.97367858886719 Val_Reconstruction : 99.56915664672852 Val_KL : 3.404521107673645\n","Epoch: 746/8000  Traning Loss: 105.44906806945801  Train_Reconstruction: 102.00455474853516  Train_KL: 3.4445134699344635  Validation Loss : 103.27263259887695 Val_Reconstruction : 99.85539245605469 Val_KL : 3.4172372817993164\n","Epoch: 747/8000  Traning Loss: 105.7345666885376  Train_Reconstruction: 102.27849864959717  Train_KL: 3.4560681879520416  Validation Loss : 103.4207649230957 Val_Reconstruction : 99.98510360717773 Val_KL : 3.4356616735458374\n","Epoch: 748/8000  Traning Loss: 105.62106132507324  Train_Reconstruction: 102.16972541809082  Train_KL: 3.4513363242149353  Validation Loss : 103.24730682373047 Val_Reconstruction : 99.845703125 Val_KL : 3.401602268218994\n","Epoch: 749/8000  Traning Loss: 105.33936786651611  Train_Reconstruction: 101.91182613372803  Train_KL: 3.4275426864624023  Validation Loss : 102.66886520385742 Val_Reconstruction : 99.27421569824219 Val_KL : 3.3946518898010254\n","Epoch: 750/8000  Traning Loss: 105.41546726226807  Train_Reconstruction: 101.97258281707764  Train_KL: 3.442883998155594  Validation Loss : 103.0402603149414 Val_Reconstruction : 99.61597061157227 Val_KL : 3.424290657043457\n","Epoch: 751/8000  Traning Loss: 105.36608600616455  Train_Reconstruction: 101.93446350097656  Train_KL: 3.4316235184669495  Validation Loss : 103.05203628540039 Val_Reconstruction : 99.65336608886719 Val_KL : 3.398668646812439\n","Epoch: 752/8000  Traning Loss: 105.23417472839355  Train_Reconstruction: 101.79148960113525  Train_KL: 3.4426849484443665  Validation Loss : 102.75704956054688 Val_Reconstruction : 99.31748962402344 Val_KL : 3.4395605325698853\n","Epoch: 753/8000  Traning Loss: 105.02413558959961  Train_Reconstruction: 101.5757064819336  Train_KL: 3.44842928647995  Validation Loss : 102.42511749267578 Val_Reconstruction : 99.02920532226562 Val_KL : 3.395913243293762\n","Epoch: 754/8000  Traning Loss: 105.03713989257812  Train_Reconstruction: 101.5903959274292  Train_KL: 3.44674488902092  Validation Loss : 102.60197448730469 Val_Reconstruction : 99.18842697143555 Val_KL : 3.413548231124878\n","Epoch: 755/8000  Traning Loss: 105.33802318572998  Train_Reconstruction: 101.89547729492188  Train_KL: 3.442544996738434  Validation Loss : 103.05713272094727 Val_Reconstruction : 99.64322280883789 Val_KL : 3.4139097929000854\n","Epoch: 756/8000  Traning Loss: 105.5363540649414  Train_Reconstruction: 102.11569309234619  Train_KL: 3.420660525560379  Validation Loss : 103.39053726196289 Val_Reconstruction : 99.97808074951172 Val_KL : 3.4124555587768555\n","Epoch: 757/8000  Traning Loss: 105.38657665252686  Train_Reconstruction: 101.920823097229  Train_KL: 3.4657537639141083  Validation Loss : 102.64371871948242 Val_Reconstruction : 99.21774291992188 Val_KL : 3.425974488258362\n","Epoch: 758/8000  Traning Loss: 105.20989322662354  Train_Reconstruction: 101.76715087890625  Train_KL: 3.442741721868515  Validation Loss : 102.82271957397461 Val_Reconstruction : 99.41473770141602 Val_KL : 3.4079829454421997\n","Epoch: 759/8000  Traning Loss: 104.95313739776611  Train_Reconstruction: 101.52086544036865  Train_KL: 3.4322731494903564  Validation Loss : 102.20256423950195 Val_Reconstruction : 98.80147933959961 Val_KL : 3.4010884761810303\n","Epoch: 760/8000  Traning Loss: 105.03395462036133  Train_Reconstruction: 101.60295677185059  Train_KL: 3.430998742580414  Validation Loss : 102.26556015014648 Val_Reconstruction : 98.86910247802734 Val_KL : 3.3964571952819824\n","Epoch: 761/8000  Traning Loss: 104.96390056610107  Train_Reconstruction: 101.53653049468994  Train_KL: 3.427370935678482  Validation Loss : 102.37914657592773 Val_Reconstruction : 98.98587799072266 Val_KL : 3.393270492553711\n","Epoch: 762/8000  Traning Loss: 104.7895860671997  Train_Reconstruction: 101.35058879852295  Train_KL: 3.4389972388744354  Validation Loss : 102.3961181640625 Val_Reconstruction : 98.98661041259766 Val_KL : 3.4095072746276855\n","Epoch: 763/8000  Traning Loss: 104.97176170349121  Train_Reconstruction: 101.54411888122559  Train_KL: 3.4276430010795593  Validation Loss : 103.05421447753906 Val_Reconstruction : 99.66279220581055 Val_KL : 3.391421675682068\n","Epoch: 764/8000  Traning Loss: 104.94495582580566  Train_Reconstruction: 101.49512004852295  Train_KL: 3.4498367607593536  Validation Loss : 102.50919342041016 Val_Reconstruction : 99.08418273925781 Val_KL : 3.42501163482666\n","Epoch: 765/8000  Traning Loss: 104.80084991455078  Train_Reconstruction: 101.35456085205078  Train_KL: 3.446290135383606  Validation Loss : 102.30448150634766 Val_Reconstruction : 98.91087341308594 Val_KL : 3.3936115503311157\n","Epoch: 766/8000  Traning Loss: 104.95898723602295  Train_Reconstruction: 101.51525783538818  Train_KL: 3.443728446960449  Validation Loss : 102.45596694946289 Val_Reconstruction : 99.03062438964844 Val_KL : 3.4253422021865845\n","Epoch: 767/8000  Traning Loss: 105.51207637786865  Train_Reconstruction: 102.08161735534668  Train_KL: 3.4304602444171906  Validation Loss : 103.04239273071289 Val_Reconstruction : 99.66452026367188 Val_KL : 3.3778713941574097\n","Epoch: 768/8000  Traning Loss: 104.87166023254395  Train_Reconstruction: 101.42564487457275  Train_KL: 3.4460159838199615  Validation Loss : 102.15332412719727 Val_Reconstruction : 98.73785400390625 Val_KL : 3.415472626686096\n","Epoch: 769/8000  Traning Loss: 104.74755764007568  Train_Reconstruction: 101.30872917175293  Train_KL: 3.438828766345978  Validation Loss : 102.30711364746094 Val_Reconstruction : 98.91450119018555 Val_KL : 3.392610788345337\n","Epoch: 770/8000  Traning Loss: 104.72098636627197  Train_Reconstruction: 101.2711067199707  Train_KL: 3.4498786330223083  Validation Loss : 102.2588119506836 Val_Reconstruction : 98.81694793701172 Val_KL : 3.4418612718582153\n","Epoch: 771/8000  Traning Loss: 104.72008514404297  Train_Reconstruction: 101.26068496704102  Train_KL: 3.459400027990341  Validation Loss : 102.01846694946289 Val_Reconstruction : 98.60325241088867 Val_KL : 3.415212631225586\n","Epoch: 772/8000  Traning Loss: 104.81959056854248  Train_Reconstruction: 101.38240623474121  Train_KL: 3.437184453010559  Validation Loss : 102.2187385559082 Val_Reconstruction : 98.8204116821289 Val_KL : 3.3983267545700073\n","Epoch: 773/8000  Traning Loss: 104.8756332397461  Train_Reconstruction: 101.4303092956543  Train_KL: 3.445323407649994  Validation Loss : 102.61918640136719 Val_Reconstruction : 99.20014953613281 Val_KL : 3.419037938117981\n","Epoch: 774/8000  Traning Loss: 105.01984119415283  Train_Reconstruction: 101.58248519897461  Train_KL: 3.4373565018177032  Validation Loss : 102.8769302368164 Val_Reconstruction : 99.47800827026367 Val_KL : 3.3989202976226807\n","Epoch: 775/8000  Traning Loss: 105.02673625946045  Train_Reconstruction: 101.5782699584961  Train_KL: 3.4484662115573883  Validation Loss : 102.44065856933594 Val_Reconstruction : 99.01822662353516 Val_KL : 3.4224305152893066\n","Epoch: 776/8000  Traning Loss: 105.2662878036499  Train_Reconstruction: 101.80802154541016  Train_KL: 3.4582653045654297  Validation Loss : 102.63866424560547 Val_Reconstruction : 99.22242736816406 Val_KL : 3.4162386655807495\n","Epoch: 777/8000  Traning Loss: 104.96845245361328  Train_Reconstruction: 101.5379228591919  Train_KL: 3.4305295944213867  Validation Loss : 102.38126373291016 Val_Reconstruction : 98.98838806152344 Val_KL : 3.3928765058517456\n","Epoch: 778/8000  Traning Loss: 104.88063049316406  Train_Reconstruction: 101.4602222442627  Train_KL: 3.420409083366394  Validation Loss : 102.12161636352539 Val_Reconstruction : 98.72146224975586 Val_KL : 3.4001559019088745\n","Epoch: 779/8000  Traning Loss: 105.02625465393066  Train_Reconstruction: 101.59117698669434  Train_KL: 3.435077041387558  Validation Loss : 102.47971725463867 Val_Reconstruction : 99.08475494384766 Val_KL : 3.394960403442383\n","Epoch: 780/8000  Traning Loss: 105.42597198486328  Train_Reconstruction: 101.99101543426514  Train_KL: 3.434955060482025  Validation Loss : 102.96708297729492 Val_Reconstruction : 99.53270721435547 Val_KL : 3.4343751668930054\n","Epoch: 781/8000  Traning Loss: 104.96056365966797  Train_Reconstruction: 101.50089359283447  Train_KL: 3.4596692621707916  Validation Loss : 102.39435195922852 Val_Reconstruction : 98.97715377807617 Val_KL : 3.4172003269195557\n","Epoch: 782/8000  Traning Loss: 104.85803890228271  Train_Reconstruction: 101.41015529632568  Train_KL: 3.4478847980499268  Validation Loss : 102.41839599609375 Val_Reconstruction : 98.99835968017578 Val_KL : 3.4200379848480225\n","Epoch: 783/8000  Traning Loss: 105.1132459640503  Train_Reconstruction: 101.66096591949463  Train_KL: 3.4522801637649536  Validation Loss : 102.20197296142578 Val_Reconstruction : 98.79006958007812 Val_KL : 3.411903977394104\n","Epoch: 784/8000  Traning Loss: 104.69680595397949  Train_Reconstruction: 101.2388334274292  Train_KL: 3.457973301410675  Validation Loss : 102.01491928100586 Val_Reconstruction : 98.59441375732422 Val_KL : 3.4205033779144287\n","Epoch: 785/8000  Traning Loss: 104.3818359375  Train_Reconstruction: 100.93540000915527  Train_KL: 3.4464355409145355  Validation Loss : 101.8946304321289 Val_Reconstruction : 98.4673957824707 Val_KL : 3.4272350072860718\n","Epoch: 786/8000  Traning Loss: 104.63255310058594  Train_Reconstruction: 101.1875410079956  Train_KL: 3.4450114369392395  Validation Loss : 102.05190658569336 Val_Reconstruction : 98.65760803222656 Val_KL : 3.3942956924438477\n","Epoch: 787/8000  Traning Loss: 104.74462509155273  Train_Reconstruction: 101.30660343170166  Train_KL: 3.4380216002464294  Validation Loss : 102.66003799438477 Val_Reconstruction : 99.26227569580078 Val_KL : 3.39776349067688\n","Epoch: 788/8000  Traning Loss: 105.09753704071045  Train_Reconstruction: 101.66213798522949  Train_KL: 3.4354000091552734  Validation Loss : 102.39805603027344 Val_Reconstruction : 98.99560928344727 Val_KL : 3.4024447202682495\n","Epoch: 789/8000  Traning Loss: 104.67424774169922  Train_Reconstruction: 101.23271465301514  Train_KL: 3.4415328204631805  Validation Loss : 102.42354202270508 Val_Reconstruction : 99.02345657348633 Val_KL : 3.4000840187072754\n","Epoch: 790/8000  Traning Loss: 104.43702030181885  Train_Reconstruction: 101.01035404205322  Train_KL: 3.426665425300598  Validation Loss : 101.83807373046875 Val_Reconstruction : 98.45067596435547 Val_KL : 3.387395977973938\n","Epoch: 791/8000  Traning Loss: 104.57353401184082  Train_Reconstruction: 101.13498306274414  Train_KL: 3.4385508000850677  Validation Loss : 102.18829727172852 Val_Reconstruction : 98.7731704711914 Val_KL : 3.4151268005371094\n","Epoch: 792/8000  Traning Loss: 104.63202095031738  Train_Reconstruction: 101.18841552734375  Train_KL: 3.443604916334152  Validation Loss : 101.85210800170898 Val_Reconstruction : 98.44571304321289 Val_KL : 3.4063960313796997\n","Epoch: 793/8000  Traning Loss: 104.37422180175781  Train_Reconstruction: 100.93361854553223  Train_KL: 3.440602868795395  Validation Loss : 101.77492904663086 Val_Reconstruction : 98.34734344482422 Val_KL : 3.427583694458008\n","Epoch: 794/8000  Traning Loss: 104.3402795791626  Train_Reconstruction: 100.88134002685547  Train_KL: 3.458939164876938  Validation Loss : 101.79492568969727 Val_Reconstruction : 98.37349700927734 Val_KL : 3.421428680419922\n","Epoch: 795/8000  Traning Loss: 104.3504695892334  Train_Reconstruction: 100.92287349700928  Train_KL: 3.427596092224121  Validation Loss : 102.03056335449219 Val_Reconstruction : 98.64909362792969 Val_KL : 3.381468653678894\n","Epoch: 796/8000  Traning Loss: 104.43894386291504  Train_Reconstruction: 101.00628471374512  Train_KL: 3.4326597452163696  Validation Loss : 102.05762100219727 Val_Reconstruction : 98.63272094726562 Val_KL : 3.4249004125595093\n","Epoch: 797/8000  Traning Loss: 104.41125202178955  Train_Reconstruction: 100.94206523895264  Train_KL: 3.46918722987175  Validation Loss : 101.85819244384766 Val_Reconstruction : 98.41666030883789 Val_KL : 3.44153368473053\n","Epoch: 798/8000  Traning Loss: 104.82585906982422  Train_Reconstruction: 101.37411212921143  Train_KL: 3.451745957136154  Validation Loss : 102.0197525024414 Val_Reconstruction : 98.60884094238281 Val_KL : 3.410911798477173\n","Epoch: 799/8000  Traning Loss: 104.74932670593262  Train_Reconstruction: 101.30416774749756  Train_KL: 3.4451590478420258  Validation Loss : 101.93109893798828 Val_Reconstruction : 98.52508544921875 Val_KL : 3.406014084815979\n","Epoch: 800/8000  Traning Loss: 104.50639629364014  Train_Reconstruction: 101.07875156402588  Train_KL: 3.4276444017887115  Validation Loss : 101.77835083007812 Val_Reconstruction : 98.36375427246094 Val_KL : 3.4145946502685547\n","Epoch: 801/8000  Traning Loss: 104.41591835021973  Train_Reconstruction: 100.97570323944092  Train_KL: 3.4402140974998474  Validation Loss : 101.97705459594727 Val_Reconstruction : 98.5914421081543 Val_KL : 3.385612726211548\n","Epoch: 802/8000  Traning Loss: 104.22648525238037  Train_Reconstruction: 100.7714786529541  Train_KL: 3.4550053477287292  Validation Loss : 101.864501953125 Val_Reconstruction : 98.42829132080078 Val_KL : 3.4362136125564575\n","Epoch: 803/8000  Traning Loss: 104.40975856781006  Train_Reconstruction: 100.96018123626709  Train_KL: 3.4495773017406464  Validation Loss : 101.88457489013672 Val_Reconstruction : 98.48532485961914 Val_KL : 3.399251341819763\n","Epoch: 804/8000  Traning Loss: 104.4786376953125  Train_Reconstruction: 101.02593898773193  Train_KL: 3.452698200941086  Validation Loss : 102.31697845458984 Val_Reconstruction : 98.89031982421875 Val_KL : 3.42665696144104\n","Epoch: 805/8000  Traning Loss: 104.37110137939453  Train_Reconstruction: 100.92332935333252  Train_KL: 3.4477717578411102  Validation Loss : 101.78531265258789 Val_Reconstruction : 98.3675651550293 Val_KL : 3.4177474975585938\n","Epoch: 806/8000  Traning Loss: 104.26320171356201  Train_Reconstruction: 100.81658458709717  Train_KL: 3.4466162621974945  Validation Loss : 102.2477798461914 Val_Reconstruction : 98.85075759887695 Val_KL : 3.3970240354537964\n","Epoch: 807/8000  Traning Loss: 104.69186401367188  Train_Reconstruction: 101.26232624053955  Train_KL: 3.429537057876587  Validation Loss : 102.72000503540039 Val_Reconstruction : 99.30083847045898 Val_KL : 3.4191651344299316\n","Epoch: 808/8000  Traning Loss: 104.8894910812378  Train_Reconstruction: 101.43770122528076  Train_KL: 3.45179083943367  Validation Loss : 102.21256637573242 Val_Reconstruction : 98.80996704101562 Val_KL : 3.4026005268096924\n","Epoch: 809/8000  Traning Loss: 104.8591480255127  Train_Reconstruction: 101.41689395904541  Train_KL: 3.4422543942928314  Validation Loss : 102.4942512512207 Val_Reconstruction : 99.06442642211914 Val_KL : 3.4298237562179565\n","Epoch: 810/8000  Traning Loss: 104.65392780303955  Train_Reconstruction: 101.18974494934082  Train_KL: 3.4641825556755066  Validation Loss : 102.19610214233398 Val_Reconstruction : 98.78196716308594 Val_KL : 3.4141368865966797\n","Epoch: 811/8000  Traning Loss: 104.83081150054932  Train_Reconstruction: 101.37503051757812  Train_KL: 3.4557809829711914  Validation Loss : 102.2968521118164 Val_Reconstruction : 98.87472534179688 Val_KL : 3.422126054763794\n","Epoch: 812/8000  Traning Loss: 104.76666927337646  Train_Reconstruction: 101.3218879699707  Train_KL: 3.444779723882675  Validation Loss : 102.28519821166992 Val_Reconstruction : 98.8727798461914 Val_KL : 3.4124181270599365\n","Epoch: 813/8000  Traning Loss: 104.70960712432861  Train_Reconstruction: 101.26117515563965  Train_KL: 3.448432147502899  Validation Loss : 102.49707412719727 Val_Reconstruction : 99.06121063232422 Val_KL : 3.4358657598495483\n","Epoch: 814/8000  Traning Loss: 104.70632648468018  Train_Reconstruction: 101.24180221557617  Train_KL: 3.464525192975998  Validation Loss : 102.31374740600586 Val_Reconstruction : 98.91017532348633 Val_KL : 3.4035706520080566\n","Epoch: 815/8000  Traning Loss: 104.40359497070312  Train_Reconstruction: 100.95135498046875  Train_KL: 3.4522408843040466  Validation Loss : 101.96832656860352 Val_Reconstruction : 98.54846572875977 Val_KL : 3.419861078262329\n","Epoch: 816/8000  Traning Loss: 104.22107982635498  Train_Reconstruction: 100.77066326141357  Train_KL: 3.4504177570343018  Validation Loss : 101.74121475219727 Val_Reconstruction : 98.32555389404297 Val_KL : 3.4156596660614014\n","Epoch: 817/8000  Traning Loss: 104.22450828552246  Train_Reconstruction: 100.77786064147949  Train_KL: 3.4466475546360016  Validation Loss : 101.87126922607422 Val_Reconstruction : 98.46406555175781 Val_KL : 3.407203435897827\n","Epoch: 818/8000  Traning Loss: 104.24964332580566  Train_Reconstruction: 100.81518745422363  Train_KL: 3.434455543756485  Validation Loss : 102.19065856933594 Val_Reconstruction : 98.8119125366211 Val_KL : 3.3787471055984497\n","Epoch: 819/8000  Traning Loss: 104.06334781646729  Train_Reconstruction: 100.62161350250244  Train_KL: 3.441734254360199  Validation Loss : 101.5270881652832 Val_Reconstruction : 98.09978103637695 Val_KL : 3.427306294441223\n","Epoch: 820/8000  Traning Loss: 103.88510608673096  Train_Reconstruction: 100.4463005065918  Train_KL: 3.438805729150772  Validation Loss : 101.43430709838867 Val_Reconstruction : 98.03401184082031 Val_KL : 3.4002950191497803\n","Epoch: 821/8000  Traning Loss: 104.27947902679443  Train_Reconstruction: 100.82750606536865  Train_KL: 3.4519738852977753  Validation Loss : 101.92860794067383 Val_Reconstruction : 98.51586151123047 Val_KL : 3.4127451181411743\n","Epoch: 822/8000  Traning Loss: 104.47164630889893  Train_Reconstruction: 101.0261640548706  Train_KL: 3.4454815685749054  Validation Loss : 102.2381706237793 Val_Reconstruction : 98.81964111328125 Val_KL : 3.4185301065444946\n","Epoch: 823/8000  Traning Loss: 104.5184907913208  Train_Reconstruction: 101.0629415512085  Train_KL: 3.4555479288101196  Validation Loss : 101.8673095703125 Val_Reconstruction : 98.45691680908203 Val_KL : 3.410392999649048\n","Epoch: 824/8000  Traning Loss: 104.12725925445557  Train_Reconstruction: 100.68758583068848  Train_KL: 3.439674824476242  Validation Loss : 101.70228958129883 Val_Reconstruction : 98.27753829956055 Val_KL : 3.4247517585754395\n","Epoch: 825/8000  Traning Loss: 104.11086463928223  Train_Reconstruction: 100.65939712524414  Train_KL: 3.4514676928520203  Validation Loss : 101.90699005126953 Val_Reconstruction : 98.5233268737793 Val_KL : 3.383664131164551\n","Epoch: 826/8000  Traning Loss: 104.11762619018555  Train_Reconstruction: 100.68458652496338  Train_KL: 3.433040052652359  Validation Loss : 101.63116836547852 Val_Reconstruction : 98.22035598754883 Val_KL : 3.410812497138977\n","Epoch: 827/8000  Traning Loss: 104.07896041870117  Train_Reconstruction: 100.64401531219482  Train_KL: 3.434944808483124  Validation Loss : 101.84144592285156 Val_Reconstruction : 98.43214416503906 Val_KL : 3.409299850463867\n","Epoch: 828/8000  Traning Loss: 104.05754089355469  Train_Reconstruction: 100.60799026489258  Train_KL: 3.4495497345924377  Validation Loss : 101.72135162353516 Val_Reconstruction : 98.30727767944336 Val_KL : 3.4140747785568237\n","Epoch: 829/8000  Traning Loss: 104.18057155609131  Train_Reconstruction: 100.7345142364502  Train_KL: 3.4460556507110596  Validation Loss : 101.86340713500977 Val_Reconstruction : 98.4585952758789 Val_KL : 3.404812455177307\n","Epoch: 830/8000  Traning Loss: 104.15467548370361  Train_Reconstruction: 100.72089004516602  Train_KL: 3.4337852597236633  Validation Loss : 101.80238342285156 Val_Reconstruction : 98.40947341918945 Val_KL : 3.392910122871399\n","Epoch: 831/8000  Traning Loss: 104.21198272705078  Train_Reconstruction: 100.76575756072998  Train_KL: 3.4462254643440247  Validation Loss : 102.10803985595703 Val_Reconstruction : 98.69762802124023 Val_KL : 3.410409688949585\n","Epoch: 832/8000  Traning Loss: 104.23626327514648  Train_Reconstruction: 100.78578472137451  Train_KL: 3.4504791498184204  Validation Loss : 101.54988861083984 Val_Reconstruction : 98.12621307373047 Val_KL : 3.4236754179000854\n","Epoch: 833/8000  Traning Loss: 103.88669967651367  Train_Reconstruction: 100.43656635284424  Train_KL: 3.45013165473938  Validation Loss : 101.29850769042969 Val_Reconstruction : 97.88602066040039 Val_KL : 3.41248619556427\n","Epoch: 834/8000  Traning Loss: 104.02523136138916  Train_Reconstruction: 100.58334732055664  Train_KL: 3.441884398460388  Validation Loss : 101.76156234741211 Val_Reconstruction : 98.35322952270508 Val_KL : 3.4083327054977417\n","Epoch: 835/8000  Traning Loss: 104.34204006195068  Train_Reconstruction: 100.89041328430176  Train_KL: 3.451628178358078  Validation Loss : 102.14570999145508 Val_Reconstruction : 98.73094940185547 Val_KL : 3.4147608280181885\n","Epoch: 836/8000  Traning Loss: 104.16520309448242  Train_Reconstruction: 100.73069190979004  Train_KL: 3.434510201215744  Validation Loss : 101.55097198486328 Val_Reconstruction : 98.14885711669922 Val_KL : 3.402116298675537\n","Epoch: 837/8000  Traning Loss: 104.04061508178711  Train_Reconstruction: 100.60680961608887  Train_KL: 3.4338067770004272  Validation Loss : 101.6629524230957 Val_Reconstruction : 98.25181579589844 Val_KL : 3.411136269569397\n","Epoch: 838/8000  Traning Loss: 104.12847518920898  Train_Reconstruction: 100.67717933654785  Train_KL: 3.4512962996959686  Validation Loss : 101.90091705322266 Val_Reconstruction : 98.49037170410156 Val_KL : 3.4105457067489624\n","Epoch: 839/8000  Traning Loss: 103.95529174804688  Train_Reconstruction: 100.52846717834473  Train_KL: 3.4268245697021484  Validation Loss : 101.57675552368164 Val_Reconstruction : 98.17090606689453 Val_KL : 3.4058516025543213\n","Epoch: 840/8000  Traning Loss: 104.04845142364502  Train_Reconstruction: 100.60640716552734  Train_KL: 3.442043721675873  Validation Loss : 101.43888473510742 Val_Reconstruction : 98.02642059326172 Val_KL : 3.412464737892151\n","Epoch: 841/8000  Traning Loss: 104.03168296813965  Train_Reconstruction: 100.5925645828247  Train_KL: 3.4391174614429474  Validation Loss : 101.55596160888672 Val_Reconstruction : 98.14791870117188 Val_KL : 3.4080435037612915\n","Epoch: 842/8000  Traning Loss: 104.00351619720459  Train_Reconstruction: 100.54769992828369  Train_KL: 3.455816090106964  Validation Loss : 101.44815063476562 Val_Reconstruction : 98.02840423583984 Val_KL : 3.4197434186935425\n","Epoch: 843/8000  Traning Loss: 103.82377910614014  Train_Reconstruction: 100.38178825378418  Train_KL: 3.44199076294899  Validation Loss : 101.45565795898438 Val_Reconstruction : 98.04069900512695 Val_KL : 3.41495680809021\n","Epoch: 844/8000  Traning Loss: 103.75371265411377  Train_Reconstruction: 100.30124759674072  Train_KL: 3.452465295791626  Validation Loss : 101.70667266845703 Val_Reconstruction : 98.28943634033203 Val_KL : 3.417237162590027\n","Epoch: 845/8000  Traning Loss: 103.97053623199463  Train_Reconstruction: 100.52427005767822  Train_KL: 3.4462671279907227  Validation Loss : 101.873046875 Val_Reconstruction : 98.46210098266602 Val_KL : 3.4109461307525635\n","Epoch: 846/8000  Traning Loss: 103.82701110839844  Train_Reconstruction: 100.37998008728027  Train_KL: 3.447031617164612  Validation Loss : 101.33279800415039 Val_Reconstruction : 97.90506362915039 Val_KL : 3.4277342557907104\n","Epoch: 847/8000  Traning Loss: 103.78848266601562  Train_Reconstruction: 100.34823799133301  Train_KL: 3.440244734287262  Validation Loss : 101.55702209472656 Val_Reconstruction : 98.15213394165039 Val_KL : 3.4048880338668823\n","Epoch: 848/8000  Traning Loss: 103.81943416595459  Train_Reconstruction: 100.36604118347168  Train_KL: 3.4533933997154236  Validation Loss : 101.45875930786133 Val_Reconstruction : 98.05190658569336 Val_KL : 3.406849980354309\n","Epoch: 849/8000  Traning Loss: 103.76632118225098  Train_Reconstruction: 100.31574058532715  Train_KL: 3.450580418109894  Validation Loss : 101.41125106811523 Val_Reconstruction : 97.99236297607422 Val_KL : 3.418889284133911\n","Epoch: 850/8000  Traning Loss: 103.74280071258545  Train_Reconstruction: 100.28538799285889  Train_KL: 3.4574129581451416  Validation Loss : 101.22675323486328 Val_Reconstruction : 97.80766296386719 Val_KL : 3.4190902709960938\n","Epoch: 851/8000  Traning Loss: 103.6994857788086  Train_Reconstruction: 100.24813938140869  Train_KL: 3.451344847679138  Validation Loss : 101.53358840942383 Val_Reconstruction : 98.11272430419922 Val_KL : 3.4208643436431885\n","Epoch: 852/8000  Traning Loss: 103.59598541259766  Train_Reconstruction: 100.15973281860352  Train_KL: 3.436252623796463  Validation Loss : 101.05827713012695 Val_Reconstruction : 97.64033889770508 Val_KL : 3.417937755584717\n","Epoch: 853/8000  Traning Loss: 103.42028141021729  Train_Reconstruction: 99.94919395446777  Train_KL: 3.4710869193077087  Validation Loss : 101.09545135498047 Val_Reconstruction : 97.65672302246094 Val_KL : 3.438730835914612\n","Epoch: 854/8000  Traning Loss: 103.51115894317627  Train_Reconstruction: 100.06298160552979  Train_KL: 3.448177307844162  Validation Loss : 101.26222229003906 Val_Reconstruction : 97.85279083251953 Val_KL : 3.40943443775177\n","Epoch: 855/8000  Traning Loss: 103.75408172607422  Train_Reconstruction: 100.29319667816162  Train_KL: 3.4608840942382812  Validation Loss : 101.40663146972656 Val_Reconstruction : 97.98392868041992 Val_KL : 3.422700881958008\n","Epoch: 856/8000  Traning Loss: 103.60708808898926  Train_Reconstruction: 100.15752506256104  Train_KL: 3.44956174492836  Validation Loss : 101.10265731811523 Val_Reconstruction : 97.68336486816406 Val_KL : 3.4192938804626465\n","Epoch: 857/8000  Traning Loss: 103.58024215698242  Train_Reconstruction: 100.12778663635254  Train_KL: 3.452454447746277  Validation Loss : 101.1590347290039 Val_Reconstruction : 97.7321548461914 Val_KL : 3.4268780946731567\n","Epoch: 858/8000  Traning Loss: 103.53719902038574  Train_Reconstruction: 100.07388877868652  Train_KL: 3.463310271501541  Validation Loss : 101.49092102050781 Val_Reconstruction : 98.0517692565918 Val_KL : 3.4391520023345947\n","Epoch: 859/8000  Traning Loss: 103.84893798828125  Train_Reconstruction: 100.40071296691895  Train_KL: 3.4482245445251465  Validation Loss : 101.56824493408203 Val_Reconstruction : 98.17194366455078 Val_KL : 3.396302342414856\n","Epoch: 860/8000  Traning Loss: 104.30825233459473  Train_Reconstruction: 100.86757278442383  Train_KL: 3.440677523612976  Validation Loss : 101.88548278808594 Val_Reconstruction : 98.47780990600586 Val_KL : 3.4076714515686035\n","Epoch: 861/8000  Traning Loss: 104.22655487060547  Train_Reconstruction: 100.8025598526001  Train_KL: 3.423996090888977  Validation Loss : 101.5174446105957 Val_Reconstruction : 98.13085556030273 Val_KL : 3.3865878582000732\n","Epoch: 862/8000  Traning Loss: 103.94623279571533  Train_Reconstruction: 100.50059700012207  Train_KL: 3.445636510848999  Validation Loss : 101.69062423706055 Val_Reconstruction : 98.26061248779297 Val_KL : 3.430010199546814\n","Epoch: 863/8000  Traning Loss: 104.1916913986206  Train_Reconstruction: 100.73338985443115  Train_KL: 3.4583010971546173  Validation Loss : 101.84711456298828 Val_Reconstruction : 98.41548156738281 Val_KL : 3.4316338300704956\n","Epoch: 864/8000  Traning Loss: 103.70747375488281  Train_Reconstruction: 100.24736404418945  Train_KL: 3.4601097106933594  Validation Loss : 101.39400482177734 Val_Reconstruction : 97.98144912719727 Val_KL : 3.412554144859314\n","Epoch: 865/8000  Traning Loss: 103.74897766113281  Train_Reconstruction: 100.31470012664795  Train_KL: 3.434279203414917  Validation Loss : 101.5695571899414 Val_Reconstruction : 98.18305206298828 Val_KL : 3.386501908302307\n","Epoch: 866/8000  Traning Loss: 103.74418830871582  Train_Reconstruction: 100.30869770050049  Train_KL: 3.435491532087326  Validation Loss : 101.3519515991211 Val_Reconstruction : 97.94282531738281 Val_KL : 3.409124732017517\n","Epoch: 867/8000  Traning Loss: 103.55820751190186  Train_Reconstruction: 100.10405921936035  Train_KL: 3.4541483223438263  Validation Loss : 101.19932556152344 Val_Reconstruction : 97.76770401000977 Val_KL : 3.4316210746765137\n","Epoch: 868/8000  Traning Loss: 103.33135032653809  Train_Reconstruction: 99.89741611480713  Train_KL: 3.433935433626175  Validation Loss : 100.95118713378906 Val_Reconstruction : 97.5593147277832 Val_KL : 3.39186954498291\n","Epoch: 869/8000  Traning Loss: 103.31988906860352  Train_Reconstruction: 99.878737449646  Train_KL: 3.441151440143585  Validation Loss : 100.89925384521484 Val_Reconstruction : 97.47278594970703 Val_KL : 3.4264676570892334\n","Epoch: 870/8000  Traning Loss: 103.40549373626709  Train_Reconstruction: 99.95149803161621  Train_KL: 3.453995645046234  Validation Loss : 101.33566665649414 Val_Reconstruction : 97.910400390625 Val_KL : 3.425265908241272\n","Epoch: 871/8000  Traning Loss: 103.370774269104  Train_Reconstruction: 99.92190074920654  Train_KL: 3.4488751590251923  Validation Loss : 100.95246887207031 Val_Reconstruction : 97.53611755371094 Val_KL : 3.416350483894348\n","Epoch: 872/8000  Traning Loss: 103.45481395721436  Train_Reconstruction: 100.00173568725586  Train_KL: 3.453078418970108  Validation Loss : 101.17538833618164 Val_Reconstruction : 97.75653839111328 Val_KL : 3.4188531637191772\n","Epoch: 873/8000  Traning Loss: 103.4723310470581  Train_Reconstruction: 100.01458740234375  Train_KL: 3.4577440321445465  Validation Loss : 101.16334915161133 Val_Reconstruction : 97.73273849487305 Val_KL : 3.430608868598938\n","Epoch: 874/8000  Traning Loss: 103.46053218841553  Train_Reconstruction: 99.99537181854248  Train_KL: 3.4651595652103424  Validation Loss : 101.06059646606445 Val_Reconstruction : 97.64029312133789 Val_KL : 3.4203025102615356\n","Epoch: 875/8000  Traning Loss: 103.55048942565918  Train_Reconstruction: 100.09357452392578  Train_KL: 3.456914573907852  Validation Loss : 101.29682922363281 Val_Reconstruction : 97.85912704467773 Val_KL : 3.437702775001526\n","Epoch: 876/8000  Traning Loss: 103.4907455444336  Train_Reconstruction: 100.03660678863525  Train_KL: 3.4541370272636414  Validation Loss : 101.0042610168457 Val_Reconstruction : 97.60464477539062 Val_KL : 3.3996158838272095\n","Epoch: 877/8000  Traning Loss: 103.37626361846924  Train_Reconstruction: 99.92491054534912  Train_KL: 3.451352298259735  Validation Loss : 100.97185516357422 Val_Reconstruction : 97.54618453979492 Val_KL : 3.4256722927093506\n","Epoch: 878/8000  Traning Loss: 103.34796810150146  Train_Reconstruction: 99.90557193756104  Train_KL: 3.442397326231003  Validation Loss : 100.88758850097656 Val_Reconstruction : 97.48507690429688 Val_KL : 3.4025124311447144\n","Epoch: 879/8000  Traning Loss: 103.38364505767822  Train_Reconstruction: 99.9365406036377  Train_KL: 3.447105288505554  Validation Loss : 101.04964828491211 Val_Reconstruction : 97.62703323364258 Val_KL : 3.422614812850952\n","Epoch: 880/8000  Traning Loss: 103.40663146972656  Train_Reconstruction: 99.95899486541748  Train_KL: 3.4476376473903656  Validation Loss : 101.09075927734375 Val_Reconstruction : 97.6764030456543 Val_KL : 3.414356827735901\n","Epoch: 881/8000  Traning Loss: 103.8449935913086  Train_Reconstruction: 100.4040174484253  Train_KL: 3.440975159406662  Validation Loss : 101.63274765014648 Val_Reconstruction : 98.22042465209961 Val_KL : 3.412323236465454\n","Epoch: 882/8000  Traning Loss: 103.33675861358643  Train_Reconstruction: 99.87995910644531  Train_KL: 3.4567987620830536  Validation Loss : 100.73636245727539 Val_Reconstruction : 97.31595230102539 Val_KL : 3.4204113483428955\n","Epoch: 883/8000  Traning Loss: 103.03482437133789  Train_Reconstruction: 99.58577823638916  Train_KL: 3.4490458965301514  Validation Loss : 101.03219604492188 Val_Reconstruction : 97.61539840698242 Val_KL : 3.41679847240448\n","Epoch: 884/8000  Traning Loss: 103.2794828414917  Train_Reconstruction: 99.83164691925049  Train_KL: 3.447835624217987  Validation Loss : 101.39733123779297 Val_Reconstruction : 97.98527526855469 Val_KL : 3.4120556116104126\n","Epoch: 885/8000  Traning Loss: 103.61550521850586  Train_Reconstruction: 100.17409229278564  Train_KL: 3.4414124488830566  Validation Loss : 101.61438751220703 Val_Reconstruction : 98.20509719848633 Val_KL : 3.409291386604309\n","Epoch: 886/8000  Traning Loss: 103.39385414123535  Train_Reconstruction: 99.94388103485107  Train_KL: 3.449973851442337  Validation Loss : 100.81818771362305 Val_Reconstruction : 97.39190292358398 Val_KL : 3.4262858629226685\n","Epoch: 887/8000  Traning Loss: 103.24478816986084  Train_Reconstruction: 99.7918815612793  Train_KL: 3.452907621860504  Validation Loss : 101.01213836669922 Val_Reconstruction : 97.61783218383789 Val_KL : 3.394303798675537\n","Epoch: 888/8000  Traning Loss: 103.41154193878174  Train_Reconstruction: 99.96745586395264  Train_KL: 3.4440853893756866  Validation Loss : 101.14280700683594 Val_Reconstruction : 97.71436309814453 Val_KL : 3.4284437894821167\n","Epoch: 889/8000  Traning Loss: 103.29540634155273  Train_Reconstruction: 99.82886409759521  Train_KL: 3.466542214155197  Validation Loss : 100.75073623657227 Val_Reconstruction : 97.3243637084961 Val_KL : 3.4263747930526733\n","Epoch: 890/8000  Traning Loss: 103.1595811843872  Train_Reconstruction: 99.71875667572021  Train_KL: 3.440825581550598  Validation Loss : 100.88993453979492 Val_Reconstruction : 97.48588562011719 Val_KL : 3.4040502309799194\n","Epoch: 891/8000  Traning Loss: 103.28004360198975  Train_Reconstruction: 99.83117198944092  Train_KL: 3.4488710463047028  Validation Loss : 100.92433547973633 Val_Reconstruction : 97.51816177368164 Val_KL : 3.4061721563339233\n","Epoch: 892/8000  Traning Loss: 103.2586555480957  Train_Reconstruction: 99.81187725067139  Train_KL: 3.4467784762382507  Validation Loss : 100.72650909423828 Val_Reconstruction : 97.29650497436523 Val_KL : 3.430004835128784\n","Epoch: 893/8000  Traning Loss: 103.50234031677246  Train_Reconstruction: 100.05724239349365  Train_KL: 3.4450989365577698  Validation Loss : 101.13742446899414 Val_Reconstruction : 97.73347854614258 Val_KL : 3.403944492340088\n","Epoch: 894/8000  Traning Loss: 103.52541732788086  Train_Reconstruction: 100.05675315856934  Train_KL: 3.4686646461486816  Validation Loss : 101.05509185791016 Val_Reconstruction : 97.60881042480469 Val_KL : 3.4462825059890747\n","Epoch: 895/8000  Traning Loss: 103.19811916351318  Train_Reconstruction: 99.74322986602783  Train_KL: 3.4548897445201874  Validation Loss : 100.95124053955078 Val_Reconstruction : 97.55270385742188 Val_KL : 3.3985373973846436\n","Epoch: 896/8000  Traning Loss: 103.38174247741699  Train_Reconstruction: 99.93100070953369  Train_KL: 3.4507409036159515  Validation Loss : 101.10383224487305 Val_Reconstruction : 97.66924667358398 Val_KL : 3.434587836265564\n","Epoch: 897/8000  Traning Loss: 103.3325662612915  Train_Reconstruction: 99.8776798248291  Train_KL: 3.45488640666008  Validation Loss : 100.97621154785156 Val_Reconstruction : 97.5680160522461 Val_KL : 3.408195972442627\n","Epoch: 898/8000  Traning Loss: 103.21192932128906  Train_Reconstruction: 99.7850112915039  Train_KL: 3.4269192218780518  Validation Loss : 100.78623580932617 Val_Reconstruction : 97.3921127319336 Val_KL : 3.3941233158111572\n","Epoch: 899/8000  Traning Loss: 103.22219181060791  Train_Reconstruction: 99.7640027999878  Train_KL: 3.4581886529922485  Validation Loss : 100.8750228881836 Val_Reconstruction : 97.43402862548828 Val_KL : 3.440993547439575\n","Epoch: 900/8000  Traning Loss: 103.34451866149902  Train_Reconstruction: 99.8828239440918  Train_KL: 3.4616944789886475  Validation Loss : 101.0862808227539 Val_Reconstruction : 97.67580795288086 Val_KL : 3.410473585128784\n","Epoch: 901/8000  Traning Loss: 103.10251045227051  Train_Reconstruction: 99.64984226226807  Train_KL: 3.4526672959327698  Validation Loss : 100.56342697143555 Val_Reconstruction : 97.14386749267578 Val_KL : 3.4195609092712402\n","Epoch: 902/8000  Traning Loss: 103.00517177581787  Train_Reconstruction: 99.54983806610107  Train_KL: 3.4553345143795013  Validation Loss : 100.57383346557617 Val_Reconstruction : 97.15185928344727 Val_KL : 3.421974539756775\n","Epoch: 903/8000  Traning Loss: 102.93833255767822  Train_Reconstruction: 99.47553730010986  Train_KL: 3.4627948701381683  Validation Loss : 100.72506713867188 Val_Reconstruction : 97.30928802490234 Val_KL : 3.4157785177230835\n","Epoch: 904/8000  Traning Loss: 103.11021327972412  Train_Reconstruction: 99.65538501739502  Train_KL: 3.4548279643058777  Validation Loss : 101.05083847045898 Val_Reconstruction : 97.62388610839844 Val_KL : 3.4269551038742065\n","Epoch: 905/8000  Traning Loss: 103.31480693817139  Train_Reconstruction: 99.8499345779419  Train_KL: 3.464871436357498  Validation Loss : 101.12602233886719 Val_Reconstruction : 97.69816207885742 Val_KL : 3.4278606176376343\n","Epoch: 906/8000  Traning Loss: 103.39669895172119  Train_Reconstruction: 99.9340467453003  Train_KL: 3.4626523554325104  Validation Loss : 101.32025146484375 Val_Reconstruction : 97.8913459777832 Val_KL : 3.4289069175720215\n","Epoch: 907/8000  Traning Loss: 103.47923183441162  Train_Reconstruction: 100.02293300628662  Train_KL: 3.456299990415573  Validation Loss : 101.26459121704102 Val_Reconstruction : 97.83712005615234 Val_KL : 3.4274739027023315\n","Epoch: 908/8000  Traning Loss: 103.50965404510498  Train_Reconstruction: 100.04949569702148  Train_KL: 3.46015727519989  Validation Loss : 101.07511138916016 Val_Reconstruction : 97.63659286499023 Val_KL : 3.438517451286316\n","Epoch: 909/8000  Traning Loss: 103.1679048538208  Train_Reconstruction: 99.70090198516846  Train_KL: 3.4670033156871796  Validation Loss : 100.89791107177734 Val_Reconstruction : 97.46687316894531 Val_KL : 3.431037664413452\n","Epoch: 910/8000  Traning Loss: 103.58547687530518  Train_Reconstruction: 100.1390438079834  Train_KL: 3.446433365345001  Validation Loss : 101.7243766784668 Val_Reconstruction : 98.31122207641602 Val_KL : 3.413155198097229\n","Epoch: 911/8000  Traning Loss: 103.66111850738525  Train_Reconstruction: 100.20945930480957  Train_KL: 3.451658934354782  Validation Loss : 101.26497650146484 Val_Reconstruction : 97.84764862060547 Val_KL : 3.4173280000686646\n","Epoch: 912/8000  Traning Loss: 103.17563533782959  Train_Reconstruction: 99.7143087387085  Train_KL: 3.4613271057605743  Validation Loss : 100.96627807617188 Val_Reconstruction : 97.53450012207031 Val_KL : 3.4317785501480103\n","Epoch: 913/8000  Traning Loss: 103.40998935699463  Train_Reconstruction: 99.96106815338135  Train_KL: 3.448920726776123  Validation Loss : 101.2426643371582 Val_Reconstruction : 97.83172607421875 Val_KL : 3.4109389781951904\n","Epoch: 914/8000  Traning Loss: 103.46831703186035  Train_Reconstruction: 100.0180311203003  Train_KL: 3.4502869248390198  Validation Loss : 101.406494140625 Val_Reconstruction : 97.98783111572266 Val_KL : 3.4186625480651855\n","Epoch: 915/8000  Traning Loss: 103.16079044342041  Train_Reconstruction: 99.71678161621094  Train_KL: 3.4440080523490906  Validation Loss : 100.77432250976562 Val_Reconstruction : 97.36266326904297 Val_KL : 3.4116604328155518\n","Epoch: 916/8000  Traning Loss: 102.97986125946045  Train_Reconstruction: 99.5277795791626  Train_KL: 3.4520816802978516  Validation Loss : 101.01602935791016 Val_Reconstruction : 97.59489822387695 Val_KL : 3.4211333990097046\n","Epoch: 917/8000  Traning Loss: 103.13971614837646  Train_Reconstruction: 99.67701816558838  Train_KL: 3.4626984000205994  Validation Loss : 101.41273880004883 Val_Reconstruction : 97.98486328125 Val_KL : 3.4278749227523804\n","Epoch: 918/8000  Traning Loss: 103.52299308776855  Train_Reconstruction: 100.06727695465088  Train_KL: 3.4557164311408997  Validation Loss : 101.04779815673828 Val_Reconstruction : 97.62537384033203 Val_KL : 3.422425389289856\n","Epoch: 919/8000  Traning Loss: 103.15522480010986  Train_Reconstruction: 99.6896162033081  Train_KL: 3.4656085669994354  Validation Loss : 100.75481414794922 Val_Reconstruction : 97.32419967651367 Val_KL : 3.430613160133362\n","Epoch: 920/8000  Traning Loss: 102.88270664215088  Train_Reconstruction: 99.4321346282959  Train_KL: 3.450572371482849  Validation Loss : 100.61048126220703 Val_Reconstruction : 97.18821334838867 Val_KL : 3.4222711324691772\n","Epoch: 921/8000  Traning Loss: 102.85200786590576  Train_Reconstruction: 99.39719581604004  Train_KL: 3.4548125863075256  Validation Loss : 100.77128601074219 Val_Reconstruction : 97.3393440246582 Val_KL : 3.4319422245025635\n","Epoch: 922/8000  Traning Loss: 103.13693332672119  Train_Reconstruction: 99.67642784118652  Train_KL: 3.460505485534668  Validation Loss : 100.97757720947266 Val_Reconstruction : 97.5561294555664 Val_KL : 3.4214454889297485\n","Epoch: 923/8000  Traning Loss: 102.92400741577148  Train_Reconstruction: 99.47667217254639  Train_KL: 3.4473348259925842  Validation Loss : 100.68460845947266 Val_Reconstruction : 97.27159881591797 Val_KL : 3.4130114316940308\n","Epoch: 924/8000  Traning Loss: 102.86668968200684  Train_Reconstruction: 99.41965103149414  Train_KL: 3.4470378160476685  Validation Loss : 100.58165740966797 Val_Reconstruction : 97.17824172973633 Val_KL : 3.403412342071533\n","Epoch: 925/8000  Traning Loss: 102.79904270172119  Train_Reconstruction: 99.35761070251465  Train_KL: 3.441431403160095  Validation Loss : 100.30155944824219 Val_Reconstruction : 96.90133285522461 Val_KL : 3.4002280235290527\n","Epoch: 926/8000  Traning Loss: 102.8925609588623  Train_Reconstruction: 99.44223976135254  Train_KL: 3.450321614742279  Validation Loss : 100.87764358520508 Val_Reconstruction : 97.43753433227539 Val_KL : 3.440108060836792\n","Epoch: 927/8000  Traning Loss: 103.0593957901001  Train_Reconstruction: 99.59462356567383  Train_KL: 3.464772790670395  Validation Loss : 100.64141845703125 Val_Reconstruction : 97.21348571777344 Val_KL : 3.427931785583496\n","Epoch: 928/8000  Traning Loss: 103.05340099334717  Train_Reconstruction: 99.60447311401367  Train_KL: 3.4489279091358185  Validation Loss : 101.04873657226562 Val_Reconstruction : 97.64194869995117 Val_KL : 3.4067848920822144\n","Epoch: 929/8000  Traning Loss: 102.84592056274414  Train_Reconstruction: 99.40589618682861  Train_KL: 3.440023124217987  Validation Loss : 100.32540893554688 Val_Reconstruction : 96.9137191772461 Val_KL : 3.411692261695862\n","Epoch: 930/8000  Traning Loss: 102.94094848632812  Train_Reconstruction: 99.49697875976562  Train_KL: 3.443970501422882  Validation Loss : 100.90037155151367 Val_Reconstruction : 97.48716735839844 Val_KL : 3.413204312324524\n","Epoch: 931/8000  Traning Loss: 103.12392616271973  Train_Reconstruction: 99.64577198028564  Train_KL: 3.4781543016433716  Validation Loss : 100.71617126464844 Val_Reconstruction : 97.25745391845703 Val_KL : 3.4587182998657227\n","Epoch: 932/8000  Traning Loss: 103.10159873962402  Train_Reconstruction: 99.63591766357422  Train_KL: 3.4656819105148315  Validation Loss : 100.72876358032227 Val_Reconstruction : 97.30952835083008 Val_KL : 3.4192333221435547\n","Epoch: 933/8000  Traning Loss: 102.89264869689941  Train_Reconstruction: 99.42887783050537  Train_KL: 3.4637705385684967  Validation Loss : 100.71801376342773 Val_Reconstruction : 97.28921508789062 Val_KL : 3.4287999868392944\n","Epoch: 934/8000  Traning Loss: 102.95259189605713  Train_Reconstruction: 99.51337146759033  Train_KL: 3.4392193257808685  Validation Loss : 100.39581680297852 Val_Reconstruction : 96.97919464111328 Val_KL : 3.4166245460510254\n","Epoch: 935/8000  Traning Loss: 102.66861915588379  Train_Reconstruction: 99.21337699890137  Train_KL: 3.455241084098816  Validation Loss : 100.56132507324219 Val_Reconstruction : 97.15621185302734 Val_KL : 3.405111789703369\n","Epoch: 936/8000  Traning Loss: 102.80248832702637  Train_Reconstruction: 99.36218452453613  Train_KL: 3.4403035938739777  Validation Loss : 100.56334686279297 Val_Reconstruction : 97.13374328613281 Val_KL : 3.429604649543762\n","Epoch: 937/8000  Traning Loss: 102.69674396514893  Train_Reconstruction: 99.23523807525635  Train_KL: 3.461505353450775  Validation Loss : 100.21830368041992 Val_Reconstruction : 96.8068962097168 Val_KL : 3.4114071130752563\n","Epoch: 938/8000  Traning Loss: 102.92238998413086  Train_Reconstruction: 99.47266292572021  Train_KL: 3.4497268199920654  Validation Loss : 100.67508697509766 Val_Reconstruction : 97.2624626159668 Val_KL : 3.4126263856887817\n","Epoch: 939/8000  Traning Loss: 103.00591468811035  Train_Reconstruction: 99.56552314758301  Train_KL: 3.4403910040855408  Validation Loss : 100.68151092529297 Val_Reconstruction : 97.27526092529297 Val_KL : 3.4062501192092896\n","Epoch: 940/8000  Traning Loss: 102.95887565612793  Train_Reconstruction: 99.50956439971924  Train_KL: 3.4493104219436646  Validation Loss : 101.26153945922852 Val_Reconstruction : 97.83560562133789 Val_KL : 3.4259321689605713\n","Epoch: 941/8000  Traning Loss: 103.29743576049805  Train_Reconstruction: 99.85227012634277  Train_KL: 3.445164740085602  Validation Loss : 101.10240173339844 Val_Reconstruction : 97.69190979003906 Val_KL : 3.410492181777954\n","Epoch: 942/8000  Traning Loss: 103.18505954742432  Train_Reconstruction: 99.73401165008545  Train_KL: 3.451047420501709  Validation Loss : 100.98806381225586 Val_Reconstruction : 97.5682373046875 Val_KL : 3.419827103614807\n","Epoch: 943/8000  Traning Loss: 102.83160591125488  Train_Reconstruction: 99.3646411895752  Train_KL: 3.4669635593891144  Validation Loss : 100.43281173706055 Val_Reconstruction : 96.99151992797852 Val_KL : 3.4412944316864014\n","Epoch: 944/8000  Traning Loss: 102.89941692352295  Train_Reconstruction: 99.41649913787842  Train_KL: 3.4829185903072357  Validation Loss : 100.49239730834961 Val_Reconstruction : 97.07203674316406 Val_KL : 3.4203609228134155\n","Epoch: 945/8000  Traning Loss: 102.74270153045654  Train_Reconstruction: 99.30124568939209  Train_KL: 3.4414549469947815  Validation Loss : 100.4417953491211 Val_Reconstruction : 97.04130172729492 Val_KL : 3.400493025779724\n","Epoch: 946/8000  Traning Loss: 102.59383773803711  Train_Reconstruction: 99.13470554351807  Train_KL: 3.4591317176818848  Validation Loss : 100.21522521972656 Val_Reconstruction : 96.80022430419922 Val_KL : 3.4149996042251587\n","Epoch: 947/8000  Traning Loss: 102.46675395965576  Train_Reconstruction: 99.03222465515137  Train_KL: 3.434528261423111  Validation Loss : 100.31403732299805 Val_Reconstruction : 96.91039657592773 Val_KL : 3.4036412239074707\n","Epoch: 948/8000  Traning Loss: 102.67281246185303  Train_Reconstruction: 99.22324752807617  Train_KL: 3.4495646953582764  Validation Loss : 100.49974822998047 Val_Reconstruction : 97.08158874511719 Val_KL : 3.418159008026123\n","Epoch: 949/8000  Traning Loss: 102.63894176483154  Train_Reconstruction: 99.17756652832031  Train_KL: 3.4613757133483887  Validation Loss : 100.33855819702148 Val_Reconstruction : 96.91688537597656 Val_KL : 3.4216734170913696\n","Epoch: 950/8000  Traning Loss: 102.83020973205566  Train_Reconstruction: 99.38345718383789  Train_KL: 3.446752190589905  Validation Loss : 100.88214111328125 Val_Reconstruction : 97.47991180419922 Val_KL : 3.402229070663452\n","Epoch: 951/8000  Traning Loss: 103.18102264404297  Train_Reconstruction: 99.72715091705322  Train_KL: 3.4538706243038177  Validation Loss : 100.83615493774414 Val_Reconstruction : 97.41217041015625 Val_KL : 3.423986554145813\n","Epoch: 952/8000  Traning Loss: 102.94221019744873  Train_Reconstruction: 99.49787712097168  Train_KL: 3.4443338811397552  Validation Loss : 100.82326126098633 Val_Reconstruction : 97.40808486938477 Val_KL : 3.4151771068573\n","Epoch: 953/8000  Traning Loss: 102.64821243286133  Train_Reconstruction: 99.20291137695312  Train_KL: 3.445300370454788  Validation Loss : 100.3060531616211 Val_Reconstruction : 96.8885383605957 Val_KL : 3.417515277862549\n","Epoch: 954/8000  Traning Loss: 102.61697673797607  Train_Reconstruction: 99.1807222366333  Train_KL: 3.4362537562847137  Validation Loss : 100.74145126342773 Val_Reconstruction : 97.34128189086914 Val_KL : 3.400169849395752\n","Epoch: 955/8000  Traning Loss: 102.98096561431885  Train_Reconstruction: 99.53672409057617  Train_KL: 3.44424307346344  Validation Loss : 100.41669464111328 Val_Reconstruction : 97.00723266601562 Val_KL : 3.409460186958313\n","Epoch: 956/8000  Traning Loss: 102.6881103515625  Train_Reconstruction: 99.23284816741943  Train_KL: 3.4552628695964813  Validation Loss : 100.55850982666016 Val_Reconstruction : 97.11763763427734 Val_KL : 3.4408726692199707\n","Epoch: 957/8000  Traning Loss: 102.62766551971436  Train_Reconstruction: 99.16614246368408  Train_KL: 3.4615232348442078  Validation Loss : 100.36370086669922 Val_Reconstruction : 96.93990707397461 Val_KL : 3.42379629611969\n","Epoch: 958/8000  Traning Loss: 102.61835670471191  Train_Reconstruction: 99.15948963165283  Train_KL: 3.4588664174079895  Validation Loss : 100.40326309204102 Val_Reconstruction : 96.99053573608398 Val_KL : 3.4127286672592163\n","Epoch: 959/8000  Traning Loss: 102.70728397369385  Train_Reconstruction: 99.25284004211426  Train_KL: 3.454444468021393  Validation Loss : 100.5933952331543 Val_Reconstruction : 97.1743278503418 Val_KL : 3.419064998626709\n","Epoch: 960/8000  Traning Loss: 102.66789627075195  Train_Reconstruction: 99.21811580657959  Train_KL: 3.449780821800232  Validation Loss : 100.31634902954102 Val_Reconstruction : 96.90398025512695 Val_KL : 3.4123690128326416\n","Epoch: 961/8000  Traning Loss: 102.51655197143555  Train_Reconstruction: 99.05959987640381  Train_KL: 3.4569508731365204  Validation Loss : 100.0401382446289 Val_Reconstruction : 96.60405731201172 Val_KL : 3.4360833168029785\n","Epoch: 962/8000  Traning Loss: 102.66867733001709  Train_Reconstruction: 99.2252368927002  Train_KL: 3.4434405863285065  Validation Loss : 100.75438690185547 Val_Reconstruction : 97.35197830200195 Val_KL : 3.402409553527832\n","Epoch: 963/8000  Traning Loss: 102.6277723312378  Train_Reconstruction: 99.17411804199219  Train_KL: 3.453654170036316  Validation Loss : 100.25600051879883 Val_Reconstruction : 96.82559204101562 Val_KL : 3.4304081201553345\n","Epoch: 964/8000  Traning Loss: 102.7674207687378  Train_Reconstruction: 99.30671215057373  Train_KL: 3.4607078433036804  Validation Loss : 100.82940292358398 Val_Reconstruction : 97.40848159790039 Val_KL : 3.4209223985671997\n","Epoch: 965/8000  Traning Loss: 103.00044441223145  Train_Reconstruction: 99.54641437530518  Train_KL: 3.454029470682144  Validation Loss : 100.94211196899414 Val_Reconstruction : 97.52720642089844 Val_KL : 3.4149049520492554\n","Epoch: 966/8000  Traning Loss: 102.60534763336182  Train_Reconstruction: 99.1663408279419  Train_KL: 3.4390057623386383  Validation Loss : 100.52570343017578 Val_Reconstruction : 97.11233901977539 Val_KL : 3.4133628606796265\n","Epoch: 967/8000  Traning Loss: 102.48430252075195  Train_Reconstruction: 99.01364231109619  Train_KL: 3.4706602692604065  Validation Loss : 100.09089660644531 Val_Reconstruction : 96.65634155273438 Val_KL : 3.4345531463623047\n","Epoch: 968/8000  Traning Loss: 102.44311904907227  Train_Reconstruction: 98.9911470413208  Train_KL: 3.4519726037979126  Validation Loss : 100.44161605834961 Val_Reconstruction : 97.0438461303711 Val_KL : 3.397769331932068\n","Epoch: 969/8000  Traning Loss: 102.44795417785645  Train_Reconstruction: 98.99264717102051  Train_KL: 3.455306738615036  Validation Loss : 100.126220703125 Val_Reconstruction : 96.68859100341797 Val_KL : 3.4376301765441895\n","Epoch: 970/8000  Traning Loss: 102.60192680358887  Train_Reconstruction: 99.14568519592285  Train_KL: 3.456240713596344  Validation Loss : 100.4932746887207 Val_Reconstruction : 97.08773040771484 Val_KL : 3.40554141998291\n","Epoch: 971/8000  Traning Loss: 102.75335788726807  Train_Reconstruction: 99.30796146392822  Train_KL: 3.445396602153778  Validation Loss : 100.33366394042969 Val_Reconstruction : 96.92407608032227 Val_KL : 3.4095886945724487\n","Epoch: 972/8000  Traning Loss: 102.56168842315674  Train_Reconstruction: 99.09927463531494  Train_KL: 3.4624133706092834  Validation Loss : 100.22565460205078 Val_Reconstruction : 96.81155776977539 Val_KL : 3.4140950441360474\n","Epoch: 973/8000  Traning Loss: 102.39768886566162  Train_Reconstruction: 98.95021438598633  Train_KL: 3.4474750757217407  Validation Loss : 100.19796371459961 Val_Reconstruction : 96.7801513671875 Val_KL : 3.4178131818771362\n","Epoch: 974/8000  Traning Loss: 102.53492546081543  Train_Reconstruction: 99.08183670043945  Train_KL: 3.453090101480484  Validation Loss : 100.33272552490234 Val_Reconstruction : 96.91787338256836 Val_KL : 3.4148519039154053\n","Epoch: 975/8000  Traning Loss: 102.40751552581787  Train_Reconstruction: 98.95560836791992  Train_KL: 3.451908230781555  Validation Loss : 100.19951248168945 Val_Reconstruction : 96.7740707397461 Val_KL : 3.4254422187805176\n","Epoch: 976/8000  Traning Loss: 102.67893505096436  Train_Reconstruction: 99.2349910736084  Train_KL: 3.443943291902542  Validation Loss : 100.64260482788086 Val_Reconstruction : 97.23386001586914 Val_KL : 3.4087454080581665\n","Epoch: 977/8000  Traning Loss: 102.62424182891846  Train_Reconstruction: 99.17137813568115  Train_KL: 3.4528634548187256  Validation Loss : 100.62643814086914 Val_Reconstruction : 97.19852066040039 Val_KL : 3.427920937538147\n","Epoch: 978/8000  Traning Loss: 102.61351585388184  Train_Reconstruction: 99.17076110839844  Train_KL: 3.442755252122879  Validation Loss : 100.28567123413086 Val_Reconstruction : 96.8817253112793 Val_KL : 3.4039448499679565\n","Epoch: 979/8000  Traning Loss: 102.60585975646973  Train_Reconstruction: 99.15883445739746  Train_KL: 3.447024315595627  Validation Loss : 100.87818908691406 Val_Reconstruction : 97.46039962768555 Val_KL : 3.4177902936935425\n","Epoch: 980/8000  Traning Loss: 102.70883369445801  Train_Reconstruction: 99.24974632263184  Train_KL: 3.4590872824192047  Validation Loss : 100.4002685546875 Val_Reconstruction : 96.98007583618164 Val_KL : 3.4201942682266235\n","Epoch: 981/8000  Traning Loss: 102.61179637908936  Train_Reconstruction: 99.15783214569092  Train_KL: 3.453963041305542  Validation Loss : 100.96480941772461 Val_Reconstruction : 97.54731750488281 Val_KL : 3.417493462562561\n","Epoch: 982/8000  Traning Loss: 102.4916582107544  Train_Reconstruction: 99.04062175750732  Train_KL: 3.4510360062122345  Validation Loss : 100.24044799804688 Val_Reconstruction : 96.83922958374023 Val_KL : 3.4012198448181152\n","Epoch: 983/8000  Traning Loss: 102.1876802444458  Train_Reconstruction: 98.7398509979248  Train_KL: 3.447829633951187  Validation Loss : 99.91615295410156 Val_Reconstruction : 96.5057601928711 Val_KL : 3.410390615463257\n","Epoch: 984/8000  Traning Loss: 102.1510238647461  Train_Reconstruction: 98.7125129699707  Train_KL: 3.438510924577713  Validation Loss : 99.80335235595703 Val_Reconstruction : 96.39498901367188 Val_KL : 3.4083645343780518\n","Epoch: 985/8000  Traning Loss: 102.28475952148438  Train_Reconstruction: 98.83030033111572  Train_KL: 3.454459011554718  Validation Loss : 100.12286758422852 Val_Reconstruction : 96.70074844360352 Val_KL : 3.4221185445785522\n","Epoch: 986/8000  Traning Loss: 102.48490715026855  Train_Reconstruction: 99.02396392822266  Train_KL: 3.46094286441803  Validation Loss : 100.54530715942383 Val_Reconstruction : 97.09785842895508 Val_KL : 3.4474496841430664\n","Epoch: 987/8000  Traning Loss: 103.0648021697998  Train_Reconstruction: 99.60452270507812  Train_KL: 3.460280030965805  Validation Loss : 101.43077087402344 Val_Reconstruction : 98.01247024536133 Val_KL : 3.4183017015457153\n","Epoch: 988/8000  Traning Loss: 102.55715847015381  Train_Reconstruction: 99.09381675720215  Train_KL: 3.4633410274982452  Validation Loss : 100.27116394042969 Val_Reconstruction : 96.83551025390625 Val_KL : 3.4356508255004883\n","Epoch: 989/8000  Traning Loss: 101.99409866333008  Train_Reconstruction: 98.54054832458496  Train_KL: 3.4535494446754456  Validation Loss : 99.6822738647461 Val_Reconstruction : 96.25702285766602 Val_KL : 3.4252517223358154\n","Epoch: 990/8000  Traning Loss: 102.03034496307373  Train_Reconstruction: 98.57346534729004  Train_KL: 3.4568794071674347  Validation Loss : 99.98047637939453 Val_Reconstruction : 96.55702590942383 Val_KL : 3.4234508275985718\n","Epoch: 991/8000  Traning Loss: 102.05482196807861  Train_Reconstruction: 98.60552883148193  Train_KL: 3.4492943584918976  Validation Loss : 99.93394470214844 Val_Reconstruction : 96.5217399597168 Val_KL : 3.412207007408142\n","Epoch: 992/8000  Traning Loss: 102.16323184967041  Train_Reconstruction: 98.69330215454102  Train_KL: 3.4699289798736572  Validation Loss : 100.01094436645508 Val_Reconstruction : 96.56941986083984 Val_KL : 3.4415228366851807\n","Epoch: 993/8000  Traning Loss: 102.34334754943848  Train_Reconstruction: 98.88736534118652  Train_KL: 3.4559808373451233  Validation Loss : 100.56624984741211 Val_Reconstruction : 97.15593719482422 Val_KL : 3.4103143215179443\n","Epoch: 994/8000  Traning Loss: 102.71886157989502  Train_Reconstruction: 99.2875623703003  Train_KL: 3.431298941373825  Validation Loss : 100.63309860229492 Val_Reconstruction : 97.2489128112793 Val_KL : 3.384187936782837\n","Epoch: 995/8000  Traning Loss: 102.5740270614624  Train_Reconstruction: 99.1258316040039  Train_KL: 3.4481943547725677  Validation Loss : 100.6072883605957 Val_Reconstruction : 97.17280960083008 Val_KL : 3.434475541114807\n","Epoch: 996/8000  Traning Loss: 102.46751403808594  Train_Reconstruction: 99.00911617279053  Train_KL: 3.4583974182605743  Validation Loss : 100.63454818725586 Val_Reconstruction : 97.22994232177734 Val_KL : 3.404604911804199\n","Epoch: 997/8000  Traning Loss: 102.36084270477295  Train_Reconstruction: 98.9038610458374  Train_KL: 3.4569825530052185  Validation Loss : 100.0782356262207 Val_Reconstruction : 96.64509963989258 Val_KL : 3.433137536048889\n","Epoch: 998/8000  Traning Loss: 102.1963062286377  Train_Reconstruction: 98.74256038665771  Train_KL: 3.4537460803985596  Validation Loss : 99.92203903198242 Val_Reconstruction : 96.50149917602539 Val_KL : 3.420538067817688\n","Epoch: 999/8000  Traning Loss: 102.06544589996338  Train_Reconstruction: 98.60923767089844  Train_KL: 3.456209182739258  Validation Loss : 99.85526275634766 Val_Reconstruction : 96.44458389282227 Val_KL : 3.4106807708740234\n","Epoch: 1000/8000  Traning Loss: 102.08318996429443  Train_Reconstruction: 98.63665390014648  Train_KL: 3.4465366303920746  Validation Loss : 99.9857177734375 Val_Reconstruction : 96.56344985961914 Val_KL : 3.4222670793533325\n","Epoch: 1001/8000  Traning Loss: 102.16447925567627  Train_Reconstruction: 98.7178373336792  Train_KL: 3.446641981601715  Validation Loss : 100.08549118041992 Val_Reconstruction : 96.67869567871094 Val_KL : 3.4067975282669067\n","Epoch: 1002/8000  Traning Loss: 102.73584175109863  Train_Reconstruction: 99.27595233917236  Train_KL: 3.459889680147171  Validation Loss : 100.73393630981445 Val_Reconstruction : 97.28485488891602 Val_KL : 3.449080228805542\n","Epoch: 1003/8000  Traning Loss: 102.94804763793945  Train_Reconstruction: 99.48152828216553  Train_KL: 3.4665195643901825  Validation Loss : 100.62017822265625 Val_Reconstruction : 97.20606231689453 Val_KL : 3.41411554813385\n","Epoch: 1004/8000  Traning Loss: 102.88543224334717  Train_Reconstruction: 99.41448497772217  Train_KL: 3.470945656299591  Validation Loss : 100.37390518188477 Val_Reconstruction : 96.93975448608398 Val_KL : 3.434148073196411\n","Epoch: 1005/8000  Traning Loss: 103.12046241760254  Train_Reconstruction: 99.65952777862549  Train_KL: 3.4609349966049194  Validation Loss : 100.11836624145508 Val_Reconstruction : 96.70452117919922 Val_KL : 3.413844108581543\n","Epoch: 1006/8000  Traning Loss: 102.83574390411377  Train_Reconstruction: 99.38229751586914  Train_KL: 3.4534454941749573  Validation Loss : 100.12057113647461 Val_Reconstruction : 96.69761657714844 Val_KL : 3.4229533672332764\n","Epoch: 1007/8000  Traning Loss: 102.13833618164062  Train_Reconstruction: 98.68368911743164  Train_KL: 3.4546462893486023  Validation Loss : 99.69544219970703 Val_Reconstruction : 96.26438903808594 Val_KL : 3.4310550689697266\n","Epoch: 1008/8000  Traning Loss: 101.9710340499878  Train_Reconstruction: 98.51016426086426  Train_KL: 3.460869163274765  Validation Loss : 100.10413360595703 Val_Reconstruction : 96.68519592285156 Val_KL : 3.4189406633377075\n","Epoch: 1009/8000  Traning Loss: 102.51279640197754  Train_Reconstruction: 99.0719051361084  Train_KL: 3.4408910274505615  Validation Loss : 100.78873062133789 Val_Reconstruction : 97.36497497558594 Val_KL : 3.4237558841705322\n","Epoch: 1010/8000  Traning Loss: 102.70290184020996  Train_Reconstruction: 99.24171733856201  Train_KL: 3.4611838459968567  Validation Loss : 101.02582550048828 Val_Reconstruction : 97.59519958496094 Val_KL : 3.4306260347366333\n","Epoch: 1011/8000  Traning Loss: 103.28177070617676  Train_Reconstruction: 99.82594776153564  Train_KL: 3.455822229385376  Validation Loss : 100.87775421142578 Val_Reconstruction : 97.45022583007812 Val_KL : 3.4275306463241577\n","Epoch: 1012/8000  Traning Loss: 102.53504371643066  Train_Reconstruction: 99.07357120513916  Train_KL: 3.461471676826477  Validation Loss : 100.65565872192383 Val_Reconstruction : 97.2204818725586 Val_KL : 3.4351755380630493\n","Epoch: 1013/8000  Traning Loss: 102.09102630615234  Train_Reconstruction: 98.63573551177979  Train_KL: 3.4552902579307556  Validation Loss : 99.8440933227539 Val_Reconstruction : 96.4235954284668 Val_KL : 3.4204962253570557\n","Epoch: 1014/8000  Traning Loss: 102.07100200653076  Train_Reconstruction: 98.61978912353516  Train_KL: 3.451214075088501  Validation Loss : 99.96014022827148 Val_Reconstruction : 96.53794479370117 Val_KL : 3.422195553779602\n","Epoch: 1015/8000  Traning Loss: 102.03369045257568  Train_Reconstruction: 98.58282089233398  Train_KL: 3.4508684277534485  Validation Loss : 99.67734146118164 Val_Reconstruction : 96.2675666809082 Val_KL : 3.4097756147384644\n","Epoch: 1016/8000  Traning Loss: 102.00971698760986  Train_Reconstruction: 98.57370948791504  Train_KL: 3.436007469892502  Validation Loss : 99.77716445922852 Val_Reconstruction : 96.37174224853516 Val_KL : 3.4054198265075684\n","Epoch: 1017/8000  Traning Loss: 101.89877033233643  Train_Reconstruction: 98.44718265533447  Train_KL: 3.451587527990341  Validation Loss : 99.60971069335938 Val_Reconstruction : 96.1955451965332 Val_KL : 3.414164423942566\n","Epoch: 1018/8000  Traning Loss: 102.13391590118408  Train_Reconstruction: 98.67299842834473  Train_KL: 3.4609175324440002  Validation Loss : 100.21991348266602 Val_Reconstruction : 96.78269958496094 Val_KL : 3.4372142553329468\n","Epoch: 1019/8000  Traning Loss: 102.25715923309326  Train_Reconstruction: 98.79558563232422  Train_KL: 3.461571604013443  Validation Loss : 100.02143096923828 Val_Reconstruction : 96.60509872436523 Val_KL : 3.4163306951522827\n","Epoch: 1020/8000  Traning Loss: 102.19430255889893  Train_Reconstruction: 98.73826503753662  Train_KL: 3.4560362696647644  Validation Loss : 100.10417556762695 Val_Reconstruction : 96.67789840698242 Val_KL : 3.4262784719467163\n","Epoch: 1021/8000  Traning Loss: 101.97951984405518  Train_Reconstruction: 98.52577209472656  Train_KL: 3.4537472128868103  Validation Loss : 99.96625518798828 Val_Reconstruction : 96.55344009399414 Val_KL : 3.4128153324127197\n","Epoch: 1022/8000  Traning Loss: 102.2696180343628  Train_Reconstruction: 98.82549381256104  Train_KL: 3.44412362575531  Validation Loss : 100.22353744506836 Val_Reconstruction : 96.8089714050293 Val_KL : 3.41456937789917\n","Epoch: 1023/8000  Traning Loss: 102.12263870239258  Train_Reconstruction: 98.65285205841064  Train_KL: 3.4697844088077545  Validation Loss : 99.85698318481445 Val_Reconstruction : 96.40583419799805 Val_KL : 3.451146364212036\n","Epoch: 1024/8000  Traning Loss: 101.8987627029419  Train_Reconstruction: 98.44363021850586  Train_KL: 3.45513254404068  Validation Loss : 99.58886337280273 Val_Reconstruction : 96.16895294189453 Val_KL : 3.419912099838257\n","Epoch: 1025/8000  Traning Loss: 101.92992496490479  Train_Reconstruction: 98.45759391784668  Train_KL: 3.472330719232559  Validation Loss : 100.14661407470703 Val_Reconstruction : 96.69754409790039 Val_KL : 3.4490692615509033\n","Epoch: 1026/8000  Traning Loss: 102.03871536254883  Train_Reconstruction: 98.58844566345215  Train_KL: 3.450269967317581  Validation Loss : 99.90808486938477 Val_Reconstruction : 96.49762725830078 Val_KL : 3.410459280014038\n","Epoch: 1027/8000  Traning Loss: 101.8654260635376  Train_Reconstruction: 98.40670108795166  Train_KL: 3.458727240562439  Validation Loss : 99.82317352294922 Val_Reconstruction : 96.3882064819336 Val_KL : 3.4349660873413086\n","Epoch: 1028/8000  Traning Loss: 101.73871803283691  Train_Reconstruction: 98.2724256515503  Train_KL: 3.4662908613681793  Validation Loss : 99.72606658935547 Val_Reconstruction : 96.29819107055664 Val_KL : 3.4278753995895386\n","Epoch: 1029/8000  Traning Loss: 101.79719734191895  Train_Reconstruction: 98.33915710449219  Train_KL: 3.458039343357086  Validation Loss : 99.9241943359375 Val_Reconstruction : 96.50578689575195 Val_KL : 3.41840922832489\n","Epoch: 1030/8000  Traning Loss: 102.2471694946289  Train_Reconstruction: 98.80055046081543  Train_KL: 3.446619540452957  Validation Loss : 99.9234848022461 Val_Reconstruction : 96.52241897583008 Val_KL : 3.401065468788147\n","Epoch: 1031/8000  Traning Loss: 102.05619049072266  Train_Reconstruction: 98.6211051940918  Train_KL: 3.435084581375122  Validation Loss : 99.95880889892578 Val_Reconstruction : 96.55722427368164 Val_KL : 3.401585340499878\n","Epoch: 1032/8000  Traning Loss: 102.34632587432861  Train_Reconstruction: 98.88649368286133  Train_KL: 3.459832042455673  Validation Loss : 100.49480819702148 Val_Reconstruction : 97.04108810424805 Val_KL : 3.4537177085876465\n","Epoch: 1033/8000  Traning Loss: 102.85940074920654  Train_Reconstruction: 99.40524673461914  Train_KL: 3.454152911901474  Validation Loss : 101.59962844848633 Val_Reconstruction : 98.19635391235352 Val_KL : 3.4032716751098633\n","Epoch: 1034/8000  Traning Loss: 103.5298547744751  Train_Reconstruction: 100.0676851272583  Train_KL: 3.462169975042343  Validation Loss : 101.23000717163086 Val_Reconstruction : 97.79010009765625 Val_KL : 3.439908981323242\n","Epoch: 1035/8000  Traning Loss: 102.63377571105957  Train_Reconstruction: 99.18163013458252  Train_KL: 3.4521449506282806  Validation Loss : 100.41269302368164 Val_Reconstruction : 97.00584030151367 Val_KL : 3.4068514108657837\n","Epoch: 1036/8000  Traning Loss: 102.26303100585938  Train_Reconstruction: 98.81093215942383  Train_KL: 3.452099621295929  Validation Loss : 100.25104141235352 Val_Reconstruction : 96.81209182739258 Val_KL : 3.4389500617980957\n","Epoch: 1037/8000  Traning Loss: 102.20947360992432  Train_Reconstruction: 98.75163078308105  Train_KL: 3.4578439593315125  Validation Loss : 100.11010360717773 Val_Reconstruction : 96.703125 Val_KL : 3.406978964805603\n","Epoch: 1038/8000  Traning Loss: 102.43626022338867  Train_Reconstruction: 98.9813003540039  Train_KL: 3.4549604058265686  Validation Loss : 100.36659622192383 Val_Reconstruction : 96.93560409545898 Val_KL : 3.430991530418396\n","Epoch: 1039/8000  Traning Loss: 102.57777118682861  Train_Reconstruction: 99.12537574768066  Train_KL: 3.452395647764206  Validation Loss : 100.31643676757812 Val_Reconstruction : 96.89668273925781 Val_KL : 3.4197570085525513\n","Epoch: 1040/8000  Traning Loss: 102.13845539093018  Train_Reconstruction: 98.67725276947021  Train_KL: 3.461202472448349  Validation Loss : 100.2480697631836 Val_Reconstruction : 96.82136154174805 Val_KL : 3.426710844039917\n","Epoch: 1041/8000  Traning Loss: 101.84716510772705  Train_Reconstruction: 98.38963031768799  Train_KL: 3.457534432411194  Validation Loss : 99.71928405761719 Val_Reconstruction : 96.29214859008789 Val_KL : 3.4271373748779297\n","Epoch: 1042/8000  Traning Loss: 101.65260791778564  Train_Reconstruction: 98.1913652420044  Train_KL: 3.461242228746414  Validation Loss : 99.60325241088867 Val_Reconstruction : 96.17621612548828 Val_KL : 3.4270377159118652\n","Epoch: 1043/8000  Traning Loss: 101.99400424957275  Train_Reconstruction: 98.53200149536133  Train_KL: 3.462002456188202  Validation Loss : 100.19784164428711 Val_Reconstruction : 96.77819061279297 Val_KL : 3.4196503162384033\n","Epoch: 1044/8000  Traning Loss: 102.25124931335449  Train_Reconstruction: 98.79765319824219  Train_KL: 3.4535949528217316  Validation Loss : 100.06145858764648 Val_Reconstruction : 96.63639068603516 Val_KL : 3.42506742477417\n","Epoch: 1045/8000  Traning Loss: 101.89013195037842  Train_Reconstruction: 98.42567920684814  Train_KL: 3.464452713727951  Validation Loss : 99.6480941772461 Val_Reconstruction : 96.21003341674805 Val_KL : 3.4380617141723633\n","Epoch: 1046/8000  Traning Loss: 102.06110191345215  Train_Reconstruction: 98.60721778869629  Train_KL: 3.4538842141628265  Validation Loss : 100.11475372314453 Val_Reconstruction : 96.68826293945312 Val_KL : 3.4264895915985107\n","Epoch: 1047/8000  Traning Loss: 102.19530010223389  Train_Reconstruction: 98.73885154724121  Train_KL: 3.4564487636089325  Validation Loss : 99.91115188598633 Val_Reconstruction : 96.50365447998047 Val_KL : 3.4074960947036743\n","Epoch: 1048/8000  Traning Loss: 101.92834758758545  Train_Reconstruction: 98.4798994064331  Train_KL: 3.4484487771987915  Validation Loss : 99.59622573852539 Val_Reconstruction : 96.17516708374023 Val_KL : 3.421057939529419\n","Epoch: 1049/8000  Traning Loss: 101.73793601989746  Train_Reconstruction: 98.27855110168457  Train_KL: 3.459384709596634  Validation Loss : 99.76644897460938 Val_Reconstruction : 96.34712982177734 Val_KL : 3.4193190336227417\n","Epoch: 1050/8000  Traning Loss: 101.52491188049316  Train_Reconstruction: 98.06870937347412  Train_KL: 3.4562023282051086  Validation Loss : 99.39624786376953 Val_Reconstruction : 95.95137023925781 Val_KL : 3.44487464427948\n","Epoch: 1051/8000  Traning Loss: 101.66348552703857  Train_Reconstruction: 98.1900863647461  Train_KL: 3.4733998775482178  Validation Loss : 99.83260726928711 Val_Reconstruction : 96.40289688110352 Val_KL : 3.429708242416382\n","Epoch: 1052/8000  Traning Loss: 101.8309679031372  Train_Reconstruction: 98.37399673461914  Train_KL: 3.4569713175296783  Validation Loss : 99.29812240600586 Val_Reconstruction : 95.87486267089844 Val_KL : 3.42326021194458\n","Epoch: 1053/8000  Traning Loss: 101.5620174407959  Train_Reconstruction: 98.10958194732666  Train_KL: 3.4524368345737457  Validation Loss : 99.52067184448242 Val_Reconstruction : 96.12413787841797 Val_KL : 3.396535277366638\n","Epoch: 1054/8000  Traning Loss: 101.59120655059814  Train_Reconstruction: 98.14064121246338  Train_KL: 3.4505648612976074  Validation Loss : 99.98942184448242 Val_Reconstruction : 96.56136703491211 Val_KL : 3.4280552864074707\n","Epoch: 1055/8000  Traning Loss: 101.77157688140869  Train_Reconstruction: 98.30079936981201  Train_KL: 3.4707771241664886  Validation Loss : 99.63566970825195 Val_Reconstruction : 96.20332717895508 Val_KL : 3.432339310646057\n","Epoch: 1056/8000  Traning Loss: 101.71973323822021  Train_Reconstruction: 98.25721645355225  Train_KL: 3.462516129016876  Validation Loss : 99.66324234008789 Val_Reconstruction : 96.23929595947266 Val_KL : 3.4239436388015747\n","Epoch: 1057/8000  Traning Loss: 101.48072528839111  Train_Reconstruction: 98.0226583480835  Train_KL: 3.4580668807029724  Validation Loss : 99.22371292114258 Val_Reconstruction : 95.80466842651367 Val_KL : 3.419046640396118\n","Epoch: 1058/8000  Traning Loss: 101.46263408660889  Train_Reconstruction: 98.005539894104  Train_KL: 3.4570940136909485  Validation Loss : 99.57268524169922 Val_Reconstruction : 96.15364456176758 Val_KL : 3.419041872024536\n","Epoch: 1059/8000  Traning Loss: 101.88417911529541  Train_Reconstruction: 98.43385887145996  Train_KL: 3.450319766998291  Validation Loss : 100.38872528076172 Val_Reconstruction : 96.97048950195312 Val_KL : 3.4182358980178833\n","Epoch: 1060/8000  Traning Loss: 102.09627723693848  Train_Reconstruction: 98.63701725006104  Train_KL: 3.459259480237961  Validation Loss : 99.65923690795898 Val_Reconstruction : 96.23001480102539 Val_KL : 3.429221510887146\n","Epoch: 1061/8000  Traning Loss: 101.56869697570801  Train_Reconstruction: 98.12395095825195  Train_KL: 3.4447450041770935  Validation Loss : 99.31158828735352 Val_Reconstruction : 95.89595413208008 Val_KL : 3.4156346321105957\n","Epoch: 1062/8000  Traning Loss: 101.4304838180542  Train_Reconstruction: 97.9637222290039  Train_KL: 3.4667617678642273  Validation Loss : 99.17954635620117 Val_Reconstruction : 95.74551391601562 Val_KL : 3.434032678604126\n","Epoch: 1063/8000  Traning Loss: 101.70023441314697  Train_Reconstruction: 98.22438335418701  Train_KL: 3.4758507907390594  Validation Loss : 99.78267669677734 Val_Reconstruction : 96.32467651367188 Val_KL : 3.457999110221863\n","Epoch: 1064/8000  Traning Loss: 101.93323993682861  Train_Reconstruction: 98.46139430999756  Train_KL: 3.471846252679825  Validation Loss : 99.7080192565918 Val_Reconstruction : 96.27559280395508 Val_KL : 3.4324262142181396\n","Epoch: 1065/8000  Traning Loss: 102.13161659240723  Train_Reconstruction: 98.66360759735107  Train_KL: 3.468009650707245  Validation Loss : 100.15933227539062 Val_Reconstruction : 96.7345962524414 Val_KL : 3.4247360229492188\n","Epoch: 1066/8000  Traning Loss: 101.69557094573975  Train_Reconstruction: 98.24367809295654  Train_KL: 3.4518931806087494  Validation Loss : 99.29672622680664 Val_Reconstruction : 95.88934707641602 Val_KL : 3.4073771238327026\n","Epoch: 1067/8000  Traning Loss: 101.37736701965332  Train_Reconstruction: 97.92841911315918  Train_KL: 3.4489480555057526  Validation Loss : 99.18690872192383 Val_Reconstruction : 95.78150939941406 Val_KL : 3.405398368835449\n","Epoch: 1068/8000  Traning Loss: 101.39815139770508  Train_Reconstruction: 97.95227432250977  Train_KL: 3.4458754658699036  Validation Loss : 99.20686721801758 Val_Reconstruction : 95.77716827392578 Val_KL : 3.429701089859009\n","Epoch: 1069/8000  Traning Loss: 101.27443981170654  Train_Reconstruction: 97.80261611938477  Train_KL: 3.471824288368225  Validation Loss : 98.96471786499023 Val_Reconstruction : 95.51375961303711 Val_KL : 3.450958728790283\n","Epoch: 1070/8000  Traning Loss: 101.20050239562988  Train_Reconstruction: 97.73133754730225  Train_KL: 3.4691653847694397  Validation Loss : 99.37821197509766 Val_Reconstruction : 95.94478607177734 Val_KL : 3.4334243535995483\n","Epoch: 1071/8000  Traning Loss: 101.51935482025146  Train_Reconstruction: 98.06445693969727  Train_KL: 3.4548975229263306  Validation Loss : 99.26851272583008 Val_Reconstruction : 95.8537368774414 Val_KL : 3.4147756099700928\n","Epoch: 1072/8000  Traning Loss: 101.3770523071289  Train_Reconstruction: 97.91074848175049  Train_KL: 3.466303735971451  Validation Loss : 99.16767501831055 Val_Reconstruction : 95.73734664916992 Val_KL : 3.43032705783844\n","Epoch: 1073/8000  Traning Loss: 101.46532344818115  Train_Reconstruction: 98.01351547241211  Train_KL: 3.4518075585365295  Validation Loss : 99.47662734985352 Val_Reconstruction : 96.06542587280273 Val_KL : 3.411199688911438\n","Epoch: 1074/8000  Traning Loss: 101.58282661437988  Train_Reconstruction: 98.12148857116699  Train_KL: 3.461338222026825  Validation Loss : 99.27790451049805 Val_Reconstruction : 95.83552551269531 Val_KL : 3.4423763751983643\n","Epoch: 1075/8000  Traning Loss: 101.4899263381958  Train_Reconstruction: 98.02072429656982  Train_KL: 3.4692027270793915  Validation Loss : 99.3067398071289 Val_Reconstruction : 95.88572311401367 Val_KL : 3.4210184812545776\n","Epoch: 1076/8000  Traning Loss: 101.53313159942627  Train_Reconstruction: 98.08613204956055  Train_KL: 3.446999341249466  Validation Loss : 99.177490234375 Val_Reconstruction : 95.769287109375 Val_KL : 3.4082027673721313\n","Epoch: 1077/8000  Traning Loss: 101.90637016296387  Train_Reconstruction: 98.4474458694458  Train_KL: 3.4589231610298157  Validation Loss : 99.90492630004883 Val_Reconstruction : 96.47653579711914 Val_KL : 3.428391218185425\n","Epoch: 1078/8000  Traning Loss: 101.91236400604248  Train_Reconstruction: 98.45561504364014  Train_KL: 3.456749349832535  Validation Loss : 99.69742584228516 Val_Reconstruction : 96.27187728881836 Val_KL : 3.425550699234009\n","Epoch: 1079/8000  Traning Loss: 101.78157424926758  Train_Reconstruction: 98.31163215637207  Train_KL: 3.469941556453705  Validation Loss : 99.62217712402344 Val_Reconstruction : 96.18482971191406 Val_KL : 3.4373464584350586\n","Epoch: 1080/8000  Traning Loss: 101.80146980285645  Train_Reconstruction: 98.34845924377441  Train_KL: 3.4530102610588074  Validation Loss : 99.97734451293945 Val_Reconstruction : 96.54758834838867 Val_KL : 3.429757833480835\n","Epoch: 1081/8000  Traning Loss: 101.62866020202637  Train_Reconstruction: 98.1559534072876  Train_KL: 3.4727054834365845  Validation Loss : 99.59589767456055 Val_Reconstruction : 96.16131591796875 Val_KL : 3.434581995010376\n","Epoch: 1082/8000  Traning Loss: 101.3544225692749  Train_Reconstruction: 97.90578365325928  Train_KL: 3.4486383199691772  Validation Loss : 99.70088195800781 Val_Reconstruction : 96.27513885498047 Val_KL : 3.425742745399475\n","Epoch: 1083/8000  Traning Loss: 101.44262599945068  Train_Reconstruction: 97.97899341583252  Train_KL: 3.4636311531066895  Validation Loss : 99.13895034790039 Val_Reconstruction : 95.71133804321289 Val_KL : 3.4276143312454224\n","Epoch: 1084/8000  Traning Loss: 101.1469087600708  Train_Reconstruction: 97.69467163085938  Train_KL: 3.4522365033626556  Validation Loss : 98.88588333129883 Val_Reconstruction : 95.47551727294922 Val_KL : 3.4103668928146362\n","Epoch: 1085/8000  Traning Loss: 101.4188175201416  Train_Reconstruction: 97.97212505340576  Train_KL: 3.446692109107971  Validation Loss : 99.8826675415039 Val_Reconstruction : 96.47946166992188 Val_KL : 3.4032071828842163\n","Epoch: 1086/8000  Traning Loss: 102.00123691558838  Train_Reconstruction: 98.54883098602295  Train_KL: 3.452404737472534  Validation Loss : 99.93526840209961 Val_Reconstruction : 96.49151992797852 Val_KL : 3.443748950958252\n","Epoch: 1087/8000  Traning Loss: 101.58672714233398  Train_Reconstruction: 98.11616039276123  Train_KL: 3.4705666303634644  Validation Loss : 99.53633499145508 Val_Reconstruction : 96.1224136352539 Val_KL : 3.4139236211776733\n","Epoch: 1088/8000  Traning Loss: 101.5621862411499  Train_Reconstruction: 98.11845207214355  Train_KL: 3.443733811378479  Validation Loss : 99.30147552490234 Val_Reconstruction : 95.87773513793945 Val_KL : 3.423737049102783\n","Epoch: 1089/8000  Traning Loss: 101.44167041778564  Train_Reconstruction: 97.99862098693848  Train_KL: 3.4430490732192993  Validation Loss : 99.26658248901367 Val_Reconstruction : 95.86434555053711 Val_KL : 3.402235984802246\n","Epoch: 1090/8000  Traning Loss: 101.83401203155518  Train_Reconstruction: 98.36866474151611  Train_KL: 3.4653491377830505  Validation Loss : 100.03379440307617 Val_Reconstruction : 96.58516693115234 Val_KL : 3.4486254453659058\n","Epoch: 1091/8000  Traning Loss: 102.10554027557373  Train_Reconstruction: 98.63308334350586  Train_KL: 3.4724575579166412  Validation Loss : 100.16497802734375 Val_Reconstruction : 96.75682830810547 Val_KL : 3.408151388168335\n","Epoch: 1092/8000  Traning Loss: 101.57029056549072  Train_Reconstruction: 98.12474250793457  Train_KL: 3.445548504590988  Validation Loss : 99.84505844116211 Val_Reconstruction : 96.41490936279297 Val_KL : 3.4301496744155884\n","Epoch: 1093/8000  Traning Loss: 101.11980724334717  Train_Reconstruction: 97.65030765533447  Train_KL: 3.4695002734661102  Validation Loss : 98.9471435546875 Val_Reconstruction : 95.5316162109375 Val_KL : 3.4155290126800537\n","Epoch: 1094/8000  Traning Loss: 101.28884220123291  Train_Reconstruction: 97.83136367797852  Train_KL: 3.4574784636497498  Validation Loss : 99.64940643310547 Val_Reconstruction : 96.2291259765625 Val_KL : 3.42028272151947\n","Epoch: 1095/8000  Traning Loss: 101.52169036865234  Train_Reconstruction: 98.0537462234497  Train_KL: 3.467944234609604  Validation Loss : 99.36548233032227 Val_Reconstruction : 95.92516326904297 Val_KL : 3.440319061279297\n","Epoch: 1096/8000  Traning Loss: 101.51599788665771  Train_Reconstruction: 98.04803943634033  Train_KL: 3.4679582118988037  Validation Loss : 99.62102508544922 Val_Reconstruction : 96.20005798339844 Val_KL : 3.4209675788879395\n","Epoch: 1097/8000  Traning Loss: 101.79316997528076  Train_Reconstruction: 98.34247016906738  Train_KL: 3.4506997764110565  Validation Loss : 99.88562393188477 Val_Reconstruction : 96.4593276977539 Val_KL : 3.426297426223755\n","Epoch: 1098/8000  Traning Loss: 102.03023624420166  Train_Reconstruction: 98.56472492218018  Train_KL: 3.465511441230774  Validation Loss : 99.8219108581543 Val_Reconstruction : 96.4089126586914 Val_KL : 3.4130001068115234\n","Epoch: 1099/8000  Traning Loss: 101.79576301574707  Train_Reconstruction: 98.34730625152588  Train_KL: 3.4484560787677765  Validation Loss : 100.05084991455078 Val_Reconstruction : 96.63546752929688 Val_KL : 3.4153835773468018\n","Epoch: 1100/8000  Traning Loss: 101.33261203765869  Train_Reconstruction: 97.87097835540771  Train_KL: 3.461634188890457  Validation Loss : 99.2349967956543 Val_Reconstruction : 95.80854415893555 Val_KL : 3.4264503717422485\n","Epoch: 1101/8000  Traning Loss: 101.33688449859619  Train_Reconstruction: 97.8890027999878  Train_KL: 3.4478811621665955  Validation Loss : 99.46603012084961 Val_Reconstruction : 96.05858612060547 Val_KL : 3.4074442386627197\n","Epoch: 1102/8000  Traning Loss: 101.38360214233398  Train_Reconstruction: 97.91743659973145  Train_KL: 3.466166079044342  Validation Loss : 98.97038269042969 Val_Reconstruction : 95.52670288085938 Val_KL : 3.4436813592910767\n","Epoch: 1103/8000  Traning Loss: 101.30763912200928  Train_Reconstruction: 97.83773708343506  Train_KL: 3.4699014127254486  Validation Loss : 98.95251846313477 Val_Reconstruction : 95.52305603027344 Val_KL : 3.4294605255126953\n","Epoch: 1104/8000  Traning Loss: 101.2023696899414  Train_Reconstruction: 97.74749088287354  Train_KL: 3.454879254102707  Validation Loss : 99.14362335205078 Val_Reconstruction : 95.73215866088867 Val_KL : 3.4114630222320557\n","Epoch: 1105/8000  Traning Loss: 101.04248523712158  Train_Reconstruction: 97.58086013793945  Train_KL: 3.4616250097751617  Validation Loss : 98.92097091674805 Val_Reconstruction : 95.50145721435547 Val_KL : 3.419516682624817\n","Epoch: 1106/8000  Traning Loss: 101.10885906219482  Train_Reconstruction: 97.6618824005127  Train_KL: 3.446976274251938  Validation Loss : 98.93368148803711 Val_Reconstruction : 95.51630020141602 Val_KL : 3.417382001876831\n","Epoch: 1107/8000  Traning Loss: 101.00931739807129  Train_Reconstruction: 97.55442523956299  Train_KL: 3.4548920691013336  Validation Loss : 99.1977767944336 Val_Reconstruction : 95.77701187133789 Val_KL : 3.4207643270492554\n","Epoch: 1108/8000  Traning Loss: 101.14663600921631  Train_Reconstruction: 97.67643547058105  Train_KL: 3.470200091600418  Validation Loss : 98.81976318359375 Val_Reconstruction : 95.38924407958984 Val_KL : 3.4305206537246704\n","Epoch: 1109/8000  Traning Loss: 101.04147434234619  Train_Reconstruction: 97.58436107635498  Train_KL: 3.4571122229099274  Validation Loss : 98.91504669189453 Val_Reconstruction : 95.50157165527344 Val_KL : 3.4134734869003296\n","Epoch: 1110/8000  Traning Loss: 101.09505081176758  Train_Reconstruction: 97.64077758789062  Train_KL: 3.4542734026908875  Validation Loss : 99.31673431396484 Val_Reconstruction : 95.88822937011719 Val_KL : 3.4285064935684204\n","Epoch: 1111/8000  Traning Loss: 101.19777774810791  Train_Reconstruction: 97.72752571105957  Train_KL: 3.4702519178390503  Validation Loss : 99.17464828491211 Val_Reconstruction : 95.74760437011719 Val_KL : 3.4270442724227905\n","Epoch: 1112/8000  Traning Loss: 101.19011974334717  Train_Reconstruction: 97.72815990447998  Train_KL: 3.461960554122925  Validation Loss : 99.04764556884766 Val_Reconstruction : 95.63377380371094 Val_KL : 3.413870692253113\n","Epoch: 1113/8000  Traning Loss: 101.03304958343506  Train_Reconstruction: 97.56850337982178  Train_KL: 3.4645454585552216  Validation Loss : 99.02914810180664 Val_Reconstruction : 95.59967803955078 Val_KL : 3.4294692277908325\n","Epoch: 1114/8000  Traning Loss: 101.12531185150146  Train_Reconstruction: 97.66458320617676  Train_KL: 3.4607284665107727  Validation Loss : 98.84442520141602 Val_Reconstruction : 95.41972351074219 Val_KL : 3.424703598022461\n","Epoch: 1115/8000  Traning Loss: 100.98213195800781  Train_Reconstruction: 97.52809619903564  Train_KL: 3.4540362656116486  Validation Loss : 99.10126113891602 Val_Reconstruction : 95.68030166625977 Val_KL : 3.420959949493408\n","Epoch: 1116/8000  Traning Loss: 100.96557235717773  Train_Reconstruction: 97.49571132659912  Train_KL: 3.469862014055252  Validation Loss : 99.05718612670898 Val_Reconstruction : 95.62903594970703 Val_KL : 3.4281485080718994\n","Epoch: 1117/8000  Traning Loss: 100.97829055786133  Train_Reconstruction: 97.52274513244629  Train_KL: 3.4555462300777435  Validation Loss : 99.13359451293945 Val_Reconstruction : 95.70650482177734 Val_KL : 3.4270896911621094\n","Epoch: 1118/8000  Traning Loss: 101.09808254241943  Train_Reconstruction: 97.6341667175293  Train_KL: 3.463915526866913  Validation Loss : 98.98556900024414 Val_Reconstruction : 95.56034469604492 Val_KL : 3.4252262115478516\n","Epoch: 1119/8000  Traning Loss: 101.07364654541016  Train_Reconstruction: 97.5959300994873  Train_KL: 3.47771617770195  Validation Loss : 99.11674118041992 Val_Reconstruction : 95.66176986694336 Val_KL : 3.454970955848694\n","Epoch: 1120/8000  Traning Loss: 101.41253757476807  Train_Reconstruction: 97.92868041992188  Train_KL: 3.483858346939087  Validation Loss : 99.44400787353516 Val_Reconstruction : 96.01594924926758 Val_KL : 3.4280600547790527\n","Epoch: 1121/8000  Traning Loss: 101.4226245880127  Train_Reconstruction: 97.96421337127686  Train_KL: 3.4584109485149384  Validation Loss : 99.26139831542969 Val_Reconstruction : 95.82582092285156 Val_KL : 3.435578227043152\n","Epoch: 1122/8000  Traning Loss: 101.29645824432373  Train_Reconstruction: 97.83828067779541  Train_KL: 3.4581759572029114  Validation Loss : 99.37203598022461 Val_Reconstruction : 95.96353530883789 Val_KL : 3.408501386642456\n","Epoch: 1123/8000  Traning Loss: 101.25734806060791  Train_Reconstruction: 97.80982875823975  Train_KL: 3.4475184082984924  Validation Loss : 99.14726257324219 Val_Reconstruction : 95.72370147705078 Val_KL : 3.4235613346099854\n","Epoch: 1124/8000  Traning Loss: 101.13083934783936  Train_Reconstruction: 97.6726427078247  Train_KL: 3.458197683095932  Validation Loss : 98.97637939453125 Val_Reconstruction : 95.54360580444336 Val_KL : 3.4327725172042847\n","Epoch: 1125/8000  Traning Loss: 101.30795383453369  Train_Reconstruction: 97.85072326660156  Train_KL: 3.4572292864322662  Validation Loss : 98.93425750732422 Val_Reconstruction : 95.5274887084961 Val_KL : 3.4067691564559937\n","Epoch: 1126/8000  Traning Loss: 101.13060283660889  Train_Reconstruction: 97.68696212768555  Train_KL: 3.443640649318695  Validation Loss : 99.0789794921875 Val_Reconstruction : 95.66770935058594 Val_KL : 3.411268472671509\n","Epoch: 1127/8000  Traning Loss: 101.1798095703125  Train_Reconstruction: 97.7268009185791  Train_KL: 3.4530097246170044  Validation Loss : 99.02251052856445 Val_Reconstruction : 95.58929443359375 Val_KL : 3.433212399482727\n","Epoch: 1128/8000  Traning Loss: 101.13108539581299  Train_Reconstruction: 97.65788459777832  Train_KL: 3.4731995463371277  Validation Loss : 98.9242935180664 Val_Reconstruction : 95.48675155639648 Val_KL : 3.4375436305999756\n","Epoch: 1129/8000  Traning Loss: 101.12362384796143  Train_Reconstruction: 97.66532135009766  Train_KL: 3.458302527666092  Validation Loss : 98.91071319580078 Val_Reconstruction : 95.49385452270508 Val_KL : 3.4168580770492554\n","Epoch: 1130/8000  Traning Loss: 100.97769260406494  Train_Reconstruction: 97.52291774749756  Train_KL: 3.4547741413116455  Validation Loss : 98.89847183227539 Val_Reconstruction : 95.47004699707031 Val_KL : 3.4284249544143677\n","Epoch: 1131/8000  Traning Loss: 101.00546169281006  Train_Reconstruction: 97.54393863677979  Train_KL: 3.461523175239563  Validation Loss : 99.02578735351562 Val_Reconstruction : 95.59519958496094 Val_KL : 3.4305866956710815\n","Epoch: 1132/8000  Traning Loss: 101.04068851470947  Train_Reconstruction: 97.57163906097412  Train_KL: 3.4690492153167725  Validation Loss : 99.06867218017578 Val_Reconstruction : 95.62998962402344 Val_KL : 3.4386813640594482\n","Epoch: 1133/8000  Traning Loss: 101.1549596786499  Train_Reconstruction: 97.69417190551758  Train_KL: 3.4607877135276794  Validation Loss : 99.25249099731445 Val_Reconstruction : 95.84046173095703 Val_KL : 3.4120302200317383\n","Epoch: 1134/8000  Traning Loss: 101.15078258514404  Train_Reconstruction: 97.69929695129395  Train_KL: 3.451486438512802  Validation Loss : 99.14567184448242 Val_Reconstruction : 95.71788024902344 Val_KL : 3.4277926683425903\n","Epoch: 1135/8000  Traning Loss: 101.33350944519043  Train_Reconstruction: 97.86794567108154  Train_KL: 3.465563118457794  Validation Loss : 99.32846069335938 Val_Reconstruction : 95.88934707641602 Val_KL : 3.439116954803467\n","Epoch: 1136/8000  Traning Loss: 101.35693168640137  Train_Reconstruction: 97.89836978912354  Train_KL: 3.458561360836029  Validation Loss : 99.39363479614258 Val_Reconstruction : 95.97542572021484 Val_KL : 3.4182108640670776\n","Epoch: 1137/8000  Traning Loss: 101.09319496154785  Train_Reconstruction: 97.6364974975586  Train_KL: 3.4566964507102966  Validation Loss : 99.14371490478516 Val_Reconstruction : 95.72182846069336 Val_KL : 3.421888589859009\n","Epoch: 1138/8000  Traning Loss: 100.80688858032227  Train_Reconstruction: 97.34415340423584  Train_KL: 3.4627358615398407  Validation Loss : 99.03884506225586 Val_Reconstruction : 95.61376571655273 Val_KL : 3.4250776767730713\n","Epoch: 1139/8000  Traning Loss: 100.95851707458496  Train_Reconstruction: 97.49531269073486  Train_KL: 3.4632046222686768  Validation Loss : 99.02840805053711 Val_Reconstruction : 95.6050796508789 Val_KL : 3.4233261346817017\n","Epoch: 1140/8000  Traning Loss: 101.17031383514404  Train_Reconstruction: 97.7102518081665  Train_KL: 3.460061341524124  Validation Loss : 99.39128112792969 Val_Reconstruction : 95.94980239868164 Val_KL : 3.4414809942245483\n","Epoch: 1141/8000  Traning Loss: 101.16578578948975  Train_Reconstruction: 97.70180892944336  Train_KL: 3.4639776051044464  Validation Loss : 99.24470901489258 Val_Reconstruction : 95.80632019042969 Val_KL : 3.438387155532837\n","Epoch: 1142/8000  Traning Loss: 100.93509006500244  Train_Reconstruction: 97.46816921234131  Train_KL: 3.4669211208820343  Validation Loss : 98.67350769042969 Val_Reconstruction : 95.2362174987793 Val_KL : 3.4372904300689697\n","Epoch: 1143/8000  Traning Loss: 100.80061054229736  Train_Reconstruction: 97.32325553894043  Train_KL: 3.4773560762405396  Validation Loss : 98.54325103759766 Val_Reconstruction : 95.10420227050781 Val_KL : 3.439047932624817\n","Epoch: 1144/8000  Traning Loss: 100.56668472290039  Train_Reconstruction: 97.0947093963623  Train_KL: 3.471976637840271  Validation Loss : 98.67671585083008 Val_Reconstruction : 95.2352409362793 Val_KL : 3.4414753913879395\n","Epoch: 1145/8000  Traning Loss: 100.55376434326172  Train_Reconstruction: 97.08804035186768  Train_KL: 3.465725898742676  Validation Loss : 98.69226837158203 Val_Reconstruction : 95.26504898071289 Val_KL : 3.427217960357666\n","Epoch: 1146/8000  Traning Loss: 100.73078060150146  Train_Reconstruction: 97.2661828994751  Train_KL: 3.464597851037979  Validation Loss : 98.74945068359375 Val_Reconstruction : 95.30798721313477 Val_KL : 3.441462755203247\n","Epoch: 1147/8000  Traning Loss: 100.77511405944824  Train_Reconstruction: 97.294358253479  Train_KL: 3.4807564318180084  Validation Loss : 98.55684280395508 Val_Reconstruction : 95.11351013183594 Val_KL : 3.443331718444824\n","Epoch: 1148/8000  Traning Loss: 100.85129833221436  Train_Reconstruction: 97.37907218933105  Train_KL: 3.4722266495227814  Validation Loss : 99.00585556030273 Val_Reconstruction : 95.56061172485352 Val_KL : 3.445244550704956\n","Epoch: 1149/8000  Traning Loss: 101.62530517578125  Train_Reconstruction: 98.15847110748291  Train_KL: 3.46683531999588  Validation Loss : 99.7695198059082 Val_Reconstruction : 96.3514289855957 Val_KL : 3.4180926084518433\n","Epoch: 1150/8000  Traning Loss: 102.22996807098389  Train_Reconstruction: 98.75905227661133  Train_KL: 3.470914751291275  Validation Loss : 100.47061920166016 Val_Reconstruction : 97.0302505493164 Val_KL : 3.4403679370880127\n","Epoch: 1151/8000  Traning Loss: 102.0627269744873  Train_Reconstruction: 98.60109901428223  Train_KL: 3.461627185344696  Validation Loss : 99.7053451538086 Val_Reconstruction : 96.27914047241211 Val_KL : 3.4262036085128784\n","Epoch: 1152/8000  Traning Loss: 101.75282573699951  Train_Reconstruction: 98.29335117340088  Train_KL: 3.4594752192497253  Validation Loss : 99.28913116455078 Val_Reconstruction : 95.85347747802734 Val_KL : 3.435655951499939\n","Epoch: 1153/8000  Traning Loss: 101.23494052886963  Train_Reconstruction: 97.76758480072021  Train_KL: 3.467355489730835  Validation Loss : 99.03422164916992 Val_Reconstruction : 95.59556198120117 Val_KL : 3.4386574029922485\n","Epoch: 1154/8000  Traning Loss: 100.89298725128174  Train_Reconstruction: 97.43949699401855  Train_KL: 3.4534903466701508  Validation Loss : 98.7700309753418 Val_Reconstruction : 95.35496520996094 Val_KL : 3.4150670766830444\n","Epoch: 1155/8000  Traning Loss: 100.98874759674072  Train_Reconstruction: 97.52471733093262  Train_KL: 3.4640310406684875  Validation Loss : 98.6546630859375 Val_Reconstruction : 95.21994018554688 Val_KL : 3.434725522994995\n","Epoch: 1156/8000  Traning Loss: 100.92255783081055  Train_Reconstruction: 97.45762634277344  Train_KL: 3.4649329781532288  Validation Loss : 99.16006851196289 Val_Reconstruction : 95.7399673461914 Val_KL : 3.4201018810272217\n","Epoch: 1157/8000  Traning Loss: 100.7605676651001  Train_Reconstruction: 97.29750728607178  Train_KL: 3.4630613923072815  Validation Loss : 98.78125762939453 Val_Reconstruction : 95.34891891479492 Val_KL : 3.4323394298553467\n","Epoch: 1158/8000  Traning Loss: 100.97110366821289  Train_Reconstruction: 97.51492214202881  Train_KL: 3.4561829268932343  Validation Loss : 99.19798278808594 Val_Reconstruction : 95.7755355834961 Val_KL : 3.4224464893341064\n","Epoch: 1159/8000  Traning Loss: 100.857008934021  Train_Reconstruction: 97.40681838989258  Train_KL: 3.4501913487911224  Validation Loss : 98.97493743896484 Val_Reconstruction : 95.55992126464844 Val_KL : 3.4150174856185913\n","Epoch: 1160/8000  Traning Loss: 100.91987609863281  Train_Reconstruction: 97.4723129272461  Train_KL: 3.4475649297237396  Validation Loss : 98.77174758911133 Val_Reconstruction : 95.36272811889648 Val_KL : 3.409019112586975\n","Epoch: 1161/8000  Traning Loss: 100.67540454864502  Train_Reconstruction: 97.19701766967773  Train_KL: 3.478386700153351  Validation Loss : 98.94949722290039 Val_Reconstruction : 95.51560592651367 Val_KL : 3.433889627456665\n","Epoch: 1162/8000  Traning Loss: 100.88125896453857  Train_Reconstruction: 97.40931415557861  Train_KL: 3.4719457626342773  Validation Loss : 98.7751235961914 Val_Reconstruction : 95.34027099609375 Val_KL : 3.4348514080047607\n","Epoch: 1163/8000  Traning Loss: 100.70545101165771  Train_Reconstruction: 97.2506160736084  Train_KL: 3.454835444688797  Validation Loss : 98.9096450805664 Val_Reconstruction : 95.47797775268555 Val_KL : 3.4316694736480713\n","Epoch: 1164/8000  Traning Loss: 101.06447505950928  Train_Reconstruction: 97.597900390625  Train_KL: 3.466575801372528  Validation Loss : 99.11941909790039 Val_Reconstruction : 95.7001724243164 Val_KL : 3.4192458391189575\n","Epoch: 1165/8000  Traning Loss: 101.08126068115234  Train_Reconstruction: 97.60795307159424  Train_KL: 3.473306804895401  Validation Loss : 99.2995834350586 Val_Reconstruction : 95.8557357788086 Val_KL : 3.443848729133606\n","Epoch: 1166/8000  Traning Loss: 101.0381326675415  Train_Reconstruction: 97.56670665740967  Train_KL: 3.4714256525039673  Validation Loss : 99.12947082519531 Val_Reconstruction : 95.70437622070312 Val_KL : 3.4250954389572144\n","Epoch: 1167/8000  Traning Loss: 100.96895790100098  Train_Reconstruction: 97.49573421478271  Train_KL: 3.473223716020584  Validation Loss : 99.17316818237305 Val_Reconstruction : 95.74517059326172 Val_KL : 3.427995204925537\n","Epoch: 1168/8000  Traning Loss: 100.84745788574219  Train_Reconstruction: 97.38717460632324  Train_KL: 3.4602838158607483  Validation Loss : 98.80506134033203 Val_Reconstruction : 95.38409805297852 Val_KL : 3.4209630489349365\n","Epoch: 1169/8000  Traning Loss: 100.89247989654541  Train_Reconstruction: 97.43885231018066  Train_KL: 3.453627735376358  Validation Loss : 98.89229202270508 Val_Reconstruction : 95.48318481445312 Val_KL : 3.4091044664382935\n","Epoch: 1170/8000  Traning Loss: 100.93936824798584  Train_Reconstruction: 97.48173236846924  Train_KL: 3.4576355516910553  Validation Loss : 99.07482528686523 Val_Reconstruction : 95.64927673339844 Val_KL : 3.4255484342575073\n","Epoch: 1171/8000  Traning Loss: 100.73570728302002  Train_Reconstruction: 97.28470993041992  Train_KL: 3.4509967863559723  Validation Loss : 98.65314102172852 Val_Reconstruction : 95.23779678344727 Val_KL : 3.4153448343276978\n","Epoch: 1172/8000  Traning Loss: 100.84765625  Train_Reconstruction: 97.36972904205322  Train_KL: 3.477928251028061  Validation Loss : 99.58951950073242 Val_Reconstruction : 96.12935638427734 Val_KL : 3.4601638317108154\n","Epoch: 1173/8000  Traning Loss: 101.06405448913574  Train_Reconstruction: 97.5722770690918  Train_KL: 3.4917781352996826  Validation Loss : 99.69294357299805 Val_Reconstruction : 96.24990463256836 Val_KL : 3.4430373907089233\n","Epoch: 1174/8000  Traning Loss: 101.16273784637451  Train_Reconstruction: 97.68680381774902  Train_KL: 3.4759353399276733  Validation Loss : 99.19200134277344 Val_Reconstruction : 95.74602890014648 Val_KL : 3.44597327709198\n","Epoch: 1175/8000  Traning Loss: 101.04680061340332  Train_Reconstruction: 97.58141326904297  Train_KL: 3.4653872549533844  Validation Loss : 98.9849624633789 Val_Reconstruction : 95.5506362915039 Val_KL : 3.434326410293579\n","Epoch: 1176/8000  Traning Loss: 100.99959087371826  Train_Reconstruction: 97.5309419631958  Train_KL: 3.4686498939990997  Validation Loss : 98.60115051269531 Val_Reconstruction : 95.18176651000977 Val_KL : 3.4193836450576782\n","Epoch: 1177/8000  Traning Loss: 100.89425373077393  Train_Reconstruction: 97.42998886108398  Train_KL: 3.464264392852783  Validation Loss : 98.93846893310547 Val_Reconstruction : 95.51280212402344 Val_KL : 3.4256651401519775\n","Epoch: 1178/8000  Traning Loss: 100.93722820281982  Train_Reconstruction: 97.46709632873535  Train_KL: 3.4701319336891174  Validation Loss : 98.89269638061523 Val_Reconstruction : 95.4418830871582 Val_KL : 3.4508148431777954\n","Epoch: 1179/8000  Traning Loss: 101.12642002105713  Train_Reconstruction: 97.6592960357666  Train_KL: 3.467123866081238  Validation Loss : 99.22185897827148 Val_Reconstruction : 95.7877197265625 Val_KL : 3.4341390132904053\n","Epoch: 1180/8000  Traning Loss: 101.11763668060303  Train_Reconstruction: 97.64986038208008  Train_KL: 3.467776268720627  Validation Loss : 99.16702270507812 Val_Reconstruction : 95.75328063964844 Val_KL : 3.4137420654296875\n","Epoch: 1181/8000  Traning Loss: 101.17814350128174  Train_Reconstruction: 97.71581649780273  Train_KL: 3.462328255176544  Validation Loss : 99.46917724609375 Val_Reconstruction : 96.04408645629883 Val_KL : 3.4250890016555786\n","Epoch: 1182/8000  Traning Loss: 101.26042175292969  Train_Reconstruction: 97.79421901702881  Train_KL: 3.4662022590637207  Validation Loss : 99.07213973999023 Val_Reconstruction : 95.63789749145508 Val_KL : 3.434240221977234\n","Epoch: 1183/8000  Traning Loss: 100.84065532684326  Train_Reconstruction: 97.37027549743652  Train_KL: 3.4703806340694427  Validation Loss : 99.07764053344727 Val_Reconstruction : 95.64773559570312 Val_KL : 3.4299029111862183\n","Epoch: 1184/8000  Traning Loss: 100.93646240234375  Train_Reconstruction: 97.4744119644165  Train_KL: 3.46204936504364  Validation Loss : 98.89398574829102 Val_Reconstruction : 95.4734115600586 Val_KL : 3.420575976371765\n","Epoch: 1185/8000  Traning Loss: 101.08641910552979  Train_Reconstruction: 97.62821102142334  Train_KL: 3.4582068622112274  Validation Loss : 98.7924690246582 Val_Reconstruction : 95.37631607055664 Val_KL : 3.416153073310852\n","Epoch: 1186/8000  Traning Loss: 100.60070133209229  Train_Reconstruction: 97.13819313049316  Train_KL: 3.4625082910060883  Validation Loss : 98.46440887451172 Val_Reconstruction : 95.04239273071289 Val_KL : 3.4220186471939087\n","Epoch: 1187/8000  Traning Loss: 100.44530773162842  Train_Reconstruction: 96.97813510894775  Train_KL: 3.467171937227249  Validation Loss : 98.41962051391602 Val_Reconstruction : 95.00028991699219 Val_KL : 3.419332504272461\n","Epoch: 1188/8000  Traning Loss: 100.36832237243652  Train_Reconstruction: 96.8942928314209  Train_KL: 3.474031001329422  Validation Loss : 98.59527969360352 Val_Reconstruction : 95.14556121826172 Val_KL : 3.4497177600860596\n","Epoch: 1189/8000  Traning Loss: 100.31428813934326  Train_Reconstruction: 96.84292316436768  Train_KL: 3.471363812685013  Validation Loss : 98.5899772644043 Val_Reconstruction : 95.16197967529297 Val_KL : 3.427995800971985\n","Epoch: 1190/8000  Traning Loss: 100.47172260284424  Train_Reconstruction: 97.00818634033203  Train_KL: 3.463536113500595  Validation Loss : 98.81487655639648 Val_Reconstruction : 95.37859725952148 Val_KL : 3.4362759590148926\n","Epoch: 1191/8000  Traning Loss: 100.9559736251831  Train_Reconstruction: 97.47758769989014  Train_KL: 3.478386402130127  Validation Loss : 99.27226257324219 Val_Reconstruction : 95.84660339355469 Val_KL : 3.4256584644317627\n","Epoch: 1192/8000  Traning Loss: 101.20817470550537  Train_Reconstruction: 97.7402868270874  Train_KL: 3.4678871631622314  Validation Loss : 99.15396881103516 Val_Reconstruction : 95.72060775756836 Val_KL : 3.4333609342575073\n","Epoch: 1193/8000  Traning Loss: 100.65797138214111  Train_Reconstruction: 97.19728946685791  Train_KL: 3.460680365562439  Validation Loss : 98.32566833496094 Val_Reconstruction : 94.90396118164062 Val_KL : 3.4217073917388916\n","Epoch: 1194/8000  Traning Loss: 100.23575019836426  Train_Reconstruction: 96.78367233276367  Train_KL: 3.4520777761936188  Validation Loss : 98.16361618041992 Val_Reconstruction : 94.72506713867188 Val_KL : 3.438551664352417\n","Epoch: 1195/8000  Traning Loss: 100.32308673858643  Train_Reconstruction: 96.84520053863525  Train_KL: 3.4778860807418823  Validation Loss : 98.82685089111328 Val_Reconstruction : 95.3918685913086 Val_KL : 3.434983253479004\n","Epoch: 1196/8000  Traning Loss: 100.78808116912842  Train_Reconstruction: 97.33274555206299  Train_KL: 3.4553354680538177  Validation Loss : 99.14961242675781 Val_Reconstruction : 95.73259353637695 Val_KL : 3.4170172214508057\n","Epoch: 1197/8000  Traning Loss: 100.67752647399902  Train_Reconstruction: 97.21502304077148  Train_KL: 3.46250256896019  Validation Loss : 98.79326248168945 Val_Reconstruction : 95.36251068115234 Val_KL : 3.4307507276535034\n","Epoch: 1198/8000  Traning Loss: 100.62131595611572  Train_Reconstruction: 97.15226745605469  Train_KL: 3.4690485894680023  Validation Loss : 98.87282180786133 Val_Reconstruction : 95.44787979125977 Val_KL : 3.4249415397644043\n","Epoch: 1199/8000  Traning Loss: 100.43163299560547  Train_Reconstruction: 96.97768020629883  Train_KL: 3.453954041004181  Validation Loss : 98.45467758178711 Val_Reconstruction : 95.03202438354492 Val_KL : 3.4226505756378174\n","Epoch: 1200/8000  Traning Loss: 100.77779865264893  Train_Reconstruction: 97.31339645385742  Train_KL: 3.4644022583961487  Validation Loss : 98.97465133666992 Val_Reconstruction : 95.52483367919922 Val_KL : 3.4498172998428345\n","Epoch: 1201/8000  Traning Loss: 101.09756374359131  Train_Reconstruction: 97.6265001296997  Train_KL: 3.4710628390312195  Validation Loss : 98.80133056640625 Val_Reconstruction : 95.3830337524414 Val_KL : 3.4182991981506348\n","Epoch: 1202/8000  Traning Loss: 100.55696868896484  Train_Reconstruction: 97.09382724761963  Train_KL: 3.463141620159149  Validation Loss : 98.78668212890625 Val_Reconstruction : 95.32797622680664 Val_KL : 3.458706021308899\n","Epoch: 1203/8000  Traning Loss: 100.6395206451416  Train_Reconstruction: 97.17297267913818  Train_KL: 3.4665490090847015  Validation Loss : 98.6097297668457 Val_Reconstruction : 95.19010162353516 Val_KL : 3.419626235961914\n","Epoch: 1204/8000  Traning Loss: 100.52314281463623  Train_Reconstruction: 97.05951309204102  Train_KL: 3.46363028883934  Validation Loss : 98.7187385559082 Val_Reconstruction : 95.2800064086914 Val_KL : 3.4387301206588745\n","Epoch: 1205/8000  Traning Loss: 100.33992767333984  Train_Reconstruction: 96.85144233703613  Train_KL: 3.4884859919548035  Validation Loss : 98.50170135498047 Val_Reconstruction : 95.0543327331543 Val_KL : 3.447367548942566\n","Epoch: 1206/8000  Traning Loss: 100.29183673858643  Train_Reconstruction: 96.8287353515625  Train_KL: 3.463101387023926  Validation Loss : 98.61814880371094 Val_Reconstruction : 95.19270706176758 Val_KL : 3.425440788269043\n","Epoch: 1207/8000  Traning Loss: 100.35748481750488  Train_Reconstruction: 96.89025211334229  Train_KL: 3.4672333002090454  Validation Loss : 98.44048309326172 Val_Reconstruction : 95.02013397216797 Val_KL : 3.4203461408615112\n","Epoch: 1208/8000  Traning Loss: 100.75315189361572  Train_Reconstruction: 97.2904462814331  Train_KL: 3.462705761194229  Validation Loss : 98.9394416809082 Val_Reconstruction : 95.51676940917969 Val_KL : 3.422674775123596\n","Epoch: 1209/8000  Traning Loss: 101.38489723205566  Train_Reconstruction: 97.92224407196045  Train_KL: 3.462653338909149  Validation Loss : 99.20146942138672 Val_Reconstruction : 95.78136825561523 Val_KL : 3.420101761817932\n","Epoch: 1210/8000  Traning Loss: 101.244215965271  Train_Reconstruction: 97.77535724639893  Train_KL: 3.4688596725463867  Validation Loss : 99.19738006591797 Val_Reconstruction : 95.74652862548828 Val_KL : 3.4508488178253174\n","Epoch: 1211/8000  Traning Loss: 101.04528045654297  Train_Reconstruction: 97.57442951202393  Train_KL: 3.470852315425873  Validation Loss : 99.02100372314453 Val_Reconstruction : 95.60021209716797 Val_KL : 3.4207946062088013\n","Epoch: 1212/8000  Traning Loss: 101.3170747756958  Train_Reconstruction: 97.85974216461182  Train_KL: 3.457332104444504  Validation Loss : 99.21923446655273 Val_Reconstruction : 95.80231094360352 Val_KL : 3.4169247150421143\n","Epoch: 1213/8000  Traning Loss: 101.34203815460205  Train_Reconstruction: 97.8979024887085  Train_KL: 3.44413623213768  Validation Loss : 98.9271469116211 Val_Reconstruction : 95.5172004699707 Val_KL : 3.409945249557495\n","Epoch: 1214/8000  Traning Loss: 100.81569290161133  Train_Reconstruction: 97.35980892181396  Train_KL: 3.4558832943439484  Validation Loss : 98.66920852661133 Val_Reconstruction : 95.23907470703125 Val_KL : 3.4301341772079468\n","Epoch: 1215/8000  Traning Loss: 100.69891738891602  Train_Reconstruction: 97.22117710113525  Train_KL: 3.47773876786232  Validation Loss : 98.98287963867188 Val_Reconstruction : 95.54612350463867 Val_KL : 3.436756730079651\n","Epoch: 1216/8000  Traning Loss: 100.59793376922607  Train_Reconstruction: 97.13665962219238  Train_KL: 3.46127250790596  Validation Loss : 98.8003921508789 Val_Reconstruction : 95.36853408813477 Val_KL : 3.431857466697693\n","Epoch: 1217/8000  Traning Loss: 100.84060764312744  Train_Reconstruction: 97.37445831298828  Train_KL: 3.466148465871811  Validation Loss : 98.68570709228516 Val_Reconstruction : 95.25238037109375 Val_KL : 3.4333250522613525\n","Epoch: 1218/8000  Traning Loss: 100.78941631317139  Train_Reconstruction: 97.33563709259033  Train_KL: 3.4537795186042786  Validation Loss : 99.2682113647461 Val_Reconstruction : 95.83306121826172 Val_KL : 3.435146689414978\n","Epoch: 1219/8000  Traning Loss: 100.51464557647705  Train_Reconstruction: 97.0426139831543  Train_KL: 3.4720305800437927  Validation Loss : 98.36006164550781 Val_Reconstruction : 94.91928100585938 Val_KL : 3.440782308578491\n","Epoch: 1220/8000  Traning Loss: 100.26722717285156  Train_Reconstruction: 96.80614757537842  Train_KL: 3.4610789120197296  Validation Loss : 98.61659240722656 Val_Reconstruction : 95.19583511352539 Val_KL : 3.4207578897476196\n","Epoch: 1221/8000  Traning Loss: 100.23269844055176  Train_Reconstruction: 96.77207660675049  Train_KL: 3.460621416568756  Validation Loss : 98.3551139831543 Val_Reconstruction : 94.93106460571289 Val_KL : 3.424049139022827\n","Epoch: 1222/8000  Traning Loss: 100.32031440734863  Train_Reconstruction: 96.86377429962158  Train_KL: 3.4565402567386627  Validation Loss : 98.72822570800781 Val_Reconstruction : 95.29756164550781 Val_KL : 3.4306668043136597\n","Epoch: 1223/8000  Traning Loss: 100.27983474731445  Train_Reconstruction: 96.80577754974365  Train_KL: 3.4740577340126038  Validation Loss : 98.43790435791016 Val_Reconstruction : 94.98733139038086 Val_KL : 3.4505739212036133\n","Epoch: 1224/8000  Traning Loss: 100.24332332611084  Train_Reconstruction: 96.76620674133301  Train_KL: 3.4771171510219574  Validation Loss : 98.36083221435547 Val_Reconstruction : 94.92754745483398 Val_KL : 3.4332876205444336\n","Epoch: 1225/8000  Traning Loss: 100.23329448699951  Train_Reconstruction: 96.77231788635254  Train_KL: 3.4609769880771637  Validation Loss : 98.33757400512695 Val_Reconstruction : 94.91500473022461 Val_KL : 3.422568917274475\n","Epoch: 1226/8000  Traning Loss: 100.42397022247314  Train_Reconstruction: 96.95722484588623  Train_KL: 3.4667468070983887  Validation Loss : 98.58643341064453 Val_Reconstruction : 95.15692520141602 Val_KL : 3.4295088052749634\n","Epoch: 1227/8000  Traning Loss: 100.63178634643555  Train_Reconstruction: 97.18277359008789  Train_KL: 3.449011951684952  Validation Loss : 98.48262786865234 Val_Reconstruction : 95.05299377441406 Val_KL : 3.429635524749756\n","Epoch: 1228/8000  Traning Loss: 100.41613578796387  Train_Reconstruction: 96.9423999786377  Train_KL: 3.4737352430820465  Validation Loss : 98.61714172363281 Val_Reconstruction : 95.18507385253906 Val_KL : 3.4320682287216187\n","Epoch: 1229/8000  Traning Loss: 100.36140060424805  Train_Reconstruction: 96.89463233947754  Train_KL: 3.466767966747284  Validation Loss : 98.41142272949219 Val_Reconstruction : 94.98156356811523 Val_KL : 3.429860830307007\n","Epoch: 1230/8000  Traning Loss: 100.22781085968018  Train_Reconstruction: 96.76583480834961  Train_KL: 3.4619752168655396  Validation Loss : 98.4155502319336 Val_Reconstruction : 94.99402618408203 Val_KL : 3.4215224981307983\n","Epoch: 1231/8000  Traning Loss: 100.44522094726562  Train_Reconstruction: 96.98610210418701  Train_KL: 3.4591196179389954  Validation Loss : 98.67438507080078 Val_Reconstruction : 95.25202178955078 Val_KL : 3.422364592552185\n","Epoch: 1232/8000  Traning Loss: 100.4430799484253  Train_Reconstruction: 97.00013065338135  Train_KL: 3.4429489374160767  Validation Loss : 99.07833099365234 Val_Reconstruction : 95.68451309204102 Val_KL : 3.3938156366348267\n","Epoch: 1233/8000  Traning Loss: 101.18738842010498  Train_Reconstruction: 97.72266578674316  Train_KL: 3.4647238850593567  Validation Loss : 100.07988739013672 Val_Reconstruction : 96.6304817199707 Val_KL : 3.4494069814682007\n","Epoch: 1234/8000  Traning Loss: 101.4570837020874  Train_Reconstruction: 97.9940128326416  Train_KL: 3.463071048259735  Validation Loss : 99.40692901611328 Val_Reconstruction : 95.9843635559082 Val_KL : 3.422563910484314\n","Epoch: 1235/8000  Traning Loss: 101.07318782806396  Train_Reconstruction: 97.60437774658203  Train_KL: 3.4688102304935455  Validation Loss : 99.07843017578125 Val_Reconstruction : 95.63342666625977 Val_KL : 3.445004463195801\n","Epoch: 1236/8000  Traning Loss: 100.5998945236206  Train_Reconstruction: 97.13626670837402  Train_KL: 3.4636276364326477  Validation Loss : 99.11239624023438 Val_Reconstruction : 95.69824981689453 Val_KL : 3.414146304130554\n","Epoch: 1237/8000  Traning Loss: 100.72230815887451  Train_Reconstruction: 97.27777194976807  Train_KL: 3.4445362389087677  Validation Loss : 98.61935806274414 Val_Reconstruction : 95.18666076660156 Val_KL : 3.4326976537704468\n","Epoch: 1238/8000  Traning Loss: 100.3494462966919  Train_Reconstruction: 96.88494300842285  Train_KL: 3.464504301548004  Validation Loss : 98.46037673950195 Val_Reconstruction : 95.0414924621582 Val_KL : 3.418882369995117\n","Epoch: 1239/8000  Traning Loss: 100.23975563049316  Train_Reconstruction: 96.7764368057251  Train_KL: 3.4633185267448425  Validation Loss : 98.39273834228516 Val_Reconstruction : 94.95324325561523 Val_KL : 3.4394928216934204\n","Epoch: 1240/8000  Traning Loss: 100.22467803955078  Train_Reconstruction: 96.75446033477783  Train_KL: 3.470217764377594  Validation Loss : 98.42134857177734 Val_Reconstruction : 94.99763488769531 Val_KL : 3.4237136840820312\n","Epoch: 1241/8000  Traning Loss: 100.84630584716797  Train_Reconstruction: 97.37775802612305  Train_KL: 3.468546599149704  Validation Loss : 99.20797348022461 Val_Reconstruction : 95.766845703125 Val_KL : 3.44112491607666\n","Epoch: 1242/8000  Traning Loss: 100.61876010894775  Train_Reconstruction: 97.15051937103271  Train_KL: 3.468241423368454  Validation Loss : 98.21273422241211 Val_Reconstruction : 94.80054473876953 Val_KL : 3.4121896028518677\n","Epoch: 1243/8000  Traning Loss: 100.19209957122803  Train_Reconstruction: 96.74771404266357  Train_KL: 3.444384276866913  Validation Loss : 98.1528091430664 Val_Reconstruction : 94.73091125488281 Val_KL : 3.4218982458114624\n","Epoch: 1244/8000  Traning Loss: 100.0522518157959  Train_Reconstruction: 96.58271884918213  Train_KL: 3.4695334136486053  Validation Loss : 98.37663269042969 Val_Reconstruction : 94.92840194702148 Val_KL : 3.4482311010360718\n","Epoch: 1245/8000  Traning Loss: 100.10163879394531  Train_Reconstruction: 96.6259593963623  Train_KL: 3.4756792783737183  Validation Loss : 98.26339721679688 Val_Reconstruction : 94.82404327392578 Val_KL : 3.439354419708252\n","Epoch: 1246/8000  Traning Loss: 100.03786087036133  Train_Reconstruction: 96.5647144317627  Train_KL: 3.473146378993988  Validation Loss : 98.40849685668945 Val_Reconstruction : 94.96687698364258 Val_KL : 3.441617965698242\n","Epoch: 1247/8000  Traning Loss: 100.00820732116699  Train_Reconstruction: 96.52453327178955  Train_KL: 3.483673244714737  Validation Loss : 98.43942260742188 Val_Reconstruction : 94.9907455444336 Val_KL : 3.4486767053604126\n","Epoch: 1248/8000  Traning Loss: 100.30989170074463  Train_Reconstruction: 96.85546970367432  Train_KL: 3.454422742128372  Validation Loss : 98.4174575805664 Val_Reconstruction : 94.98821640014648 Val_KL : 3.429240107536316\n","Epoch: 1249/8000  Traning Loss: 100.2913064956665  Train_Reconstruction: 96.81047439575195  Train_KL: 3.4808314740657806  Validation Loss : 98.63328170776367 Val_Reconstruction : 95.19320678710938 Val_KL : 3.4400763511657715\n","Epoch: 1250/8000  Traning Loss: 100.35014629364014  Train_Reconstruction: 96.90138339996338  Train_KL: 3.4487614035606384  Validation Loss : 98.45133972167969 Val_Reconstruction : 95.03239440917969 Val_KL : 3.4189456701278687\n","Epoch: 1251/8000  Traning Loss: 100.02963352203369  Train_Reconstruction: 96.55018138885498  Train_KL: 3.4794530272483826  Validation Loss : 98.04170227050781 Val_Reconstruction : 94.57991409301758 Val_KL : 3.4617886543273926\n","Epoch: 1252/8000  Traning Loss: 99.85418224334717  Train_Reconstruction: 96.37207508087158  Train_KL: 3.4821062088012695  Validation Loss : 97.95312881469727 Val_Reconstruction : 94.50590896606445 Val_KL : 3.447218656539917\n","Epoch: 1253/8000  Traning Loss: 99.76681613922119  Train_Reconstruction: 96.29352378845215  Train_KL: 3.473292797803879  Validation Loss : 98.05936813354492 Val_Reconstruction : 94.61942291259766 Val_KL : 3.439945697784424\n","Epoch: 1254/8000  Traning Loss: 99.86850070953369  Train_Reconstruction: 96.39279651641846  Train_KL: 3.475704401731491  Validation Loss : 98.15875244140625 Val_Reconstruction : 94.7070426940918 Val_KL : 3.4517102241516113\n","Epoch: 1255/8000  Traning Loss: 100.16108417510986  Train_Reconstruction: 96.66636657714844  Train_KL: 3.4947157204151154  Validation Loss : 98.5501708984375 Val_Reconstruction : 95.10081100463867 Val_KL : 3.4493608474731445\n","Epoch: 1256/8000  Traning Loss: 100.10951519012451  Train_Reconstruction: 96.64471340179443  Train_KL: 3.4648018181324005  Validation Loss : 98.56112289428711 Val_Reconstruction : 95.12866973876953 Val_KL : 3.432451367378235\n","Epoch: 1257/8000  Traning Loss: 100.51722240447998  Train_Reconstruction: 97.03865909576416  Train_KL: 3.478563964366913  Validation Loss : 98.82615661621094 Val_Reconstruction : 95.39644622802734 Val_KL : 3.4297115802764893\n","Epoch: 1258/8000  Traning Loss: 100.65459156036377  Train_Reconstruction: 97.19735527038574  Train_KL: 3.457236886024475  Validation Loss : 98.79232025146484 Val_Reconstruction : 95.3740005493164 Val_KL : 3.4183197021484375\n","Epoch: 1259/8000  Traning Loss: 100.68167400360107  Train_Reconstruction: 97.23376655578613  Train_KL: 3.447907119989395  Validation Loss : 98.77901840209961 Val_Reconstruction : 95.37024688720703 Val_KL : 3.4087748527526855\n","Epoch: 1260/8000  Traning Loss: 100.11425685882568  Train_Reconstruction: 96.64310550689697  Train_KL: 3.4711514711380005  Validation Loss : 98.01425170898438 Val_Reconstruction : 94.56393051147461 Val_KL : 3.450323700904846\n","Epoch: 1261/8000  Traning Loss: 99.93557357788086  Train_Reconstruction: 96.45010280609131  Train_KL: 3.485472232103348  Validation Loss : 98.12508392333984 Val_Reconstruction : 94.70053482055664 Val_KL : 3.4245492219924927\n","Epoch: 1262/8000  Traning Loss: 100.12652778625488  Train_Reconstruction: 96.67710590362549  Train_KL: 3.4494218230247498  Validation Loss : 98.28414535522461 Val_Reconstruction : 94.86779403686523 Val_KL : 3.416353702545166\n","Epoch: 1263/8000  Traning Loss: 100.13421535491943  Train_Reconstruction: 96.67527770996094  Train_KL: 3.4589368402957916  Validation Loss : 98.27436065673828 Val_Reconstruction : 94.85149002075195 Val_KL : 3.4228715896606445\n","Epoch: 1264/8000  Traning Loss: 100.13264751434326  Train_Reconstruction: 96.66804885864258  Train_KL: 3.464598000049591  Validation Loss : 98.27957916259766 Val_Reconstruction : 94.83235931396484 Val_KL : 3.447221040725708\n","Epoch: 1265/8000  Traning Loss: 100.03307342529297  Train_Reconstruction: 96.54065036773682  Train_KL: 3.4924236834049225  Validation Loss : 98.06738662719727 Val_Reconstruction : 94.61170959472656 Val_KL : 3.45567786693573\n","Epoch: 1266/8000  Traning Loss: 100.25271701812744  Train_Reconstruction: 96.78494167327881  Train_KL: 3.4677754044532776  Validation Loss : 98.81595230102539 Val_Reconstruction : 95.38324356079102 Val_KL : 3.4327093362808228\n","Epoch: 1267/8000  Traning Loss: 100.89041900634766  Train_Reconstruction: 97.4115219116211  Train_KL: 3.478894829750061  Validation Loss : 98.87345886230469 Val_Reconstruction : 95.4134521484375 Val_KL : 3.4600090980529785\n","Epoch: 1268/8000  Traning Loss: 100.94544124603271  Train_Reconstruction: 97.45578479766846  Train_KL: 3.489656925201416  Validation Loss : 99.12012481689453 Val_Reconstruction : 95.67707443237305 Val_KL : 3.443050980567932\n","Epoch: 1269/8000  Traning Loss: 100.39681911468506  Train_Reconstruction: 96.94032859802246  Train_KL: 3.456490397453308  Validation Loss : 98.4637451171875 Val_Reconstruction : 95.04521179199219 Val_KL : 3.4185333251953125\n","Epoch: 1270/8000  Traning Loss: 99.84465408325195  Train_Reconstruction: 96.37828731536865  Train_KL: 3.466366082429886  Validation Loss : 97.94964218139648 Val_Reconstruction : 94.51960372924805 Val_KL : 3.430039167404175\n","Epoch: 1271/8000  Traning Loss: 99.98535060882568  Train_Reconstruction: 96.52237892150879  Train_KL: 3.462970793247223  Validation Loss : 98.11405944824219 Val_Reconstruction : 94.68371200561523 Val_KL : 3.4303466081619263\n","Epoch: 1272/8000  Traning Loss: 99.87849426269531  Train_Reconstruction: 96.4010705947876  Train_KL: 3.4774234890937805  Validation Loss : 97.75342178344727 Val_Reconstruction : 94.30749130249023 Val_KL : 3.4459296464920044\n","Epoch: 1273/8000  Traning Loss: 100.0154857635498  Train_Reconstruction: 96.53347587585449  Train_KL: 3.482010453939438  Validation Loss : 98.89126205444336 Val_Reconstruction : 95.44220352172852 Val_KL : 3.44905948638916\n","Epoch: 1274/8000  Traning Loss: 100.31445217132568  Train_Reconstruction: 96.8367919921875  Train_KL: 3.477660745382309  Validation Loss : 98.35452651977539 Val_Reconstruction : 94.91306686401367 Val_KL : 3.4414597749710083\n","Epoch: 1275/8000  Traning Loss: 99.95589828491211  Train_Reconstruction: 96.48604488372803  Train_KL: 3.469853550195694  Validation Loss : 97.9505615234375 Val_Reconstruction : 94.5206527709961 Val_KL : 3.4299060106277466\n","Epoch: 1276/8000  Traning Loss: 99.74455070495605  Train_Reconstruction: 96.28820419311523  Train_KL: 3.45634663105011  Validation Loss : 97.83530044555664 Val_Reconstruction : 94.41376495361328 Val_KL : 3.421534538269043\n","Epoch: 1277/8000  Traning Loss: 99.68161869049072  Train_Reconstruction: 96.22181224822998  Train_KL: 3.459806442260742  Validation Loss : 97.75584411621094 Val_Reconstruction : 94.32373046875 Val_KL : 3.432114362716675\n","Epoch: 1278/8000  Traning Loss: 99.74758338928223  Train_Reconstruction: 96.27343082427979  Train_KL: 3.474152684211731  Validation Loss : 98.3052978515625 Val_Reconstruction : 94.86281204223633 Val_KL : 3.44248366355896\n","Epoch: 1279/8000  Traning Loss: 100.10285472869873  Train_Reconstruction: 96.62874507904053  Train_KL: 3.474109411239624  Validation Loss : 98.13998794555664 Val_Reconstruction : 94.703857421875 Val_KL : 3.436129331588745\n","Epoch: 1280/8000  Traning Loss: 100.15943241119385  Train_Reconstruction: 96.68998527526855  Train_KL: 3.469446986913681  Validation Loss : 98.53384017944336 Val_Reconstruction : 95.10170364379883 Val_KL : 3.4321378469467163\n","Epoch: 1281/8000  Traning Loss: 100.42556953430176  Train_Reconstruction: 96.94339942932129  Train_KL: 3.4821706116199493  Validation Loss : 99.02421569824219 Val_Reconstruction : 95.58150100708008 Val_KL : 3.4427130222320557\n","Epoch: 1282/8000  Traning Loss: 100.64833450317383  Train_Reconstruction: 97.16545963287354  Train_KL: 3.482874482870102  Validation Loss : 98.90386962890625 Val_Reconstruction : 95.46602630615234 Val_KL : 3.4378437995910645\n","Epoch: 1283/8000  Traning Loss: 100.85716152191162  Train_Reconstruction: 97.39557647705078  Train_KL: 3.461584657430649  Validation Loss : 98.977294921875 Val_Reconstruction : 95.5445327758789 Val_KL : 3.432762861251831\n","Epoch: 1284/8000  Traning Loss: 100.2696008682251  Train_Reconstruction: 96.79778003692627  Train_KL: 3.4718220233917236  Validation Loss : 98.51112365722656 Val_Reconstruction : 95.06914901733398 Val_KL : 3.4419732093811035\n","Epoch: 1285/8000  Traning Loss: 100.32745170593262  Train_Reconstruction: 96.8544864654541  Train_KL: 3.4729658365249634  Validation Loss : 98.80810546875 Val_Reconstruction : 95.36962890625 Val_KL : 3.4384766817092896\n","Epoch: 1286/8000  Traning Loss: 100.43152809143066  Train_Reconstruction: 96.96130657196045  Train_KL: 3.4702205061912537  Validation Loss : 98.44703674316406 Val_Reconstruction : 95.01852035522461 Val_KL : 3.428513765335083\n","Epoch: 1287/8000  Traning Loss: 100.02487182617188  Train_Reconstruction: 96.56414604187012  Train_KL: 3.4607253670692444  Validation Loss : 98.50713348388672 Val_Reconstruction : 95.09289932250977 Val_KL : 3.414233922958374\n","Epoch: 1288/8000  Traning Loss: 100.60139846801758  Train_Reconstruction: 97.13980197906494  Train_KL: 3.46159765124321  Validation Loss : 98.62829208374023 Val_Reconstruction : 95.20537567138672 Val_KL : 3.4229153394699097\n","Epoch: 1289/8000  Traning Loss: 100.15846061706543  Train_Reconstruction: 96.69547367095947  Train_KL: 3.4629871249198914  Validation Loss : 97.92161560058594 Val_Reconstruction : 94.50071334838867 Val_KL : 3.4209017753601074\n","Epoch: 1290/8000  Traning Loss: 99.77178001403809  Train_Reconstruction: 96.30748271942139  Train_KL: 3.4642968475818634  Validation Loss : 97.87300109863281 Val_Reconstruction : 94.44271087646484 Val_KL : 3.430292844772339\n","Epoch: 1291/8000  Traning Loss: 99.75440406799316  Train_Reconstruction: 96.28723239898682  Train_KL: 3.4671712517738342  Validation Loss : 98.02642822265625 Val_Reconstruction : 94.58274841308594 Val_KL : 3.4436784982681274\n","Epoch: 1292/8000  Traning Loss: 99.8698673248291  Train_Reconstruction: 96.39204788208008  Train_KL: 3.477819859981537  Validation Loss : 98.2708969116211 Val_Reconstruction : 94.83590698242188 Val_KL : 3.434989809989929\n","Epoch: 1293/8000  Traning Loss: 100.26383876800537  Train_Reconstruction: 96.78514099121094  Train_KL: 3.478698879480362  Validation Loss : 98.7156867980957 Val_Reconstruction : 95.26539611816406 Val_KL : 3.4502912759780884\n","Epoch: 1294/8000  Traning Loss: 100.2312593460083  Train_Reconstruction: 96.74916648864746  Train_KL: 3.4820936918258667  Validation Loss : 98.39628601074219 Val_Reconstruction : 94.95758819580078 Val_KL : 3.438698649406433\n","Epoch: 1295/8000  Traning Loss: 100.09700298309326  Train_Reconstruction: 96.62441158294678  Train_KL: 3.4725921750068665  Validation Loss : 98.48813247680664 Val_Reconstruction : 95.05205154418945 Val_KL : 3.4360824823379517\n","Epoch: 1296/8000  Traning Loss: 100.31118774414062  Train_Reconstruction: 96.84946346282959  Train_KL: 3.4617246985435486  Validation Loss : 97.99541473388672 Val_Reconstruction : 94.57136535644531 Val_KL : 3.424049496650696\n","Epoch: 1297/8000  Traning Loss: 100.20836734771729  Train_Reconstruction: 96.73665809631348  Train_KL: 3.4717081487178802  Validation Loss : 98.47381591796875 Val_Reconstruction : 95.03460311889648 Val_KL : 3.439212441444397\n","Epoch: 1298/8000  Traning Loss: 100.12918949127197  Train_Reconstruction: 96.64733791351318  Train_KL: 3.4818506836891174  Validation Loss : 98.41202926635742 Val_Reconstruction : 94.96028518676758 Val_KL : 3.451743721961975\n","Epoch: 1299/8000  Traning Loss: 99.77535152435303  Train_Reconstruction: 96.30021381378174  Train_KL: 3.4751376807689667  Validation Loss : 98.12066268920898 Val_Reconstruction : 94.68562316894531 Val_KL : 3.4350396394729614\n","Epoch: 1300/8000  Traning Loss: 99.87557888031006  Train_Reconstruction: 96.40681457519531  Train_KL: 3.4687644839286804  Validation Loss : 98.33888244628906 Val_Reconstruction : 94.89902114868164 Val_KL : 3.439860701560974\n","Epoch: 1301/8000  Traning Loss: 99.85443210601807  Train_Reconstruction: 96.3744888305664  Train_KL: 3.479942947626114  Validation Loss : 98.38418579101562 Val_Reconstruction : 94.94315719604492 Val_KL : 3.441028594970703\n","Epoch: 1302/8000  Traning Loss: 100.00924777984619  Train_Reconstruction: 96.52537059783936  Train_KL: 3.4838767647743225  Validation Loss : 98.10979080200195 Val_Reconstruction : 94.66449356079102 Val_KL : 3.4452991485595703\n","Epoch: 1303/8000  Traning Loss: 99.84427547454834  Train_Reconstruction: 96.3735408782959  Train_KL: 3.4707336127758026  Validation Loss : 98.08508682250977 Val_Reconstruction : 94.65939331054688 Val_KL : 3.425696849822998\n","Epoch: 1304/8000  Traning Loss: 100.21289825439453  Train_Reconstruction: 96.75159168243408  Train_KL: 3.4613062739372253  Validation Loss : 99.07495880126953 Val_Reconstruction : 95.65530776977539 Val_KL : 3.4196510314941406\n","Epoch: 1305/8000  Traning Loss: 100.9486494064331  Train_Reconstruction: 97.48139190673828  Train_KL: 3.4672562777996063  Validation Loss : 98.7464485168457 Val_Reconstruction : 95.31988906860352 Val_KL : 3.426559090614319\n","Epoch: 1306/8000  Traning Loss: 100.61227321624756  Train_Reconstruction: 97.13386726379395  Train_KL: 3.4784066677093506  Validation Loss : 99.00640487670898 Val_Reconstruction : 95.55194854736328 Val_KL : 3.4544565677642822\n","Epoch: 1307/8000  Traning Loss: 100.46916198730469  Train_Reconstruction: 96.98526573181152  Train_KL: 3.483897089958191  Validation Loss : 98.79238891601562 Val_Reconstruction : 95.3583869934082 Val_KL : 3.4340020418167114\n","Epoch: 1308/8000  Traning Loss: 100.2931432723999  Train_Reconstruction: 96.82420635223389  Train_KL: 3.468935877084732  Validation Loss : 98.46601867675781 Val_Reconstruction : 95.02729415893555 Val_KL : 3.4387274980545044\n","Epoch: 1309/8000  Traning Loss: 100.28443908691406  Train_Reconstruction: 96.81646251678467  Train_KL: 3.467978835105896  Validation Loss : 98.39762878417969 Val_Reconstruction : 94.95455932617188 Val_KL : 3.4430683851242065\n","Epoch: 1310/8000  Traning Loss: 100.09161186218262  Train_Reconstruction: 96.61750411987305  Train_KL: 3.4741081595420837  Validation Loss : 98.50165557861328 Val_Reconstruction : 95.0759048461914 Val_KL : 3.425750732421875\n","Epoch: 1311/8000  Traning Loss: 99.94127178192139  Train_Reconstruction: 96.46997928619385  Train_KL: 3.4712925851345062  Validation Loss : 98.23263168334961 Val_Reconstruction : 94.77934265136719 Val_KL : 3.45328950881958\n","Epoch: 1312/8000  Traning Loss: 100.09234523773193  Train_Reconstruction: 96.62150478363037  Train_KL: 3.470839649438858  Validation Loss : 98.3674087524414 Val_Reconstruction : 94.94992446899414 Val_KL : 3.4174840450286865\n","Epoch: 1313/8000  Traning Loss: 100.30354404449463  Train_Reconstruction: 96.85303401947021  Train_KL: 3.450508326292038  Validation Loss : 98.48989868164062 Val_Reconstruction : 95.0731315612793 Val_KL : 3.4167665243148804\n","Epoch: 1314/8000  Traning Loss: 100.26380062103271  Train_Reconstruction: 96.7907772064209  Train_KL: 3.4730239510536194  Validation Loss : 98.51857376098633 Val_Reconstruction : 95.05465316772461 Val_KL : 3.4639220237731934\n","Epoch: 1315/8000  Traning Loss: 100.09473133087158  Train_Reconstruction: 96.61740016937256  Train_KL: 3.4773323833942413  Validation Loss : 98.25546264648438 Val_Reconstruction : 94.83467102050781 Val_KL : 3.420791268348694\n","Epoch: 1316/8000  Traning Loss: 99.89526557922363  Train_Reconstruction: 96.43448257446289  Train_KL: 3.460782766342163  Validation Loss : 98.12193298339844 Val_Reconstruction : 94.70355224609375 Val_KL : 3.418381452560425\n","Epoch: 1317/8000  Traning Loss: 99.78031921386719  Train_Reconstruction: 96.31003761291504  Train_KL: 3.4702822864055634  Validation Loss : 98.05439376831055 Val_Reconstruction : 94.60659790039062 Val_KL : 3.447796106338501\n","Epoch: 1318/8000  Traning Loss: 99.74613285064697  Train_Reconstruction: 96.25676536560059  Train_KL: 3.4893676340579987  Validation Loss : 97.8925666809082 Val_Reconstruction : 94.42531204223633 Val_KL : 3.4672573804855347\n","Epoch: 1319/8000  Traning Loss: 100.1073350906372  Train_Reconstruction: 96.6208963394165  Train_KL: 3.486437886953354  Validation Loss : 98.45329284667969 Val_Reconstruction : 95.01591491699219 Val_KL : 3.4373799562454224\n","Epoch: 1320/8000  Traning Loss: 100.06643581390381  Train_Reconstruction: 96.59206295013428  Train_KL: 3.474372446537018  Validation Loss : 98.1879997253418 Val_Reconstruction : 94.74973678588867 Val_KL : 3.4382612705230713\n","Epoch: 1321/8000  Traning Loss: 99.60474872589111  Train_Reconstruction: 96.13808917999268  Train_KL: 3.4666607677936554  Validation Loss : 97.7972526550293 Val_Reconstruction : 94.36276245117188 Val_KL : 3.4344924688339233\n","Epoch: 1322/8000  Traning Loss: 99.51319789886475  Train_Reconstruction: 96.03472805023193  Train_KL: 3.478468894958496  Validation Loss : 97.84380722045898 Val_Reconstruction : 94.41169357299805 Val_KL : 3.432113766670227\n","Epoch: 1323/8000  Traning Loss: 99.56679344177246  Train_Reconstruction: 96.09055042266846  Train_KL: 3.476245015859604  Validation Loss : 97.62299346923828 Val_Reconstruction : 94.19166946411133 Val_KL : 3.4313247203826904\n","Epoch: 1324/8000  Traning Loss: 99.43552017211914  Train_Reconstruction: 95.96315002441406  Train_KL: 3.4723712503910065  Validation Loss : 97.78145217895508 Val_Reconstruction : 94.33805847167969 Val_KL : 3.4433923959732056\n","Epoch: 1325/8000  Traning Loss: 99.32842445373535  Train_Reconstruction: 95.864501953125  Train_KL: 3.463921844959259  Validation Loss : 97.44615173339844 Val_Reconstruction : 94.03072738647461 Val_KL : 3.4154231548309326\n","Epoch: 1326/8000  Traning Loss: 99.49986743927002  Train_Reconstruction: 96.03926944732666  Train_KL: 3.4605990052223206  Validation Loss : 97.80923080444336 Val_Reconstruction : 94.36574935913086 Val_KL : 3.4434807300567627\n","Epoch: 1327/8000  Traning Loss: 99.63368129730225  Train_Reconstruction: 96.14878559112549  Train_KL: 3.484895884990692  Validation Loss : 98.15225219726562 Val_Reconstruction : 94.7174301147461 Val_KL : 3.434821605682373\n","Epoch: 1328/8000  Traning Loss: 100.12948036193848  Train_Reconstruction: 96.65470504760742  Train_KL: 3.4747743606567383  Validation Loss : 97.98959732055664 Val_Reconstruction : 94.53739166259766 Val_KL : 3.4522048234939575\n","Epoch: 1329/8000  Traning Loss: 100.11277198791504  Train_Reconstruction: 96.6411304473877  Train_KL: 3.4716419875621796  Validation Loss : 98.08152389526367 Val_Reconstruction : 94.65771484375 Val_KL : 3.42380952835083\n","Epoch: 1330/8000  Traning Loss: 100.14720249176025  Train_Reconstruction: 96.67653274536133  Train_KL: 3.4706697165966034  Validation Loss : 98.01299285888672 Val_Reconstruction : 94.56869506835938 Val_KL : 3.444298267364502\n","Epoch: 1331/8000  Traning Loss: 99.88522720336914  Train_Reconstruction: 96.41978359222412  Train_KL: 3.465442717075348  Validation Loss : 98.09866714477539 Val_Reconstruction : 94.68465042114258 Val_KL : 3.4140180349349976\n","Epoch: 1332/8000  Traning Loss: 99.83131980895996  Train_Reconstruction: 96.37297058105469  Train_KL: 3.458347737789154  Validation Loss : 98.03933715820312 Val_Reconstruction : 94.59841918945312 Val_KL : 3.4409191608428955\n","Epoch: 1333/8000  Traning Loss: 100.10623836517334  Train_Reconstruction: 96.61139869689941  Train_KL: 3.494840443134308  Validation Loss : 98.4852066040039 Val_Reconstruction : 95.02789688110352 Val_KL : 3.457306742668152\n","Epoch: 1334/8000  Traning Loss: 100.20574951171875  Train_Reconstruction: 96.73696994781494  Train_KL: 3.468779057264328  Validation Loss : 98.22447204589844 Val_Reconstruction : 94.80961990356445 Val_KL : 3.414855122566223\n","Epoch: 1335/8000  Traning Loss: 99.95941829681396  Train_Reconstruction: 96.50802230834961  Train_KL: 3.451396405696869  Validation Loss : 97.98800659179688 Val_Reconstruction : 94.5709228515625 Val_KL : 3.417082667350769\n","Epoch: 1336/8000  Traning Loss: 99.67809200286865  Train_Reconstruction: 96.209885597229  Train_KL: 3.468206435441971  Validation Loss : 97.79751205444336 Val_Reconstruction : 94.3563461303711 Val_KL : 3.441166400909424\n","Epoch: 1337/8000  Traning Loss: 99.54395580291748  Train_Reconstruction: 96.06575012207031  Train_KL: 3.4782084822654724  Validation Loss : 97.58111953735352 Val_Reconstruction : 94.13837814331055 Val_KL : 3.4427406787872314\n","Epoch: 1338/8000  Traning Loss: 99.49707984924316  Train_Reconstruction: 96.02270889282227  Train_KL: 3.474371463060379  Validation Loss : 97.67723846435547 Val_Reconstruction : 94.23545837402344 Val_KL : 3.441781759262085\n","Epoch: 1339/8000  Traning Loss: 99.7382459640503  Train_Reconstruction: 96.25482940673828  Train_KL: 3.483416348695755  Validation Loss : 98.3285026550293 Val_Reconstruction : 94.90261459350586 Val_KL : 3.42588472366333\n","Epoch: 1340/8000  Traning Loss: 99.71404361724854  Train_Reconstruction: 96.24798488616943  Train_KL: 3.466059386730194  Validation Loss : 98.05401611328125 Val_Reconstruction : 94.6338996887207 Val_KL : 3.42011296749115\n","Epoch: 1341/8000  Traning Loss: 99.4837760925293  Train_Reconstruction: 96.01351547241211  Train_KL: 3.470260262489319  Validation Loss : 97.73057556152344 Val_Reconstruction : 94.28569412231445 Val_KL : 3.4448816776275635\n","Epoch: 1342/8000  Traning Loss: 99.5946159362793  Train_Reconstruction: 96.1218204498291  Train_KL: 3.4727956652641296  Validation Loss : 97.85366821289062 Val_Reconstruction : 94.42763137817383 Val_KL : 3.426035761833191\n","Epoch: 1343/8000  Traning Loss: 99.86682224273682  Train_Reconstruction: 96.4132490158081  Train_KL: 3.45357283949852  Validation Loss : 97.90496063232422 Val_Reconstruction : 94.48648452758789 Val_KL : 3.4184762239456177\n","Epoch: 1344/8000  Traning Loss: 99.65195178985596  Train_Reconstruction: 96.19070243835449  Train_KL: 3.461248964071274  Validation Loss : 97.79184341430664 Val_Reconstruction : 94.36352920532227 Val_KL : 3.428315758705139\n","Epoch: 1345/8000  Traning Loss: 99.57516574859619  Train_Reconstruction: 96.10057163238525  Train_KL: 3.4745943546295166  Validation Loss : 97.83269500732422 Val_Reconstruction : 94.40194320678711 Val_KL : 3.4307507276535034\n","Epoch: 1346/8000  Traning Loss: 99.40891456604004  Train_Reconstruction: 95.94295692443848  Train_KL: 3.465957373380661  Validation Loss : 97.87342071533203 Val_Reconstruction : 94.43918228149414 Val_KL : 3.434242010116577\n","Epoch: 1347/8000  Traning Loss: 99.68141078948975  Train_Reconstruction: 96.2010726928711  Train_KL: 3.4803385138511658  Validation Loss : 97.97054672241211 Val_Reconstruction : 94.54478073120117 Val_KL : 3.4257649183273315\n","Epoch: 1348/8000  Traning Loss: 99.64696311950684  Train_Reconstruction: 96.18711948394775  Train_KL: 3.459842711687088  Validation Loss : 97.50091552734375 Val_Reconstruction : 94.07419967651367 Val_KL : 3.4267152547836304\n","Epoch: 1349/8000  Traning Loss: 99.30927753448486  Train_Reconstruction: 95.84673404693604  Train_KL: 3.462543159723282  Validation Loss : 97.4280891418457 Val_Reconstruction : 93.99309539794922 Val_KL : 3.4349942207336426\n","Epoch: 1350/8000  Traning Loss: 99.4987211227417  Train_Reconstruction: 96.01660823822021  Train_KL: 3.48211133480072  Validation Loss : 97.76004028320312 Val_Reconstruction : 94.31814193725586 Val_KL : 3.4418996572494507\n","Epoch: 1351/8000  Traning Loss: 99.69201564788818  Train_Reconstruction: 96.2144422531128  Train_KL: 3.4775737524032593  Validation Loss : 98.2048110961914 Val_Reconstruction : 94.75634384155273 Val_KL : 3.448469042778015\n","Epoch: 1352/8000  Traning Loss: 100.15823745727539  Train_Reconstruction: 96.65993595123291  Train_KL: 3.4983007311820984  Validation Loss : 98.67502975463867 Val_Reconstruction : 95.22620391845703 Val_KL : 3.4488279819488525\n","Epoch: 1353/8000  Traning Loss: 100.21678733825684  Train_Reconstruction: 96.74583911895752  Train_KL: 3.470947742462158  Validation Loss : 98.28593063354492 Val_Reconstruction : 94.85317611694336 Val_KL : 3.4327545166015625\n","Epoch: 1354/8000  Traning Loss: 99.70363807678223  Train_Reconstruction: 96.2464427947998  Train_KL: 3.457195967435837  Validation Loss : 97.83179092407227 Val_Reconstruction : 94.4158935546875 Val_KL : 3.415899634361267\n","Epoch: 1355/8000  Traning Loss: 99.62387275695801  Train_Reconstruction: 96.16061592102051  Train_KL: 3.4632575511932373  Validation Loss : 98.36899185180664 Val_Reconstruction : 94.93521499633789 Val_KL : 3.433777093887329\n","Epoch: 1356/8000  Traning Loss: 99.80856895446777  Train_Reconstruction: 96.33003616333008  Train_KL: 3.4785328805446625  Validation Loss : 98.34524917602539 Val_Reconstruction : 94.92542266845703 Val_KL : 3.4198256731033325\n","Epoch: 1357/8000  Traning Loss: 99.7431697845459  Train_Reconstruction: 96.2740364074707  Train_KL: 3.4691323339939117  Validation Loss : 97.73562240600586 Val_Reconstruction : 94.29056930541992 Val_KL : 3.4450559616088867\n","Epoch: 1358/8000  Traning Loss: 99.82960891723633  Train_Reconstruction: 96.36065673828125  Train_KL: 3.468952715396881  Validation Loss : 97.8557243347168 Val_Reconstruction : 94.43122100830078 Val_KL : 3.424500584602356\n","Epoch: 1359/8000  Traning Loss: 99.95045757293701  Train_Reconstruction: 96.48393154144287  Train_KL: 3.4665257334709167  Validation Loss : 98.1514663696289 Val_Reconstruction : 94.71047973632812 Val_KL : 3.4409897327423096\n","Epoch: 1360/8000  Traning Loss: 99.59330558776855  Train_Reconstruction: 96.12669849395752  Train_KL: 3.4666071236133575  Validation Loss : 98.34424209594727 Val_Reconstruction : 94.9140510559082 Val_KL : 3.430190086364746\n","Epoch: 1361/8000  Traning Loss: 99.80463409423828  Train_Reconstruction: 96.327880859375  Train_KL: 3.476754367351532  Validation Loss : 98.23614120483398 Val_Reconstruction : 94.79382705688477 Val_KL : 3.442314386367798\n","Epoch: 1362/8000  Traning Loss: 99.7963514328003  Train_Reconstruction: 96.31876373291016  Train_KL: 3.4775880575180054  Validation Loss : 98.19761276245117 Val_Reconstruction : 94.7652359008789 Val_KL : 3.4323768615722656\n","Epoch: 1363/8000  Traning Loss: 99.39606666564941  Train_Reconstruction: 95.93691062927246  Train_KL: 3.4591579139232635  Validation Loss : 97.47755432128906 Val_Reconstruction : 94.04958724975586 Val_KL : 3.427967071533203\n","Epoch: 1364/8000  Traning Loss: 99.29062175750732  Train_Reconstruction: 95.82076644897461  Train_KL: 3.469854950904846  Validation Loss : 97.72661972045898 Val_Reconstruction : 94.29127883911133 Val_KL : 3.435339570045471\n","Epoch: 1365/8000  Traning Loss: 99.53652095794678  Train_Reconstruction: 96.06951713562012  Train_KL: 3.4670028388500214  Validation Loss : 97.92135620117188 Val_Reconstruction : 94.49908065795898 Val_KL : 3.422278046607971\n","Epoch: 1366/8000  Traning Loss: 99.51592636108398  Train_Reconstruction: 96.05061149597168  Train_KL: 3.465315729379654  Validation Loss : 97.55730819702148 Val_Reconstruction : 94.12736511230469 Val_KL : 3.4299418926239014\n","Epoch: 1367/8000  Traning Loss: 99.63607978820801  Train_Reconstruction: 96.17117309570312  Train_KL: 3.4649065732955933  Validation Loss : 97.80721664428711 Val_Reconstruction : 94.36718368530273 Val_KL : 3.44003427028656\n","Epoch: 1368/8000  Traning Loss: 99.64242172241211  Train_Reconstruction: 96.16970920562744  Train_KL: 3.4727125763893127  Validation Loss : 97.9365348815918 Val_Reconstruction : 94.50305557250977 Val_KL : 3.433477759361267\n","Epoch: 1369/8000  Traning Loss: 99.44272708892822  Train_Reconstruction: 95.97410583496094  Train_KL: 3.4686209559440613  Validation Loss : 97.55243301391602 Val_Reconstruction : 94.11087799072266 Val_KL : 3.4415557384490967\n","Epoch: 1370/8000  Traning Loss: 99.6300277709961  Train_Reconstruction: 96.13270664215088  Train_KL: 3.4973220229148865  Validation Loss : 97.60477447509766 Val_Reconstruction : 94.14183807373047 Val_KL : 3.462937355041504\n","Epoch: 1371/8000  Traning Loss: 99.80430126190186  Train_Reconstruction: 96.32611751556396  Train_KL: 3.4781830310821533  Validation Loss : 98.14007568359375 Val_Reconstruction : 94.7111701965332 Val_KL : 3.4289075136184692\n","Epoch: 1372/8000  Traning Loss: 99.61457824707031  Train_Reconstruction: 96.14304256439209  Train_KL: 3.4715356826782227  Validation Loss : 97.6556510925293 Val_Reconstruction : 94.21589279174805 Val_KL : 3.4397605657577515\n","Epoch: 1373/8000  Traning Loss: 99.18698406219482  Train_Reconstruction: 95.70567512512207  Train_KL: 3.481310397386551  Validation Loss : 97.47054672241211 Val_Reconstruction : 94.03361892700195 Val_KL : 3.4369261264801025\n","Epoch: 1374/8000  Traning Loss: 99.11284828186035  Train_Reconstruction: 95.64180278778076  Train_KL: 3.4710454046726227  Validation Loss : 97.57046890258789 Val_Reconstruction : 94.14781188964844 Val_KL : 3.4226572513580322\n","Epoch: 1375/8000  Traning Loss: 99.18813896179199  Train_Reconstruction: 95.7143440246582  Train_KL: 3.4737946689128876  Validation Loss : 97.33395004272461 Val_Reconstruction : 93.90926361083984 Val_KL : 3.424686074256897\n","Epoch: 1376/8000  Traning Loss: 99.04761600494385  Train_Reconstruction: 95.56957721710205  Train_KL: 3.4780388176441193  Validation Loss : 97.38424301147461 Val_Reconstruction : 93.93439865112305 Val_KL : 3.4498443603515625\n","Epoch: 1377/8000  Traning Loss: 99.1335678100586  Train_Reconstruction: 95.65227127075195  Train_KL: 3.4812976121902466  Validation Loss : 97.6385269165039 Val_Reconstruction : 94.21294021606445 Val_KL : 3.4255834817886353\n","Epoch: 1378/8000  Traning Loss: 99.27784061431885  Train_Reconstruction: 95.81813144683838  Train_KL: 3.4597082138061523  Validation Loss : 97.70550155639648 Val_Reconstruction : 94.2869758605957 Val_KL : 3.418526768684387\n","Epoch: 1379/8000  Traning Loss: 99.32580471038818  Train_Reconstruction: 95.8572130203247  Train_KL: 3.4685913920402527  Validation Loss : 97.74203872680664 Val_Reconstruction : 94.31193923950195 Val_KL : 3.430099844932556\n","Epoch: 1380/8000  Traning Loss: 99.79584407806396  Train_Reconstruction: 96.31737995147705  Train_KL: 3.4784646928310394  Validation Loss : 98.15213012695312 Val_Reconstruction : 94.72151947021484 Val_KL : 3.430611252784729\n","Epoch: 1381/8000  Traning Loss: 99.35656642913818  Train_Reconstruction: 95.90660953521729  Train_KL: 3.449957400560379  Validation Loss : 97.48267364501953 Val_Reconstruction : 94.07445526123047 Val_KL : 3.4082199335098267\n","Epoch: 1382/8000  Traning Loss: 99.43050003051758  Train_Reconstruction: 95.9566707611084  Train_KL: 3.473829209804535  Validation Loss : 97.67290878295898 Val_Reconstruction : 94.23378372192383 Val_KL : 3.439127564430237\n","Epoch: 1383/8000  Traning Loss: 99.63203525543213  Train_Reconstruction: 96.15590286254883  Train_KL: 3.4761317670345306  Validation Loss : 97.83494186401367 Val_Reconstruction : 94.40678405761719 Val_KL : 3.4281595945358276\n","Epoch: 1384/8000  Traning Loss: 99.88778018951416  Train_Reconstruction: 96.41727447509766  Train_KL: 3.470503121614456  Validation Loss : 98.38718795776367 Val_Reconstruction : 94.94918441772461 Val_KL : 3.4380033016204834\n","Epoch: 1385/8000  Traning Loss: 99.76340579986572  Train_Reconstruction: 96.29555034637451  Train_KL: 3.4678544104099274  Validation Loss : 97.91858673095703 Val_Reconstruction : 94.48118209838867 Val_KL : 3.4374074935913086\n","Epoch: 1386/8000  Traning Loss: 99.50329685211182  Train_Reconstruction: 96.02933406829834  Train_KL: 3.4739619195461273  Validation Loss : 98.2305793762207 Val_Reconstruction : 94.81820678710938 Val_KL : 3.4123729467391968\n","Epoch: 1387/8000  Traning Loss: 99.38853931427002  Train_Reconstruction: 95.92211437225342  Train_KL: 3.4664256274700165  Validation Loss : 97.56553649902344 Val_Reconstruction : 94.13108825683594 Val_KL : 3.4344482421875\n","Epoch: 1388/8000  Traning Loss: 99.15701103210449  Train_Reconstruction: 95.68162822723389  Train_KL: 3.4753811955451965  Validation Loss : 97.55068969726562 Val_Reconstruction : 94.10637283325195 Val_KL : 3.4443132877349854\n","Epoch: 1389/8000  Traning Loss: 99.28466510772705  Train_Reconstruction: 95.80785274505615  Train_KL: 3.47681200504303  Validation Loss : 97.68278121948242 Val_Reconstruction : 94.24211502075195 Val_KL : 3.440666913986206\n","Epoch: 1390/8000  Traning Loss: 99.12247943878174  Train_Reconstruction: 95.65444374084473  Train_KL: 3.4680342078208923  Validation Loss : 97.52117919921875 Val_Reconstruction : 94.08970260620117 Val_KL : 3.431477427482605\n","Epoch: 1391/8000  Traning Loss: 99.20662021636963  Train_Reconstruction: 95.74130153656006  Train_KL: 3.4653187692165375  Validation Loss : 97.58742904663086 Val_Reconstruction : 94.16211318969727 Val_KL : 3.4253175258636475\n","Epoch: 1392/8000  Traning Loss: 100.0436544418335  Train_Reconstruction: 96.5778636932373  Train_KL: 3.465790569782257  Validation Loss : 99.31539535522461 Val_Reconstruction : 95.88197708129883 Val_KL : 3.4334158897399902\n","Epoch: 1393/8000  Traning Loss: 100.28742408752441  Train_Reconstruction: 96.82049179077148  Train_KL: 3.4669314324855804  Validation Loss : 98.19780731201172 Val_Reconstruction : 94.77374649047852 Val_KL : 3.4240599870681763\n","Epoch: 1394/8000  Traning Loss: 99.53841972351074  Train_Reconstruction: 96.07271194458008  Train_KL: 3.465707391500473  Validation Loss : 97.67055130004883 Val_Reconstruction : 94.23993301391602 Val_KL : 3.4306191205978394\n","Epoch: 1395/8000  Traning Loss: 99.05075263977051  Train_Reconstruction: 95.58252429962158  Train_KL: 3.468228816986084  Validation Loss : 97.28311538696289 Val_Reconstruction : 93.8458137512207 Val_KL : 3.4373003244400024\n","Epoch: 1396/8000  Traning Loss: 99.16272354125977  Train_Reconstruction: 95.68775367736816  Train_KL: 3.47496896982193  Validation Loss : 97.47288513183594 Val_Reconstruction : 94.04365158081055 Val_KL : 3.429231643676758\n","Epoch: 1397/8000  Traning Loss: 99.03647708892822  Train_Reconstruction: 95.56650161743164  Train_KL: 3.4699757397174835  Validation Loss : 97.54109954833984 Val_Reconstruction : 94.09088897705078 Val_KL : 3.450212597846985\n","Epoch: 1398/8000  Traning Loss: 99.21547985076904  Train_Reconstruction: 95.71912384033203  Train_KL: 3.4963561594486237  Validation Loss : 97.53718185424805 Val_Reconstruction : 94.08456039428711 Val_KL : 3.4526190757751465\n","Epoch: 1399/8000  Traning Loss: 99.04757881164551  Train_Reconstruction: 95.59203910827637  Train_KL: 3.455539345741272  Validation Loss : 97.45132827758789 Val_Reconstruction : 94.03510284423828 Val_KL : 3.4162278175354004\n","Epoch: 1400/8000  Traning Loss: 99.10035228729248  Train_Reconstruction: 95.61715698242188  Train_KL: 3.4831953048706055  Validation Loss : 97.44467163085938 Val_Reconstruction : 93.98916244506836 Val_KL : 3.455510377883911\n","Epoch: 1401/8000  Traning Loss: 99.20950889587402  Train_Reconstruction: 95.7460241317749  Train_KL: 3.4634837806224823  Validation Loss : 97.53410339355469 Val_Reconstruction : 94.1276741027832 Val_KL : 3.4064284563064575\n","Epoch: 1402/8000  Traning Loss: 99.2438554763794  Train_Reconstruction: 95.77559280395508  Train_KL: 3.4682635068893433  Validation Loss : 97.57381057739258 Val_Reconstruction : 94.12186050415039 Val_KL : 3.45194935798645\n","Epoch: 1403/8000  Traning Loss: 99.2634048461914  Train_Reconstruction: 95.78245830535889  Train_KL: 3.4809470176696777  Validation Loss : 97.64801406860352 Val_Reconstruction : 94.21866989135742 Val_KL : 3.4293469190597534\n","Epoch: 1404/8000  Traning Loss: 99.42378520965576  Train_Reconstruction: 95.95579433441162  Train_KL: 3.4679914116859436  Validation Loss : 97.86174011230469 Val_Reconstruction : 94.42681884765625 Val_KL : 3.4349234104156494\n","Epoch: 1405/8000  Traning Loss: 99.61847019195557  Train_Reconstruction: 96.14170837402344  Train_KL: 3.4767629504203796  Validation Loss : 97.72342300415039 Val_Reconstruction : 94.29166030883789 Val_KL : 3.4317620992660522\n","Epoch: 1406/8000  Traning Loss: 99.50299263000488  Train_Reconstruction: 96.02697944641113  Train_KL: 3.476013481616974  Validation Loss : 97.64783477783203 Val_Reconstruction : 94.20523834228516 Val_KL : 3.442597985267639\n","Epoch: 1407/8000  Traning Loss: 99.3643217086792  Train_Reconstruction: 95.89790916442871  Train_KL: 3.4664130210876465  Validation Loss : 97.91091918945312 Val_Reconstruction : 94.50221252441406 Val_KL : 3.4087077379226685\n","Epoch: 1408/8000  Traning Loss: 99.34616088867188  Train_Reconstruction: 95.8879919052124  Train_KL: 3.4581689536571503  Validation Loss : 97.42577362060547 Val_Reconstruction : 94.0094108581543 Val_KL : 3.416363835334778\n","Epoch: 1409/8000  Traning Loss: 99.04741764068604  Train_Reconstruction: 95.56885623931885  Train_KL: 3.478562891483307  Validation Loss : 97.4225959777832 Val_Reconstruction : 93.98192977905273 Val_KL : 3.440663456916809\n","Epoch: 1410/8000  Traning Loss: 98.9596004486084  Train_Reconstruction: 95.4762020111084  Train_KL: 3.4833991825580597  Validation Loss : 97.18531036376953 Val_Reconstruction : 93.74534606933594 Val_KL : 3.4399659633636475\n","Epoch: 1411/8000  Traning Loss: 98.96741485595703  Train_Reconstruction: 95.48776054382324  Train_KL: 3.4796552658081055  Validation Loss : 97.23212814331055 Val_Reconstruction : 93.79326248168945 Val_KL : 3.438864827156067\n","Epoch: 1412/8000  Traning Loss: 98.92809200286865  Train_Reconstruction: 95.44499492645264  Train_KL: 3.4830974638462067  Validation Loss : 97.43264770507812 Val_Reconstruction : 93.98333358764648 Val_KL : 3.4493151903152466\n","Epoch: 1413/8000  Traning Loss: 99.05707359313965  Train_Reconstruction: 95.56889343261719  Train_KL: 3.48818102478981  Validation Loss : 97.5329360961914 Val_Reconstruction : 94.10082626342773 Val_KL : 3.4321082830429077\n","Epoch: 1414/8000  Traning Loss: 99.39653491973877  Train_Reconstruction: 95.92659378051758  Train_KL: 3.469940423965454  Validation Loss : 97.5531005859375 Val_Reconstruction : 94.11974716186523 Val_KL : 3.4333542585372925\n","Epoch: 1415/8000  Traning Loss: 99.34330654144287  Train_Reconstruction: 95.85661506652832  Train_KL: 3.486691504716873  Validation Loss : 97.79826736450195 Val_Reconstruction : 94.34303665161133 Val_KL : 3.4552308320999146\n","Epoch: 1416/8000  Traning Loss: 99.40835189819336  Train_Reconstruction: 95.92900371551514  Train_KL: 3.479347139596939  Validation Loss : 98.20576477050781 Val_Reconstruction : 94.77661895751953 Val_KL : 3.4291460514068604\n","Epoch: 1417/8000  Traning Loss: 99.61955070495605  Train_Reconstruction: 96.16079139709473  Train_KL: 3.4587588906288147  Validation Loss : 98.03141784667969 Val_Reconstruction : 94.60723114013672 Val_KL : 3.4241859912872314\n","Epoch: 1418/8000  Traning Loss: 99.59593200683594  Train_Reconstruction: 96.13091945648193  Train_KL: 3.465012490749359  Validation Loss : 98.046142578125 Val_Reconstruction : 94.62380981445312 Val_KL : 3.422332763671875\n","Epoch: 1419/8000  Traning Loss: 99.63903999328613  Train_Reconstruction: 96.17727947235107  Train_KL: 3.4617599844932556  Validation Loss : 98.23361206054688 Val_Reconstruction : 94.81067657470703 Val_KL : 3.4229350090026855\n","Epoch: 1420/8000  Traning Loss: 99.44313335418701  Train_Reconstruction: 95.9694299697876  Train_KL: 3.4737033247947693  Validation Loss : 97.64058303833008 Val_Reconstruction : 94.20410537719727 Val_KL : 3.4364798069000244\n","Epoch: 1421/8000  Traning Loss: 98.97231578826904  Train_Reconstruction: 95.5149564743042  Train_KL: 3.4573596119880676  Validation Loss : 97.2523078918457 Val_Reconstruction : 93.83267974853516 Val_KL : 3.419628620147705\n","Epoch: 1422/8000  Traning Loss: 98.99821281433105  Train_Reconstruction: 95.51882839202881  Train_KL: 3.4793847501277924  Validation Loss : 97.57183837890625 Val_Reconstruction : 94.11930465698242 Val_KL : 3.4525344371795654\n","Epoch: 1423/8000  Traning Loss: 99.1627893447876  Train_Reconstruction: 95.6822738647461  Train_KL: 3.480514109134674  Validation Loss : 97.3834114074707 Val_Reconstruction : 93.94223022460938 Val_KL : 3.441183567047119\n","Epoch: 1424/8000  Traning Loss: 99.13689708709717  Train_Reconstruction: 95.64946937561035  Train_KL: 3.4874288737773895  Validation Loss : 97.56497955322266 Val_Reconstruction : 94.11347198486328 Val_KL : 3.4515072107315063\n","Epoch: 1425/8000  Traning Loss: 99.00730419158936  Train_Reconstruction: 95.51680088043213  Train_KL: 3.49050435423851  Validation Loss : 97.08466339111328 Val_Reconstruction : 93.63302230834961 Val_KL : 3.4516396522521973\n","Epoch: 1426/8000  Traning Loss: 98.9670934677124  Train_Reconstruction: 95.4931812286377  Train_KL: 3.473912537097931  Validation Loss : 97.23268127441406 Val_Reconstruction : 93.7844123840332 Val_KL : 3.448270559310913\n","Epoch: 1427/8000  Traning Loss: 98.96541118621826  Train_Reconstruction: 95.48163795471191  Train_KL: 3.483773320913315  Validation Loss : 97.32548904418945 Val_Reconstruction : 93.87691879272461 Val_KL : 3.4485703706741333\n","Epoch: 1428/8000  Traning Loss: 98.87963676452637  Train_Reconstruction: 95.41518688201904  Train_KL: 3.4644495248794556  Validation Loss : 97.08788681030273 Val_Reconstruction : 93.66794204711914 Val_KL : 3.419943928718567\n","Epoch: 1429/8000  Traning Loss: 98.98665237426758  Train_Reconstruction: 95.53179168701172  Train_KL: 3.45486056804657  Validation Loss : 97.23553466796875 Val_Reconstruction : 93.81240844726562 Val_KL : 3.423124074935913\n","Epoch: 1430/8000  Traning Loss: 99.30748081207275  Train_Reconstruction: 95.81682395935059  Train_KL: 3.490656793117523  Validation Loss : 97.64865112304688 Val_Reconstruction : 94.19587707519531 Val_KL : 3.4527747631073\n","Epoch: 1431/8000  Traning Loss: 99.189697265625  Train_Reconstruction: 95.70644760131836  Train_KL: 3.4832504391670227  Validation Loss : 97.35306167602539 Val_Reconstruction : 93.92739486694336 Val_KL : 3.4256690740585327\n","Epoch: 1432/8000  Traning Loss: 99.05086421966553  Train_Reconstruction: 95.59079265594482  Train_KL: 3.4600708186626434  Validation Loss : 97.14978790283203 Val_Reconstruction : 93.72554779052734 Val_KL : 3.4242414236068726\n","Epoch: 1433/8000  Traning Loss: 99.1697883605957  Train_Reconstruction: 95.69545650482178  Train_KL: 3.4743331372737885  Validation Loss : 97.7147331237793 Val_Reconstruction : 94.2793197631836 Val_KL : 3.43541419506073\n","Epoch: 1434/8000  Traning Loss: 99.20555686950684  Train_Reconstruction: 95.73331546783447  Train_KL: 3.4722421169281006  Validation Loss : 97.16682815551758 Val_Reconstruction : 93.73881530761719 Val_KL : 3.4280141592025757\n","Epoch: 1435/8000  Traning Loss: 98.82431602478027  Train_Reconstruction: 95.35864448547363  Train_KL: 3.4656718969345093  Validation Loss : 97.33255767822266 Val_Reconstruction : 93.89105224609375 Val_KL : 3.4415076971054077\n","Epoch: 1436/8000  Traning Loss: 98.78429889678955  Train_Reconstruction: 95.30426025390625  Train_KL: 3.480039596557617  Validation Loss : 96.91528701782227 Val_Reconstruction : 93.47371673583984 Val_KL : 3.4415736198425293\n","Epoch: 1437/8000  Traning Loss: 98.81851196289062  Train_Reconstruction: 95.34195518493652  Train_KL: 3.476556658744812  Validation Loss : 97.09445571899414 Val_Reconstruction : 93.65765380859375 Val_KL : 3.436800003051758\n","Epoch: 1438/8000  Traning Loss: 98.9208869934082  Train_Reconstruction: 95.4488353729248  Train_KL: 3.472052037715912  Validation Loss : 97.29594802856445 Val_Reconstruction : 93.8727798461914 Val_KL : 3.423168420791626\n","Epoch: 1439/8000  Traning Loss: 98.93159484863281  Train_Reconstruction: 95.46690940856934  Train_KL: 3.4646857380867004  Validation Loss : 97.25665283203125 Val_Reconstruction : 93.81331253051758 Val_KL : 3.443338394165039\n","Epoch: 1440/8000  Traning Loss: 98.86357593536377  Train_Reconstruction: 95.39203262329102  Train_KL: 3.4715432226657867  Validation Loss : 97.31386947631836 Val_Reconstruction : 93.87350082397461 Val_KL : 3.4403679370880127\n","Epoch: 1441/8000  Traning Loss: 99.15351009368896  Train_Reconstruction: 95.68374347686768  Train_KL: 3.4697668254375458  Validation Loss : 97.51073837280273 Val_Reconstruction : 94.0733528137207 Val_KL : 3.4373849630355835\n","Epoch: 1442/8000  Traning Loss: 99.4675407409668  Train_Reconstruction: 95.98180866241455  Train_KL: 3.48573237657547  Validation Loss : 98.03275299072266 Val_Reconstruction : 94.58903503417969 Val_KL : 3.443719267845154\n","Epoch: 1443/8000  Traning Loss: 99.55707359313965  Train_Reconstruction: 96.07427597045898  Train_KL: 3.4827974140644073  Validation Loss : 97.74176025390625 Val_Reconstruction : 94.29676818847656 Val_KL : 3.4449888467788696\n","Epoch: 1444/8000  Traning Loss: 99.09333038330078  Train_Reconstruction: 95.61830711364746  Train_KL: 3.475023865699768  Validation Loss : 97.28115844726562 Val_Reconstruction : 93.83834838867188 Val_KL : 3.442810297012329\n","Epoch: 1445/8000  Traning Loss: 98.8227481842041  Train_Reconstruction: 95.35948085784912  Train_KL: 3.463267505168915  Validation Loss : 97.15715789794922 Val_Reconstruction : 93.73412322998047 Val_KL : 3.423032760620117\n","Epoch: 1446/8000  Traning Loss: 98.87028121948242  Train_Reconstruction: 95.3976240158081  Train_KL: 3.4726582169532776  Validation Loss : 97.07878112792969 Val_Reconstruction : 93.62021255493164 Val_KL : 3.4585680961608887\n","Epoch: 1447/8000  Traning Loss: 98.82997226715088  Train_Reconstruction: 95.34385776519775  Train_KL: 3.486115127801895  Validation Loss : 97.23175048828125 Val_Reconstruction : 93.78810119628906 Val_KL : 3.4436497688293457\n","Epoch: 1448/8000  Traning Loss: 99.181960105896  Train_Reconstruction: 95.70382976531982  Train_KL: 3.478131353855133  Validation Loss : 97.8983268737793 Val_Reconstruction : 94.46442031860352 Val_KL : 3.4339078664779663\n","Epoch: 1449/8000  Traning Loss: 99.94962310791016  Train_Reconstruction: 96.47048282623291  Train_KL: 3.4791400730609894  Validation Loss : 98.18344116210938 Val_Reconstruction : 94.73400497436523 Val_KL : 3.449436664581299\n","Epoch: 1450/8000  Traning Loss: 99.69786357879639  Train_Reconstruction: 96.21705150604248  Train_KL: 3.480812668800354  Validation Loss : 97.65163803100586 Val_Reconstruction : 94.21974563598633 Val_KL : 3.4318898916244507\n","Epoch: 1451/8000  Traning Loss: 99.1440019607544  Train_Reconstruction: 95.68234443664551  Train_KL: 3.4616565704345703  Validation Loss : 97.25519180297852 Val_Reconstruction : 93.83079147338867 Val_KL : 3.4244004487991333\n","Epoch: 1452/8000  Traning Loss: 98.87704944610596  Train_Reconstruction: 95.41062068939209  Train_KL: 3.466429114341736  Validation Loss : 97.35686492919922 Val_Reconstruction : 93.92750549316406 Val_KL : 3.4293594360351562\n","Epoch: 1453/8000  Traning Loss: 98.97221088409424  Train_Reconstruction: 95.49781227111816  Train_KL: 3.474399209022522  Validation Loss : 97.4224739074707 Val_Reconstruction : 93.97835159301758 Val_KL : 3.4441230297088623\n","Epoch: 1454/8000  Traning Loss: 98.86554050445557  Train_Reconstruction: 95.38461399078369  Train_KL: 3.480926126241684  Validation Loss : 97.45544052124023 Val_Reconstruction : 94.00296020507812 Val_KL : 3.452480912208557\n","Epoch: 1455/8000  Traning Loss: 98.84926128387451  Train_Reconstruction: 95.37212371826172  Train_KL: 3.477136939764023  Validation Loss : 97.36188507080078 Val_Reconstruction : 93.93973541259766 Val_KL : 3.422150135040283\n","Epoch: 1456/8000  Traning Loss: 99.20970058441162  Train_Reconstruction: 95.74972438812256  Train_KL: 3.459976404905319  Validation Loss : 97.62239074707031 Val_Reconstruction : 94.20703125 Val_KL : 3.4153610467910767\n","Epoch: 1457/8000  Traning Loss: 99.15895080566406  Train_Reconstruction: 95.68041896820068  Train_KL: 3.4785315990448  Validation Loss : 97.5579719543457 Val_Reconstruction : 94.09647369384766 Val_KL : 3.4615012407302856\n","Epoch: 1458/8000  Traning Loss: 98.83803653717041  Train_Reconstruction: 95.34728622436523  Train_KL: 3.490749478340149  Validation Loss : 97.30361938476562 Val_Reconstruction : 93.85528182983398 Val_KL : 3.448334813117981\n","Epoch: 1459/8000  Traning Loss: 99.4520959854126  Train_Reconstruction: 95.98322200775146  Train_KL: 3.4688757359981537  Validation Loss : 97.52618408203125 Val_Reconstruction : 94.09354019165039 Val_KL : 3.432641863822937\n","Epoch: 1460/8000  Traning Loss: 99.39921379089355  Train_Reconstruction: 95.93124961853027  Train_KL: 3.467963457107544  Validation Loss : 97.61014556884766 Val_Reconstruction : 94.17939376831055 Val_KL : 3.43075168132782\n","Epoch: 1461/8000  Traning Loss: 98.94436931610107  Train_Reconstruction: 95.47023582458496  Train_KL: 3.4741326570510864  Validation Loss : 97.16108322143555 Val_Reconstruction : 93.70625686645508 Val_KL : 3.4548263549804688\n","Epoch: 1462/8000  Traning Loss: 98.62015628814697  Train_Reconstruction: 95.1312608718872  Train_KL: 3.488895833492279  Validation Loss : 97.16024017333984 Val_Reconstruction : 93.71986389160156 Val_KL : 3.440372943878174\n","Epoch: 1463/8000  Traning Loss: 99.10757446289062  Train_Reconstruction: 95.64461898803711  Train_KL: 3.462955117225647  Validation Loss : 98.26142501831055 Val_Reconstruction : 94.84614944458008 Val_KL : 3.4152735471725464\n","Epoch: 1464/8000  Traning Loss: 99.25742530822754  Train_Reconstruction: 95.79859638214111  Train_KL: 3.458828717470169  Validation Loss : 98.13740539550781 Val_Reconstruction : 94.71995162963867 Val_KL : 3.4174537658691406\n","Epoch: 1465/8000  Traning Loss: 99.3434066772461  Train_Reconstruction: 95.87901210784912  Train_KL: 3.464394450187683  Validation Loss : 97.47709655761719 Val_Reconstruction : 94.03759384155273 Val_KL : 3.4395042657852173\n","Epoch: 1466/8000  Traning Loss: 99.01796531677246  Train_Reconstruction: 95.53448390960693  Train_KL: 3.483480542898178  Validation Loss : 97.31659698486328 Val_Reconstruction : 93.86771392822266 Val_KL : 3.448885440826416\n","Epoch: 1467/8000  Traning Loss: 98.86370658874512  Train_Reconstruction: 95.37495994567871  Train_KL: 3.4887463748455048  Validation Loss : 97.39379119873047 Val_Reconstruction : 93.93890762329102 Val_KL : 3.454880952835083\n","Epoch: 1468/8000  Traning Loss: 98.58702564239502  Train_Reconstruction: 95.10715293884277  Train_KL: 3.479871928691864  Validation Loss : 96.98088836669922 Val_Reconstruction : 93.54197692871094 Val_KL : 3.438912272453308\n","Epoch: 1469/8000  Traning Loss: 98.45843696594238  Train_Reconstruction: 94.97499942779541  Train_KL: 3.4834370017051697  Validation Loss : 96.77935028076172 Val_Reconstruction : 93.33361053466797 Val_KL : 3.4457404613494873\n","Epoch: 1470/8000  Traning Loss: 98.49161052703857  Train_Reconstruction: 95.0239486694336  Train_KL: 3.467663496732712  Validation Loss : 96.93978500366211 Val_Reconstruction : 93.51654434204102 Val_KL : 3.423240542411804\n","Epoch: 1471/8000  Traning Loss: 98.76282024383545  Train_Reconstruction: 95.28330326080322  Train_KL: 3.479516863822937  Validation Loss : 97.17581558227539 Val_Reconstruction : 93.71697998046875 Val_KL : 3.4588361978530884\n","Epoch: 1472/8000  Traning Loss: 99.09308433532715  Train_Reconstruction: 95.60912799835205  Train_KL: 3.4839557707309723  Validation Loss : 97.44647216796875 Val_Reconstruction : 94.01959609985352 Val_KL : 3.4268769025802612\n","Epoch: 1473/8000  Traning Loss: 99.23252010345459  Train_Reconstruction: 95.76902961730957  Train_KL: 3.4634895026683807  Validation Loss : 97.44989013671875 Val_Reconstruction : 94.01518249511719 Val_KL : 3.434708595275879\n","Epoch: 1474/8000  Traning Loss: 98.98052883148193  Train_Reconstruction: 95.49653816223145  Train_KL: 3.4839902818202972  Validation Loss : 97.19833374023438 Val_Reconstruction : 93.73977661132812 Val_KL : 3.458555817604065\n","Epoch: 1475/8000  Traning Loss: 98.88072681427002  Train_Reconstruction: 95.41036605834961  Train_KL: 3.470361143350601  Validation Loss : 97.18206405639648 Val_Reconstruction : 93.75594329833984 Val_KL : 3.4261229038238525\n","Epoch: 1476/8000  Traning Loss: 98.9094123840332  Train_Reconstruction: 95.44088840484619  Train_KL: 3.468523293733597  Validation Loss : 97.24560165405273 Val_Reconstruction : 93.82024383544922 Val_KL : 3.4253604412078857\n","Epoch: 1477/8000  Traning Loss: 98.83310508728027  Train_Reconstruction: 95.35625171661377  Train_KL: 3.4768543243408203  Validation Loss : 97.22822952270508 Val_Reconstruction : 93.78662490844727 Val_KL : 3.44160532951355\n","Epoch: 1478/8000  Traning Loss: 98.50486660003662  Train_Reconstruction: 95.01772499084473  Train_KL: 3.4871414601802826  Validation Loss : 96.91773223876953 Val_Reconstruction : 93.46745681762695 Val_KL : 3.4502766132354736\n","Epoch: 1479/8000  Traning Loss: 98.56492614746094  Train_Reconstruction: 95.07983875274658  Train_KL: 3.485087752342224  Validation Loss : 96.77561569213867 Val_Reconstruction : 93.3363265991211 Val_KL : 3.4392894506454468\n","Epoch: 1480/8000  Traning Loss: 98.84173583984375  Train_Reconstruction: 95.36758232116699  Train_KL: 3.4741518795490265  Validation Loss : 97.47313690185547 Val_Reconstruction : 94.02799987792969 Val_KL : 3.445138692855835\n","Epoch: 1481/8000  Traning Loss: 98.83977603912354  Train_Reconstruction: 95.35544013977051  Train_KL: 3.484335482120514  Validation Loss : 97.3021240234375 Val_Reconstruction : 93.86878967285156 Val_KL : 3.4333345890045166\n","Epoch: 1482/8000  Traning Loss: 98.76394367218018  Train_Reconstruction: 95.29075241088867  Train_KL: 3.473190814256668  Validation Loss : 97.11224365234375 Val_Reconstruction : 93.66697692871094 Val_KL : 3.445265769958496\n","Epoch: 1483/8000  Traning Loss: 98.89845371246338  Train_Reconstruction: 95.41423511505127  Train_KL: 3.484217345714569  Validation Loss : 97.39460754394531 Val_Reconstruction : 93.94136047363281 Val_KL : 3.4532506465911865\n","Epoch: 1484/8000  Traning Loss: 98.92043685913086  Train_Reconstruction: 95.4317398071289  Train_KL: 3.4886971414089203  Validation Loss : 97.09236526489258 Val_Reconstruction : 93.65056991577148 Val_KL : 3.441793203353882\n","Epoch: 1485/8000  Traning Loss: 99.22697448730469  Train_Reconstruction: 95.74483394622803  Train_KL: 3.4821407794952393  Validation Loss : 97.7635726928711 Val_Reconstruction : 94.31921005249023 Val_KL : 3.444360136985779\n","Epoch: 1486/8000  Traning Loss: 99.40938663482666  Train_Reconstruction: 95.93652629852295  Train_KL: 3.4728585481643677  Validation Loss : 98.14747619628906 Val_Reconstruction : 94.7073860168457 Val_KL : 3.4400880336761475\n","Epoch: 1487/8000  Traning Loss: 99.43841743469238  Train_Reconstruction: 95.95449924468994  Train_KL: 3.483918607234955  Validation Loss : 97.79008102416992 Val_Reconstruction : 94.34912109375 Val_KL : 3.440958619117737\n","Epoch: 1488/8000  Traning Loss: 99.45364570617676  Train_Reconstruction: 95.97104549407959  Train_KL: 3.482598662376404  Validation Loss : 97.68728256225586 Val_Reconstruction : 94.2519760131836 Val_KL : 3.4353079795837402\n","Epoch: 1489/8000  Traning Loss: 99.79420852661133  Train_Reconstruction: 96.3303337097168  Train_KL: 3.4638757705688477  Validation Loss : 98.44033432006836 Val_Reconstruction : 95.02223205566406 Val_KL : 3.4181008338928223\n","Epoch: 1490/8000  Traning Loss: 100.24686908721924  Train_Reconstruction: 96.78910827636719  Train_KL: 3.457759141921997  Validation Loss : 99.0424575805664 Val_Reconstruction : 95.60710906982422 Val_KL : 3.4353485107421875\n","Epoch: 1491/8000  Traning Loss: 100.6075611114502  Train_Reconstruction: 97.11039066314697  Train_KL: 3.497169405221939  Validation Loss : 98.88396072387695 Val_Reconstruction : 95.42543029785156 Val_KL : 3.458530068397522\n","Epoch: 1492/8000  Traning Loss: 99.84422302246094  Train_Reconstruction: 96.37928199768066  Train_KL: 3.4649395048618317  Validation Loss : 98.11908340454102 Val_Reconstruction : 94.71428680419922 Val_KL : 3.4047958850860596\n","Epoch: 1493/8000  Traning Loss: 99.21875953674316  Train_Reconstruction: 95.76367378234863  Train_KL: 3.4550853967666626  Validation Loss : 97.71046447753906 Val_Reconstruction : 94.28009033203125 Val_KL : 3.4303756952285767\n","Epoch: 1494/8000  Traning Loss: 98.72747421264648  Train_Reconstruction: 95.25225639343262  Train_KL: 3.4752183854579926  Validation Loss : 97.07670974731445 Val_Reconstruction : 93.63209915161133 Val_KL : 3.444608449935913\n","Epoch: 1495/8000  Traning Loss: 98.55453777313232  Train_Reconstruction: 95.0680923461914  Train_KL: 3.486445724964142  Validation Loss : 96.98115921020508 Val_Reconstruction : 93.52518463134766 Val_KL : 3.455976128578186\n","Epoch: 1496/8000  Traning Loss: 98.39698886871338  Train_Reconstruction: 94.91384696960449  Train_KL: 3.483141541481018  Validation Loss : 96.97381591796875 Val_Reconstruction : 93.54176330566406 Val_KL : 3.4320526123046875\n","Epoch: 1497/8000  Traning Loss: 98.67520236968994  Train_Reconstruction: 95.198073387146  Train_KL: 3.4771280586719513  Validation Loss : 97.28702163696289 Val_Reconstruction : 93.83687591552734 Val_KL : 3.4501445293426514\n","Epoch: 1498/8000  Traning Loss: 98.69084358215332  Train_Reconstruction: 95.20307731628418  Train_KL: 3.487767606973648  Validation Loss : 97.21642684936523 Val_Reconstruction : 93.76687240600586 Val_KL : 3.449553370475769\n","Epoch: 1499/8000  Traning Loss: 98.65308666229248  Train_Reconstruction: 95.16415214538574  Train_KL: 3.4889350533485413  Validation Loss : 97.02019882202148 Val_Reconstruction : 93.5742416381836 Val_KL : 3.44595730304718\n","Epoch: 1500/8000  Traning Loss: 98.61216831207275  Train_Reconstruction: 95.14132976531982  Train_KL: 3.470839262008667  Validation Loss : 96.9400405883789 Val_Reconstruction : 93.51354598999023 Val_KL : 3.426493525505066\n","Epoch: 1501/8000  Traning Loss: 98.76317310333252  Train_Reconstruction: 95.29033756256104  Train_KL: 3.472835659980774  Validation Loss : 97.75188827514648 Val_Reconstruction : 94.31575393676758 Val_KL : 3.436135172843933\n","Epoch: 1502/8000  Traning Loss: 98.99641799926758  Train_Reconstruction: 95.51982688903809  Train_KL: 3.4765904247760773  Validation Loss : 97.39890670776367 Val_Reconstruction : 93.96509170532227 Val_KL : 3.4338170289993286\n","Epoch: 1503/8000  Traning Loss: 99.31464576721191  Train_Reconstruction: 95.83377742767334  Train_KL: 3.480869084596634  Validation Loss : 97.7301139831543 Val_Reconstruction : 94.27921295166016 Val_KL : 3.450901985168457\n","Epoch: 1504/8000  Traning Loss: 98.92372417449951  Train_Reconstruction: 95.44054794311523  Train_KL: 3.483176201581955  Validation Loss : 96.87324905395508 Val_Reconstruction : 93.44147872924805 Val_KL : 3.4317692518234253\n","Epoch: 1505/8000  Traning Loss: 98.40648746490479  Train_Reconstruction: 94.9394588470459  Train_KL: 3.4670292735099792  Validation Loss : 96.76401138305664 Val_Reconstruction : 93.32458877563477 Val_KL : 3.43942129611969\n","Epoch: 1506/8000  Traning Loss: 98.50340175628662  Train_Reconstruction: 95.01824378967285  Train_KL: 3.4851591885089874  Validation Loss : 97.0340690612793 Val_Reconstruction : 93.58247375488281 Val_KL : 3.4515953063964844\n","Epoch: 1507/8000  Traning Loss: 98.9273042678833  Train_Reconstruction: 95.45540809631348  Train_KL: 3.4718951880931854  Validation Loss : 97.39270782470703 Val_Reconstruction : 93.95824432373047 Val_KL : 3.434463381767273\n","Epoch: 1508/8000  Traning Loss: 99.31755352020264  Train_Reconstruction: 95.82340431213379  Train_KL: 3.494150072336197  Validation Loss : 98.20434951782227 Val_Reconstruction : 94.75050354003906 Val_KL : 3.4538447856903076\n","Epoch: 1509/8000  Traning Loss: 98.88424873352051  Train_Reconstruction: 95.39218330383301  Train_KL: 3.4920663237571716  Validation Loss : 97.40866088867188 Val_Reconstruction : 93.96525955200195 Val_KL : 3.443399429321289\n","Epoch: 1510/8000  Traning Loss: 98.70113754272461  Train_Reconstruction: 95.23266887664795  Train_KL: 3.468468427658081  Validation Loss : 97.06802749633789 Val_Reconstruction : 93.63602066040039 Val_KL : 3.4320050477981567\n","Epoch: 1511/8000  Traning Loss: 98.50149726867676  Train_Reconstruction: 95.0252914428711  Train_KL: 3.47620490193367  Validation Loss : 97.0711441040039 Val_Reconstruction : 93.63135147094727 Val_KL : 3.4397915601730347\n","Epoch: 1512/8000  Traning Loss: 98.83322620391846  Train_Reconstruction: 95.36308860778809  Train_KL: 3.4701386094093323  Validation Loss : 97.40114974975586 Val_Reconstruction : 93.97649765014648 Val_KL : 3.4246504306793213\n","Epoch: 1513/8000  Traning Loss: 98.67791366577148  Train_Reconstruction: 95.21393585205078  Train_KL: 3.4639783799648285  Validation Loss : 96.97243881225586 Val_Reconstruction : 93.53805160522461 Val_KL : 3.4343879222869873\n","Epoch: 1514/8000  Traning Loss: 99.08148384094238  Train_Reconstruction: 95.61335754394531  Train_KL: 3.468125641345978  Validation Loss : 97.54225540161133 Val_Reconstruction : 94.10396957397461 Val_KL : 3.4382853507995605\n","Epoch: 1515/8000  Traning Loss: 99.14439868927002  Train_Reconstruction: 95.66385459899902  Train_KL: 3.480542689561844  Validation Loss : 97.06262588500977 Val_Reconstruction : 93.62594604492188 Val_KL : 3.436677932739258\n","Epoch: 1516/8000  Traning Loss: 98.64265632629395  Train_Reconstruction: 95.15947341918945  Train_KL: 3.4831830263137817  Validation Loss : 97.08943557739258 Val_Reconstruction : 93.64695739746094 Val_KL : 3.4424773454666138\n","Epoch: 1517/8000  Traning Loss: 98.71826934814453  Train_Reconstruction: 95.24263000488281  Train_KL: 3.47563898563385  Validation Loss : 97.3098030090332 Val_Reconstruction : 93.87804412841797 Val_KL : 3.431759238243103\n","Epoch: 1518/8000  Traning Loss: 98.81881809234619  Train_Reconstruction: 95.34371280670166  Train_KL: 3.4751067459583282  Validation Loss : 97.20794296264648 Val_Reconstruction : 93.76302719116211 Val_KL : 3.4449130296707153\n","Epoch: 1519/8000  Traning Loss: 98.8831377029419  Train_Reconstruction: 95.39839839935303  Train_KL: 3.4847390055656433  Validation Loss : 97.46764373779297 Val_Reconstruction : 94.0195541381836 Val_KL : 3.448089838027954\n","Epoch: 1520/8000  Traning Loss: 98.90520191192627  Train_Reconstruction: 95.42903900146484  Train_KL: 3.4761635661125183  Validation Loss : 97.80199813842773 Val_Reconstruction : 94.3709487915039 Val_KL : 3.431047558784485\n","Epoch: 1521/8000  Traning Loss: 98.82598114013672  Train_Reconstruction: 95.35624980926514  Train_KL: 3.4697320461273193  Validation Loss : 97.5057487487793 Val_Reconstruction : 94.0842170715332 Val_KL : 3.4215290546417236\n","Epoch: 1522/8000  Traning Loss: 98.79534244537354  Train_Reconstruction: 95.33034896850586  Train_KL: 3.464993178844452  Validation Loss : 97.0974006652832 Val_Reconstruction : 93.65862274169922 Val_KL : 3.438781261444092\n","Epoch: 1523/8000  Traning Loss: 98.61240577697754  Train_Reconstruction: 95.13219833374023  Train_KL: 3.480205535888672  Validation Loss : 96.803466796875 Val_Reconstruction : 93.3651237487793 Val_KL : 3.4383411407470703\n","Epoch: 1524/8000  Traning Loss: 98.40845966339111  Train_Reconstruction: 94.94293022155762  Train_KL: 3.465529054403305  Validation Loss : 96.85979461669922 Val_Reconstruction : 93.43108749389648 Val_KL : 3.4287071228027344\n","Epoch: 1525/8000  Traning Loss: 98.61124038696289  Train_Reconstruction: 95.13485622406006  Train_KL: 3.4763834476470947  Validation Loss : 97.39231872558594 Val_Reconstruction : 93.95309066772461 Val_KL : 3.439226269721985\n","Epoch: 1526/8000  Traning Loss: 98.73553562164307  Train_Reconstruction: 95.26815795898438  Train_KL: 3.4673782885074615  Validation Loss : 97.4066162109375 Val_Reconstruction : 93.98006820678711 Val_KL : 3.426544427871704\n","Epoch: 1527/8000  Traning Loss: 98.60491275787354  Train_Reconstruction: 95.11716842651367  Train_KL: 3.487742602825165  Validation Loss : 96.87791442871094 Val_Reconstruction : 93.42121887207031 Val_KL : 3.456696391105652\n","Epoch: 1528/8000  Traning Loss: 98.54978275299072  Train_Reconstruction: 95.06586360931396  Train_KL: 3.483917683362961  Validation Loss : 97.26447677612305 Val_Reconstruction : 93.83201599121094 Val_KL : 3.432461380958557\n","Epoch: 1529/8000  Traning Loss: 98.6724271774292  Train_Reconstruction: 95.20826625823975  Train_KL: 3.4641612768173218  Validation Loss : 96.99632263183594 Val_Reconstruction : 93.57633209228516 Val_KL : 3.4199914932250977\n","Epoch: 1530/8000  Traning Loss: 98.4481430053711  Train_Reconstruction: 94.97119808197021  Train_KL: 3.4769442975521088  Validation Loss : 96.94415283203125 Val_Reconstruction : 93.49744033813477 Val_KL : 3.446711301803589\n","Epoch: 1531/8000  Traning Loss: 98.45092296600342  Train_Reconstruction: 94.9631118774414  Train_KL: 3.487810879945755  Validation Loss : 96.69338989257812 Val_Reconstruction : 93.23896408081055 Val_KL : 3.454423427581787\n","Epoch: 1532/8000  Traning Loss: 98.41156196594238  Train_Reconstruction: 94.92526817321777  Train_KL: 3.486293137073517  Validation Loss : 96.95695495605469 Val_Reconstruction : 93.5224494934082 Val_KL : 3.4345057010650635\n","Epoch: 1533/8000  Traning Loss: 98.47210311889648  Train_Reconstruction: 95.00208473205566  Train_KL: 3.470018118619919  Validation Loss : 96.80739212036133 Val_Reconstruction : 93.3907699584961 Val_KL : 3.416619300842285\n","Epoch: 1534/8000  Traning Loss: 99.24399757385254  Train_Reconstruction: 95.77460765838623  Train_KL: 3.4693886935710907  Validation Loss : 97.5142936706543 Val_Reconstruction : 94.06124877929688 Val_KL : 3.4530428647994995\n","Epoch: 1535/8000  Traning Loss: 99.04905033111572  Train_Reconstruction: 95.58244132995605  Train_KL: 3.466607987880707  Validation Loss : 97.33271408081055 Val_Reconstruction : 93.90967559814453 Val_KL : 3.4230360984802246\n","Epoch: 1536/8000  Traning Loss: 98.81549072265625  Train_Reconstruction: 95.35219383239746  Train_KL: 3.4632970094680786  Validation Loss : 96.97110748291016 Val_Reconstruction : 93.5400161743164 Val_KL : 3.431090235710144\n","Epoch: 1537/8000  Traning Loss: 98.51801204681396  Train_Reconstruction: 95.02958679199219  Train_KL: 3.4884259402751923  Validation Loss : 96.57402038574219 Val_Reconstruction : 93.10790252685547 Val_KL : 3.4661210775375366\n","Epoch: 1538/8000  Traning Loss: 98.26338195800781  Train_Reconstruction: 94.77824401855469  Train_KL: 3.4851371943950653  Validation Loss : 96.81321716308594 Val_Reconstruction : 93.38535690307617 Val_KL : 3.4278584718704224\n","Epoch: 1539/8000  Traning Loss: 98.56281661987305  Train_Reconstruction: 95.09932231903076  Train_KL: 3.463493287563324  Validation Loss : 97.13517379760742 Val_Reconstruction : 93.7096939086914 Val_KL : 3.4254801273345947\n","Epoch: 1540/8000  Traning Loss: 98.89374732971191  Train_Reconstruction: 95.42305850982666  Train_KL: 3.4706881642341614  Validation Loss : 97.57146072387695 Val_Reconstruction : 94.12344360351562 Val_KL : 3.4480170011520386\n","Epoch: 1541/8000  Traning Loss: 99.26377487182617  Train_Reconstruction: 95.7825231552124  Train_KL: 3.4812531769275665  Validation Loss : 98.13190841674805 Val_Reconstruction : 94.68844223022461 Val_KL : 3.443463683128357\n","Epoch: 1542/8000  Traning Loss: 99.22403049468994  Train_Reconstruction: 95.76511669158936  Train_KL: 3.4589133858680725  Validation Loss : 97.74442672729492 Val_Reconstruction : 94.32794570922852 Val_KL : 3.4164804220199585\n","Epoch: 1543/8000  Traning Loss: 98.7990198135376  Train_Reconstruction: 95.32132720947266  Train_KL: 3.4776925146579742  Validation Loss : 97.27847290039062 Val_Reconstruction : 93.82425308227539 Val_KL : 3.4542195796966553\n","Epoch: 1544/8000  Traning Loss: 98.5849781036377  Train_Reconstruction: 95.1034927368164  Train_KL: 3.4814854562282562  Validation Loss : 97.14497756958008 Val_Reconstruction : 93.71310424804688 Val_KL : 3.4318712949752808\n","Epoch: 1545/8000  Traning Loss: 98.79219627380371  Train_Reconstruction: 95.32291507720947  Train_KL: 3.469281643629074  Validation Loss : 97.54146575927734 Val_Reconstruction : 94.10760116577148 Val_KL : 3.433863639831543\n","Epoch: 1546/8000  Traning Loss: 99.03155422210693  Train_Reconstruction: 95.54935455322266  Train_KL: 3.482199639081955  Validation Loss : 97.63643646240234 Val_Reconstruction : 94.19937515258789 Val_KL : 3.437062382698059\n","Epoch: 1547/8000  Traning Loss: 98.80696105957031  Train_Reconstruction: 95.32403659820557  Train_KL: 3.482922911643982  Validation Loss : 97.28567123413086 Val_Reconstruction : 93.85363388061523 Val_KL : 3.4320380687713623\n","Epoch: 1548/8000  Traning Loss: 98.89289283752441  Train_Reconstruction: 95.43050193786621  Train_KL: 3.4623906016349792  Validation Loss : 97.80578231811523 Val_Reconstruction : 94.38766860961914 Val_KL : 3.418112874031067\n","Epoch: 1549/8000  Traning Loss: 98.67629146575928  Train_Reconstruction: 95.20320987701416  Train_KL: 3.4730820655822754  Validation Loss : 97.26746368408203 Val_Reconstruction : 93.81412887573242 Val_KL : 3.4533358812332153\n","Epoch: 1550/8000  Traning Loss: 98.47898769378662  Train_Reconstruction: 94.98910427093506  Train_KL: 3.4898821115493774  Validation Loss : 97.06925964355469 Val_Reconstruction : 93.6358528137207 Val_KL : 3.4334059953689575\n","Epoch: 1551/8000  Traning Loss: 98.40385913848877  Train_Reconstruction: 94.92992973327637  Train_KL: 3.4739299416542053  Validation Loss : 96.79146957397461 Val_Reconstruction : 93.3614616394043 Val_KL : 3.4300081729888916\n","Epoch: 1552/8000  Traning Loss: 98.49592018127441  Train_Reconstruction: 95.0225133895874  Train_KL: 3.4734062254428864  Validation Loss : 96.91595077514648 Val_Reconstruction : 93.4828872680664 Val_KL : 3.4330663681030273\n","Epoch: 1553/8000  Traning Loss: 98.64506912231445  Train_Reconstruction: 95.16926670074463  Train_KL: 3.4758020639419556  Validation Loss : 96.92546081542969 Val_Reconstruction : 93.4981689453125 Val_KL : 3.4272937774658203\n","Epoch: 1554/8000  Traning Loss: 98.56891059875488  Train_Reconstruction: 95.09524250030518  Train_KL: 3.473668783903122  Validation Loss : 96.99264144897461 Val_Reconstruction : 93.56241607666016 Val_KL : 3.430223822593689\n","Epoch: 1555/8000  Traning Loss: 98.58596611022949  Train_Reconstruction: 95.1221981048584  Train_KL: 3.4637687504291534  Validation Loss : 97.0988883972168 Val_Reconstruction : 93.68552017211914 Val_KL : 3.4133657217025757\n","Epoch: 1556/8000  Traning Loss: 98.41452884674072  Train_Reconstruction: 94.96060180664062  Train_KL: 3.4539266228675842  Validation Loss : 96.7659797668457 Val_Reconstruction : 93.34671020507812 Val_KL : 3.4192670583724976\n","Epoch: 1557/8000  Traning Loss: 98.4547004699707  Train_Reconstruction: 94.98897075653076  Train_KL: 3.4657305479049683  Validation Loss : 97.15691375732422 Val_Reconstruction : 93.72015762329102 Val_KL : 3.4367541074752808\n","Epoch: 1558/8000  Traning Loss: 98.71446990966797  Train_Reconstruction: 95.22753429412842  Train_KL: 3.486934572458267  Validation Loss : 97.10413360595703 Val_Reconstruction : 93.65580749511719 Val_KL : 3.4483267068862915\n","Epoch: 1559/8000  Traning Loss: 99.2229528427124  Train_Reconstruction: 95.74062824249268  Train_KL: 3.482324004173279  Validation Loss : 97.72580337524414 Val_Reconstruction : 94.28315734863281 Val_KL : 3.442649245262146\n","Epoch: 1560/8000  Traning Loss: 98.71783828735352  Train_Reconstruction: 95.23696327209473  Train_KL: 3.480874627828598  Validation Loss : 96.61235046386719 Val_Reconstruction : 93.17296981811523 Val_KL : 3.439382553100586\n","Epoch: 1561/8000  Traning Loss: 98.31581687927246  Train_Reconstruction: 94.84017181396484  Train_KL: 3.475645750761032  Validation Loss : 96.74933624267578 Val_Reconstruction : 93.32134246826172 Val_KL : 3.4279953241348267\n","Epoch: 1562/8000  Traning Loss: 98.35651016235352  Train_Reconstruction: 94.89202499389648  Train_KL: 3.464486390352249  Validation Loss : 96.96859359741211 Val_Reconstruction : 93.53772735595703 Val_KL : 3.4308671951293945\n","Epoch: 1563/8000  Traning Loss: 98.4630708694458  Train_Reconstruction: 94.98115921020508  Train_KL: 3.48191174864769  Validation Loss : 97.23339080810547 Val_Reconstruction : 93.78457260131836 Val_KL : 3.4488167762756348\n","Epoch: 1564/8000  Traning Loss: 98.43578052520752  Train_Reconstruction: 94.9525899887085  Train_KL: 3.483190208673477  Validation Loss : 96.79908752441406 Val_Reconstruction : 93.35995101928711 Val_KL : 3.4391356706619263\n","Epoch: 1565/8000  Traning Loss: 98.00494289398193  Train_Reconstruction: 94.53800201416016  Train_KL: 3.4669414162635803  Validation Loss : 96.36430358886719 Val_Reconstruction : 92.92618179321289 Val_KL : 3.438123106956482\n","Epoch: 1566/8000  Traning Loss: 97.97686290740967  Train_Reconstruction: 94.49236297607422  Train_KL: 3.4845004081726074  Validation Loss : 96.38616943359375 Val_Reconstruction : 92.93943786621094 Val_KL : 3.4467310905456543\n","Epoch: 1567/8000  Traning Loss: 97.99872589111328  Train_Reconstruction: 94.52870178222656  Train_KL: 3.4700251817703247  Validation Loss : 96.38153076171875 Val_Reconstruction : 92.95846939086914 Val_KL : 3.423061490058899\n","Epoch: 1568/8000  Traning Loss: 98.1994981765747  Train_Reconstruction: 94.72334671020508  Train_KL: 3.4761516451835632  Validation Loss : 96.93011474609375 Val_Reconstruction : 93.47953796386719 Val_KL : 3.4505776166915894\n","Epoch: 1569/8000  Traning Loss: 98.63196659088135  Train_Reconstruction: 95.15449237823486  Train_KL: 3.4774751365184784  Validation Loss : 97.43505477905273 Val_Reconstruction : 94.00871276855469 Val_KL : 3.426342487335205\n","Epoch: 1570/8000  Traning Loss: 98.67221641540527  Train_Reconstruction: 95.2041826248169  Train_KL: 3.4680337607860565  Validation Loss : 96.96264266967773 Val_Reconstruction : 93.52117538452148 Val_KL : 3.441469430923462\n","Epoch: 1571/8000  Traning Loss: 98.36566925048828  Train_Reconstruction: 94.89763355255127  Train_KL: 3.4680361449718475  Validation Loss : 96.76245880126953 Val_Reconstruction : 93.33543014526367 Val_KL : 3.42703115940094\n","Epoch: 1572/8000  Traning Loss: 98.36437702178955  Train_Reconstruction: 94.88105392456055  Train_KL: 3.4833236634731293  Validation Loss : 96.48979568481445 Val_Reconstruction : 93.03281021118164 Val_KL : 3.456986904144287\n","Epoch: 1573/8000  Traning Loss: 98.2324104309082  Train_Reconstruction: 94.74348640441895  Train_KL: 3.4889233708381653  Validation Loss : 96.59402847290039 Val_Reconstruction : 93.16212463378906 Val_KL : 3.4319045543670654\n","Epoch: 1574/8000  Traning Loss: 98.06522369384766  Train_Reconstruction: 94.59489250183105  Train_KL: 3.470330834388733  Validation Loss : 96.48454284667969 Val_Reconstruction : 93.04182815551758 Val_KL : 3.442714214324951\n","Epoch: 1575/8000  Traning Loss: 98.03827953338623  Train_Reconstruction: 94.55432033538818  Train_KL: 3.4839604198932648  Validation Loss : 96.49853515625 Val_Reconstruction : 93.04981231689453 Val_KL : 3.4487218856811523\n","Epoch: 1576/8000  Traning Loss: 98.06973838806152  Train_Reconstruction: 94.59952163696289  Train_KL: 3.470217376947403  Validation Loss : 96.53427505493164 Val_Reconstruction : 93.11618041992188 Val_KL : 3.418093204498291\n","Epoch: 1577/8000  Traning Loss: 98.1000280380249  Train_Reconstruction: 94.62489891052246  Train_KL: 3.475130021572113  Validation Loss : 96.67790222167969 Val_Reconstruction : 93.23795318603516 Val_KL : 3.439948797225952\n","Epoch: 1578/8000  Traning Loss: 97.91122341156006  Train_Reconstruction: 94.42977619171143  Train_KL: 3.481447160243988  Validation Loss : 96.52940368652344 Val_Reconstruction : 93.0818977355957 Val_KL : 3.4475083351135254\n","Epoch: 1579/8000  Traning Loss: 97.92665481567383  Train_Reconstruction: 94.43588542938232  Train_KL: 3.490768611431122  Validation Loss : 96.77777099609375 Val_Reconstruction : 93.31438827514648 Val_KL : 3.4633809328079224\n","Epoch: 1580/8000  Traning Loss: 98.49059104919434  Train_Reconstruction: 94.99576091766357  Train_KL: 3.4948292672634125  Validation Loss : 97.46191024780273 Val_Reconstruction : 94.01223373413086 Val_KL : 3.4496744871139526\n","Epoch: 1581/8000  Traning Loss: 99.15030002593994  Train_Reconstruction: 95.68047618865967  Train_KL: 3.4698236286640167  Validation Loss : 98.50048065185547 Val_Reconstruction : 95.05763626098633 Val_KL : 3.442842483520508\n","Epoch: 1582/8000  Traning Loss: 99.50762271881104  Train_Reconstruction: 96.01376056671143  Train_KL: 3.493860810995102  Validation Loss : 98.10770034790039 Val_Reconstruction : 94.65530014038086 Val_KL : 3.4524037837982178\n","Epoch: 1583/8000  Traning Loss: 98.80529499053955  Train_Reconstruction: 95.31726741790771  Train_KL: 3.488026946783066  Validation Loss : 97.00846862792969 Val_Reconstruction : 93.56026840209961 Val_KL : 3.44819974899292\n","Epoch: 1584/8000  Traning Loss: 98.21551609039307  Train_Reconstruction: 94.73210620880127  Train_KL: 3.483409136533737  Validation Loss : 96.8749885559082 Val_Reconstruction : 93.43186950683594 Val_KL : 3.4431217908859253\n","Epoch: 1585/8000  Traning Loss: 98.11190700531006  Train_Reconstruction: 94.64353561401367  Train_KL: 3.4683724343776703  Validation Loss : 97.06213760375977 Val_Reconstruction : 93.63504409790039 Val_KL : 3.427096128463745\n","Epoch: 1586/8000  Traning Loss: 98.0581865310669  Train_Reconstruction: 94.57850933074951  Train_KL: 3.4796780347824097  Validation Loss : 96.61312484741211 Val_Reconstruction : 93.15925598144531 Val_KL : 3.4538673162460327\n","Epoch: 1587/8000  Traning Loss: 98.36377620697021  Train_Reconstruction: 94.87956523895264  Train_KL: 3.484210819005966  Validation Loss : 96.97641372680664 Val_Reconstruction : 93.52109909057617 Val_KL : 3.455312490463257\n","Epoch: 1588/8000  Traning Loss: 98.10170078277588  Train_Reconstruction: 94.62301540374756  Train_KL: 3.4786859154701233  Validation Loss : 96.41788101196289 Val_Reconstruction : 92.98789596557617 Val_KL : 3.429985523223877\n","Epoch: 1589/8000  Traning Loss: 97.96225547790527  Train_Reconstruction: 94.48285007476807  Train_KL: 3.479405790567398  Validation Loss : 96.77492141723633 Val_Reconstruction : 93.33238220214844 Val_KL : 3.4425415992736816\n","Epoch: 1590/8000  Traning Loss: 98.07596492767334  Train_Reconstruction: 94.5932092666626  Train_KL: 3.4827550649642944  Validation Loss : 96.68323516845703 Val_Reconstruction : 93.2480697631836 Val_KL : 3.435164451599121\n","Epoch: 1591/8000  Traning Loss: 98.17069911956787  Train_Reconstruction: 94.69793510437012  Train_KL: 3.472763180732727  Validation Loss : 96.51299667358398 Val_Reconstruction : 93.08034896850586 Val_KL : 3.432647466659546\n","Epoch: 1592/8000  Traning Loss: 98.34262561798096  Train_Reconstruction: 94.86535358428955  Train_KL: 3.4772724509239197  Validation Loss : 97.43367004394531 Val_Reconstruction : 93.99395751953125 Val_KL : 3.4397157430648804\n","Epoch: 1593/8000  Traning Loss: 98.69373035430908  Train_Reconstruction: 95.22409152984619  Train_KL: 3.4696370661258698  Validation Loss : 97.64187622070312 Val_Reconstruction : 94.22228240966797 Val_KL : 3.419595241546631\n","Epoch: 1594/8000  Traning Loss: 98.80672550201416  Train_Reconstruction: 95.3382511138916  Train_KL: 3.4684736728668213  Validation Loss : 97.09395217895508 Val_Reconstruction : 93.64935302734375 Val_KL : 3.444600462913513\n","Epoch: 1595/8000  Traning Loss: 98.60256862640381  Train_Reconstruction: 95.12504005432129  Train_KL: 3.477529078722  Validation Loss : 96.85196304321289 Val_Reconstruction : 93.41877365112305 Val_KL : 3.433189034461975\n","Epoch: 1596/8000  Traning Loss: 98.2448616027832  Train_Reconstruction: 94.77311611175537  Train_KL: 3.471745491027832  Validation Loss : 96.56072998046875 Val_Reconstruction : 93.13174057006836 Val_KL : 3.428991675376892\n","Epoch: 1597/8000  Traning Loss: 98.22179698944092  Train_Reconstruction: 94.74487018585205  Train_KL: 3.476928174495697  Validation Loss : 96.89259719848633 Val_Reconstruction : 93.44295120239258 Val_KL : 3.4496439695358276\n","Epoch: 1598/8000  Traning Loss: 98.15263175964355  Train_Reconstruction: 94.67259311676025  Train_KL: 3.480039596557617  Validation Loss : 96.72036361694336 Val_Reconstruction : 93.29292297363281 Val_KL : 3.427443265914917\n","Epoch: 1599/8000  Traning Loss: 98.17089080810547  Train_Reconstruction: 94.69903945922852  Train_KL: 3.471851885318756  Validation Loss : 96.53986358642578 Val_Reconstruction : 93.0843391418457 Val_KL : 3.4555234909057617\n","Epoch: 1600/8000  Traning Loss: 98.15485000610352  Train_Reconstruction: 94.6544599533081  Train_KL: 3.5003906190395355  Validation Loss : 96.56538391113281 Val_Reconstruction : 93.10292053222656 Val_KL : 3.4624658823013306\n","Epoch: 1601/8000  Traning Loss: 97.97991466522217  Train_Reconstruction: 94.50093746185303  Train_KL: 3.4789757132530212  Validation Loss : 96.3922233581543 Val_Reconstruction : 92.9632453918457 Val_KL : 3.42898166179657\n","Epoch: 1602/8000  Traning Loss: 97.84914588928223  Train_Reconstruction: 94.37448310852051  Train_KL: 3.4746634364128113  Validation Loss : 96.43387603759766 Val_Reconstruction : 92.98929214477539 Val_KL : 3.444585084915161\n","Epoch: 1603/8000  Traning Loss: 97.80040168762207  Train_Reconstruction: 94.3193130493164  Train_KL: 3.4810883700847626  Validation Loss : 96.34274673461914 Val_Reconstruction : 92.912353515625 Val_KL : 3.4303925037384033\n","Epoch: 1604/8000  Traning Loss: 97.8530626296997  Train_Reconstruction: 94.36254119873047  Train_KL: 3.4905207455158234  Validation Loss : 96.29614639282227 Val_Reconstruction : 92.8320541381836 Val_KL : 3.464094638824463\n","Epoch: 1605/8000  Traning Loss: 98.0830602645874  Train_Reconstruction: 94.59809398651123  Train_KL: 3.484967291355133  Validation Loss : 96.64303970336914 Val_Reconstruction : 93.21049880981445 Val_KL : 3.4325400590896606\n","Epoch: 1606/8000  Traning Loss: 98.33914184570312  Train_Reconstruction: 94.87204551696777  Train_KL: 3.4670982360839844  Validation Loss : 96.74826049804688 Val_Reconstruction : 93.31843948364258 Val_KL : 3.429820418357849\n","Epoch: 1607/8000  Traning Loss: 98.33018779754639  Train_Reconstruction: 94.85142230987549  Train_KL: 3.4787658154964447  Validation Loss : 96.4339828491211 Val_Reconstruction : 93.00625610351562 Val_KL : 3.4277251958847046\n","Epoch: 1608/8000  Traning Loss: 98.09187412261963  Train_Reconstruction: 94.61217975616455  Train_KL: 3.4796933829784393  Validation Loss : 96.54991149902344 Val_Reconstruction : 93.1070556640625 Val_KL : 3.4428560733795166\n","Epoch: 1609/8000  Traning Loss: 98.23183631896973  Train_Reconstruction: 94.74839687347412  Train_KL: 3.4834397435188293  Validation Loss : 96.73539733886719 Val_Reconstruction : 93.28128051757812 Val_KL : 3.4541162252426147\n","Epoch: 1610/8000  Traning Loss: 98.39311599731445  Train_Reconstruction: 94.92166042327881  Train_KL: 3.4714560210704803  Validation Loss : 96.8038330078125 Val_Reconstruction : 93.37598037719727 Val_KL : 3.427855134010315\n","Epoch: 1611/8000  Traning Loss: 98.26338768005371  Train_Reconstruction: 94.79220962524414  Train_KL: 3.4711786806583405  Validation Loss : 96.40131759643555 Val_Reconstruction : 92.97958374023438 Val_KL : 3.4217323064804077\n","Epoch: 1612/8000  Traning Loss: 98.0326337814331  Train_Reconstruction: 94.56282424926758  Train_KL: 3.4698080718517303  Validation Loss : 96.19810485839844 Val_Reconstruction : 92.75876998901367 Val_KL : 3.439334988594055\n","Epoch: 1613/8000  Traning Loss: 98.06143760681152  Train_Reconstruction: 94.57195949554443  Train_KL: 3.489476352930069  Validation Loss : 96.84029769897461 Val_Reconstruction : 93.39779663085938 Val_KL : 3.4425023794174194\n","Epoch: 1614/8000  Traning Loss: 98.39264106750488  Train_Reconstruction: 94.92186260223389  Train_KL: 3.4707798957824707  Validation Loss : 97.0279426574707 Val_Reconstruction : 93.60078811645508 Val_KL : 3.4271531105041504\n","Epoch: 1615/8000  Traning Loss: 98.42953491210938  Train_Reconstruction: 94.95865631103516  Train_KL: 3.4708783626556396  Validation Loss : 96.85310745239258 Val_Reconstruction : 93.42546844482422 Val_KL : 3.427638292312622\n","Epoch: 1616/8000  Traning Loss: 98.40195846557617  Train_Reconstruction: 94.92856502532959  Train_KL: 3.4733937680721283  Validation Loss : 96.88058471679688 Val_Reconstruction : 93.4419174194336 Val_KL : 3.438665270805359\n","Epoch: 1617/8000  Traning Loss: 98.10524082183838  Train_Reconstruction: 94.62404251098633  Train_KL: 3.48119780421257  Validation Loss : 96.66151809692383 Val_Reconstruction : 93.21277236938477 Val_KL : 3.4487463235855103\n","Epoch: 1618/8000  Traning Loss: 97.76909732818604  Train_Reconstruction: 94.29275894165039  Train_KL: 3.4763394594192505  Validation Loss : 96.35104751586914 Val_Reconstruction : 92.9287223815918 Val_KL : 3.422324061393738\n","Epoch: 1619/8000  Traning Loss: 97.80302143096924  Train_Reconstruction: 94.33315944671631  Train_KL: 3.469862163066864  Validation Loss : 96.27270889282227 Val_Reconstruction : 92.82062149047852 Val_KL : 3.4520856142044067\n","Epoch: 1620/8000  Traning Loss: 97.78117752075195  Train_Reconstruction: 94.27492427825928  Train_KL: 3.5062527656555176  Validation Loss : 96.36296463012695 Val_Reconstruction : 92.89262771606445 Val_KL : 3.470337390899658\n","Epoch: 1621/8000  Traning Loss: 97.95155239105225  Train_Reconstruction: 94.45863342285156  Train_KL: 3.4929183423519135  Validation Loss : 96.47062683105469 Val_Reconstruction : 93.01238632202148 Val_KL : 3.45823872089386\n","Epoch: 1622/8000  Traning Loss: 98.00105857849121  Train_Reconstruction: 94.52199172973633  Train_KL: 3.479066163301468  Validation Loss : 96.36792373657227 Val_Reconstruction : 92.92455673217773 Val_KL : 3.443366765975952\n","Epoch: 1623/8000  Traning Loss: 97.92844676971436  Train_Reconstruction: 94.45512580871582  Train_KL: 3.473319709300995  Validation Loss : 96.3653678894043 Val_Reconstruction : 92.91946411132812 Val_KL : 3.445904016494751\n","Epoch: 1624/8000  Traning Loss: 97.86990928649902  Train_Reconstruction: 94.3765172958374  Train_KL: 3.493391513824463  Validation Loss : 96.31326675415039 Val_Reconstruction : 92.85217666625977 Val_KL : 3.461090087890625\n","Epoch: 1625/8000  Traning Loss: 97.81060028076172  Train_Reconstruction: 94.31958961486816  Train_KL: 3.4910105168819427  Validation Loss : 96.3156623840332 Val_Reconstruction : 92.87560653686523 Val_KL : 3.4400572776794434\n","Epoch: 1626/8000  Traning Loss: 97.74669361114502  Train_Reconstruction: 94.2693338394165  Train_KL: 3.47735932469368  Validation Loss : 96.3625717163086 Val_Reconstruction : 92.93304061889648 Val_KL : 3.429534077644348\n","Epoch: 1627/8000  Traning Loss: 98.00424766540527  Train_Reconstruction: 94.52894020080566  Train_KL: 3.4753067791461945  Validation Loss : 96.82870483398438 Val_Reconstruction : 93.38799285888672 Val_KL : 3.4407132863998413\n","Epoch: 1628/8000  Traning Loss: 98.45083713531494  Train_Reconstruction: 94.98071765899658  Train_KL: 3.4701212644577026  Validation Loss : 97.36002731323242 Val_Reconstruction : 93.92248153686523 Val_KL : 3.4375452995300293\n","Epoch: 1629/8000  Traning Loss: 98.48332691192627  Train_Reconstruction: 95.01058197021484  Train_KL: 3.4727447628974915  Validation Loss : 96.60153198242188 Val_Reconstruction : 93.17060470581055 Val_KL : 3.4309258460998535\n","Epoch: 1630/8000  Traning Loss: 97.7685604095459  Train_Reconstruction: 94.29296970367432  Train_KL: 3.475591480731964  Validation Loss : 96.0731086730957 Val_Reconstruction : 92.63742065429688 Val_KL : 3.4356882572174072\n","Epoch: 1631/8000  Traning Loss: 97.67443180084229  Train_Reconstruction: 94.1920518875122  Train_KL: 3.4823800027370453  Validation Loss : 96.09877014160156 Val_Reconstruction : 92.65512084960938 Val_KL : 3.4436497688293457\n","Epoch: 1632/8000  Traning Loss: 97.5792064666748  Train_Reconstruction: 94.09776973724365  Train_KL: 3.4814362823963165  Validation Loss : 96.0967025756836 Val_Reconstruction : 92.66029739379883 Val_KL : 3.436404228210449\n","Epoch: 1633/8000  Traning Loss: 97.49538993835449  Train_Reconstruction: 94.01965713500977  Train_KL: 3.475732445716858  Validation Loss : 95.94818496704102 Val_Reconstruction : 92.50578689575195 Val_KL : 3.442399501800537\n","Epoch: 1634/8000  Traning Loss: 97.61280250549316  Train_Reconstruction: 94.12634658813477  Train_KL: 3.4864553213119507  Validation Loss : 96.0988883972168 Val_Reconstruction : 92.65365219116211 Val_KL : 3.4452379941940308\n","Epoch: 1635/8000  Traning Loss: 97.81679344177246  Train_Reconstruction: 94.3383264541626  Train_KL: 3.478465050458908  Validation Loss : 95.90221786499023 Val_Reconstruction : 92.46054458618164 Val_KL : 3.4416732788085938\n","Epoch: 1636/8000  Traning Loss: 97.61318874359131  Train_Reconstruction: 94.13390350341797  Train_KL: 3.479284167289734  Validation Loss : 96.50091934204102 Val_Reconstruction : 93.06573486328125 Val_KL : 3.435182571411133\n","Epoch: 1637/8000  Traning Loss: 97.91734027862549  Train_Reconstruction: 94.43830585479736  Train_KL: 3.4790335595607758  Validation Loss : 96.31459045410156 Val_Reconstruction : 92.8754997253418 Val_KL : 3.439094066619873\n","Epoch: 1638/8000  Traning Loss: 97.98424911499023  Train_Reconstruction: 94.51749324798584  Train_KL: 3.4667557775974274  Validation Loss : 96.50164413452148 Val_Reconstruction : 93.06820678710938 Val_KL : 3.4334388971328735\n","Epoch: 1639/8000  Traning Loss: 97.80541515350342  Train_Reconstruction: 94.32167720794678  Train_KL: 3.4837377667427063  Validation Loss : 96.34842300415039 Val_Reconstruction : 92.89917373657227 Val_KL : 3.4492493867874146\n","Epoch: 1640/8000  Traning Loss: 97.94897651672363  Train_Reconstruction: 94.46544647216797  Train_KL: 3.4835307598114014  Validation Loss : 96.44508361816406 Val_Reconstruction : 93.0029411315918 Val_KL : 3.4421417713165283\n","Epoch: 1641/8000  Traning Loss: 97.84793853759766  Train_Reconstruction: 94.3758897781372  Train_KL: 3.472047805786133  Validation Loss : 96.40539932250977 Val_Reconstruction : 92.97536849975586 Val_KL : 3.4300326108932495\n","Epoch: 1642/8000  Traning Loss: 97.78657245635986  Train_Reconstruction: 94.30449390411377  Train_KL: 3.482078969478607  Validation Loss : 96.6904525756836 Val_Reconstruction : 93.2497329711914 Val_KL : 3.4407167434692383\n","Epoch: 1643/8000  Traning Loss: 98.06842613220215  Train_Reconstruction: 94.5929069519043  Train_KL: 3.475518077611923  Validation Loss : 96.52253341674805 Val_Reconstruction : 93.08734130859375 Val_KL : 3.4351905584335327\n","Epoch: 1644/8000  Traning Loss: 97.94247245788574  Train_Reconstruction: 94.45828819274902  Train_KL: 3.484185129404068  Validation Loss : 96.4813117980957 Val_Reconstruction : 93.03091812133789 Val_KL : 3.450393795967102\n","Epoch: 1645/8000  Traning Loss: 98.16879081726074  Train_Reconstruction: 94.68194007873535  Train_KL: 3.4868518710136414  Validation Loss : 97.08745193481445 Val_Reconstruction : 93.65331268310547 Val_KL : 3.4341373443603516\n","Epoch: 1646/8000  Traning Loss: 98.53120803833008  Train_Reconstruction: 95.054931640625  Train_KL: 3.4762765765190125  Validation Loss : 97.0303840637207 Val_Reconstruction : 93.59656143188477 Val_KL : 3.433823823928833\n","Epoch: 1647/8000  Traning Loss: 98.46640968322754  Train_Reconstruction: 94.98910427093506  Train_KL: 3.4773061275482178  Validation Loss : 96.66244888305664 Val_Reconstruction : 93.2236442565918 Val_KL : 3.4388033151626587\n","Epoch: 1648/8000  Traning Loss: 98.350510597229  Train_Reconstruction: 94.87344074249268  Train_KL: 3.4770708680152893  Validation Loss : 97.29861450195312 Val_Reconstruction : 93.87414169311523 Val_KL : 3.424470067024231\n","Epoch: 1649/8000  Traning Loss: 98.15549182891846  Train_Reconstruction: 94.68582916259766  Train_KL: 3.46966215968132  Validation Loss : 97.2418441772461 Val_Reconstruction : 93.81839370727539 Val_KL : 3.4234503507614136\n","Epoch: 1650/8000  Traning Loss: 98.55467987060547  Train_Reconstruction: 95.0902452468872  Train_KL: 3.4644361436367035  Validation Loss : 97.14068222045898 Val_Reconstruction : 93.72011184692383 Val_KL : 3.420570135116577\n","Epoch: 1651/8000  Traning Loss: 99.09777069091797  Train_Reconstruction: 95.62010478973389  Train_KL: 3.4776657223701477  Validation Loss : 97.49554824829102 Val_Reconstruction : 94.05309677124023 Val_KL : 3.4424487352371216\n","Epoch: 1652/8000  Traning Loss: 98.41618919372559  Train_Reconstruction: 94.93498706817627  Train_KL: 3.4812003076076508  Validation Loss : 96.94178009033203 Val_Reconstruction : 93.5189094543457 Val_KL : 3.4228713512420654\n","Epoch: 1653/8000  Traning Loss: 98.1215238571167  Train_Reconstruction: 94.63733100891113  Train_KL: 3.4841927587985992  Validation Loss : 96.5323600769043 Val_Reconstruction : 93.08003234863281 Val_KL : 3.452328085899353\n","Epoch: 1654/8000  Traning Loss: 97.76994895935059  Train_Reconstruction: 94.28024291992188  Train_KL: 3.4897064566612244  Validation Loss : 96.25077819824219 Val_Reconstruction : 92.81560516357422 Val_KL : 3.435175657272339\n","Epoch: 1655/8000  Traning Loss: 97.73149299621582  Train_Reconstruction: 94.25446033477783  Train_KL: 3.4770321249961853  Validation Loss : 96.77062606811523 Val_Reconstruction : 93.32963562011719 Val_KL : 3.440989136695862\n","Epoch: 1656/8000  Traning Loss: 98.62527084350586  Train_Reconstruction: 95.14390087127686  Train_KL: 3.4813703894615173  Validation Loss : 97.68406295776367 Val_Reconstruction : 94.25088119506836 Val_KL : 3.433180809020996\n","Epoch: 1657/8000  Traning Loss: 98.84759044647217  Train_Reconstruction: 95.37229347229004  Train_KL: 3.475298911333084  Validation Loss : 97.34000015258789 Val_Reconstruction : 93.90019226074219 Val_KL : 3.4398072957992554\n","Epoch: 1658/8000  Traning Loss: 98.6153039932251  Train_Reconstruction: 95.13334560394287  Train_KL: 3.481958895921707  Validation Loss : 97.38986206054688 Val_Reconstruction : 93.94109725952148 Val_KL : 3.4487626552581787\n","Epoch: 1659/8000  Traning Loss: 98.19068622589111  Train_Reconstruction: 94.69515037536621  Train_KL: 3.4955367147922516  Validation Loss : 96.59817886352539 Val_Reconstruction : 93.1295051574707 Val_KL : 3.4686728715896606\n","Epoch: 1660/8000  Traning Loss: 97.87897300720215  Train_Reconstruction: 94.39223957061768  Train_KL: 3.4867335855960846  Validation Loss : 96.29003143310547 Val_Reconstruction : 92.85067749023438 Val_KL : 3.439353346824646\n","Epoch: 1661/8000  Traning Loss: 97.65836524963379  Train_Reconstruction: 94.1739854812622  Train_KL: 3.484380841255188  Validation Loss : 96.28354263305664 Val_Reconstruction : 92.83683013916016 Val_KL : 3.4467121362686157\n","Epoch: 1662/8000  Traning Loss: 97.731520652771  Train_Reconstruction: 94.24560928344727  Train_KL: 3.4859108924865723  Validation Loss : 96.3779067993164 Val_Reconstruction : 92.9411849975586 Val_KL : 3.436721444129944\n","Epoch: 1663/8000  Traning Loss: 97.7994794845581  Train_Reconstruction: 94.33716487884521  Train_KL: 3.4623148441314697  Validation Loss : 96.4488296508789 Val_Reconstruction : 93.02496719360352 Val_KL : 3.4238637685775757\n","Epoch: 1664/8000  Traning Loss: 97.68921375274658  Train_Reconstruction: 94.19403553009033  Train_KL: 3.4951784312725067  Validation Loss : 96.6162223815918 Val_Reconstruction : 93.14384841918945 Val_KL : 3.4723730087280273\n","Epoch: 1665/8000  Traning Loss: 98.16969776153564  Train_Reconstruction: 94.660325050354  Train_KL: 3.5093732476234436  Validation Loss : 96.97294616699219 Val_Reconstruction : 93.51571655273438 Val_KL : 3.4572304487228394\n","Epoch: 1666/8000  Traning Loss: 98.28945732116699  Train_Reconstruction: 94.82272148132324  Train_KL: 3.466735929250717  Validation Loss : 96.86091232299805 Val_Reconstruction : 93.45368576049805 Val_KL : 3.407228946685791\n","Epoch: 1667/8000  Traning Loss: 97.83837795257568  Train_Reconstruction: 94.37003517150879  Train_KL: 3.468342125415802  Validation Loss : 96.57790756225586 Val_Reconstruction : 93.15251541137695 Val_KL : 3.4253923892974854\n","Epoch: 1668/8000  Traning Loss: 97.9899730682373  Train_Reconstruction: 94.52430152893066  Train_KL: 3.465671807527542  Validation Loss : 96.67547607421875 Val_Reconstruction : 93.24048614501953 Val_KL : 3.434989333152771\n","Epoch: 1669/8000  Traning Loss: 98.0093994140625  Train_Reconstruction: 94.53070545196533  Train_KL: 3.47869336605072  Validation Loss : 96.66871643066406 Val_Reconstruction : 93.22953414916992 Val_KL : 3.4391846656799316\n","Epoch: 1670/8000  Traning Loss: 97.99354934692383  Train_Reconstruction: 94.51423168182373  Train_KL: 3.4793185889720917  Validation Loss : 96.69702911376953 Val_Reconstruction : 93.2632942199707 Val_KL : 3.433732271194458\n","Epoch: 1671/8000  Traning Loss: 98.1625337600708  Train_Reconstruction: 94.67229747772217  Train_KL: 3.4902370870113373  Validation Loss : 96.66051483154297 Val_Reconstruction : 93.21105575561523 Val_KL : 3.4494580030441284\n","Epoch: 1672/8000  Traning Loss: 98.27390193939209  Train_Reconstruction: 94.80734443664551  Train_KL: 3.466558128595352  Validation Loss : 96.5216293334961 Val_Reconstruction : 93.09757995605469 Val_KL : 3.4240487813949585\n","Epoch: 1673/8000  Traning Loss: 97.75798225402832  Train_Reconstruction: 94.27983856201172  Train_KL: 3.478144019842148  Validation Loss : 96.41484069824219 Val_Reconstruction : 92.95862197875977 Val_KL : 3.456218719482422\n","Epoch: 1674/8000  Traning Loss: 97.78192901611328  Train_Reconstruction: 94.28261947631836  Train_KL: 3.499308794736862  Validation Loss : 96.49296951293945 Val_Reconstruction : 93.04018020629883 Val_KL : 3.4527896642684937\n","Epoch: 1675/8000  Traning Loss: 97.90621376037598  Train_Reconstruction: 94.4347562789917  Train_KL: 3.471457749605179  Validation Loss : 96.27566528320312 Val_Reconstruction : 92.84673309326172 Val_KL : 3.4289294481277466\n","Epoch: 1676/8000  Traning Loss: 98.0618896484375  Train_Reconstruction: 94.59391784667969  Train_KL: 3.467972993850708  Validation Loss : 96.74519348144531 Val_Reconstruction : 93.3144416809082 Val_KL : 3.430752396583557\n","Epoch: 1677/8000  Traning Loss: 97.91290283203125  Train_Reconstruction: 94.43178081512451  Train_KL: 3.4811197221279144  Validation Loss : 96.21700286865234 Val_Reconstruction : 92.77513122558594 Val_KL : 3.441872477531433\n","Epoch: 1678/8000  Traning Loss: 97.62844467163086  Train_Reconstruction: 94.15005779266357  Train_KL: 3.4783860743045807  Validation Loss : 96.10577392578125 Val_Reconstruction : 92.67076110839844 Val_KL : 3.4350110292434692\n","Epoch: 1679/8000  Traning Loss: 97.47723484039307  Train_Reconstruction: 94.00265407562256  Train_KL: 3.474580854177475  Validation Loss : 96.18115615844727 Val_Reconstruction : 92.74163055419922 Val_KL : 3.439527988433838\n","Epoch: 1680/8000  Traning Loss: 97.82549667358398  Train_Reconstruction: 94.34109878540039  Train_KL: 3.484399288892746  Validation Loss : 96.09993743896484 Val_Reconstruction : 92.64846801757812 Val_KL : 3.4514691829681396\n","Epoch: 1681/8000  Traning Loss: 97.77112483978271  Train_Reconstruction: 94.29417037963867  Train_KL: 3.4769546687602997  Validation Loss : 96.32378387451172 Val_Reconstruction : 92.88677597045898 Val_KL : 3.437008261680603\n","Epoch: 1682/8000  Traning Loss: 97.80524158477783  Train_Reconstruction: 94.33962440490723  Train_KL: 3.465616375207901  Validation Loss : 96.31703186035156 Val_Reconstruction : 92.8875732421875 Val_KL : 3.4294583797454834\n","Epoch: 1683/8000  Traning Loss: 97.61809635162354  Train_Reconstruction: 94.1398754119873  Train_KL: 3.478221297264099  Validation Loss : 96.42673110961914 Val_Reconstruction : 92.98446655273438 Val_KL : 3.442262887954712\n","Epoch: 1684/8000  Traning Loss: 97.5798511505127  Train_Reconstruction: 94.08568000793457  Train_KL: 3.4941707253456116  Validation Loss : 96.19714736938477 Val_Reconstruction : 92.74012756347656 Val_KL : 3.4570201635360718\n","Epoch: 1685/8000  Traning Loss: 97.66232013702393  Train_Reconstruction: 94.18024826049805  Train_KL: 3.4820730090141296  Validation Loss : 96.53275299072266 Val_Reconstruction : 93.09638977050781 Val_KL : 3.436363458633423\n","Epoch: 1686/8000  Traning Loss: 97.79382514953613  Train_Reconstruction: 94.31131553649902  Train_KL: 3.482509046792984  Validation Loss : 96.46686553955078 Val_Reconstruction : 93.01955795288086 Val_KL : 3.4473084211349487\n","Epoch: 1687/8000  Traning Loss: 97.74855995178223  Train_Reconstruction: 94.26668930053711  Train_KL: 3.481870472431183  Validation Loss : 96.65329360961914 Val_Reconstruction : 93.22859573364258 Val_KL : 3.4247004985809326\n","Epoch: 1688/8000  Traning Loss: 97.79128742218018  Train_Reconstruction: 94.30943393707275  Train_KL: 3.481853663921356  Validation Loss : 96.25064849853516 Val_Reconstruction : 92.81266784667969 Val_KL : 3.4379783868789673\n","Epoch: 1689/8000  Traning Loss: 97.49837112426758  Train_Reconstruction: 94.03318881988525  Train_KL: 3.465181201696396  Validation Loss : 96.46463775634766 Val_Reconstruction : 93.04068756103516 Val_KL : 3.423949718475342\n","Epoch: 1690/8000  Traning Loss: 97.6662187576294  Train_Reconstruction: 94.18983936309814  Train_KL: 3.4763797223567963  Validation Loss : 96.57744598388672 Val_Reconstruction : 93.13906478881836 Val_KL : 3.4383797645568848\n","Epoch: 1691/8000  Traning Loss: 97.75191020965576  Train_Reconstruction: 94.2672929763794  Train_KL: 3.4846165776252747  Validation Loss : 96.29495239257812 Val_Reconstruction : 92.84913635253906 Val_KL : 3.4458167552948\n","Epoch: 1692/8000  Traning Loss: 97.55350875854492  Train_Reconstruction: 94.07712936401367  Train_KL: 3.476378709077835  Validation Loss : 96.11407089233398 Val_Reconstruction : 92.67939758300781 Val_KL : 3.434673547744751\n","Epoch: 1693/8000  Traning Loss: 97.76860332489014  Train_Reconstruction: 94.28224277496338  Train_KL: 3.486359864473343  Validation Loss : 96.62738037109375 Val_Reconstruction : 93.17614364624023 Val_KL : 3.4512351751327515\n","Epoch: 1694/8000  Traning Loss: 97.7759895324707  Train_Reconstruction: 94.28970336914062  Train_KL: 3.4862860441207886  Validation Loss : 96.25786972045898 Val_Reconstruction : 92.8145866394043 Val_KL : 3.4432820081710815\n","Epoch: 1695/8000  Traning Loss: 97.69055938720703  Train_Reconstruction: 94.20844841003418  Train_KL: 3.482111304998398  Validation Loss : 96.25341415405273 Val_Reconstruction : 92.82481002807617 Val_KL : 3.4286051988601685\n","Epoch: 1696/8000  Traning Loss: 97.85009288787842  Train_Reconstruction: 94.37644004821777  Train_KL: 3.473652631044388  Validation Loss : 96.5330924987793 Val_Reconstruction : 93.08999633789062 Val_KL : 3.443095564842224\n","Epoch: 1697/8000  Traning Loss: 97.75241470336914  Train_Reconstruction: 94.26655197143555  Train_KL: 3.485862284898758  Validation Loss : 96.0972900390625 Val_Reconstruction : 92.65545654296875 Val_KL : 3.4418357610702515\n","Epoch: 1698/8000  Traning Loss: 97.414794921875  Train_Reconstruction: 93.93238067626953  Train_KL: 3.482413947582245  Validation Loss : 96.2265625 Val_Reconstruction : 92.77913665771484 Val_KL : 3.447426199913025\n","Epoch: 1699/8000  Traning Loss: 97.42591571807861  Train_Reconstruction: 93.92978191375732  Train_KL: 3.4961330592632294  Validation Loss : 96.40394592285156 Val_Reconstruction : 92.95162200927734 Val_KL : 3.452325224876404\n","Epoch: 1700/8000  Traning Loss: 97.76446628570557  Train_Reconstruction: 94.26695346832275  Train_KL: 3.497512549161911  Validation Loss : 96.46376037597656 Val_Reconstruction : 93.00935363769531 Val_KL : 3.4544053077697754\n","Epoch: 1701/8000  Traning Loss: 97.70084476470947  Train_Reconstruction: 94.21159553527832  Train_KL: 3.4892486333847046  Validation Loss : 96.46221923828125 Val_Reconstruction : 93.02290344238281 Val_KL : 3.439314842224121\n","Epoch: 1702/8000  Traning Loss: 97.77215385437012  Train_Reconstruction: 94.29518795013428  Train_KL: 3.4769664108753204  Validation Loss : 96.48259735107422 Val_Reconstruction : 93.04178619384766 Val_KL : 3.440810203552246\n","Epoch: 1703/8000  Traning Loss: 97.87617683410645  Train_Reconstruction: 94.39576148986816  Train_KL: 3.480414867401123  Validation Loss : 96.62990951538086 Val_Reconstruction : 93.18877410888672 Val_KL : 3.441138744354248\n","Epoch: 1704/8000  Traning Loss: 98.02336883544922  Train_Reconstruction: 94.55172538757324  Train_KL: 3.4716426134109497  Validation Loss : 96.71046829223633 Val_Reconstruction : 93.28727340698242 Val_KL : 3.423192858695984\n","Epoch: 1705/8000  Traning Loss: 97.98585414886475  Train_Reconstruction: 94.50853443145752  Train_KL: 3.4773207008838654  Validation Loss : 96.92246627807617 Val_Reconstruction : 93.47163772583008 Val_KL : 3.450828433036804\n","Epoch: 1706/8000  Traning Loss: 98.08915042877197  Train_Reconstruction: 94.58791446685791  Train_KL: 3.501237154006958  Validation Loss : 96.6751594543457 Val_Reconstruction : 93.22580337524414 Val_KL : 3.4493590593338013\n","Epoch: 1707/8000  Traning Loss: 97.5155668258667  Train_Reconstruction: 94.0427417755127  Train_KL: 3.472825288772583  Validation Loss : 95.9237289428711 Val_Reconstruction : 92.51041030883789 Val_KL : 3.4133152961730957\n","Epoch: 1708/8000  Traning Loss: 97.52214050292969  Train_Reconstruction: 94.04914283752441  Train_KL: 3.4729971289634705  Validation Loss : 96.14428329467773 Val_Reconstruction : 92.69228744506836 Val_KL : 3.4519978761672974\n","Epoch: 1709/8000  Traning Loss: 97.58282947540283  Train_Reconstruction: 94.08742332458496  Train_KL: 3.495407372713089  Validation Loss : 95.98575210571289 Val_Reconstruction : 92.53275680541992 Val_KL : 3.4529967308044434\n","Epoch: 1710/8000  Traning Loss: 97.57173538208008  Train_Reconstruction: 94.08475017547607  Train_KL: 3.486985683441162  Validation Loss : 96.3993148803711 Val_Reconstruction : 92.95548248291016 Val_KL : 3.443835139274597\n","Epoch: 1711/8000  Traning Loss: 97.66764640808105  Train_Reconstruction: 94.17482852935791  Train_KL: 3.4928185641765594  Validation Loss : 96.58700561523438 Val_Reconstruction : 93.14580154418945 Val_KL : 3.441203474998474\n","Epoch: 1712/8000  Traning Loss: 97.70622634887695  Train_Reconstruction: 94.23181247711182  Train_KL: 3.4744136929512024  Validation Loss : 96.24245071411133 Val_Reconstruction : 92.80332946777344 Val_KL : 3.439120292663574\n","Epoch: 1713/8000  Traning Loss: 97.3905439376831  Train_Reconstruction: 93.9079179763794  Train_KL: 3.4826270937919617  Validation Loss : 96.20623779296875 Val_Reconstruction : 92.76974105834961 Val_KL : 3.436497211456299\n","Epoch: 1714/8000  Traning Loss: 97.46079635620117  Train_Reconstruction: 93.98176574707031  Train_KL: 3.479029655456543  Validation Loss : 95.98001861572266 Val_Reconstruction : 92.54066848754883 Val_KL : 3.4393506050109863\n","Epoch: 1715/8000  Traning Loss: 97.27730560302734  Train_Reconstruction: 93.79290199279785  Train_KL: 3.4844034910202026  Validation Loss : 95.89748764038086 Val_Reconstruction : 92.46481323242188 Val_KL : 3.432675838470459\n","Epoch: 1716/8000  Traning Loss: 97.35038375854492  Train_Reconstruction: 93.87908744812012  Train_KL: 3.4712961614131927  Validation Loss : 95.98975372314453 Val_Reconstruction : 92.5678596496582 Val_KL : 3.4218952655792236\n","Epoch: 1717/8000  Traning Loss: 97.5729112625122  Train_Reconstruction: 94.09329319000244  Train_KL: 3.4796187579631805  Validation Loss : 96.07387924194336 Val_Reconstruction : 92.62100219726562 Val_KL : 3.452879548072815\n","Epoch: 1718/8000  Traning Loss: 97.63617706298828  Train_Reconstruction: 94.14836025238037  Train_KL: 3.4878166019916534  Validation Loss : 96.08961868286133 Val_Reconstruction : 92.64566040039062 Val_KL : 3.443960189819336\n","Epoch: 1719/8000  Traning Loss: 97.5642318725586  Train_Reconstruction: 94.07970905303955  Train_KL: 3.4845228791236877  Validation Loss : 96.0400505065918 Val_Reconstruction : 92.59617233276367 Val_KL : 3.443877100944519\n","Epoch: 1720/8000  Traning Loss: 97.76172828674316  Train_Reconstruction: 94.27245330810547  Train_KL: 3.489274173974991  Validation Loss : 96.20257949829102 Val_Reconstruction : 92.75326919555664 Val_KL : 3.4493112564086914\n","Epoch: 1721/8000  Traning Loss: 97.90754413604736  Train_Reconstruction: 94.42468070983887  Train_KL: 3.482862740755081  Validation Loss : 96.47820281982422 Val_Reconstruction : 93.02375030517578 Val_KL : 3.454455614089966\n","Epoch: 1722/8000  Traning Loss: 97.70178413391113  Train_Reconstruction: 94.21387195587158  Train_KL: 3.487912029027939  Validation Loss : 96.36369323730469 Val_Reconstruction : 92.92171859741211 Val_KL : 3.441977381706238\n","Epoch: 1723/8000  Traning Loss: 97.63410472869873  Train_Reconstruction: 94.152419090271  Train_KL: 3.481685131788254  Validation Loss : 95.95367431640625 Val_Reconstruction : 92.51288223266602 Val_KL : 3.4407894611358643\n","Epoch: 1724/8000  Traning Loss: 97.59406566619873  Train_Reconstruction: 94.1184663772583  Train_KL: 3.475599616765976  Validation Loss : 96.1360092163086 Val_Reconstruction : 92.70378494262695 Val_KL : 3.4322245121002197\n","Epoch: 1725/8000  Traning Loss: 97.65693664550781  Train_Reconstruction: 94.18063068389893  Train_KL: 3.4763055741786957  Validation Loss : 96.18922805786133 Val_Reconstruction : 92.75448608398438 Val_KL : 3.4347429275512695\n","Epoch: 1726/8000  Traning Loss: 97.83059310913086  Train_Reconstruction: 94.36211204528809  Train_KL: 3.468480557203293  Validation Loss : 96.25350570678711 Val_Reconstruction : 92.82111358642578 Val_KL : 3.432390809059143\n","Epoch: 1727/8000  Traning Loss: 97.82149028778076  Train_Reconstruction: 94.3309555053711  Train_KL: 3.490534693002701  Validation Loss : 96.43518829345703 Val_Reconstruction : 92.9787826538086 Val_KL : 3.4564069509506226\n","Epoch: 1728/8000  Traning Loss: 97.95794105529785  Train_Reconstruction: 94.4731388092041  Train_KL: 3.484801799058914  Validation Loss : 96.73806762695312 Val_Reconstruction : 93.30186462402344 Val_KL : 3.4362024068832397\n","Epoch: 1729/8000  Traning Loss: 98.1929178237915  Train_Reconstruction: 94.71070003509521  Train_KL: 3.48221755027771  Validation Loss : 96.48147583007812 Val_Reconstruction : 93.03055191040039 Val_KL : 3.450922727584839\n","Epoch: 1730/8000  Traning Loss: 97.63953304290771  Train_Reconstruction: 94.1573839187622  Train_KL: 3.4821485579013824  Validation Loss : 95.76552963256836 Val_Reconstruction : 92.32452774047852 Val_KL : 3.440999746322632\n","Epoch: 1731/8000  Traning Loss: 97.62028980255127  Train_Reconstruction: 94.1375093460083  Train_KL: 3.482781022787094  Validation Loss : 96.15304565429688 Val_Reconstruction : 92.71259689331055 Val_KL : 3.4404460191726685\n","Epoch: 1732/8000  Traning Loss: 97.59109115600586  Train_Reconstruction: 94.12195873260498  Train_KL: 3.469131350517273  Validation Loss : 96.32085800170898 Val_Reconstruction : 92.8974609375 Val_KL : 3.4233986139297485\n","Epoch: 1733/8000  Traning Loss: 97.75812721252441  Train_Reconstruction: 94.27446365356445  Train_KL: 3.4836624562740326  Validation Loss : 95.95625686645508 Val_Reconstruction : 92.50740051269531 Val_KL : 3.448857545852661\n","Epoch: 1734/8000  Traning Loss: 97.98780345916748  Train_Reconstruction: 94.49611568450928  Train_KL: 3.4916893541812897  Validation Loss : 96.55816650390625 Val_Reconstruction : 93.11513900756836 Val_KL : 3.4430298805236816\n","Epoch: 1735/8000  Traning Loss: 97.86358547210693  Train_Reconstruction: 94.38441944122314  Train_KL: 3.479165703058243  Validation Loss : 96.53080749511719 Val_Reconstruction : 93.08855056762695 Val_KL : 3.4422565698623657\n","Epoch: 1736/8000  Traning Loss: 97.72264862060547  Train_Reconstruction: 94.23283863067627  Train_KL: 3.4898087680339813  Validation Loss : 96.55765914916992 Val_Reconstruction : 93.10481262207031 Val_KL : 3.4528493881225586\n","Epoch: 1737/8000  Traning Loss: 97.59124946594238  Train_Reconstruction: 94.09924125671387  Train_KL: 3.492008715867996  Validation Loss : 95.96308898925781 Val_Reconstruction : 92.50606536865234 Val_KL : 3.457023024559021\n","Epoch: 1738/8000  Traning Loss: 97.49586296081543  Train_Reconstruction: 94.00209522247314  Train_KL: 3.493767946958542  Validation Loss : 95.92806243896484 Val_Reconstruction : 92.48851776123047 Val_KL : 3.439545512199402\n","Epoch: 1739/8000  Traning Loss: 97.55117607116699  Train_Reconstruction: 94.08145904541016  Train_KL: 3.469716489315033  Validation Loss : 96.23784637451172 Val_Reconstruction : 92.81042861938477 Val_KL : 3.42741596698761\n","Epoch: 1740/8000  Traning Loss: 97.52336406707764  Train_Reconstruction: 94.04209804534912  Train_KL: 3.48126557469368  Validation Loss : 96.33266830444336 Val_Reconstruction : 92.88422775268555 Val_KL : 3.448439598083496\n","Epoch: 1741/8000  Traning Loss: 97.37916564941406  Train_Reconstruction: 93.89409255981445  Train_KL: 3.4850739240646362  Validation Loss : 95.98834991455078 Val_Reconstruction : 92.55392837524414 Val_KL : 3.434420585632324\n","Epoch: 1742/8000  Traning Loss: 97.3477029800415  Train_Reconstruction: 93.86685466766357  Train_KL: 3.4808488488197327  Validation Loss : 96.0029296875 Val_Reconstruction : 92.5630111694336 Val_KL : 3.439919114112854\n","Epoch: 1743/8000  Traning Loss: 97.53899765014648  Train_Reconstruction: 94.05556106567383  Train_KL: 3.48343625664711  Validation Loss : 96.1129150390625 Val_Reconstruction : 92.65682983398438 Val_KL : 3.4560850858688354\n","Epoch: 1744/8000  Traning Loss: 97.63407135009766  Train_Reconstruction: 94.16099548339844  Train_KL: 3.473075717687607  Validation Loss : 96.13631820678711 Val_Reconstruction : 92.70623779296875 Val_KL : 3.4300806522369385\n","Epoch: 1745/8000  Traning Loss: 97.61127853393555  Train_Reconstruction: 94.13346099853516  Train_KL: 3.4778164327144623  Validation Loss : 96.45351791381836 Val_Reconstruction : 93.0048828125 Val_KL : 3.448632597923279\n","Epoch: 1746/8000  Traning Loss: 97.59809494018555  Train_Reconstruction: 94.1131534576416  Train_KL: 3.4849418997764587  Validation Loss : 95.95494842529297 Val_Reconstruction : 92.52323150634766 Val_KL : 3.431715965270996\n","Epoch: 1747/8000  Traning Loss: 97.35355567932129  Train_Reconstruction: 93.88123512268066  Train_KL: 3.47231987118721  Validation Loss : 96.37250900268555 Val_Reconstruction : 92.93707656860352 Val_KL : 3.435434341430664\n","Epoch: 1748/8000  Traning Loss: 97.66578388214111  Train_Reconstruction: 94.18004417419434  Train_KL: 3.4857383370399475  Validation Loss : 96.55787658691406 Val_Reconstruction : 93.11542129516602 Val_KL : 3.442455768585205\n","Epoch: 1749/8000  Traning Loss: 98.54493236541748  Train_Reconstruction: 95.06436252593994  Train_KL: 3.4805704951286316  Validation Loss : 97.5439224243164 Val_Reconstruction : 94.10849380493164 Val_KL : 3.43542742729187\n","Epoch: 1750/8000  Traning Loss: 98.22132301330566  Train_Reconstruction: 94.73916721343994  Train_KL: 3.482154965400696  Validation Loss : 96.15626907348633 Val_Reconstruction : 92.70971298217773 Val_KL : 3.446556568145752\n","Epoch: 1751/8000  Traning Loss: 97.3999137878418  Train_Reconstruction: 93.91933536529541  Train_KL: 3.4805785715579987  Validation Loss : 95.75151062011719 Val_Reconstruction : 92.3242416381836 Val_KL : 3.427268385887146\n","Epoch: 1752/8000  Traning Loss: 97.19944190979004  Train_Reconstruction: 93.72349548339844  Train_KL: 3.4759455919265747  Validation Loss : 95.87831115722656 Val_Reconstruction : 92.43603515625 Val_KL : 3.4422767162323\n","Epoch: 1753/8000  Traning Loss: 97.18435573577881  Train_Reconstruction: 93.70175266265869  Train_KL: 3.482603907585144  Validation Loss : 95.7178726196289 Val_Reconstruction : 92.26388549804688 Val_KL : 3.4539889097213745\n","Epoch: 1754/8000  Traning Loss: 97.28609561920166  Train_Reconstruction: 93.79758930206299  Train_KL: 3.4885065853595734  Validation Loss : 95.66893768310547 Val_Reconstruction : 92.21781921386719 Val_KL : 3.4511168003082275\n","Epoch: 1755/8000  Traning Loss: 97.40668678283691  Train_Reconstruction: 93.93668842315674  Train_KL: 3.469997763633728  Validation Loss : 96.23501968383789 Val_Reconstruction : 92.81368637084961 Val_KL : 3.421335458755493\n","Epoch: 1756/8000  Traning Loss: 97.85888481140137  Train_Reconstruction: 94.37344741821289  Train_KL: 3.485436260700226  Validation Loss : 96.48046493530273 Val_Reconstruction : 93.02674102783203 Val_KL : 3.4537218809127808\n","Epoch: 1757/8000  Traning Loss: 97.90236186981201  Train_Reconstruction: 94.42203998565674  Train_KL: 3.480322688817978  Validation Loss : 96.1738052368164 Val_Reconstruction : 92.74777221679688 Val_KL : 3.4260315895080566\n","Epoch: 1758/8000  Traning Loss: 97.3230447769165  Train_Reconstruction: 93.8525037765503  Train_KL: 3.4705394506454468  Validation Loss : 96.22217559814453 Val_Reconstruction : 92.7811393737793 Val_KL : 3.441037893295288\n","Epoch: 1759/8000  Traning Loss: 97.41452312469482  Train_Reconstruction: 93.92488765716553  Train_KL: 3.4896360337734222  Validation Loss : 96.72062683105469 Val_Reconstruction : 93.28115463256836 Val_KL : 3.439475417137146\n","Epoch: 1760/8000  Traning Loss: 98.40167331695557  Train_Reconstruction: 94.92025852203369  Train_KL: 3.481414943933487  Validation Loss : 97.37706756591797 Val_Reconstruction : 93.94535827636719 Val_KL : 3.4317115545272827\n","Epoch: 1761/8000  Traning Loss: 98.29409122467041  Train_Reconstruction: 94.80476093292236  Train_KL: 3.4893317222595215  Validation Loss : 96.93806838989258 Val_Reconstruction : 93.48606872558594 Val_KL : 3.451999306678772\n","Epoch: 1762/8000  Traning Loss: 97.72847557067871  Train_Reconstruction: 94.2435131072998  Train_KL: 3.4849627315998077  Validation Loss : 96.193115234375 Val_Reconstruction : 92.7647705078125 Val_KL : 3.4283443689346313\n","Epoch: 1763/8000  Traning Loss: 97.86457920074463  Train_Reconstruction: 94.3994779586792  Train_KL: 3.4651020765304565  Validation Loss : 96.58123016357422 Val_Reconstruction : 93.16312026977539 Val_KL : 3.418108582496643\n","Epoch: 1764/8000  Traning Loss: 97.96714496612549  Train_Reconstruction: 94.48673439025879  Train_KL: 3.480409801006317  Validation Loss : 96.19543838500977 Val_Reconstruction : 92.73988723754883 Val_KL : 3.4555498361587524\n","Epoch: 1765/8000  Traning Loss: 97.31320190429688  Train_Reconstruction: 93.83927345275879  Train_KL: 3.4739279448986053  Validation Loss : 95.71609115600586 Val_Reconstruction : 92.29793548583984 Val_KL : 3.418156862258911\n","Epoch: 1766/8000  Traning Loss: 97.23033046722412  Train_Reconstruction: 93.75710105895996  Train_KL: 3.473229020833969  Validation Loss : 96.29470443725586 Val_Reconstruction : 92.85404586791992 Val_KL : 3.44066059589386\n","Epoch: 1767/8000  Traning Loss: 97.40102005004883  Train_Reconstruction: 93.91422176361084  Train_KL: 3.486799657344818  Validation Loss : 96.34990692138672 Val_Reconstruction : 92.89710998535156 Val_KL : 3.4527976512908936\n","Epoch: 1768/8000  Traning Loss: 97.38929271697998  Train_Reconstruction: 93.8930435180664  Train_KL: 3.4962496757507324  Validation Loss : 96.08544158935547 Val_Reconstruction : 92.6415901184082 Val_KL : 3.443854808807373\n","Epoch: 1769/8000  Traning Loss: 97.27324962615967  Train_Reconstruction: 93.79272365570068  Train_KL: 3.4805269837379456  Validation Loss : 95.81401824951172 Val_Reconstruction : 92.37237930297852 Val_KL : 3.441637635231018\n","Epoch: 1770/8000  Traning Loss: 97.1480360031128  Train_Reconstruction: 93.65346813201904  Train_KL: 3.4945662021636963  Validation Loss : 95.88383865356445 Val_Reconstruction : 92.44113159179688 Val_KL : 3.442708969116211\n","Epoch: 1771/8000  Traning Loss: 97.4917573928833  Train_Reconstruction: 94.01096725463867  Train_KL: 3.4807887077331543  Validation Loss : 96.5394515991211 Val_Reconstruction : 93.11653137207031 Val_KL : 3.4229217767715454\n","Epoch: 1772/8000  Traning Loss: 97.90388584136963  Train_Reconstruction: 94.42828845977783  Train_KL: 3.475598990917206  Validation Loss : 96.55780029296875 Val_Reconstruction : 93.12147903442383 Val_KL : 3.436318874359131\n","Epoch: 1773/8000  Traning Loss: 98.26226997375488  Train_Reconstruction: 94.78756809234619  Train_KL: 3.4747008085250854  Validation Loss : 97.43445205688477 Val_Reconstruction : 94.00889587402344 Val_KL : 3.4255555868148804\n","Epoch: 1774/8000  Traning Loss: 98.19804286956787  Train_Reconstruction: 94.72729206085205  Train_KL: 3.470750093460083  Validation Loss : 96.9742660522461 Val_Reconstruction : 93.54219436645508 Val_KL : 3.4320703744888306\n","Epoch: 1775/8000  Traning Loss: 97.77191638946533  Train_Reconstruction: 94.28405284881592  Train_KL: 3.4878645539283752  Validation Loss : 96.61691284179688 Val_Reconstruction : 93.17816925048828 Val_KL : 3.43874454498291\n","Epoch: 1776/8000  Traning Loss: 97.71293640136719  Train_Reconstruction: 94.22358989715576  Train_KL: 3.4893457293510437  Validation Loss : 96.3280258178711 Val_Reconstruction : 92.88090896606445 Val_KL : 3.447118043899536\n","Epoch: 1777/8000  Traning Loss: 97.63863372802734  Train_Reconstruction: 94.15739154815674  Train_KL: 3.481243282556534  Validation Loss : 96.33428573608398 Val_Reconstruction : 92.89183807373047 Val_KL : 3.442448377609253\n","Epoch: 1778/8000  Traning Loss: 97.40355396270752  Train_Reconstruction: 93.91126441955566  Train_KL: 3.492290735244751  Validation Loss : 96.14540481567383 Val_Reconstruction : 92.6924819946289 Val_KL : 3.4529199600219727\n","Epoch: 1779/8000  Traning Loss: 97.38429641723633  Train_Reconstruction: 93.89041137695312  Train_KL: 3.493883937597275  Validation Loss : 96.28461074829102 Val_Reconstruction : 92.83330154418945 Val_KL : 3.4513087272644043\n","Epoch: 1780/8000  Traning Loss: 97.45643615722656  Train_Reconstruction: 93.9668493270874  Train_KL: 3.489586889743805  Validation Loss : 96.04747772216797 Val_Reconstruction : 92.60413360595703 Val_KL : 3.4433473348617554\n","Epoch: 1781/8000  Traning Loss: 97.51864337921143  Train_Reconstruction: 94.03418922424316  Train_KL: 3.4844526648521423  Validation Loss : 96.14738464355469 Val_Reconstruction : 92.70624923706055 Val_KL : 3.44113552570343\n","Epoch: 1782/8000  Traning Loss: 97.38284873962402  Train_Reconstruction: 93.89956378936768  Train_KL: 3.483284682035446  Validation Loss : 96.11607360839844 Val_Reconstruction : 92.68099975585938 Val_KL : 3.435075044631958\n","Epoch: 1783/8000  Traning Loss: 97.14357948303223  Train_Reconstruction: 93.67431354522705  Train_KL: 3.4692640900611877  Validation Loss : 95.9235725402832 Val_Reconstruction : 92.49216079711914 Val_KL : 3.431410312652588\n","Epoch: 1784/8000  Traning Loss: 96.97766590118408  Train_Reconstruction: 93.50246047973633  Train_KL: 3.4752050042152405  Validation Loss : 95.56733322143555 Val_Reconstruction : 92.12237167358398 Val_KL : 3.444961667060852\n","Epoch: 1785/8000  Traning Loss: 96.9693489074707  Train_Reconstruction: 93.47697067260742  Train_KL: 3.4923775494098663  Validation Loss : 95.72802352905273 Val_Reconstruction : 92.28012084960938 Val_KL : 3.4479018449783325\n","Epoch: 1786/8000  Traning Loss: 96.98394680023193  Train_Reconstruction: 93.50442695617676  Train_KL: 3.4795199632644653  Validation Loss : 95.72967147827148 Val_Reconstruction : 92.29924392700195 Val_KL : 3.43043053150177\n","Epoch: 1787/8000  Traning Loss: 97.06860446929932  Train_Reconstruction: 93.59344387054443  Train_KL: 3.475160598754883  Validation Loss : 95.8254165649414 Val_Reconstruction : 92.37835693359375 Val_KL : 3.447059750556946\n","Epoch: 1788/8000  Traning Loss: 97.14218330383301  Train_Reconstruction: 93.6464262008667  Train_KL: 3.4957570135593414  Validation Loss : 95.76031875610352 Val_Reconstruction : 92.2973518371582 Val_KL : 3.4629679918289185\n","Epoch: 1789/8000  Traning Loss: 97.03450298309326  Train_Reconstruction: 93.53565120697021  Train_KL: 3.4988507628440857  Validation Loss : 96.22798919677734 Val_Reconstruction : 92.78863525390625 Val_KL : 3.439354419708252\n","Epoch: 1790/8000  Traning Loss: 97.43972969055176  Train_Reconstruction: 93.97006607055664  Train_KL: 3.4696632623672485  Validation Loss : 96.4400634765625 Val_Reconstruction : 93.02219009399414 Val_KL : 3.4178727865219116\n","Epoch: 1791/8000  Traning Loss: 97.1695966720581  Train_Reconstruction: 93.68105983734131  Train_KL: 3.4885358214378357  Validation Loss : 95.77789306640625 Val_Reconstruction : 92.32026672363281 Val_KL : 3.4576252698898315\n","Epoch: 1792/8000  Traning Loss: 97.10388851165771  Train_Reconstruction: 93.61641120910645  Train_KL: 3.4874769151210785  Validation Loss : 95.98055267333984 Val_Reconstruction : 92.54587173461914 Val_KL : 3.4346795082092285\n","Epoch: 1793/8000  Traning Loss: 97.1251049041748  Train_Reconstruction: 93.65128803253174  Train_KL: 3.4738185703754425  Validation Loss : 96.17741394042969 Val_Reconstruction : 92.74035263061523 Val_KL : 3.437059760093689\n","Epoch: 1794/8000  Traning Loss: 97.36709022521973  Train_Reconstruction: 93.88232517242432  Train_KL: 3.48476505279541  Validation Loss : 96.36648941040039 Val_Reconstruction : 92.92081832885742 Val_KL : 3.4456714391708374\n","Epoch: 1795/8000  Traning Loss: 97.99651718139648  Train_Reconstruction: 94.5156774520874  Train_KL: 3.4808386266231537  Validation Loss : 97.30690002441406 Val_Reconstruction : 93.86215591430664 Val_KL : 3.444742798805237\n","Epoch: 1796/8000  Traning Loss: 97.92025089263916  Train_Reconstruction: 94.43878936767578  Train_KL: 3.4814592599868774  Validation Loss : 96.22099304199219 Val_Reconstruction : 92.76803207397461 Val_KL : 3.452962636947632\n","Epoch: 1797/8000  Traning Loss: 97.36933326721191  Train_Reconstruction: 93.8710298538208  Train_KL: 3.498302459716797  Validation Loss : 95.93852615356445 Val_Reconstruction : 92.48967742919922 Val_KL : 3.448846936225891\n","Epoch: 1798/8000  Traning Loss: 97.49041557312012  Train_Reconstruction: 94.01201820373535  Train_KL: 3.478398859500885  Validation Loss : 96.11576843261719 Val_Reconstruction : 92.70220947265625 Val_KL : 3.4135587215423584\n","Epoch: 1799/8000  Traning Loss: 97.3856143951416  Train_Reconstruction: 93.907470703125  Train_KL: 3.4781437516212463  Validation Loss : 96.1281623840332 Val_Reconstruction : 92.6806640625 Val_KL : 3.447498917579651\n","Epoch: 1800/8000  Traning Loss: 97.0726089477539  Train_Reconstruction: 93.58102321624756  Train_KL: 3.4915849566459656  Validation Loss : 95.50738906860352 Val_Reconstruction : 92.06430435180664 Val_KL : 3.443084239959717\n","Epoch: 1801/8000  Traning Loss: 97.05111312866211  Train_Reconstruction: 93.56842041015625  Train_KL: 3.482692629098892  Validation Loss : 95.45178985595703 Val_Reconstruction : 92.0273551940918 Val_KL : 3.42443585395813\n","Epoch: 1802/8000  Traning Loss: 97.02722454071045  Train_Reconstruction: 93.55265998840332  Train_KL: 3.474563956260681  Validation Loss : 95.79046249389648 Val_Reconstruction : 92.3609390258789 Val_KL : 3.4295233488082886\n","Epoch: 1803/8000  Traning Loss: 97.04880428314209  Train_Reconstruction: 93.56824588775635  Train_KL: 3.4805588722229004  Validation Loss : 95.64540481567383 Val_Reconstruction : 92.21778869628906 Val_KL : 3.4276187419891357\n","Epoch: 1804/8000  Traning Loss: 97.09737300872803  Train_Reconstruction: 93.6198787689209  Train_KL: 3.4774929583072662  Validation Loss : 95.91493606567383 Val_Reconstruction : 92.47722625732422 Val_KL : 3.437710165977478\n","Epoch: 1805/8000  Traning Loss: 97.4822244644165  Train_Reconstruction: 94.00554656982422  Train_KL: 3.476678788661957  Validation Loss : 96.51544952392578 Val_Reconstruction : 93.0884780883789 Val_KL : 3.4269707202911377\n","Epoch: 1806/8000  Traning Loss: 97.48394012451172  Train_Reconstruction: 94.00410079956055  Train_KL: 3.4798385202884674  Validation Loss : 96.05101013183594 Val_Reconstruction : 92.62093734741211 Val_KL : 3.430072546005249\n","Epoch: 1807/8000  Traning Loss: 97.44385528564453  Train_Reconstruction: 93.95552635192871  Train_KL: 3.4883282482624054  Validation Loss : 95.95618438720703 Val_Reconstruction : 92.5136833190918 Val_KL : 3.442501187324524\n","Epoch: 1808/8000  Traning Loss: 97.85406589508057  Train_Reconstruction: 94.35674953460693  Train_KL: 3.4973154962062836  Validation Loss : 96.59326934814453 Val_Reconstruction : 93.13439178466797 Val_KL : 3.4588780403137207\n","Epoch: 1809/8000  Traning Loss: 97.93907928466797  Train_Reconstruction: 94.45860481262207  Train_KL: 3.4804746210575104  Validation Loss : 96.42094039916992 Val_Reconstruction : 93.00169372558594 Val_KL : 3.419246554374695\n","Epoch: 1810/8000  Traning Loss: 97.72412967681885  Train_Reconstruction: 94.2490062713623  Train_KL: 3.475124269723892  Validation Loss : 96.5394401550293 Val_Reconstruction : 93.10239791870117 Val_KL : 3.4370453357696533\n","Epoch: 1811/8000  Traning Loss: 97.50843048095703  Train_Reconstruction: 94.01592350006104  Train_KL: 3.4925073385238647  Validation Loss : 96.49800872802734 Val_Reconstruction : 93.04624938964844 Val_KL : 3.451758861541748\n","Epoch: 1812/8000  Traning Loss: 97.0763578414917  Train_Reconstruction: 93.59062099456787  Train_KL: 3.485736161470413  Validation Loss : 95.98316955566406 Val_Reconstruction : 92.55794906616211 Val_KL : 3.4252175092697144\n","Epoch: 1813/8000  Traning Loss: 97.15830516815186  Train_Reconstruction: 93.69108486175537  Train_KL: 3.4672204852104187  Validation Loss : 95.81433868408203 Val_Reconstruction : 92.38472747802734 Val_KL : 3.429609179496765\n","Epoch: 1814/8000  Traning Loss: 97.03529644012451  Train_Reconstruction: 93.55399799346924  Train_KL: 3.4812962114810944  Validation Loss : 95.6223030090332 Val_Reconstruction : 92.17462921142578 Val_KL : 3.447675347328186\n","Epoch: 1815/8000  Traning Loss: 97.25886249542236  Train_Reconstruction: 93.7728967666626  Train_KL: 3.4859659671783447  Validation Loss : 95.9453125 Val_Reconstruction : 92.5042839050293 Val_KL : 3.441027283668518\n","Epoch: 1816/8000  Traning Loss: 97.2656192779541  Train_Reconstruction: 93.76800060272217  Train_KL: 3.4976174235343933  Validation Loss : 96.08794784545898 Val_Reconstruction : 92.63676071166992 Val_KL : 3.451185941696167\n","Epoch: 1817/8000  Traning Loss: 97.20635509490967  Train_Reconstruction: 93.72462558746338  Train_KL: 3.4817305207252502  Validation Loss : 96.24542617797852 Val_Reconstruction : 92.81304931640625 Val_KL : 3.4323770999908447\n","Epoch: 1818/8000  Traning Loss: 97.29697322845459  Train_Reconstruction: 93.81794929504395  Train_KL: 3.479024261236191  Validation Loss : 96.16802215576172 Val_Reconstruction : 92.72888565063477 Val_KL : 3.439134955406189\n","Epoch: 1819/8000  Traning Loss: 97.45788478851318  Train_Reconstruction: 93.96156787872314  Train_KL: 3.4963169395923615  Validation Loss : 96.04927062988281 Val_Reconstruction : 92.58290481567383 Val_KL : 3.46636426448822\n","Epoch: 1820/8000  Traning Loss: 97.6843900680542  Train_Reconstruction: 94.19123268127441  Train_KL: 3.493156135082245  Validation Loss : 96.61019515991211 Val_Reconstruction : 93.18018341064453 Val_KL : 3.4300113916397095\n","Epoch: 1821/8000  Traning Loss: 97.67332077026367  Train_Reconstruction: 94.20714950561523  Train_KL: 3.466170012950897  Validation Loss : 96.4306640625 Val_Reconstruction : 93.00714111328125 Val_KL : 3.423521876335144\n","Epoch: 1822/8000  Traning Loss: 97.65140724182129  Train_Reconstruction: 94.17102813720703  Train_KL: 3.4803798496723175  Validation Loss : 96.41923904418945 Val_Reconstruction : 92.97324752807617 Val_KL : 3.4459891319274902\n","Epoch: 1823/8000  Traning Loss: 97.58438205718994  Train_Reconstruction: 94.09751224517822  Train_KL: 3.4868682622909546  Validation Loss : 96.41645812988281 Val_Reconstruction : 92.97289276123047 Val_KL : 3.4435657262802124\n","Epoch: 1824/8000  Traning Loss: 97.42497539520264  Train_Reconstruction: 93.93838882446289  Train_KL: 3.4865872263908386  Validation Loss : 95.93035125732422 Val_Reconstruction : 92.48402786254883 Val_KL : 3.446325421333313\n","Epoch: 1825/8000  Traning Loss: 97.30897808074951  Train_Reconstruction: 93.84543895721436  Train_KL: 3.4635374546051025  Validation Loss : 95.85898208618164 Val_Reconstruction : 92.44454193115234 Val_KL : 3.414440393447876\n","Epoch: 1826/8000  Traning Loss: 97.05037689208984  Train_Reconstruction: 93.57933902740479  Train_KL: 3.4710383117198944  Validation Loss : 95.82708740234375 Val_Reconstruction : 92.39198684692383 Val_KL : 3.435103416442871\n","Epoch: 1827/8000  Traning Loss: 97.07335376739502  Train_Reconstruction: 93.57377433776855  Train_KL: 3.4995796382427216  Validation Loss : 95.90298843383789 Val_Reconstruction : 92.44168472290039 Val_KL : 3.4613029956817627\n","Epoch: 1828/8000  Traning Loss: 97.04406261444092  Train_Reconstruction: 93.55319690704346  Train_KL: 3.4908666014671326  Validation Loss : 96.2605209350586 Val_Reconstruction : 92.82059097290039 Val_KL : 3.439931035041809\n","Epoch: 1829/8000  Traning Loss: 96.96446800231934  Train_Reconstruction: 93.48676013946533  Train_KL: 3.477708399295807  Validation Loss : 95.7384147644043 Val_Reconstruction : 92.30441665649414 Val_KL : 3.433999180793762\n","Epoch: 1830/8000  Traning Loss: 96.98160076141357  Train_Reconstruction: 93.49574089050293  Train_KL: 3.485860198736191  Validation Loss : 95.77733612060547 Val_Reconstruction : 92.32999420166016 Val_KL : 3.447342872619629\n","Epoch: 1831/8000  Traning Loss: 96.9887809753418  Train_Reconstruction: 93.49764633178711  Train_KL: 3.49113392829895  Validation Loss : 95.80204010009766 Val_Reconstruction : 92.35334396362305 Val_KL : 3.4486966133117676\n","Epoch: 1832/8000  Traning Loss: 97.65048694610596  Train_Reconstruction: 94.15478420257568  Train_KL: 3.495703548192978  Validation Loss : 96.80355453491211 Val_Reconstruction : 93.34926223754883 Val_KL : 3.454290509223938\n","Epoch: 1833/8000  Traning Loss: 97.65080547332764  Train_Reconstruction: 94.165358543396  Train_KL: 3.4854467809200287  Validation Loss : 96.50529098510742 Val_Reconstruction : 93.0796890258789 Val_KL : 3.4256017208099365\n","Epoch: 1834/8000  Traning Loss: 97.47686386108398  Train_Reconstruction: 93.9890193939209  Train_KL: 3.487843245267868  Validation Loss : 96.4500961303711 Val_Reconstruction : 92.99727249145508 Val_KL : 3.452824115753174\n","Epoch: 1835/8000  Traning Loss: 97.34933376312256  Train_Reconstruction: 93.86842727661133  Train_KL: 3.480904996395111  Validation Loss : 96.47883605957031 Val_Reconstruction : 93.0412368774414 Val_KL : 3.4376001358032227\n","Epoch: 1836/8000  Traning Loss: 97.23138427734375  Train_Reconstruction: 93.74591732025146  Train_KL: 3.48546639084816  Validation Loss : 96.23127365112305 Val_Reconstruction : 92.77845001220703 Val_KL : 3.4528238773345947\n","Epoch: 1837/8000  Traning Loss: 97.07650947570801  Train_Reconstruction: 93.58889198303223  Train_KL: 3.4876187443733215  Validation Loss : 95.99589157104492 Val_Reconstruction : 92.55871963500977 Val_KL : 3.437172532081604\n","Epoch: 1838/8000  Traning Loss: 97.1586275100708  Train_Reconstruction: 93.67796516418457  Train_KL: 3.4806621372699738  Validation Loss : 95.83376693725586 Val_Reconstruction : 92.3961181640625 Val_KL : 3.437650203704834\n","Epoch: 1839/8000  Traning Loss: 96.90641403198242  Train_Reconstruction: 93.41674041748047  Train_KL: 3.4896737933158875  Validation Loss : 95.6070671081543 Val_Reconstruction : 92.16597366333008 Val_KL : 3.4410934448242188\n","Epoch: 1840/8000  Traning Loss: 96.90859603881836  Train_Reconstruction: 93.43773555755615  Train_KL: 3.4708606600761414  Validation Loss : 95.8050308227539 Val_Reconstruction : 92.38198852539062 Val_KL : 3.4230395555496216\n","Epoch: 1841/8000  Traning Loss: 97.11368560791016  Train_Reconstruction: 93.6238842010498  Train_KL: 3.4897996485233307  Validation Loss : 95.78759002685547 Val_Reconstruction : 92.31941604614258 Val_KL : 3.4681769609451294\n","Epoch: 1842/8000  Traning Loss: 97.0192461013794  Train_Reconstruction: 93.5183572769165  Train_KL: 3.500889539718628  Validation Loss : 95.67840957641602 Val_Reconstruction : 92.23319625854492 Val_KL : 3.4452147483825684\n","Epoch: 1843/8000  Traning Loss: 97.15564823150635  Train_Reconstruction: 93.67133903503418  Train_KL: 3.484310060739517  Validation Loss : 95.82514953613281 Val_Reconstruction : 92.39546966552734 Val_KL : 3.4296791553497314\n","Epoch: 1844/8000  Traning Loss: 96.99035263061523  Train_Reconstruction: 93.50337886810303  Train_KL: 3.4869733452796936  Validation Loss : 95.5352554321289 Val_Reconstruction : 92.08384704589844 Val_KL : 3.4514098167419434\n","Epoch: 1845/8000  Traning Loss: 96.96922588348389  Train_Reconstruction: 93.48219203948975  Train_KL: 3.4870343804359436  Validation Loss : 95.67771911621094 Val_Reconstruction : 92.2336311340332 Val_KL : 3.44408643245697\n","Epoch: 1846/8000  Traning Loss: 96.87581634521484  Train_Reconstruction: 93.38132667541504  Train_KL: 3.4944894313812256  Validation Loss : 95.37306594848633 Val_Reconstruction : 91.92464828491211 Val_KL : 3.448419213294983\n","Epoch: 1847/8000  Traning Loss: 96.89377021789551  Train_Reconstruction: 93.41371059417725  Train_KL: 3.480058968067169  Validation Loss : 95.8322525024414 Val_Reconstruction : 92.39864730834961 Val_KL : 3.433606266975403\n","Epoch: 1848/8000  Traning Loss: 97.01145553588867  Train_Reconstruction: 93.52604579925537  Train_KL: 3.485409200191498  Validation Loss : 95.59944152832031 Val_Reconstruction : 92.14469528198242 Val_KL : 3.45474910736084\n","Epoch: 1849/8000  Traning Loss: 96.9371919631958  Train_Reconstruction: 93.44028663635254  Train_KL: 3.4969045519828796  Validation Loss : 95.81790161132812 Val_Reconstruction : 92.36540985107422 Val_KL : 3.452494502067566\n","Epoch: 1850/8000  Traning Loss: 97.0015115737915  Train_Reconstruction: 93.51351261138916  Train_KL: 3.487999141216278  Validation Loss : 95.98682022094727 Val_Reconstruction : 92.5500259399414 Val_KL : 3.4367945194244385\n","Epoch: 1851/8000  Traning Loss: 97.34043598175049  Train_Reconstruction: 93.86077213287354  Train_KL: 3.4796649515628815  Validation Loss : 96.21077728271484 Val_Reconstruction : 92.77847671508789 Val_KL : 3.432301163673401\n","Epoch: 1852/8000  Traning Loss: 97.86887264251709  Train_Reconstruction: 94.38937282562256  Train_KL: 3.4795002937316895  Validation Loss : 96.48287582397461 Val_Reconstruction : 93.0348129272461 Val_KL : 3.448061943054199\n","Epoch: 1853/8000  Traning Loss: 97.56845283508301  Train_Reconstruction: 94.08283138275146  Train_KL: 3.4856218099594116  Validation Loss : 96.1876335144043 Val_Reconstruction : 92.74716186523438 Val_KL : 3.4404709339141846\n","Epoch: 1854/8000  Traning Loss: 97.08212184906006  Train_Reconstruction: 93.59282875061035  Train_KL: 3.4892928302288055  Validation Loss : 95.74345779418945 Val_Reconstruction : 92.29589462280273 Val_KL : 3.4475619792938232\n","Epoch: 1855/8000  Traning Loss: 97.18943691253662  Train_Reconstruction: 93.6973705291748  Train_KL: 3.4920667111873627  Validation Loss : 96.2548713684082 Val_Reconstruction : 92.81079864501953 Val_KL : 3.4440741539001465\n","Epoch: 1856/8000  Traning Loss: 97.25949096679688  Train_Reconstruction: 93.76598167419434  Train_KL: 3.4935103356838226  Validation Loss : 95.67654037475586 Val_Reconstruction : 92.21688461303711 Val_KL : 3.4596580266952515\n","Epoch: 1857/8000  Traning Loss: 96.91030883789062  Train_Reconstruction: 93.41289901733398  Train_KL: 3.497408628463745  Validation Loss : 95.6892204284668 Val_Reconstruction : 92.2444839477539 Val_KL : 3.444737195968628\n","Epoch: 1858/8000  Traning Loss: 96.70564937591553  Train_Reconstruction: 93.22703266143799  Train_KL: 3.478617399930954  Validation Loss : 95.47745513916016 Val_Reconstruction : 92.0462875366211 Val_KL : 3.4311680793762207\n","Epoch: 1859/8000  Traning Loss: 96.72488784790039  Train_Reconstruction: 93.2445125579834  Train_KL: 3.4803734719753265  Validation Loss : 95.81904983520508 Val_Reconstruction : 92.38121032714844 Val_KL : 3.437839984893799\n","Epoch: 1860/8000  Traning Loss: 97.26525020599365  Train_Reconstruction: 93.76461410522461  Train_KL: 3.500634491443634  Validation Loss : 95.77961349487305 Val_Reconstruction : 92.31279373168945 Val_KL : 3.466817021369934\n","Epoch: 1861/8000  Traning Loss: 96.6991319656372  Train_Reconstruction: 93.20694923400879  Train_KL: 3.492181897163391  Validation Loss : 95.50025177001953 Val_Reconstruction : 92.05710220336914 Val_KL : 3.443150520324707\n","Epoch: 1862/8000  Traning Loss: 96.8076114654541  Train_Reconstruction: 93.3156909942627  Train_KL: 3.4919202625751495  Validation Loss : 95.69229125976562 Val_Reconstruction : 92.25032043457031 Val_KL : 3.44197154045105\n","Epoch: 1863/8000  Traning Loss: 97.17632293701172  Train_Reconstruction: 93.68372344970703  Train_KL: 3.492600053548813  Validation Loss : 95.87710952758789 Val_Reconstruction : 92.41626358032227 Val_KL : 3.4608476161956787\n","Epoch: 1864/8000  Traning Loss: 97.26723384857178  Train_Reconstruction: 93.78157711029053  Train_KL: 3.485656261444092  Validation Loss : 96.1370964050293 Val_Reconstruction : 92.6957015991211 Val_KL : 3.4413979053497314\n","Epoch: 1865/8000  Traning Loss: 97.5603494644165  Train_Reconstruction: 94.08556079864502  Train_KL: 3.474788874387741  Validation Loss : 95.97915649414062 Val_Reconstruction : 92.53688049316406 Val_KL : 3.442275643348694\n","Epoch: 1866/8000  Traning Loss: 97.2223482131958  Train_Reconstruction: 93.73672389984131  Train_KL: 3.485624074935913  Validation Loss : 95.89013290405273 Val_Reconstruction : 92.44486236572266 Val_KL : 3.4452720880508423\n","Epoch: 1867/8000  Traning Loss: 97.14162826538086  Train_Reconstruction: 93.65303802490234  Train_KL: 3.4885899424552917  Validation Loss : 95.68812942504883 Val_Reconstruction : 92.24687194824219 Val_KL : 3.4412572383880615\n","Epoch: 1868/8000  Traning Loss: 96.97231960296631  Train_Reconstruction: 93.4879379272461  Train_KL: 3.4843824803829193  Validation Loss : 95.52494430541992 Val_Reconstruction : 92.08193969726562 Val_KL : 3.443005919456482\n","Epoch: 1869/8000  Traning Loss: 96.96636867523193  Train_Reconstruction: 93.47797966003418  Train_KL: 3.4883896708488464  Validation Loss : 95.92918395996094 Val_Reconstruction : 92.48003005981445 Val_KL : 3.4491528272628784\n","Epoch: 1870/8000  Traning Loss: 96.99257564544678  Train_Reconstruction: 93.50081443786621  Train_KL: 3.4917608201503754  Validation Loss : 95.91157150268555 Val_Reconstruction : 92.46479415893555 Val_KL : 3.4467793703079224\n","Epoch: 1871/8000  Traning Loss: 97.1340446472168  Train_Reconstruction: 93.65117645263672  Train_KL: 3.482868582010269  Validation Loss : 96.1932601928711 Val_Reconstruction : 92.76736831665039 Val_KL : 3.4258896112442017\n","Epoch: 1872/8000  Traning Loss: 97.03115558624268  Train_Reconstruction: 93.55242824554443  Train_KL: 3.4787287414073944  Validation Loss : 96.13313674926758 Val_Reconstruction : 92.69507217407227 Val_KL : 3.438065767288208\n","Epoch: 1873/8000  Traning Loss: 96.96827507019043  Train_Reconstruction: 93.48393440246582  Train_KL: 3.484340876340866  Validation Loss : 95.64017486572266 Val_Reconstruction : 92.19448852539062 Val_KL : 3.4456852674484253\n","Epoch: 1874/8000  Traning Loss: 97.30766582489014  Train_Reconstruction: 93.82833003997803  Train_KL: 3.4793353378772736  Validation Loss : 96.18177795410156 Val_Reconstruction : 92.74609756469727 Val_KL : 3.435680627822876\n","Epoch: 1875/8000  Traning Loss: 97.40119075775146  Train_Reconstruction: 93.91872310638428  Train_KL: 3.482467442750931  Validation Loss : 95.76997375488281 Val_Reconstruction : 92.32397079467773 Val_KL : 3.446001410484314\n","Epoch: 1876/8000  Traning Loss: 97.33781242370605  Train_Reconstruction: 93.85117149353027  Train_KL: 3.486642152070999  Validation Loss : 95.9758415222168 Val_Reconstruction : 92.52685928344727 Val_KL : 3.448982834815979\n","Epoch: 1877/8000  Traning Loss: 97.09070491790771  Train_Reconstruction: 93.6032600402832  Train_KL: 3.487442374229431  Validation Loss : 95.75257873535156 Val_Reconstruction : 92.3067741394043 Val_KL : 3.4458045959472656\n","Epoch: 1878/8000  Traning Loss: 96.92986106872559  Train_Reconstruction: 93.44044399261475  Train_KL: 3.4894171357154846  Validation Loss : 95.83402252197266 Val_Reconstruction : 92.37799072265625 Val_KL : 3.456033945083618\n","Epoch: 1879/8000  Traning Loss: 96.91861629486084  Train_Reconstruction: 93.42664909362793  Train_KL: 3.491968482732773  Validation Loss : 95.79429626464844 Val_Reconstruction : 92.34418106079102 Val_KL : 3.4501134157180786\n","Epoch: 1880/8000  Traning Loss: 96.86662673950195  Train_Reconstruction: 93.37991905212402  Train_KL: 3.486708253622055  Validation Loss : 95.52876663208008 Val_Reconstruction : 92.09145736694336 Val_KL : 3.4373087882995605\n","Epoch: 1881/8000  Traning Loss: 96.74792289733887  Train_Reconstruction: 93.26959896087646  Train_KL: 3.478324681520462  Validation Loss : 95.71454620361328 Val_Reconstruction : 92.28140258789062 Val_KL : 3.4331419467926025\n","Epoch: 1882/8000  Traning Loss: 96.89033031463623  Train_Reconstruction: 93.3953628540039  Train_KL: 3.494968205690384  Validation Loss : 95.72710418701172 Val_Reconstruction : 92.26677322387695 Val_KL : 3.4603337049484253\n","Epoch: 1883/8000  Traning Loss: 96.88647079467773  Train_Reconstruction: 93.39148139953613  Train_KL: 3.4949911236763  Validation Loss : 96.18096542358398 Val_Reconstruction : 92.72660827636719 Val_KL : 3.4543567895889282\n","Epoch: 1884/8000  Traning Loss: 96.97899532318115  Train_Reconstruction: 93.5009355545044  Train_KL: 3.4780599176883698  Validation Loss : 95.58197021484375 Val_Reconstruction : 92.14917373657227 Val_KL : 3.4327971935272217\n","Epoch: 1885/8000  Traning Loss: 96.74330234527588  Train_Reconstruction: 93.26263618469238  Train_KL: 3.4806668162345886  Validation Loss : 95.65703964233398 Val_Reconstruction : 92.21905517578125 Val_KL : 3.4379847049713135\n","Epoch: 1886/8000  Traning Loss: 96.58183860778809  Train_Reconstruction: 93.09934043884277  Train_KL: 3.482497960329056  Validation Loss : 95.65851211547852 Val_Reconstruction : 92.22345733642578 Val_KL : 3.4350554943084717\n","Epoch: 1887/8000  Traning Loss: 96.63264560699463  Train_Reconstruction: 93.15303421020508  Train_KL: 3.479610413312912  Validation Loss : 95.5256462097168 Val_Reconstruction : 92.0852165222168 Val_KL : 3.4404311180114746\n","Epoch: 1888/8000  Traning Loss: 96.9799633026123  Train_Reconstruction: 93.49399757385254  Train_KL: 3.4859652519226074  Validation Loss : 95.6671142578125 Val_Reconstruction : 92.22360610961914 Val_KL : 3.4435064792633057\n","Epoch: 1889/8000  Traning Loss: 97.04654312133789  Train_Reconstruction: 93.56668949127197  Train_KL: 3.4798538088798523  Validation Loss : 95.96257781982422 Val_Reconstruction : 92.52897262573242 Val_KL : 3.4336042404174805\n","Epoch: 1890/8000  Traning Loss: 97.11570644378662  Train_Reconstruction: 93.62539672851562  Train_KL: 3.490309774875641  Validation Loss : 96.05149841308594 Val_Reconstruction : 92.59955978393555 Val_KL : 3.4519412517547607\n","Epoch: 1891/8000  Traning Loss: 97.50796031951904  Train_Reconstruction: 94.02005672454834  Train_KL: 3.487904280424118  Validation Loss : 96.0338363647461 Val_Reconstruction : 92.58674621582031 Val_KL : 3.44708788394928\n","Epoch: 1892/8000  Traning Loss: 97.44292068481445  Train_Reconstruction: 93.95435047149658  Train_KL: 3.4885700345039368  Validation Loss : 95.90700912475586 Val_Reconstruction : 92.46051788330078 Val_KL : 3.446492075920105\n","Epoch: 1893/8000  Traning Loss: 96.96193027496338  Train_Reconstruction: 93.48004531860352  Train_KL: 3.4818845987319946  Validation Loss : 95.70622253417969 Val_Reconstruction : 92.28108978271484 Val_KL : 3.425131320953369\n","Epoch: 1894/8000  Traning Loss: 97.02475547790527  Train_Reconstruction: 93.5435848236084  Train_KL: 3.4811709225177765  Validation Loss : 95.9722671508789 Val_Reconstruction : 92.53559875488281 Val_KL : 3.436671733856201\n","Epoch: 1895/8000  Traning Loss: 97.02711009979248  Train_Reconstruction: 93.5453577041626  Train_KL: 3.4817533791065216  Validation Loss : 95.96818161010742 Val_Reconstruction : 92.5312385559082 Val_KL : 3.436944007873535\n","Epoch: 1896/8000  Traning Loss: 96.98339653015137  Train_Reconstruction: 93.50457763671875  Train_KL: 3.4788187444210052  Validation Loss : 95.88724517822266 Val_Reconstruction : 92.4587173461914 Val_KL : 3.4285271167755127\n","Epoch: 1897/8000  Traning Loss: 96.92353630065918  Train_Reconstruction: 93.44187259674072  Train_KL: 3.4816633760929108  Validation Loss : 95.63355255126953 Val_Reconstruction : 92.18665313720703 Val_KL : 3.4469014406204224\n","Epoch: 1898/8000  Traning Loss: 96.6868839263916  Train_Reconstruction: 93.19622039794922  Train_KL: 3.490663170814514  Validation Loss : 95.3100357055664 Val_Reconstruction : 91.86584091186523 Val_KL : 3.4441956281661987\n","Epoch: 1899/8000  Traning Loss: 96.83023357391357  Train_Reconstruction: 93.34267997741699  Train_KL: 3.4875538647174835  Validation Loss : 95.5979232788086 Val_Reconstruction : 92.16030502319336 Val_KL : 3.4376217126846313\n","Epoch: 1900/8000  Traning Loss: 96.86759090423584  Train_Reconstruction: 93.3865156173706  Train_KL: 3.481074273586273  Validation Loss : 95.55476760864258 Val_Reconstruction : 92.1192398071289 Val_KL : 3.435529351234436\n","Epoch: 1901/8000  Traning Loss: 96.75872707366943  Train_Reconstruction: 93.27985763549805  Train_KL: 3.478869318962097  Validation Loss : 95.27688598632812 Val_Reconstruction : 91.83661651611328 Val_KL : 3.4402679204940796\n","Epoch: 1902/8000  Traning Loss: 96.83110523223877  Train_Reconstruction: 93.34403038024902  Train_KL: 3.487074702978134  Validation Loss : 95.76206588745117 Val_Reconstruction : 92.30807495117188 Val_KL : 3.4539910554885864\n","Epoch: 1903/8000  Traning Loss: 97.10378837585449  Train_Reconstruction: 93.62388134002686  Train_KL: 3.4799071550369263  Validation Loss : 96.28809356689453 Val_Reconstruction : 92.86127853393555 Val_KL : 3.426815152168274\n","Epoch: 1904/8000  Traning Loss: 97.54568290710449  Train_Reconstruction: 94.06942939758301  Train_KL: 3.4762541949748993  Validation Loss : 96.53662109375 Val_Reconstruction : 93.08890151977539 Val_KL : 3.4477200508117676\n","Epoch: 1905/8000  Traning Loss: 98.11496353149414  Train_Reconstruction: 94.62594890594482  Train_KL: 3.4890147745609283  Validation Loss : 97.2502212524414 Val_Reconstruction : 93.80567169189453 Val_KL : 3.4445499181747437\n","Epoch: 1906/8000  Traning Loss: 97.60908317565918  Train_Reconstruction: 94.12700176239014  Train_KL: 3.482081323862076  Validation Loss : 96.29102325439453 Val_Reconstruction : 92.86205673217773 Val_KL : 3.4289658069610596\n","Epoch: 1907/8000  Traning Loss: 97.24616527557373  Train_Reconstruction: 93.7674789428711  Train_KL: 3.4786863327026367  Validation Loss : 96.24354934692383 Val_Reconstruction : 92.8069953918457 Val_KL : 3.43655264377594\n","Epoch: 1908/8000  Traning Loss: 97.1014051437378  Train_Reconstruction: 93.63577270507812  Train_KL: 3.4656327068805695  Validation Loss : 95.57845687866211 Val_Reconstruction : 92.1506118774414 Val_KL : 3.4278430938720703\n","Epoch: 1909/8000  Traning Loss: 96.84875106811523  Train_Reconstruction: 93.36999893188477  Train_KL: 3.4787509739398956  Validation Loss : 95.878662109375 Val_Reconstruction : 92.42204666137695 Val_KL : 3.456615924835205\n","Epoch: 1910/8000  Traning Loss: 96.79344272613525  Train_Reconstruction: 93.29029941558838  Train_KL: 3.5031424164772034  Validation Loss : 95.77192687988281 Val_Reconstruction : 92.32470321655273 Val_KL : 3.4472204446792603\n","Epoch: 1911/8000  Traning Loss: 97.22488021850586  Train_Reconstruction: 93.7462100982666  Train_KL: 3.4786693453788757  Validation Loss : 96.2078857421875 Val_Reconstruction : 92.7684097290039 Val_KL : 3.4394742250442505\n","Epoch: 1912/8000  Traning Loss: 97.6472110748291  Train_Reconstruction: 94.16598129272461  Train_KL: 3.481229931116104  Validation Loss : 96.35162353515625 Val_Reconstruction : 92.92022705078125 Val_KL : 3.4313942193984985\n","Epoch: 1913/8000  Traning Loss: 96.99145126342773  Train_Reconstruction: 93.51694679260254  Train_KL: 3.474505692720413  Validation Loss : 95.51052474975586 Val_Reconstruction : 92.0930404663086 Val_KL : 3.4174835681915283\n","Epoch: 1914/8000  Traning Loss: 96.499098777771  Train_Reconstruction: 93.03488349914551  Train_KL: 3.4642155170440674  Validation Loss : 95.2933578491211 Val_Reconstruction : 91.86237716674805 Val_KL : 3.430981755256653\n","Epoch: 1915/8000  Traning Loss: 96.53305339813232  Train_Reconstruction: 93.03903198242188  Train_KL: 3.4940218925476074  Validation Loss : 95.56870651245117 Val_Reconstruction : 92.10972213745117 Val_KL : 3.4589825868606567\n","Epoch: 1916/8000  Traning Loss: 96.68972587585449  Train_Reconstruction: 93.19380474090576  Train_KL: 3.4959217309951782  Validation Loss : 95.79899978637695 Val_Reconstruction : 92.36294555664062 Val_KL : 3.436054825782776\n","Epoch: 1917/8000  Traning Loss: 97.11247825622559  Train_Reconstruction: 93.63461303710938  Train_KL: 3.4778659641742706  Validation Loss : 96.03438186645508 Val_Reconstruction : 92.60416793823242 Val_KL : 3.4302148818969727\n","Epoch: 1918/8000  Traning Loss: 97.4104585647583  Train_Reconstruction: 93.93362045288086  Train_KL: 3.4768393337726593  Validation Loss : 96.37741088867188 Val_Reconstruction : 92.93593978881836 Val_KL : 3.4414684772491455\n","Epoch: 1919/8000  Traning Loss: 97.02939987182617  Train_Reconstruction: 93.54847621917725  Train_KL: 3.4809228479862213  Validation Loss : 95.58557891845703 Val_Reconstruction : 92.1272201538086 Val_KL : 3.458358407020569\n","Epoch: 1920/8000  Traning Loss: 96.6745023727417  Train_Reconstruction: 93.1612958908081  Train_KL: 3.5132062435150146  Validation Loss : 95.42564010620117 Val_Reconstruction : 91.96253204345703 Val_KL : 3.4631088972091675\n","Epoch: 1921/8000  Traning Loss: 96.55168056488037  Train_Reconstruction: 93.0568904876709  Train_KL: 3.4947885274887085  Validation Loss : 95.30958938598633 Val_Reconstruction : 91.86466598510742 Val_KL : 3.4449243545532227\n","Epoch: 1922/8000  Traning Loss: 96.41708850860596  Train_Reconstruction: 92.92516422271729  Train_KL: 3.4919257164001465  Validation Loss : 95.37400436401367 Val_Reconstruction : 91.92849349975586 Val_KL : 3.445512056350708\n","Epoch: 1923/8000  Traning Loss: 96.5803918838501  Train_Reconstruction: 93.08739376068115  Train_KL: 3.4929997622966766  Validation Loss : 95.58975982666016 Val_Reconstruction : 92.13675308227539 Val_KL : 3.453005075454712\n","Epoch: 1924/8000  Traning Loss: 96.86577224731445  Train_Reconstruction: 93.38064384460449  Train_KL: 3.485128402709961  Validation Loss : 95.51251983642578 Val_Reconstruction : 92.07231903076172 Val_KL : 3.440200090408325\n","Epoch: 1925/8000  Traning Loss: 96.72863578796387  Train_Reconstruction: 93.2393159866333  Train_KL: 3.489319860935211  Validation Loss : 95.63156509399414 Val_Reconstruction : 92.18959045410156 Val_KL : 3.4419766664505005\n","Epoch: 1926/8000  Traning Loss: 96.63942337036133  Train_Reconstruction: 93.15039539337158  Train_KL: 3.4890289306640625  Validation Loss : 95.38943481445312 Val_Reconstruction : 91.94829177856445 Val_KL : 3.4411427974700928\n","Epoch: 1927/8000  Traning Loss: 96.82367134094238  Train_Reconstruction: 93.34126281738281  Train_KL: 3.482408970594406  Validation Loss : 95.64695358276367 Val_Reconstruction : 92.21925354003906 Val_KL : 3.4276983737945557\n","Epoch: 1928/8000  Traning Loss: 96.98415184020996  Train_Reconstruction: 93.50990867614746  Train_KL: 3.4742428958415985  Validation Loss : 96.14288330078125 Val_Reconstruction : 92.72416305541992 Val_KL : 3.4187203645706177\n","Epoch: 1929/8000  Traning Loss: 97.0779800415039  Train_Reconstruction: 93.61015701293945  Train_KL: 3.4678238928318024  Validation Loss : 96.01493453979492 Val_Reconstruction : 92.58815383911133 Val_KL : 3.4267797470092773\n","Epoch: 1930/8000  Traning Loss: 97.12991714477539  Train_Reconstruction: 93.64592838287354  Train_KL: 3.483987808227539  Validation Loss : 95.95152282714844 Val_Reconstruction : 92.5053825378418 Val_KL : 3.446141004562378\n","Epoch: 1931/8000  Traning Loss: 97.16797161102295  Train_Reconstruction: 93.67248439788818  Train_KL: 3.4954878389835358  Validation Loss : 96.11186981201172 Val_Reconstruction : 92.67335510253906 Val_KL : 3.4385178089141846\n","Epoch: 1932/8000  Traning Loss: 97.0639295578003  Train_Reconstruction: 93.58702564239502  Train_KL: 3.4769031703472137  Validation Loss : 95.81368637084961 Val_Reconstruction : 92.3780288696289 Val_KL : 3.435654401779175\n","Epoch: 1933/8000  Traning Loss: 96.81490230560303  Train_Reconstruction: 93.32734107971191  Train_KL: 3.4875607192516327  Validation Loss : 95.5226058959961 Val_Reconstruction : 92.08441925048828 Val_KL : 3.438188076019287\n","Epoch: 1934/8000  Traning Loss: 96.70827770233154  Train_Reconstruction: 93.22985935211182  Train_KL: 3.47841814160347  Validation Loss : 95.82074737548828 Val_Reconstruction : 92.38747787475586 Val_KL : 3.4332724809646606\n","Epoch: 1935/8000  Traning Loss: 96.69207096099854  Train_Reconstruction: 93.21477508544922  Train_KL: 3.477295935153961  Validation Loss : 95.50492477416992 Val_Reconstruction : 92.05915451049805 Val_KL : 3.4457677602767944\n","Epoch: 1936/8000  Traning Loss: 96.46332740783691  Train_Reconstruction: 92.96611499786377  Train_KL: 3.4972116351127625  Validation Loss : 95.30271530151367 Val_Reconstruction : 91.85655975341797 Val_KL : 3.4461559057235718\n","Epoch: 1937/8000  Traning Loss: 96.59483051300049  Train_Reconstruction: 93.10469722747803  Train_KL: 3.4901325702667236  Validation Loss : 95.64453125 Val_Reconstruction : 92.20291137695312 Val_KL : 3.4416221380233765\n","Epoch: 1938/8000  Traning Loss: 96.77060222625732  Train_Reconstruction: 93.27730655670166  Train_KL: 3.493295341730118  Validation Loss : 95.75755310058594 Val_Reconstruction : 92.29620742797852 Val_KL : 3.4613441228866577\n","Epoch: 1939/8000  Traning Loss: 96.90220069885254  Train_Reconstruction: 93.39623260498047  Train_KL: 3.505967080593109  Validation Loss : 95.66687774658203 Val_Reconstruction : 92.2245101928711 Val_KL : 3.4423693418502808\n","Epoch: 1940/8000  Traning Loss: 96.60442447662354  Train_Reconstruction: 93.12434768676758  Train_KL: 3.480078011751175  Validation Loss : 95.32053756713867 Val_Reconstruction : 91.89087677001953 Val_KL : 3.4296616315841675\n","Epoch: 1941/8000  Traning Loss: 96.48614692687988  Train_Reconstruction: 93.00986766815186  Train_KL: 3.4762785136699677  Validation Loss : 95.40208435058594 Val_Reconstruction : 91.9617691040039 Val_KL : 3.4403172731399536\n","Epoch: 1942/8000  Traning Loss: 96.58333492279053  Train_Reconstruction: 93.08706855773926  Train_KL: 3.4962666630744934  Validation Loss : 95.42057800292969 Val_Reconstruction : 91.9701919555664 Val_KL : 3.450387120246887\n","Epoch: 1943/8000  Traning Loss: 96.76515674591064  Train_Reconstruction: 93.27061939239502  Train_KL: 3.4945370852947235  Validation Loss : 95.40862655639648 Val_Reconstruction : 91.96535873413086 Val_KL : 3.443269729614258\n","Epoch: 1944/8000  Traning Loss: 96.58214378356934  Train_Reconstruction: 93.09377956390381  Train_KL: 3.488364189863205  Validation Loss : 95.38683319091797 Val_Reconstruction : 91.94945526123047 Val_KL : 3.437379479408264\n","Epoch: 1945/8000  Traning Loss: 96.88466453552246  Train_Reconstruction: 93.39753532409668  Train_KL: 3.4871274530887604  Validation Loss : 96.07029342651367 Val_Reconstruction : 92.61797332763672 Val_KL : 3.4523191452026367\n","Epoch: 1946/8000  Traning Loss: 96.8655309677124  Train_Reconstruction: 93.37282943725586  Train_KL: 3.4927017092704773  Validation Loss : 95.72665023803711 Val_Reconstruction : 92.27816772460938 Val_KL : 3.448482036590576\n","Epoch: 1947/8000  Traning Loss: 96.78196048736572  Train_Reconstruction: 93.29545402526855  Train_KL: 3.4865067303180695  Validation Loss : 95.62353897094727 Val_Reconstruction : 92.17863464355469 Val_KL : 3.444904088973999\n","Epoch: 1948/8000  Traning Loss: 96.66014957427979  Train_Reconstruction: 93.17349338531494  Train_KL: 3.486656576395035  Validation Loss : 95.65775680541992 Val_Reconstruction : 92.2156867980957 Val_KL : 3.4420711994171143\n","Epoch: 1949/8000  Traning Loss: 96.51110363006592  Train_Reconstruction: 93.03573799133301  Train_KL: 3.4753660559654236  Validation Loss : 95.43217086791992 Val_Reconstruction : 91.99114227294922 Val_KL : 3.4410266876220703\n","Epoch: 1950/8000  Traning Loss: 96.84085655212402  Train_Reconstruction: 93.3391227722168  Train_KL: 3.5017341673374176  Validation Loss : 96.15552139282227 Val_Reconstruction : 92.70531845092773 Val_KL : 3.4502031803131104\n","Epoch: 1951/8000  Traning Loss: 96.8543004989624  Train_Reconstruction: 93.37026500701904  Train_KL: 3.4840355217456818  Validation Loss : 95.68426895141602 Val_Reconstruction : 92.25008010864258 Val_KL : 3.434189200401306\n","Epoch: 1952/8000  Traning Loss: 96.82771492004395  Train_Reconstruction: 93.34133625030518  Train_KL: 3.4863787293434143  Validation Loss : 96.00661087036133 Val_Reconstruction : 92.56845092773438 Val_KL : 3.4381625652313232\n","Epoch: 1953/8000  Traning Loss: 96.92615222930908  Train_Reconstruction: 93.44128227233887  Train_KL: 3.4848697185516357  Validation Loss : 96.12532806396484 Val_Reconstruction : 92.67312622070312 Val_KL : 3.4522019624710083\n","Epoch: 1954/8000  Traning Loss: 96.80562019348145  Train_Reconstruction: 93.31033611297607  Train_KL: 3.495283395051956  Validation Loss : 95.5543212890625 Val_Reconstruction : 92.09497833251953 Val_KL : 3.459343910217285\n","Epoch: 1955/8000  Traning Loss: 96.94693088531494  Train_Reconstruction: 93.45495128631592  Train_KL: 3.4919801354408264  Validation Loss : 95.71263122558594 Val_Reconstruction : 92.27132797241211 Val_KL : 3.4413026571273804\n","Epoch: 1956/8000  Traning Loss: 96.72543907165527  Train_Reconstruction: 93.25230884552002  Train_KL: 3.473129838705063  Validation Loss : 95.22360610961914 Val_Reconstruction : 91.78661346435547 Val_KL : 3.4369903802871704\n","Epoch: 1957/8000  Traning Loss: 96.26419258117676  Train_Reconstruction: 92.77297306060791  Train_KL: 3.4912208020687103  Validation Loss : 95.08203506469727 Val_Reconstruction : 91.63146591186523 Val_KL : 3.450566291809082\n","Epoch: 1958/8000  Traning Loss: 96.28273677825928  Train_Reconstruction: 92.79030704498291  Train_KL: 3.492429405450821  Validation Loss : 95.04069519042969 Val_Reconstruction : 91.60103225708008 Val_KL : 3.439662456512451\n","Epoch: 1959/8000  Traning Loss: 96.34460544586182  Train_Reconstruction: 92.8551378250122  Train_KL: 3.489468365907669  Validation Loss : 95.2496223449707 Val_Reconstruction : 91.80517959594727 Val_KL : 3.4444425106048584\n","Epoch: 1960/8000  Traning Loss: 96.25849437713623  Train_Reconstruction: 92.76768112182617  Train_KL: 3.490811914205551  Validation Loss : 95.26212692260742 Val_Reconstruction : 91.81389999389648 Val_KL : 3.4482295513153076\n","Epoch: 1961/8000  Traning Loss: 96.35003757476807  Train_Reconstruction: 92.84959983825684  Train_KL: 3.5004371404647827  Validation Loss : 95.23674011230469 Val_Reconstruction : 91.79008102416992 Val_KL : 3.446655750274658\n","Epoch: 1962/8000  Traning Loss: 96.6917667388916  Train_Reconstruction: 93.1997127532959  Train_KL: 3.492054283618927  Validation Loss : 95.64490509033203 Val_Reconstruction : 92.19394302368164 Val_KL : 3.45096492767334\n","Epoch: 1963/8000  Traning Loss: 96.69806098937988  Train_Reconstruction: 93.2131404876709  Train_KL: 3.484919995069504  Validation Loss : 95.41685485839844 Val_Reconstruction : 91.97733306884766 Val_KL : 3.4395231008529663\n","Epoch: 1964/8000  Traning Loss: 96.37258434295654  Train_Reconstruction: 92.89144802093506  Train_KL: 3.4811361134052277  Validation Loss : 95.12103271484375 Val_Reconstruction : 91.67965316772461 Val_KL : 3.441379427909851\n","Epoch: 1965/8000  Traning Loss: 96.28890228271484  Train_Reconstruction: 92.79343605041504  Train_KL: 3.495464026927948  Validation Loss : 95.08232116699219 Val_Reconstruction : 91.63588333129883 Val_KL : 3.44643771648407\n","Epoch: 1966/8000  Traning Loss: 96.29785346984863  Train_Reconstruction: 92.8076753616333  Train_KL: 3.490178257226944  Validation Loss : 95.17501449584961 Val_Reconstruction : 91.73055648803711 Val_KL : 3.4444580078125\n","Epoch: 1967/8000  Traning Loss: 96.57954216003418  Train_Reconstruction: 93.09041690826416  Train_KL: 3.4891243875026703  Validation Loss : 95.38787841796875 Val_Reconstruction : 91.95098876953125 Val_KL : 3.4368929862976074\n","Epoch: 1968/8000  Traning Loss: 96.52329444885254  Train_Reconstruction: 93.03061294555664  Train_KL: 3.4926813542842865  Validation Loss : 95.42066192626953 Val_Reconstruction : 91.96236419677734 Val_KL : 3.458296060562134\n","Epoch: 1969/8000  Traning Loss: 96.270339012146  Train_Reconstruction: 92.7766342163086  Train_KL: 3.493704617023468  Validation Loss : 95.24663162231445 Val_Reconstruction : 91.80624389648438 Val_KL : 3.4403903484344482\n","Epoch: 1970/8000  Traning Loss: 96.8634386062622  Train_Reconstruction: 93.37577724456787  Train_KL: 3.4876616299152374  Validation Loss : 96.03936767578125 Val_Reconstruction : 92.6007308959961 Val_KL : 3.438636302947998\n","Epoch: 1971/8000  Traning Loss: 97.46344184875488  Train_Reconstruction: 93.97542095184326  Train_KL: 3.4880207777023315  Validation Loss : 96.30546951293945 Val_Reconstruction : 92.85943222045898 Val_KL : 3.4460402727127075\n","Epoch: 1972/8000  Traning Loss: 97.30448341369629  Train_Reconstruction: 93.81172847747803  Train_KL: 3.4927558600902557  Validation Loss : 96.13331604003906 Val_Reconstruction : 92.6931037902832 Val_KL : 3.4402111768722534\n","Epoch: 1973/8000  Traning Loss: 97.13530158996582  Train_Reconstruction: 93.65180587768555  Train_KL: 3.4834947288036346  Validation Loss : 96.01356506347656 Val_Reconstruction : 92.58599090576172 Val_KL : 3.42757248878479\n","Epoch: 1974/8000  Traning Loss: 96.89409160614014  Train_Reconstruction: 93.41073322296143  Train_KL: 3.4833585619926453  Validation Loss : 95.61792373657227 Val_Reconstruction : 92.1722183227539 Val_KL : 3.445706605911255\n","Epoch: 1975/8000  Traning Loss: 96.5718240737915  Train_Reconstruction: 93.08154392242432  Train_KL: 3.490281105041504  Validation Loss : 95.4093246459961 Val_Reconstruction : 91.96540069580078 Val_KL : 3.443920612335205\n","Epoch: 1976/8000  Traning Loss: 96.47601985931396  Train_Reconstruction: 92.99731254577637  Train_KL: 3.4787077009677887  Validation Loss : 95.25065231323242 Val_Reconstruction : 91.81586456298828 Val_KL : 3.434786081314087\n","Epoch: 1977/8000  Traning Loss: 96.6546220779419  Train_Reconstruction: 93.17193984985352  Train_KL: 3.48268124461174  Validation Loss : 95.87612915039062 Val_Reconstruction : 92.44225692749023 Val_KL : 3.433874249458313\n","Epoch: 1978/8000  Traning Loss: 96.47769927978516  Train_Reconstruction: 92.99663543701172  Train_KL: 3.4810635149478912  Validation Loss : 95.16793823242188 Val_Reconstruction : 91.7393913269043 Val_KL : 3.4285463094711304\n","Epoch: 1979/8000  Traning Loss: 96.54807186126709  Train_Reconstruction: 93.0684642791748  Train_KL: 3.479608118534088  Validation Loss : 95.52218627929688 Val_Reconstruction : 92.09162902832031 Val_KL : 3.4305588006973267\n","Epoch: 1980/8000  Traning Loss: 96.63590908050537  Train_Reconstruction: 93.16607570648193  Train_KL: 3.469833880662918  Validation Loss : 95.34028244018555 Val_Reconstruction : 91.90802383422852 Val_KL : 3.4322589635849\n","Epoch: 1981/8000  Traning Loss: 96.3029727935791  Train_Reconstruction: 92.79417610168457  Train_KL: 3.508797824382782  Validation Loss : 95.14450454711914 Val_Reconstruction : 91.67056274414062 Val_KL : 3.473942518234253\n","Epoch: 1982/8000  Traning Loss: 96.50563144683838  Train_Reconstruction: 93.01377391815186  Train_KL: 3.4918575286865234  Validation Loss : 95.7416763305664 Val_Reconstruction : 92.30674362182617 Val_KL : 3.434936046600342\n","Epoch: 1983/8000  Traning Loss: 97.09721088409424  Train_Reconstruction: 93.60968208312988  Train_KL: 3.4875291287899017  Validation Loss : 96.20569610595703 Val_Reconstruction : 92.75452041625977 Val_KL : 3.4511735439300537\n","Epoch: 1984/8000  Traning Loss: 97.35613346099854  Train_Reconstruction: 93.86355113983154  Train_KL: 3.492581158876419  Validation Loss : 96.25175857543945 Val_Reconstruction : 92.80927276611328 Val_KL : 3.4424837827682495\n","Epoch: 1985/8000  Traning Loss: 96.96778392791748  Train_Reconstruction: 93.46441268920898  Train_KL: 3.503372222185135  Validation Loss : 95.81227493286133 Val_Reconstruction : 92.35013198852539 Val_KL : 3.462142825126648\n","Epoch: 1986/8000  Traning Loss: 96.96955108642578  Train_Reconstruction: 93.47341632843018  Train_KL: 3.4961344599723816  Validation Loss : 95.72224807739258 Val_Reconstruction : 92.27090454101562 Val_KL : 3.4513421058654785\n","Epoch: 1987/8000  Traning Loss: 96.5176191329956  Train_Reconstruction: 93.02311134338379  Train_KL: 3.494507282972336  Validation Loss : 95.17213439941406 Val_Reconstruction : 91.72119140625 Val_KL : 3.4509427547454834\n","Epoch: 1988/8000  Traning Loss: 96.41616630554199  Train_Reconstruction: 92.91562175750732  Train_KL: 3.500543922185898  Validation Loss : 95.57838439941406 Val_Reconstruction : 92.11689376831055 Val_KL : 3.4614888429641724\n","Epoch: 1989/8000  Traning Loss: 96.64272403717041  Train_Reconstruction: 93.15709209442139  Train_KL: 3.485632359981537  Validation Loss : 95.64041519165039 Val_Reconstruction : 92.21023178100586 Val_KL : 3.430184006690979\n","Epoch: 1990/8000  Traning Loss: 96.93681049346924  Train_Reconstruction: 93.46021366119385  Train_KL: 3.476595848798752  Validation Loss : 95.78431701660156 Val_Reconstruction : 92.34326553344727 Val_KL : 3.4410516023635864\n","Epoch: 1991/8000  Traning Loss: 97.21397018432617  Train_Reconstruction: 93.70934581756592  Train_KL: 3.5046250224113464  Validation Loss : 96.06731414794922 Val_Reconstruction : 92.59952926635742 Val_KL : 3.4677867889404297\n","Epoch: 1992/8000  Traning Loss: 96.90991306304932  Train_Reconstruction: 93.41398048400879  Train_KL: 3.495930790901184  Validation Loss : 95.69705581665039 Val_Reconstruction : 92.25789260864258 Val_KL : 3.439161777496338\n","Epoch: 1993/8000  Traning Loss: 96.9078950881958  Train_Reconstruction: 93.42809200286865  Train_KL: 3.4798043966293335  Validation Loss : 95.79590606689453 Val_Reconstruction : 92.3567008972168 Val_KL : 3.43920636177063\n","Epoch: 1994/8000  Traning Loss: 97.11174488067627  Train_Reconstruction: 93.620285987854  Train_KL: 3.4914604127407074  Validation Loss : 95.98200225830078 Val_Reconstruction : 92.53652954101562 Val_KL : 3.4454729557037354\n","Epoch: 1995/8000  Traning Loss: 96.58775234222412  Train_Reconstruction: 93.10709953308105  Train_KL: 3.4806532859802246  Validation Loss : 95.28142929077148 Val_Reconstruction : 91.84897994995117 Val_KL : 3.432450532913208\n","Epoch: 1996/8000  Traning Loss: 96.29820823669434  Train_Reconstruction: 92.81068992614746  Train_KL: 3.4875182509422302  Validation Loss : 95.15948104858398 Val_Reconstruction : 91.7132453918457 Val_KL : 3.4462372064590454\n","Epoch: 1997/8000  Traning Loss: 96.25882816314697  Train_Reconstruction: 92.76627540588379  Train_KL: 3.492552548646927  Validation Loss : 95.24328231811523 Val_Reconstruction : 91.79253005981445 Val_KL : 3.4507519006729126\n","Epoch: 1998/8000  Traning Loss: 96.3799467086792  Train_Reconstruction: 92.89366722106934  Train_KL: 3.486278235912323  Validation Loss : 95.24240112304688 Val_Reconstruction : 91.79808044433594 Val_KL : 3.4443196058273315\n","Epoch: 1999/8000  Traning Loss: 96.40847682952881  Train_Reconstruction: 92.92849254608154  Train_KL: 3.4799841046333313  Validation Loss : 95.3595962524414 Val_Reconstruction : 91.9128303527832 Val_KL : 3.4467663764953613\n","Epoch: 2000/8000  Traning Loss: 96.64815711975098  Train_Reconstruction: 93.16444969177246  Train_KL: 3.48370760679245  Validation Loss : 95.50928497314453 Val_Reconstruction : 92.06357955932617 Val_KL : 3.445706605911255\n","Epoch: 2001/8000  Traning Loss: 96.71441650390625  Train_Reconstruction: 93.22912120819092  Train_KL: 3.4852955639362335  Validation Loss : 95.42225646972656 Val_Reconstruction : 91.96818161010742 Val_KL : 3.4540765285491943\n","Epoch: 2002/8000  Traning Loss: 96.24495601654053  Train_Reconstruction: 92.74207782745361  Train_KL: 3.502877652645111  Validation Loss : 95.07946395874023 Val_Reconstruction : 91.62020111083984 Val_KL : 3.459263324737549\n","Epoch: 2003/8000  Traning Loss: 96.47279453277588  Train_Reconstruction: 92.98456382751465  Train_KL: 3.4882307946681976  Validation Loss : 95.67511367797852 Val_Reconstruction : 92.23736953735352 Val_KL : 3.4377435445785522\n","Epoch: 2004/8000  Traning Loss: 96.57115745544434  Train_Reconstruction: 93.08233737945557  Train_KL: 3.4888206720352173  Validation Loss : 95.52882385253906 Val_Reconstruction : 92.08477020263672 Val_KL : 3.4440548419952393\n","Epoch: 2005/8000  Traning Loss: 96.26503658294678  Train_Reconstruction: 92.77475166320801  Train_KL: 3.490284711122513  Validation Loss : 95.12725067138672 Val_Reconstruction : 91.68133163452148 Val_KL : 3.445916175842285\n","Epoch: 2006/8000  Traning Loss: 96.64676570892334  Train_Reconstruction: 93.15503215789795  Train_KL: 3.4917342364788055  Validation Loss : 95.77560424804688 Val_Reconstruction : 92.3265266418457 Val_KL : 3.4490798711776733\n","Epoch: 2007/8000  Traning Loss: 96.77631950378418  Train_Reconstruction: 93.29694557189941  Train_KL: 3.479373574256897  Validation Loss : 95.46783828735352 Val_Reconstruction : 92.03373718261719 Val_KL : 3.4340980052948\n","Epoch: 2008/8000  Traning Loss: 96.60628032684326  Train_Reconstruction: 93.12621307373047  Train_KL: 3.4800675213336945  Validation Loss : 95.73498916625977 Val_Reconstruction : 92.29397583007812 Val_KL : 3.441013216972351\n","Epoch: 2009/8000  Traning Loss: 96.85319709777832  Train_Reconstruction: 93.36055850982666  Train_KL: 3.492637276649475  Validation Loss : 95.97353744506836 Val_Reconstruction : 92.51718521118164 Val_KL : 3.456355094909668\n","Epoch: 2010/8000  Traning Loss: 96.57883262634277  Train_Reconstruction: 93.0804214477539  Train_KL: 3.4984110295772552  Validation Loss : 95.35990142822266 Val_Reconstruction : 91.91498947143555 Val_KL : 3.444912552833557\n","Epoch: 2011/8000  Traning Loss: 96.45430374145508  Train_Reconstruction: 92.9666337966919  Train_KL: 3.4876707792282104  Validation Loss : 95.28430557250977 Val_Reconstruction : 91.8456916809082 Val_KL : 3.4386132955551147\n","Epoch: 2012/8000  Traning Loss: 96.28823280334473  Train_Reconstruction: 92.81301212310791  Train_KL: 3.4752207696437836  Validation Loss : 95.40579986572266 Val_Reconstruction : 91.98717880249023 Val_KL : 3.4186218976974487\n","Epoch: 2013/8000  Traning Loss: 96.2743501663208  Train_Reconstruction: 92.78265571594238  Train_KL: 3.4916957020759583  Validation Loss : 95.24419403076172 Val_Reconstruction : 91.78611373901367 Val_KL : 3.4580785036087036\n","Epoch: 2014/8000  Traning Loss: 96.19902038574219  Train_Reconstruction: 92.69622325897217  Train_KL: 3.5027966499328613  Validation Loss : 95.28398895263672 Val_Reconstruction : 91.8318977355957 Val_KL : 3.452091693878174\n","Epoch: 2015/8000  Traning Loss: 96.42314338684082  Train_Reconstruction: 92.92522811889648  Train_KL: 3.497914582490921  Validation Loss : 94.96149826049805 Val_Reconstruction : 91.50911712646484 Val_KL : 3.452378034591675\n","Epoch: 2016/8000  Traning Loss: 96.67237091064453  Train_Reconstruction: 93.17983150482178  Train_KL: 3.492538869380951  Validation Loss : 95.50616836547852 Val_Reconstruction : 92.06645202636719 Val_KL : 3.439715623855591\n","Epoch: 2017/8000  Traning Loss: 96.6580638885498  Train_Reconstruction: 93.17419052124023  Train_KL: 3.483873188495636  Validation Loss : 95.47709655761719 Val_Reconstruction : 92.03329086303711 Val_KL : 3.4438047409057617\n","Epoch: 2018/8000  Traning Loss: 96.33685779571533  Train_Reconstruction: 92.84842300415039  Train_KL: 3.4884347319602966  Validation Loss : 95.14157485961914 Val_Reconstruction : 91.68803787231445 Val_KL : 3.453539252281189\n","Epoch: 2019/8000  Traning Loss: 96.18038177490234  Train_Reconstruction: 92.69016361236572  Train_KL: 3.490217626094818  Validation Loss : 95.04490661621094 Val_Reconstruction : 91.59492492675781 Val_KL : 3.4499813318252563\n","Epoch: 2020/8000  Traning Loss: 96.094313621521  Train_Reconstruction: 92.58520889282227  Train_KL: 3.509104609489441  Validation Loss : 95.03518295288086 Val_Reconstruction : 91.57882690429688 Val_KL : 3.4563562870025635\n","Epoch: 2021/8000  Traning Loss: 96.22973537445068  Train_Reconstruction: 92.73737049102783  Train_KL: 3.49236461520195  Validation Loss : 95.20795059204102 Val_Reconstruction : 91.76133728027344 Val_KL : 3.446612596511841\n","Epoch: 2022/8000  Traning Loss: 96.12172031402588  Train_Reconstruction: 92.63446235656738  Train_KL: 3.4872575104236603  Validation Loss : 95.0249137878418 Val_Reconstruction : 91.5783920288086 Val_KL : 3.4465219974517822\n","Epoch: 2023/8000  Traning Loss: 96.23679733276367  Train_Reconstruction: 92.73830509185791  Train_KL: 3.498491734266281  Validation Loss : 95.19762420654297 Val_Reconstruction : 91.74410247802734 Val_KL : 3.4535229206085205\n","Epoch: 2024/8000  Traning Loss: 96.59506607055664  Train_Reconstruction: 93.09893321990967  Train_KL: 3.4961339831352234  Validation Loss : 95.37796401977539 Val_Reconstruction : 91.92471313476562 Val_KL : 3.4532493352890015\n","Epoch: 2025/8000  Traning Loss: 96.58576393127441  Train_Reconstruction: 93.09487628936768  Train_KL: 3.4908890426158905  Validation Loss : 95.56060028076172 Val_Reconstruction : 92.11899948120117 Val_KL : 3.4416003227233887\n","Epoch: 2026/8000  Traning Loss: 96.7327880859375  Train_Reconstruction: 93.25254249572754  Train_KL: 3.4802440106868744  Validation Loss : 95.44288635253906 Val_Reconstruction : 92.00569152832031 Val_KL : 3.4371947050094604\n","Epoch: 2027/8000  Traning Loss: 96.32413005828857  Train_Reconstruction: 92.84353446960449  Train_KL: 3.4805972278118134  Validation Loss : 95.18109893798828 Val_Reconstruction : 91.74545669555664 Val_KL : 3.4356398582458496\n","Epoch: 2028/8000  Traning Loss: 96.5529375076294  Train_Reconstruction: 93.07106399536133  Train_KL: 3.48187455534935  Validation Loss : 95.67657852172852 Val_Reconstruction : 92.24481582641602 Val_KL : 3.431763529777527\n","Epoch: 2029/8000  Traning Loss: 96.5543622970581  Train_Reconstruction: 93.07551574707031  Train_KL: 3.478847175836563  Validation Loss : 95.93371963500977 Val_Reconstruction : 92.49114227294922 Val_KL : 3.442577362060547\n","Epoch: 2030/8000  Traning Loss: 97.04151725769043  Train_Reconstruction: 93.54769515991211  Train_KL: 3.4938231110572815  Validation Loss : 96.1628646850586 Val_Reconstruction : 92.71072006225586 Val_KL : 3.4521420001983643\n","Epoch: 2031/8000  Traning Loss: 96.85551738739014  Train_Reconstruction: 93.37073993682861  Train_KL: 3.4847788512706757  Validation Loss : 95.57375717163086 Val_Reconstruction : 92.12492752075195 Val_KL : 3.4488288164138794\n","Epoch: 2032/8000  Traning Loss: 96.4462890625  Train_Reconstruction: 92.95130920410156  Train_KL: 3.494979202747345  Validation Loss : 95.23912048339844 Val_Reconstruction : 91.77467727661133 Val_KL : 3.464443325996399\n","Epoch: 2033/8000  Traning Loss: 96.30567646026611  Train_Reconstruction: 92.80093193054199  Train_KL: 3.504745662212372  Validation Loss : 95.2169303894043 Val_Reconstruction : 91.76295852661133 Val_KL : 3.4539703130722046\n","Epoch: 2034/8000  Traning Loss: 96.2178544998169  Train_Reconstruction: 92.72862815856934  Train_KL: 3.489225894212723  Validation Loss : 95.10031127929688 Val_Reconstruction : 91.66479873657227 Val_KL : 3.435511589050293\n","Epoch: 2035/8000  Traning Loss: 96.14613628387451  Train_Reconstruction: 92.66520309448242  Train_KL: 3.48093244433403  Validation Loss : 95.09363174438477 Val_Reconstruction : 91.64850997924805 Val_KL : 3.445122003555298\n","Epoch: 2036/8000  Traning Loss: 96.47746467590332  Train_Reconstruction: 92.98173427581787  Train_KL: 3.4957318902015686  Validation Loss : 95.70450592041016 Val_Reconstruction : 92.25335311889648 Val_KL : 3.451155185699463\n","Epoch: 2037/8000  Traning Loss: 96.6193380355835  Train_Reconstruction: 93.11893081665039  Train_KL: 3.5004058182239532  Validation Loss : 95.39779663085938 Val_Reconstruction : 91.94107055664062 Val_KL : 3.4567267894744873\n","Epoch: 2038/8000  Traning Loss: 96.50210475921631  Train_Reconstruction: 93.01196384429932  Train_KL: 3.4901400208473206  Validation Loss : 95.31972122192383 Val_Reconstruction : 91.89078903198242 Val_KL : 3.428932547569275\n","Epoch: 2039/8000  Traning Loss: 96.2115364074707  Train_Reconstruction: 92.72829532623291  Train_KL: 3.48324117064476  Validation Loss : 95.05157470703125 Val_Reconstruction : 91.60362243652344 Val_KL : 3.447952151298523\n","Epoch: 2040/8000  Traning Loss: 96.10084629058838  Train_Reconstruction: 92.60583114624023  Train_KL: 3.495014935731888  Validation Loss : 94.98286437988281 Val_Reconstruction : 91.52755737304688 Val_KL : 3.4553046226501465\n","Epoch: 2041/8000  Traning Loss: 96.1722183227539  Train_Reconstruction: 92.67622661590576  Train_KL: 3.4959919154644012  Validation Loss : 95.1509895324707 Val_Reconstruction : 91.70525741577148 Val_KL : 3.445733428001404\n","Epoch: 2042/8000  Traning Loss: 96.50166988372803  Train_Reconstruction: 93.01917743682861  Train_KL: 3.482492685317993  Validation Loss : 95.41799545288086 Val_Reconstruction : 91.97908401489258 Val_KL : 3.4389119148254395\n","Epoch: 2043/8000  Traning Loss: 96.38899326324463  Train_Reconstruction: 92.90239238739014  Train_KL: 3.486599624156952  Validation Loss : 95.00550842285156 Val_Reconstruction : 91.55524826049805 Val_KL : 3.4502596855163574\n","Epoch: 2044/8000  Traning Loss: 96.45495510101318  Train_Reconstruction: 92.96002960205078  Train_KL: 3.494926244020462  Validation Loss : 95.36023330688477 Val_Reconstruction : 91.91569900512695 Val_KL : 3.444533109664917\n","Epoch: 2045/8000  Traning Loss: 96.50463008880615  Train_Reconstruction: 93.01368618011475  Train_KL: 3.490943342447281  Validation Loss : 95.43401718139648 Val_Reconstruction : 91.9896240234375 Val_KL : 3.4443923234939575\n","Epoch: 2046/8000  Traning Loss: 96.49912643432617  Train_Reconstruction: 92.99625778198242  Train_KL: 3.502869188785553  Validation Loss : 95.5594253540039 Val_Reconstruction : 92.0902214050293 Val_KL : 3.46920382976532\n","Epoch: 2047/8000  Traning Loss: 96.44382762908936  Train_Reconstruction: 92.94372940063477  Train_KL: 3.5000988245010376  Validation Loss : 95.25640487670898 Val_Reconstruction : 91.80770874023438 Val_KL : 3.4486985206604004\n","Epoch: 2048/8000  Traning Loss: 96.82370853424072  Train_Reconstruction: 93.34121704101562  Train_KL: 3.482492744922638  Validation Loss : 95.9046401977539 Val_Reconstruction : 92.47402954101562 Val_KL : 3.4306108951568604\n","Epoch: 2049/8000  Traning Loss: 96.99887657165527  Train_Reconstruction: 93.50571060180664  Train_KL: 3.493166834115982  Validation Loss : 95.27694702148438 Val_Reconstruction : 91.82392501831055 Val_KL : 3.4530242681503296\n","Epoch: 2050/8000  Traning Loss: 96.27484130859375  Train_Reconstruction: 92.78340244293213  Train_KL: 3.4914391040802  Validation Loss : 95.07955551147461 Val_Reconstruction : 91.62368392944336 Val_KL : 3.455871105194092\n","Epoch: 2051/8000  Traning Loss: 96.09392642974854  Train_Reconstruction: 92.59902572631836  Train_KL: 3.4949005246162415  Validation Loss : 94.99974060058594 Val_Reconstruction : 91.53615188598633 Val_KL : 3.4635863304138184\n","Epoch: 2052/8000  Traning Loss: 96.33963584899902  Train_Reconstruction: 92.83462142944336  Train_KL: 3.5050152242183685  Validation Loss : 95.59558486938477 Val_Reconstruction : 92.1376838684082 Val_KL : 3.4579027891159058\n","Epoch: 2053/8000  Traning Loss: 96.24605369567871  Train_Reconstruction: 92.75655746459961  Train_KL: 3.489495277404785  Validation Loss : 95.16329574584961 Val_Reconstruction : 91.71537399291992 Val_KL : 3.447921395301819\n","Epoch: 2054/8000  Traning Loss: 96.3597764968872  Train_Reconstruction: 92.86830234527588  Train_KL: 3.491471976041794  Validation Loss : 95.27574157714844 Val_Reconstruction : 91.82247161865234 Val_KL : 3.453269839286804\n","Epoch: 2055/8000  Traning Loss: 96.5661849975586  Train_Reconstruction: 93.0813341140747  Train_KL: 3.484851062297821  Validation Loss : 95.88135528564453 Val_Reconstruction : 92.44884490966797 Val_KL : 3.4325116872787476\n","Epoch: 2056/8000  Traning Loss: 96.33392810821533  Train_Reconstruction: 92.85484313964844  Train_KL: 3.47908353805542  Validation Loss : 95.7309455871582 Val_Reconstruction : 92.29202651977539 Val_KL : 3.4389193058013916\n","Epoch: 2057/8000  Traning Loss: 96.30907344818115  Train_Reconstruction: 92.81159782409668  Train_KL: 3.4974760711193085  Validation Loss : 94.84029006958008 Val_Reconstruction : 91.3913803100586 Val_KL : 3.4489094018936157\n","Epoch: 2058/8000  Traning Loss: 96.1247501373291  Train_Reconstruction: 92.62447834014893  Train_KL: 3.5002715289592743  Validation Loss : 95.13972473144531 Val_Reconstruction : 91.6869010925293 Val_KL : 3.4528236389160156\n","Epoch: 2059/8000  Traning Loss: 96.15249538421631  Train_Reconstruction: 92.66093254089355  Train_KL: 3.4915632903575897  Validation Loss : 95.20533752441406 Val_Reconstruction : 91.75728607177734 Val_KL : 3.4480525255203247\n","Epoch: 2060/8000  Traning Loss: 96.22887420654297  Train_Reconstruction: 92.74301815032959  Train_KL: 3.485856831073761  Validation Loss : 95.30609512329102 Val_Reconstruction : 91.8660774230957 Val_KL : 3.44001567363739\n","Epoch: 2061/8000  Traning Loss: 96.35058212280273  Train_Reconstruction: 92.86176681518555  Train_KL: 3.488815516233444  Validation Loss : 95.23677444458008 Val_Reconstruction : 91.79022598266602 Val_KL : 3.446549415588379\n","Epoch: 2062/8000  Traning Loss: 96.16218090057373  Train_Reconstruction: 92.66752624511719  Train_KL: 3.49465411901474  Validation Loss : 94.83324432373047 Val_Reconstruction : 91.38891220092773 Val_KL : 3.4443315267562866\n","Epoch: 2063/8000  Traning Loss: 95.93659210205078  Train_Reconstruction: 92.44719314575195  Train_KL: 3.4893987476825714  Validation Loss : 94.78681564331055 Val_Reconstruction : 91.33612823486328 Val_KL : 3.4506869316101074\n","Epoch: 2064/8000  Traning Loss: 96.21445846557617  Train_Reconstruction: 92.71946811676025  Train_KL: 3.4949891567230225  Validation Loss : 95.6032829284668 Val_Reconstruction : 92.14196395874023 Val_KL : 3.461317539215088\n","Epoch: 2065/8000  Traning Loss: 96.58290481567383  Train_Reconstruction: 93.08569431304932  Train_KL: 3.497211128473282  Validation Loss : 95.27295684814453 Val_Reconstruction : 91.82276916503906 Val_KL : 3.4501872062683105\n","Epoch: 2066/8000  Traning Loss: 96.16552257537842  Train_Reconstruction: 92.67742156982422  Train_KL: 3.4881007969379425  Validation Loss : 94.92749786376953 Val_Reconstruction : 91.48514175415039 Val_KL : 3.442355155944824\n","Epoch: 2067/8000  Traning Loss: 96.26824760437012  Train_Reconstruction: 92.79053020477295  Train_KL: 3.4777173697948456  Validation Loss : 95.58745193481445 Val_Reconstruction : 92.14638137817383 Val_KL : 3.441070795059204\n","Epoch: 2068/8000  Traning Loss: 96.25815486907959  Train_Reconstruction: 92.7687931060791  Train_KL: 3.4893617928028107  Validation Loss : 95.48845291137695 Val_Reconstruction : 92.03443908691406 Val_KL : 3.45401668548584\n","Epoch: 2069/8000  Traning Loss: 96.74765110015869  Train_Reconstruction: 93.24753665924072  Train_KL: 3.5001150369644165  Validation Loss : 96.10675430297852 Val_Reconstruction : 92.65214157104492 Val_KL : 3.454612374305725\n","Epoch: 2070/8000  Traning Loss: 96.50236320495605  Train_Reconstruction: 93.00395584106445  Train_KL: 3.4984085261821747  Validation Loss : 95.43852615356445 Val_Reconstruction : 91.9921989440918 Val_KL : 3.4463272094726562\n","Epoch: 2071/8000  Traning Loss: 96.47582721710205  Train_Reconstruction: 92.99109268188477  Train_KL: 3.4847355484962463  Validation Loss : 95.08992385864258 Val_Reconstruction : 91.64668655395508 Val_KL : 3.443235397338867\n","Epoch: 2072/8000  Traning Loss: 96.33085536956787  Train_Reconstruction: 92.85089015960693  Train_KL: 3.4799661934375763  Validation Loss : 94.88472747802734 Val_Reconstruction : 91.44979476928711 Val_KL : 3.4349329471588135\n","Epoch: 2073/8000  Traning Loss: 96.31714248657227  Train_Reconstruction: 92.83405303955078  Train_KL: 3.4830896854400635  Validation Loss : 95.20293426513672 Val_Reconstruction : 91.76239013671875 Val_KL : 3.4405444860458374\n","Epoch: 2074/8000  Traning Loss: 96.09358310699463  Train_Reconstruction: 92.60789203643799  Train_KL: 3.485691785812378  Validation Loss : 95.25436782836914 Val_Reconstruction : 91.81851577758789 Val_KL : 3.43585205078125\n","Epoch: 2075/8000  Traning Loss: 96.05926609039307  Train_Reconstruction: 92.56699562072754  Train_KL: 3.49227175116539  Validation Loss : 94.91095733642578 Val_Reconstruction : 91.45623016357422 Val_KL : 3.4547237157821655\n","Epoch: 2076/8000  Traning Loss: 95.8538408279419  Train_Reconstruction: 92.360182762146  Train_KL: 3.4936583936214447  Validation Loss : 94.61137008666992 Val_Reconstruction : 91.16482543945312 Val_KL : 3.4465430974960327\n","Epoch: 2077/8000  Traning Loss: 95.87789154052734  Train_Reconstruction: 92.39589405059814  Train_KL: 3.4819973409175873  Validation Loss : 94.68604278564453 Val_Reconstruction : 91.25981903076172 Val_KL : 3.4262235164642334\n","Epoch: 2078/8000  Traning Loss: 96.18895721435547  Train_Reconstruction: 92.7091646194458  Train_KL: 3.479793667793274  Validation Loss : 95.17060470581055 Val_Reconstruction : 91.72920227050781 Val_KL : 3.441400408744812\n","Epoch: 2079/8000  Traning Loss: 96.75000286102295  Train_Reconstruction: 93.24516487121582  Train_KL: 3.50483775138855  Validation Loss : 95.88039016723633 Val_Reconstruction : 92.40996551513672 Val_KL : 3.4704242944717407\n","Epoch: 2080/8000  Traning Loss: 96.95944690704346  Train_Reconstruction: 93.4671630859375  Train_KL: 3.492282658815384  Validation Loss : 95.9952278137207 Val_Reconstruction : 92.56693267822266 Val_KL : 3.428296446800232\n","Epoch: 2081/8000  Traning Loss: 96.8685941696167  Train_Reconstruction: 93.3876600265503  Train_KL: 3.4809340834617615  Validation Loss : 95.30044937133789 Val_Reconstruction : 91.85734558105469 Val_KL : 3.4431042671203613\n","Epoch: 2082/8000  Traning Loss: 96.5658540725708  Train_Reconstruction: 93.07640743255615  Train_KL: 3.4894461631774902  Validation Loss : 95.30118179321289 Val_Reconstruction : 91.86310577392578 Val_KL : 3.4380764961242676\n","Epoch: 2083/8000  Traning Loss: 96.30427551269531  Train_Reconstruction: 92.82530498504639  Train_KL: 3.4789716005325317  Validation Loss : 95.38176727294922 Val_Reconstruction : 91.94233703613281 Val_KL : 3.439432740211487\n","Epoch: 2084/8000  Traning Loss: 96.34574604034424  Train_Reconstruction: 92.87415885925293  Train_KL: 3.4715863466262817  Validation Loss : 95.21727752685547 Val_Reconstruction : 91.78768157958984 Val_KL : 3.4295955896377563\n","Epoch: 2085/8000  Traning Loss: 96.10249614715576  Train_Reconstruction: 92.61148071289062  Train_KL: 3.491015315055847  Validation Loss : 94.87504196166992 Val_Reconstruction : 91.42175674438477 Val_KL : 3.4532846212387085\n","Epoch: 2086/8000  Traning Loss: 96.22618198394775  Train_Reconstruction: 92.71628475189209  Train_KL: 3.509897291660309  Validation Loss : 95.3261947631836 Val_Reconstruction : 91.86309051513672 Val_KL : 3.4631065130233765\n","Epoch: 2087/8000  Traning Loss: 96.2900915145874  Train_Reconstruction: 92.79136943817139  Train_KL: 3.498721122741699  Validation Loss : 95.25982666015625 Val_Reconstruction : 91.81364440917969 Val_KL : 3.4461833238601685\n","Epoch: 2088/8000  Traning Loss: 96.39992713928223  Train_Reconstruction: 92.91604614257812  Train_KL: 3.4838815331459045  Validation Loss : 95.27780532836914 Val_Reconstruction : 91.84117126464844 Val_KL : 3.436632990837097\n","Epoch: 2089/8000  Traning Loss: 96.25953006744385  Train_Reconstruction: 92.76794719696045  Train_KL: 3.4915831387043  Validation Loss : 95.06821060180664 Val_Reconstruction : 91.61885452270508 Val_KL : 3.449356198310852\n","Epoch: 2090/8000  Traning Loss: 96.02489852905273  Train_Reconstruction: 92.53705406188965  Train_KL: 3.48784476518631  Validation Loss : 94.72932052612305 Val_Reconstruction : 91.28706359863281 Val_KL : 3.442260265350342\n","Epoch: 2091/8000  Traning Loss: 95.96981143951416  Train_Reconstruction: 92.4852294921875  Train_KL: 3.4845820665359497  Validation Loss : 95.09973526000977 Val_Reconstruction : 91.65353775024414 Val_KL : 3.4462002515792847\n","Epoch: 2092/8000  Traning Loss: 96.24619007110596  Train_Reconstruction: 92.75308227539062  Train_KL: 3.493107885122299  Validation Loss : 95.03478622436523 Val_Reconstruction : 91.58109283447266 Val_KL : 3.453695774078369\n","Epoch: 2093/8000  Traning Loss: 96.2509994506836  Train_Reconstruction: 92.75019931793213  Train_KL: 3.500799834728241  Validation Loss : 95.04280090332031 Val_Reconstruction : 91.58787536621094 Val_KL : 3.4549237489700317\n","Epoch: 2094/8000  Traning Loss: 96.05600166320801  Train_Reconstruction: 92.56424331665039  Train_KL: 3.4917594492435455  Validation Loss : 95.04106140136719 Val_Reconstruction : 91.5918960571289 Val_KL : 3.4491642713546753\n","Epoch: 2095/8000  Traning Loss: 95.99982929229736  Train_Reconstruction: 92.50965213775635  Train_KL: 3.490176171064377  Validation Loss : 95.11908340454102 Val_Reconstruction : 91.67665100097656 Val_KL : 3.442432403564453\n","Epoch: 2096/8000  Traning Loss: 96.50837802886963  Train_Reconstruction: 93.02154636383057  Train_KL: 3.486830711364746  Validation Loss : 95.40553283691406 Val_Reconstruction : 91.96977233886719 Val_KL : 3.4357573986053467\n","Epoch: 2097/8000  Traning Loss: 96.84305191040039  Train_Reconstruction: 93.35665893554688  Train_KL: 3.4863944351673126  Validation Loss : 95.59191131591797 Val_Reconstruction : 92.14018630981445 Val_KL : 3.451725482940674\n","Epoch: 2098/8000  Traning Loss: 96.45097732543945  Train_Reconstruction: 92.95862102508545  Train_KL: 3.4923564791679382  Validation Loss : 95.80111694335938 Val_Reconstruction : 92.3516731262207 Val_KL : 3.449442505836487\n","Epoch: 2099/8000  Traning Loss: 96.35783386230469  Train_Reconstruction: 92.87453269958496  Train_KL: 3.4833014607429504  Validation Loss : 95.62908554077148 Val_Reconstruction : 92.1854362487793 Val_KL : 3.4436471462249756\n","Epoch: 2100/8000  Traning Loss: 96.4448709487915  Train_Reconstruction: 92.95700645446777  Train_KL: 3.4878643453121185  Validation Loss : 95.38117599487305 Val_Reconstruction : 91.93978500366211 Val_KL : 3.441388249397278\n","Epoch: 2101/8000  Traning Loss: 96.48410034179688  Train_Reconstruction: 93.00616264343262  Train_KL: 3.477938026189804  Validation Loss : 95.716552734375 Val_Reconstruction : 92.28139114379883 Val_KL : 3.435161828994751\n","Epoch: 2102/8000  Traning Loss: 96.4119930267334  Train_Reconstruction: 92.9309139251709  Train_KL: 3.481079250574112  Validation Loss : 95.13069152832031 Val_Reconstruction : 91.69297790527344 Val_KL : 3.4377158880233765\n","Epoch: 2103/8000  Traning Loss: 96.0269603729248  Train_Reconstruction: 92.53337955474854  Train_KL: 3.4935808777809143  Validation Loss : 94.95403289794922 Val_Reconstruction : 91.49956130981445 Val_KL : 3.4544711112976074\n","Epoch: 2104/8000  Traning Loss: 96.33751773834229  Train_Reconstruction: 92.84501266479492  Train_KL: 3.492504835128784  Validation Loss : 95.26768493652344 Val_Reconstruction : 91.82701110839844 Val_KL : 3.4406739473342896\n","Epoch: 2105/8000  Traning Loss: 96.25271129608154  Train_Reconstruction: 92.77474117279053  Train_KL: 3.477970778942108  Validation Loss : 95.20722961425781 Val_Reconstruction : 91.7789535522461 Val_KL : 3.4282745122909546\n","Epoch: 2106/8000  Traning Loss: 96.3562536239624  Train_Reconstruction: 92.8660078048706  Train_KL: 3.490245670080185  Validation Loss : 95.20057678222656 Val_Reconstruction : 91.74360656738281 Val_KL : 3.456969141960144\n","Epoch: 2107/8000  Traning Loss: 96.07014083862305  Train_Reconstruction: 92.58201217651367  Train_KL: 3.488129109144211  Validation Loss : 94.97296524047852 Val_Reconstruction : 91.53455352783203 Val_KL : 3.4384125471115112\n","Epoch: 2108/8000  Traning Loss: 95.88128566741943  Train_Reconstruction: 92.390625  Train_KL: 3.490659713745117  Validation Loss : 94.65787887573242 Val_Reconstruction : 91.21158218383789 Val_KL : 3.44629442691803\n","Epoch: 2109/8000  Traning Loss: 95.9456377029419  Train_Reconstruction: 92.45712661743164  Train_KL: 3.4885110557079315  Validation Loss : 94.97030639648438 Val_Reconstruction : 91.52700805664062 Val_KL : 3.4432979822158813\n","Epoch: 2110/8000  Traning Loss: 96.18192386627197  Train_Reconstruction: 92.68531799316406  Train_KL: 3.4966046512126923  Validation Loss : 95.11566925048828 Val_Reconstruction : 91.65135192871094 Val_KL : 3.464316964149475\n","Epoch: 2111/8000  Traning Loss: 96.25302124023438  Train_Reconstruction: 92.74885559082031  Train_KL: 3.5041644871234894  Validation Loss : 95.10144805908203 Val_Reconstruction : 91.64594650268555 Val_KL : 3.4554989337921143\n","Epoch: 2112/8000  Traning Loss: 96.30592727661133  Train_Reconstruction: 92.8132381439209  Train_KL: 3.492690533399582  Validation Loss : 95.03396606445312 Val_Reconstruction : 91.5804672241211 Val_KL : 3.4534987211227417\n","Epoch: 2113/8000  Traning Loss: 96.39882278442383  Train_Reconstruction: 92.91205883026123  Train_KL: 3.486764669418335  Validation Loss : 95.39496231079102 Val_Reconstruction : 91.9504623413086 Val_KL : 3.4444987773895264\n","Epoch: 2114/8000  Traning Loss: 96.05288028717041  Train_Reconstruction: 92.56023979187012  Train_KL: 3.4926396012306213  Validation Loss : 94.95754623413086 Val_Reconstruction : 91.50274276733398 Val_KL : 3.454802632331848\n","Epoch: 2115/8000  Traning Loss: 95.93102741241455  Train_Reconstruction: 92.42809772491455  Train_KL: 3.5029288828372955  Validation Loss : 95.24396896362305 Val_Reconstruction : 91.78165435791016 Val_KL : 3.4623132944107056\n","Epoch: 2116/8000  Traning Loss: 96.0201997756958  Train_Reconstruction: 92.52709293365479  Train_KL: 3.4931060075759888  Validation Loss : 95.11148834228516 Val_Reconstruction : 91.66770935058594 Val_KL : 3.4437798261642456\n","Epoch: 2117/8000  Traning Loss: 95.75651264190674  Train_Reconstruction: 92.27446460723877  Train_KL: 3.4820482432842255  Validation Loss : 94.53274154663086 Val_Reconstruction : 91.0947265625 Val_KL : 3.438015103340149\n","Epoch: 2118/8000  Traning Loss: 95.79304313659668  Train_Reconstruction: 92.30037689208984  Train_KL: 3.49266654253006  Validation Loss : 94.73276901245117 Val_Reconstruction : 91.27407455444336 Val_KL : 3.4586960077285767\n","Epoch: 2119/8000  Traning Loss: 95.7255973815918  Train_Reconstruction: 92.22816371917725  Train_KL: 3.4974345266819  Validation Loss : 94.78395080566406 Val_Reconstruction : 91.33076858520508 Val_KL : 3.4531822204589844\n","Epoch: 2120/8000  Traning Loss: 95.61637878417969  Train_Reconstruction: 92.13054943084717  Train_KL: 3.485829323530197  Validation Loss : 94.64513397216797 Val_Reconstruction : 91.20431518554688 Val_KL : 3.4408206939697266\n","Epoch: 2121/8000  Traning Loss: 95.72262859344482  Train_Reconstruction: 92.23978614807129  Train_KL: 3.4828436076641083  Validation Loss : 94.66236114501953 Val_Reconstruction : 91.21595764160156 Val_KL : 3.4464056491851807\n","Epoch: 2122/8000  Traning Loss: 95.63915634155273  Train_Reconstruction: 92.14145374298096  Train_KL: 3.4977008998394012  Validation Loss : 94.57272338867188 Val_Reconstruction : 91.11213684082031 Val_KL : 3.460590124130249\n","Epoch: 2123/8000  Traning Loss: 96.11540699005127  Train_Reconstruction: 92.62119102478027  Train_KL: 3.494216114282608  Validation Loss : 95.12842178344727 Val_Reconstruction : 91.68225479125977 Val_KL : 3.4461687803268433\n","Epoch: 2124/8000  Traning Loss: 96.13678550720215  Train_Reconstruction: 92.64950370788574  Train_KL: 3.48728284239769  Validation Loss : 95.17160034179688 Val_Reconstruction : 91.73104095458984 Val_KL : 3.4405620098114014\n","Epoch: 2125/8000  Traning Loss: 96.65476322174072  Train_Reconstruction: 93.17043495178223  Train_KL: 3.484327971935272  Validation Loss : 95.49420928955078 Val_Reconstruction : 92.04568862915039 Val_KL : 3.448518753051758\n","Epoch: 2126/8000  Traning Loss: 96.59807872772217  Train_Reconstruction: 93.10068416595459  Train_KL: 3.4973933696746826  Validation Loss : 95.40765380859375 Val_Reconstruction : 91.95273208618164 Val_KL : 3.454921007156372\n","Epoch: 2127/8000  Traning Loss: 96.01515579223633  Train_Reconstruction: 92.52755641937256  Train_KL: 3.4875993728637695  Validation Loss : 95.04798889160156 Val_Reconstruction : 91.6097526550293 Val_KL : 3.4382389783859253\n","Epoch: 2128/8000  Traning Loss: 96.2813081741333  Train_Reconstruction: 92.79808902740479  Train_KL: 3.4832178354263306  Validation Loss : 95.33541488647461 Val_Reconstruction : 91.89845657348633 Val_KL : 3.4369558095932007\n","Epoch: 2129/8000  Traning Loss: 96.43817520141602  Train_Reconstruction: 92.9489574432373  Train_KL: 3.489217519760132  Validation Loss : 95.04671859741211 Val_Reconstruction : 91.590576171875 Val_KL : 3.456145167350769\n","Epoch: 2130/8000  Traning Loss: 96.39916229248047  Train_Reconstruction: 92.90491771697998  Train_KL: 3.494244396686554  Validation Loss : 95.28134536743164 Val_Reconstruction : 91.8271255493164 Val_KL : 3.4542211294174194\n","Epoch: 2131/8000  Traning Loss: 96.37784004211426  Train_Reconstruction: 92.87838459014893  Train_KL: 3.499454826116562  Validation Loss : 95.12522888183594 Val_Reconstruction : 91.67044448852539 Val_KL : 3.4547863006591797\n","Epoch: 2132/8000  Traning Loss: 96.32095336914062  Train_Reconstruction: 92.84407329559326  Train_KL: 3.4768791794776917  Validation Loss : 95.04465866088867 Val_Reconstruction : 91.62396240234375 Val_KL : 3.4206994771957397\n","Epoch: 2133/8000  Traning Loss: 96.11844730377197  Train_Reconstruction: 92.6455774307251  Train_KL: 3.4728688299655914  Validation Loss : 94.78063583374023 Val_Reconstruction : 91.34516143798828 Val_KL : 3.4354755878448486\n","Epoch: 2134/8000  Traning Loss: 95.92332077026367  Train_Reconstruction: 92.4280481338501  Train_KL: 3.495272070169449  Validation Loss : 95.41756057739258 Val_Reconstruction : 91.9589614868164 Val_KL : 3.4585978984832764\n","Epoch: 2135/8000  Traning Loss: 96.55698013305664  Train_Reconstruction: 93.05175590515137  Train_KL: 3.5052245259284973  Validation Loss : 95.31929397583008 Val_Reconstruction : 91.86882400512695 Val_KL : 3.450469970703125\n","Epoch: 2136/8000  Traning Loss: 95.93707180023193  Train_Reconstruction: 92.43468761444092  Train_KL: 3.5023845732212067  Validation Loss : 94.9197006225586 Val_Reconstruction : 91.46271896362305 Val_KL : 3.456983208656311\n","Epoch: 2137/8000  Traning Loss: 95.77451801300049  Train_Reconstruction: 92.2807207107544  Train_KL: 3.4937973022460938  Validation Loss : 95.01899337768555 Val_Reconstruction : 91.57986068725586 Val_KL : 3.4391305446624756\n","Epoch: 2138/8000  Traning Loss: 95.9834508895874  Train_Reconstruction: 92.4936637878418  Train_KL: 3.489786922931671  Validation Loss : 94.85189056396484 Val_Reconstruction : 91.40504837036133 Val_KL : 3.446844220161438\n","Epoch: 2139/8000  Traning Loss: 95.97106266021729  Train_Reconstruction: 92.4785737991333  Train_KL: 3.4924892485141754  Validation Loss : 94.9272689819336 Val_Reconstruction : 91.48485946655273 Val_KL : 3.4424086809158325\n","Epoch: 2140/8000  Traning Loss: 96.0496768951416  Train_Reconstruction: 92.55438899993896  Train_KL: 3.4952867329120636  Validation Loss : 95.2041130065918 Val_Reconstruction : 91.76318359375 Val_KL : 3.4409316778182983\n","Epoch: 2141/8000  Traning Loss: 95.99827003479004  Train_Reconstruction: 92.5082597732544  Train_KL: 3.4900101125240326  Validation Loss : 94.65166473388672 Val_Reconstruction : 91.21245193481445 Val_KL : 3.4392141103744507\n","Epoch: 2142/8000  Traning Loss: 95.9921932220459  Train_Reconstruction: 92.49249839782715  Train_KL: 3.49969545006752  Validation Loss : 94.84610748291016 Val_Reconstruction : 91.38568496704102 Val_KL : 3.460421323776245\n","Epoch: 2143/8000  Traning Loss: 96.23565578460693  Train_Reconstruction: 92.73383903503418  Train_KL: 3.5018154680728912  Validation Loss : 95.33538436889648 Val_Reconstruction : 91.88126373291016 Val_KL : 3.4541208744049072\n","Epoch: 2144/8000  Traning Loss: 96.43506240844727  Train_Reconstruction: 92.95135879516602  Train_KL: 3.483703166246414  Validation Loss : 95.54705047607422 Val_Reconstruction : 92.11131286621094 Val_KL : 3.4357391595840454\n","Epoch: 2145/8000  Traning Loss: 96.29535484313965  Train_Reconstruction: 92.81829833984375  Train_KL: 3.4770568013191223  Validation Loss : 95.11466598510742 Val_Reconstruction : 91.67977523803711 Val_KL : 3.434889078140259\n","Epoch: 2146/8000  Traning Loss: 95.90182113647461  Train_Reconstruction: 92.40654277801514  Train_KL: 3.495279163122177  Validation Loss : 94.9830322265625 Val_Reconstruction : 91.53304290771484 Val_KL : 3.449990391731262\n","Epoch: 2147/8000  Traning Loss: 95.92135906219482  Train_Reconstruction: 92.42583847045898  Train_KL: 3.4955204725265503  Validation Loss : 94.92379760742188 Val_Reconstruction : 91.4766616821289 Val_KL : 3.4471359252929688\n","Epoch: 2148/8000  Traning Loss: 95.89390659332275  Train_Reconstruction: 92.40788173675537  Train_KL: 3.4860252737998962  Validation Loss : 94.89117050170898 Val_Reconstruction : 91.45641708374023 Val_KL : 3.4347537755966187\n","Epoch: 2149/8000  Traning Loss: 95.77215766906738  Train_Reconstruction: 92.27847385406494  Train_KL: 3.4936847388744354  Validation Loss : 94.72513580322266 Val_Reconstruction : 91.26881790161133 Val_KL : 3.456321120262146\n","Epoch: 2150/8000  Traning Loss: 95.87007999420166  Train_Reconstruction: 92.36366844177246  Train_KL: 3.50641205906868  Validation Loss : 95.16558074951172 Val_Reconstruction : 91.70884323120117 Val_KL : 3.456737995147705\n","Epoch: 2151/8000  Traning Loss: 95.9811143875122  Train_Reconstruction: 92.48715114593506  Train_KL: 3.4939653873443604  Validation Loss : 95.04119110107422 Val_Reconstruction : 91.60732650756836 Val_KL : 3.4338667392730713\n","Epoch: 2152/8000  Traning Loss: 96.16261100769043  Train_Reconstruction: 92.68285942077637  Train_KL: 3.479751318693161  Validation Loss : 95.70816802978516 Val_Reconstruction : 92.2669677734375 Val_KL : 3.441201329231262\n","Epoch: 2153/8000  Traning Loss: 95.99306297302246  Train_Reconstruction: 92.49858093261719  Train_KL: 3.4944815635681152  Validation Loss : 94.88445281982422 Val_Reconstruction : 91.4359016418457 Val_KL : 3.448552370071411\n","Epoch: 2154/8000  Traning Loss: 95.63290596008301  Train_Reconstruction: 92.14463996887207  Train_KL: 3.4882658421993256  Validation Loss : 94.61627960205078 Val_Reconstruction : 91.16825485229492 Val_KL : 3.4480273723602295\n","Epoch: 2155/8000  Traning Loss: 95.54126930236816  Train_Reconstruction: 92.05182075500488  Train_KL: 3.489449977874756  Validation Loss : 94.63972854614258 Val_Reconstruction : 91.19705581665039 Val_KL : 3.442671298980713\n","Epoch: 2156/8000  Traning Loss: 95.67058944702148  Train_Reconstruction: 92.16906261444092  Train_KL: 3.5015270113945007  Validation Loss : 94.7508316040039 Val_Reconstruction : 91.2926025390625 Val_KL : 3.4582279920578003\n","Epoch: 2157/8000  Traning Loss: 95.60934257507324  Train_Reconstruction: 92.11048316955566  Train_KL: 3.498859941959381  Validation Loss : 94.65673828125 Val_Reconstruction : 91.20809173583984 Val_KL : 3.448644518852234\n","Epoch: 2158/8000  Traning Loss: 95.55304336547852  Train_Reconstruction: 92.0614423751831  Train_KL: 3.491601288318634  Validation Loss : 94.6022834777832 Val_Reconstruction : 91.14726638793945 Val_KL : 3.4550158977508545\n","Epoch: 2159/8000  Traning Loss: 95.8435411453247  Train_Reconstruction: 92.33850383758545  Train_KL: 3.505036234855652  Validation Loss : 94.84947967529297 Val_Reconstruction : 91.38771438598633 Val_KL : 3.4617642164230347\n","Epoch: 2160/8000  Traning Loss: 96.18472003936768  Train_Reconstruction: 92.67862033843994  Train_KL: 3.5061007142066956  Validation Loss : 95.3001937866211 Val_Reconstruction : 91.83744049072266 Val_KL : 3.462753415107727\n","Epoch: 2161/8000  Traning Loss: 96.86033630371094  Train_Reconstruction: 93.36479663848877  Train_KL: 3.4955399930477142  Validation Loss : 96.14156723022461 Val_Reconstruction : 92.70214462280273 Val_KL : 3.439425468444824\n","Epoch: 2162/8000  Traning Loss: 96.49701499938965  Train_Reconstruction: 93.00271415710449  Train_KL: 3.494300961494446  Validation Loss : 95.12379455566406 Val_Reconstruction : 91.66029739379883 Val_KL : 3.463499665260315\n","Epoch: 2163/8000  Traning Loss: 95.87874984741211  Train_Reconstruction: 92.38140773773193  Train_KL: 3.4973420798778534  Validation Loss : 95.00362396240234 Val_Reconstruction : 91.55540466308594 Val_KL : 3.448220372200012\n","Epoch: 2164/8000  Traning Loss: 95.6681547164917  Train_Reconstruction: 92.18407440185547  Train_KL: 3.4840795397758484  Validation Loss : 94.89006042480469 Val_Reconstruction : 91.4507064819336 Val_KL : 3.4393563270568848\n","Epoch: 2165/8000  Traning Loss: 95.7643632888794  Train_Reconstruction: 92.26882266998291  Train_KL: 3.4955401718616486  Validation Loss : 94.89265060424805 Val_Reconstruction : 91.437255859375 Val_KL : 3.4553959369659424\n","Epoch: 2166/8000  Traning Loss: 95.83479881286621  Train_Reconstruction: 92.3411283493042  Train_KL: 3.4936708211898804  Validation Loss : 94.6220817565918 Val_Reconstruction : 91.17214965820312 Val_KL : 3.4499306678771973\n","Epoch: 2167/8000  Traning Loss: 95.96987915039062  Train_Reconstruction: 92.47586345672607  Train_KL: 3.4940143525600433  Validation Loss : 94.92715454101562 Val_Reconstruction : 91.47379684448242 Val_KL : 3.45335853099823\n","Epoch: 2168/8000  Traning Loss: 96.37011909484863  Train_Reconstruction: 92.86792278289795  Train_KL: 3.5021980106830597  Validation Loss : 95.4310531616211 Val_Reconstruction : 91.97170639038086 Val_KL : 3.4593480825424194\n","Epoch: 2169/8000  Traning Loss: 95.99974250793457  Train_Reconstruction: 92.49967384338379  Train_KL: 3.5000685453414917  Validation Loss : 94.68746566772461 Val_Reconstruction : 91.24914169311523 Val_KL : 3.4383246898651123\n","Epoch: 2170/8000  Traning Loss: 95.6357593536377  Train_Reconstruction: 92.14322757720947  Train_KL: 3.4925318360328674  Validation Loss : 94.95207214355469 Val_Reconstruction : 91.4925651550293 Val_KL : 3.45950448513031\n","Epoch: 2171/8000  Traning Loss: 95.80135726928711  Train_Reconstruction: 92.2938985824585  Train_KL: 3.5074593126773834  Validation Loss : 95.02659225463867 Val_Reconstruction : 91.57301712036133 Val_KL : 3.4535781145095825\n","Epoch: 2172/8000  Traning Loss: 95.85604286193848  Train_Reconstruction: 92.36393547058105  Train_KL: 3.4921069741249084  Validation Loss : 95.00494384765625 Val_Reconstruction : 91.55475616455078 Val_KL : 3.4501878023147583\n","Epoch: 2173/8000  Traning Loss: 96.01267528533936  Train_Reconstruction: 92.51893138885498  Train_KL: 3.493743658065796  Validation Loss : 94.92645263671875 Val_Reconstruction : 91.4701042175293 Val_KL : 3.456351161003113\n","Epoch: 2174/8000  Traning Loss: 95.91705894470215  Train_Reconstruction: 92.42551231384277  Train_KL: 3.491546720266342  Validation Loss : 95.05747985839844 Val_Reconstruction : 91.60906982421875 Val_KL : 3.448409080505371\n","Epoch: 2175/8000  Traning Loss: 96.47843170166016  Train_Reconstruction: 92.97992992401123  Train_KL: 3.4985020458698273  Validation Loss : 95.82526016235352 Val_Reconstruction : 92.37044525146484 Val_KL : 3.454813599586487\n","Epoch: 2176/8000  Traning Loss: 96.35975646972656  Train_Reconstruction: 92.85884380340576  Train_KL: 3.5009125769138336  Validation Loss : 95.27748107910156 Val_Reconstruction : 91.82255172729492 Val_KL : 3.4549310207366943\n","Epoch: 2177/8000  Traning Loss: 95.86192226409912  Train_Reconstruction: 92.36221408843994  Train_KL: 3.4997073113918304  Validation Loss : 94.90350341796875 Val_Reconstruction : 91.43975067138672 Val_KL : 3.4637550115585327\n","Epoch: 2178/8000  Traning Loss: 95.8061466217041  Train_Reconstruction: 92.29218864440918  Train_KL: 3.513958364725113  Validation Loss : 94.85793685913086 Val_Reconstruction : 91.39691162109375 Val_KL : 3.4610241651535034\n","Epoch: 2179/8000  Traning Loss: 95.78308868408203  Train_Reconstruction: 92.3030366897583  Train_KL: 3.480051666498184  Validation Loss : 94.79228210449219 Val_Reconstruction : 91.37210464477539 Val_KL : 3.42017924785614\n","Epoch: 2180/8000  Traning Loss: 95.94036102294922  Train_Reconstruction: 92.46421337127686  Train_KL: 3.4761478304862976  Validation Loss : 95.35758590698242 Val_Reconstruction : 91.91054916381836 Val_KL : 3.4470345973968506\n","Epoch: 2181/8000  Traning Loss: 96.28311061859131  Train_Reconstruction: 92.78857898712158  Train_KL: 3.494532197713852  Validation Loss : 95.52802276611328 Val_Reconstruction : 92.06795883178711 Val_KL : 3.460065484046936\n","Epoch: 2182/8000  Traning Loss: 95.9981746673584  Train_Reconstruction: 92.4974718093872  Train_KL: 3.500704377889633  Validation Loss : 94.52319717407227 Val_Reconstruction : 91.0612564086914 Val_KL : 3.461937427520752\n","Epoch: 2183/8000  Traning Loss: 95.86054420471191  Train_Reconstruction: 92.36263751983643  Train_KL: 3.497906059026718  Validation Loss : 95.00738143920898 Val_Reconstruction : 91.5577392578125 Val_KL : 3.449644684791565\n","Epoch: 2184/8000  Traning Loss: 95.81126976013184  Train_Reconstruction: 92.3147611618042  Train_KL: 3.496507078409195  Validation Loss : 94.72357559204102 Val_Reconstruction : 91.27236938476562 Val_KL : 3.45120632648468\n","Epoch: 2185/8000  Traning Loss: 95.69040393829346  Train_Reconstruction: 92.19948196411133  Train_KL: 3.490922838449478  Validation Loss : 94.77484512329102 Val_Reconstruction : 91.32937622070312 Val_KL : 3.4454705715179443\n","Epoch: 2186/8000  Traning Loss: 95.9798002243042  Train_Reconstruction: 92.48896789550781  Train_KL: 3.490829825401306  Validation Loss : 94.670654296875 Val_Reconstruction : 91.21629333496094 Val_KL : 3.4543617963790894\n","Epoch: 2187/8000  Traning Loss: 95.74177169799805  Train_Reconstruction: 92.24629878997803  Train_KL: 3.4954739212989807  Validation Loss : 94.8102798461914 Val_Reconstruction : 91.35392379760742 Val_KL : 3.45635449886322\n","Epoch: 2188/8000  Traning Loss: 95.98380947113037  Train_Reconstruction: 92.47640323638916  Train_KL: 3.5074064433574677  Validation Loss : 95.27519226074219 Val_Reconstruction : 91.80658340454102 Val_KL : 3.468607783317566\n","Epoch: 2189/8000  Traning Loss: 96.19263744354248  Train_Reconstruction: 92.70514392852783  Train_KL: 3.4874942898750305  Validation Loss : 95.05361557006836 Val_Reconstruction : 91.61623764038086 Val_KL : 3.4373773336410522\n","Epoch: 2190/8000  Traning Loss: 95.8912992477417  Train_Reconstruction: 92.41186904907227  Train_KL: 3.4794312715530396  Validation Loss : 94.81035232543945 Val_Reconstruction : 91.36032104492188 Val_KL : 3.450034022331238\n","Epoch: 2191/8000  Traning Loss: 95.82346534729004  Train_Reconstruction: 92.32594108581543  Train_KL: 3.4975233376026154  Validation Loss : 95.23735046386719 Val_Reconstruction : 91.77946853637695 Val_KL : 3.4578793048858643\n","Epoch: 2192/8000  Traning Loss: 96.0382022857666  Train_Reconstruction: 92.5372838973999  Train_KL: 3.5009182691574097  Validation Loss : 95.2187614440918 Val_Reconstruction : 91.75716018676758 Val_KL : 3.4616026878356934\n","Epoch: 2193/8000  Traning Loss: 96.21818256378174  Train_Reconstruction: 92.72494125366211  Train_KL: 3.493242710828781  Validation Loss : 95.36541366577148 Val_Reconstruction : 91.92301559448242 Val_KL : 3.442397952079773\n","Epoch: 2194/8000  Traning Loss: 96.4505729675293  Train_Reconstruction: 92.96230506896973  Train_KL: 3.4882685244083405  Validation Loss : 95.81719970703125 Val_Reconstruction : 92.3676643371582 Val_KL : 3.4495363235473633\n","Epoch: 2195/8000  Traning Loss: 96.96348476409912  Train_Reconstruction: 93.4660997390747  Train_KL: 3.4973849058151245  Validation Loss : 96.61431121826172 Val_Reconstruction : 93.15762710571289 Val_KL : 3.4566820859909058\n","Epoch: 2196/8000  Traning Loss: 97.07244205474854  Train_Reconstruction: 93.5771255493164  Train_KL: 3.4953184723854065  Validation Loss : 96.11873245239258 Val_Reconstruction : 92.66070556640625 Val_KL : 3.4580265283584595\n","Epoch: 2197/8000  Traning Loss: 96.33706760406494  Train_Reconstruction: 92.84301376342773  Train_KL: 3.494053393602371  Validation Loss : 95.44623184204102 Val_Reconstruction : 91.9997329711914 Val_KL : 3.446499466896057\n","Epoch: 2198/8000  Traning Loss: 95.96871948242188  Train_Reconstruction: 92.47024154663086  Train_KL: 3.498478591442108  Validation Loss : 94.98021697998047 Val_Reconstruction : 91.53601455688477 Val_KL : 3.4442003965377808\n","Epoch: 2199/8000  Traning Loss: 95.97673225402832  Train_Reconstruction: 92.4749813079834  Train_KL: 3.501750648021698  Validation Loss : 95.04428482055664 Val_Reconstruction : 91.5857048034668 Val_KL : 3.4585784673690796\n","Epoch: 2200/8000  Traning Loss: 96.3588695526123  Train_Reconstruction: 92.8679723739624  Train_KL: 3.4908971786499023  Validation Loss : 95.39354705810547 Val_Reconstruction : 91.94664764404297 Val_KL : 3.446900725364685\n","Epoch: 2201/8000  Traning Loss: 96.11519813537598  Train_Reconstruction: 92.6098690032959  Train_KL: 3.5053287148475647  Validation Loss : 94.7017707824707 Val_Reconstruction : 91.2242202758789 Val_KL : 3.477550745010376\n","Epoch: 2202/8000  Traning Loss: 95.6843204498291  Train_Reconstruction: 92.17257595062256  Train_KL: 3.5117430984973907  Validation Loss : 94.69627380371094 Val_Reconstruction : 91.23412704467773 Val_KL : 3.4621461629867554\n","Epoch: 2203/8000  Traning Loss: 95.87411499023438  Train_Reconstruction: 92.37463760375977  Train_KL: 3.4994777143001556  Validation Loss : 94.89212036132812 Val_Reconstruction : 91.45159149169922 Val_KL : 3.4405295848846436\n","Epoch: 2204/8000  Traning Loss: 96.24190998077393  Train_Reconstruction: 92.7550802230835  Train_KL: 3.4868300557136536  Validation Loss : 96.05870056152344 Val_Reconstruction : 92.61014938354492 Val_KL : 3.448548913002014\n","Epoch: 2205/8000  Traning Loss: 97.23552894592285  Train_Reconstruction: 93.73048114776611  Train_KL: 3.5050477385520935  Validation Loss : 95.65092849731445 Val_Reconstruction : 92.19067001342773 Val_KL : 3.4602569341659546\n","Epoch: 2206/8000  Traning Loss: 96.51950073242188  Train_Reconstruction: 93.01853084564209  Train_KL: 3.500969111919403  Validation Loss : 95.34914016723633 Val_Reconstruction : 91.90589141845703 Val_KL : 3.4432461261749268\n","Epoch: 2207/8000  Traning Loss: 95.90987491607666  Train_Reconstruction: 92.42665386199951  Train_KL: 3.4832206070423126  Validation Loss : 94.78242492675781 Val_Reconstruction : 91.3444595336914 Val_KL : 3.4379669427871704\n","Epoch: 2208/8000  Traning Loss: 95.56981563568115  Train_Reconstruction: 92.08055210113525  Train_KL: 3.4892638027668  Validation Loss : 94.58870315551758 Val_Reconstruction : 91.13322830200195 Val_KL : 3.4554744958877563\n","Epoch: 2209/8000  Traning Loss: 95.9457483291626  Train_Reconstruction: 92.44901084899902  Train_KL: 3.496738165616989  Validation Loss : 95.42336654663086 Val_Reconstruction : 91.97188186645508 Val_KL : 3.451482892036438\n","Epoch: 2210/8000  Traning Loss: 96.16410064697266  Train_Reconstruction: 92.67221355438232  Train_KL: 3.491885483264923  Validation Loss : 95.03366088867188 Val_Reconstruction : 91.59277725219727 Val_KL : 3.4408830404281616\n","Epoch: 2211/8000  Traning Loss: 96.2174425125122  Train_Reconstruction: 92.73220634460449  Train_KL: 3.4852359294891357  Validation Loss : 95.28335952758789 Val_Reconstruction : 91.83381271362305 Val_KL : 3.4495478868484497\n","Epoch: 2212/8000  Traning Loss: 96.04059886932373  Train_Reconstruction: 92.54539203643799  Train_KL: 3.4952067136764526  Validation Loss : 94.92304611206055 Val_Reconstruction : 91.46866989135742 Val_KL : 3.4543761014938354\n","Epoch: 2213/8000  Traning Loss: 95.67101955413818  Train_Reconstruction: 92.16498184204102  Train_KL: 3.506038248538971  Validation Loss : 94.54955291748047 Val_Reconstruction : 91.08283615112305 Val_KL : 3.4667149782180786\n","Epoch: 2214/8000  Traning Loss: 95.73641300201416  Train_Reconstruction: 92.23199653625488  Train_KL: 3.504414826631546  Validation Loss : 94.61206817626953 Val_Reconstruction : 91.15462875366211 Val_KL : 3.457439661026001\n","Epoch: 2215/8000  Traning Loss: 95.81485176086426  Train_Reconstruction: 92.31593036651611  Train_KL: 3.4989207983016968  Validation Loss : 95.09945678710938 Val_Reconstruction : 91.64029693603516 Val_KL : 3.4591591358184814\n","Epoch: 2216/8000  Traning Loss: 95.69923305511475  Train_Reconstruction: 92.20569896697998  Train_KL: 3.493534177541733  Validation Loss : 94.76105499267578 Val_Reconstruction : 91.31398391723633 Val_KL : 3.4470726251602173\n","Epoch: 2217/8000  Traning Loss: 95.9348783493042  Train_Reconstruction: 92.44628143310547  Train_KL: 3.488594710826874  Validation Loss : 95.05402374267578 Val_Reconstruction : 91.61610794067383 Val_KL : 3.437914729118347\n","Epoch: 2218/8000  Traning Loss: 95.8122615814209  Train_Reconstruction: 92.32979393005371  Train_KL: 3.482467859983444  Validation Loss : 95.1206169128418 Val_Reconstruction : 91.68939590454102 Val_KL : 3.4312214851379395\n","Epoch: 2219/8000  Traning Loss: 95.95565700531006  Train_Reconstruction: 92.46931076049805  Train_KL: 3.4863456785678864  Validation Loss : 95.09772872924805 Val_Reconstruction : 91.65218353271484 Val_KL : 3.4455450773239136\n","Epoch: 2220/8000  Traning Loss: 95.65345001220703  Train_Reconstruction: 92.15420246124268  Train_KL: 3.4992475509643555  Validation Loss : 94.6736946105957 Val_Reconstruction : 91.20929336547852 Val_KL : 3.4643986225128174\n","Epoch: 2221/8000  Traning Loss: 95.99510860443115  Train_Reconstruction: 92.49688339233398  Train_KL: 3.4982250928878784  Validation Loss : 95.0037612915039 Val_Reconstruction : 91.55069732666016 Val_KL : 3.4530619382858276\n","Epoch: 2222/8000  Traning Loss: 95.94106388092041  Train_Reconstruction: 92.44354724884033  Train_KL: 3.4975167214870453  Validation Loss : 94.82019424438477 Val_Reconstruction : 91.36974716186523 Val_KL : 3.4504470825195312\n","Epoch: 2223/8000  Traning Loss: 95.59255123138428  Train_Reconstruction: 92.10187530517578  Train_KL: 3.4906760454177856  Validation Loss : 94.7343635559082 Val_Reconstruction : 91.2897834777832 Val_KL : 3.444579839706421\n","Epoch: 2224/8000  Traning Loss: 95.73354721069336  Train_Reconstruction: 92.23110103607178  Train_KL: 3.5024472773075104  Validation Loss : 94.7468032836914 Val_Reconstruction : 91.27737808227539 Val_KL : 3.469426989555359\n","Epoch: 2225/8000  Traning Loss: 95.60610866546631  Train_Reconstruction: 92.10461902618408  Train_KL: 3.5014895498752594  Validation Loss : 94.58544158935547 Val_Reconstruction : 91.13165664672852 Val_KL : 3.453783392906189\n","Epoch: 2226/8000  Traning Loss: 95.71330070495605  Train_Reconstruction: 92.22201728820801  Train_KL: 3.4912832379341125  Validation Loss : 94.86054611206055 Val_Reconstruction : 91.41798782348633 Val_KL : 3.4425594806671143\n","Epoch: 2227/8000  Traning Loss: 96.12552833557129  Train_Reconstruction: 92.63846111297607  Train_KL: 3.487067073583603  Validation Loss : 94.95122528076172 Val_Reconstruction : 91.4907455444336 Val_KL : 3.4604828357696533\n","Epoch: 2228/8000  Traning Loss: 95.8476676940918  Train_Reconstruction: 92.34860324859619  Train_KL: 3.4990650713443756  Validation Loss : 94.90723419189453 Val_Reconstruction : 91.45871353149414 Val_KL : 3.4485182762145996\n","Epoch: 2229/8000  Traning Loss: 95.86745834350586  Train_Reconstruction: 92.37909889221191  Train_KL: 3.4883590936660767  Validation Loss : 95.0282096862793 Val_Reconstruction : 91.59272384643555 Val_KL : 3.4354865550994873\n","Epoch: 2230/8000  Traning Loss: 96.15048885345459  Train_Reconstruction: 92.66392803192139  Train_KL: 3.486560821533203  Validation Loss : 94.78578186035156 Val_Reconstruction : 91.34566497802734 Val_KL : 3.440118432044983\n","Epoch: 2231/8000  Traning Loss: 95.65991306304932  Train_Reconstruction: 92.1630687713623  Train_KL: 3.496844321489334  Validation Loss : 94.41647720336914 Val_Reconstruction : 90.97171401977539 Val_KL : 3.4447628259658813\n","Epoch: 2232/8000  Traning Loss: 95.43766021728516  Train_Reconstruction: 91.95395278930664  Train_KL: 3.4837068915367126  Validation Loss : 94.40438842773438 Val_Reconstruction : 90.9655532836914 Val_KL : 3.4388349056243896\n","Epoch: 2233/8000  Traning Loss: 95.71491527557373  Train_Reconstruction: 92.22387409210205  Train_KL: 3.4910423159599304  Validation Loss : 94.92564392089844 Val_Reconstruction : 91.46809387207031 Val_KL : 3.4575507640838623\n","Epoch: 2234/8000  Traning Loss: 95.98730373382568  Train_Reconstruction: 92.4770565032959  Train_KL: 3.51024729013443  Validation Loss : 95.07769393920898 Val_Reconstruction : 91.61716842651367 Val_KL : 3.4605261087417603\n","Epoch: 2235/8000  Traning Loss: 95.78583431243896  Train_Reconstruction: 92.28447151184082  Train_KL: 3.501363158226013  Validation Loss : 94.70167922973633 Val_Reconstruction : 91.25588607788086 Val_KL : 3.4457908868789673\n","Epoch: 2236/8000  Traning Loss: 95.49946212768555  Train_Reconstruction: 92.00943946838379  Train_KL: 3.4900224804878235  Validation Loss : 94.60574722290039 Val_Reconstruction : 91.15813827514648 Val_KL : 3.447607159614563\n","Epoch: 2237/8000  Traning Loss: 95.62657737731934  Train_Reconstruction: 92.14442157745361  Train_KL: 3.482155054807663  Validation Loss : 94.83991241455078 Val_Reconstruction : 91.40478134155273 Val_KL : 3.435133934020996\n","Epoch: 2238/8000  Traning Loss: 95.6178846359253  Train_Reconstruction: 92.13488960266113  Train_KL: 3.4829958975315094  Validation Loss : 94.48644256591797 Val_Reconstruction : 91.04390335083008 Val_KL : 3.4425421953201294\n","Epoch: 2239/8000  Traning Loss: 95.61759376525879  Train_Reconstruction: 92.1205530166626  Train_KL: 3.497040331363678  Validation Loss : 94.77523422241211 Val_Reconstruction : 91.3149642944336 Val_KL : 3.4602733850479126\n","Epoch: 2240/8000  Traning Loss: 95.90497398376465  Train_Reconstruction: 92.39526844024658  Train_KL: 3.5097047686576843  Validation Loss : 94.9201889038086 Val_Reconstruction : 91.45223617553711 Val_KL : 3.4679524898529053\n","Epoch: 2241/8000  Traning Loss: 95.70764446258545  Train_Reconstruction: 92.20021724700928  Train_KL: 3.5074265897274017  Validation Loss : 94.75976181030273 Val_Reconstruction : 91.30923843383789 Val_KL : 3.450524687767029\n","Epoch: 2242/8000  Traning Loss: 95.63644027709961  Train_Reconstruction: 92.14964771270752  Train_KL: 3.4867923855781555  Validation Loss : 94.45397567749023 Val_Reconstruction : 91.01726150512695 Val_KL : 3.4367138147354126\n","Epoch: 2243/8000  Traning Loss: 95.44506645202637  Train_Reconstruction: 91.95108413696289  Train_KL: 3.4939820170402527  Validation Loss : 94.57405090332031 Val_Reconstruction : 91.12715148925781 Val_KL : 3.446897268295288\n","Epoch: 2244/8000  Traning Loss: 95.48442363739014  Train_Reconstruction: 91.98948383331299  Train_KL: 3.4949391782283783  Validation Loss : 94.3738021850586 Val_Reconstruction : 90.92102813720703 Val_KL : 3.4527775049209595\n","Epoch: 2245/8000  Traning Loss: 95.36046600341797  Train_Reconstruction: 91.85931396484375  Train_KL: 3.501151919364929  Validation Loss : 94.72098922729492 Val_Reconstruction : 91.25883865356445 Val_KL : 3.4621505737304688\n","Epoch: 2246/8000  Traning Loss: 95.7477035522461  Train_Reconstruction: 92.24629211425781  Train_KL: 3.501411944627762  Validation Loss : 95.0616455078125 Val_Reconstruction : 91.6165885925293 Val_KL : 3.44505512714386\n","Epoch: 2247/8000  Traning Loss: 95.78583431243896  Train_Reconstruction: 92.29912090301514  Train_KL: 3.4867142736911774  Validation Loss : 94.43298721313477 Val_Reconstruction : 90.98350143432617 Val_KL : 3.449487090110779\n","Epoch: 2248/8000  Traning Loss: 95.6542739868164  Train_Reconstruction: 92.15824031829834  Train_KL: 3.4960347712039948  Validation Loss : 95.01864242553711 Val_Reconstruction : 91.55184173583984 Val_KL : 3.466803193092346\n","Epoch: 2249/8000  Traning Loss: 95.84728622436523  Train_Reconstruction: 92.33617115020752  Train_KL: 3.5111153721809387  Validation Loss : 95.0252571105957 Val_Reconstruction : 91.56489944458008 Val_KL : 3.460356593132019\n","Epoch: 2250/8000  Traning Loss: 96.2316370010376  Train_Reconstruction: 92.74150562286377  Train_KL: 3.490130990743637  Validation Loss : 95.06863403320312 Val_Reconstruction : 91.63253402709961 Val_KL : 3.4361010789871216\n","Epoch: 2251/8000  Traning Loss: 96.16856288909912  Train_Reconstruction: 92.67863845825195  Train_KL: 3.489924341440201  Validation Loss : 95.36298751831055 Val_Reconstruction : 91.90851593017578 Val_KL : 3.4544681310653687\n","Epoch: 2252/8000  Traning Loss: 95.76420593261719  Train_Reconstruction: 92.26477146148682  Train_KL: 3.4994344115257263  Validation Loss : 94.61864852905273 Val_Reconstruction : 91.1563835144043 Val_KL : 3.462267756462097\n","Epoch: 2253/8000  Traning Loss: 95.52528285980225  Train_Reconstruction: 92.02485752105713  Train_KL: 3.5004259645938873  Validation Loss : 94.81063842773438 Val_Reconstruction : 91.35676193237305 Val_KL : 3.4538753032684326\n","Epoch: 2254/8000  Traning Loss: 95.51444339752197  Train_Reconstruction: 92.01349449157715  Train_KL: 3.500948816537857  Validation Loss : 94.56878662109375 Val_Reconstruction : 91.11781692504883 Val_KL : 3.4509730339050293\n","Epoch: 2255/8000  Traning Loss: 95.70941925048828  Train_Reconstruction: 92.20862579345703  Train_KL: 3.5007928907871246  Validation Loss : 95.24581146240234 Val_Reconstruction : 91.78518676757812 Val_KL : 3.460623025894165\n","Epoch: 2256/8000  Traning Loss: 96.10604953765869  Train_Reconstruction: 92.61225986480713  Train_KL: 3.4937903881073  Validation Loss : 95.01562118530273 Val_Reconstruction : 91.57129669189453 Val_KL : 3.4443215131759644\n","Epoch: 2257/8000  Traning Loss: 96.54555130004883  Train_Reconstruction: 93.04714488983154  Train_KL: 3.4984059631824493  Validation Loss : 95.32963562011719 Val_Reconstruction : 91.87747573852539 Val_KL : 3.4521595239639282\n","Epoch: 2258/8000  Traning Loss: 96.1114616394043  Train_Reconstruction: 92.62796783447266  Train_KL: 3.483494311571121  Validation Loss : 94.90564727783203 Val_Reconstruction : 91.47396469116211 Val_KL : 3.4316811561584473\n","Epoch: 2259/8000  Traning Loss: 96.08953475952148  Train_Reconstruction: 92.60626602172852  Train_KL: 3.4832687377929688  Validation Loss : 95.09353256225586 Val_Reconstruction : 91.64817428588867 Val_KL : 3.4453563690185547\n","Epoch: 2260/8000  Traning Loss: 96.04973697662354  Train_Reconstruction: 92.55568981170654  Train_KL: 3.494046241044998  Validation Loss : 94.82750701904297 Val_Reconstruction : 91.3724479675293 Val_KL : 3.4550585746765137\n","Epoch: 2261/8000  Traning Loss: 95.71129894256592  Train_Reconstruction: 92.22140789031982  Train_KL: 3.489890366792679  Validation Loss : 94.6880989074707 Val_Reconstruction : 91.2498550415039 Val_KL : 3.4382455348968506\n","Epoch: 2262/8000  Traning Loss: 95.86291408538818  Train_Reconstruction: 92.37511157989502  Train_KL: 3.487800717353821  Validation Loss : 95.20604705810547 Val_Reconstruction : 91.76004028320312 Val_KL : 3.446007013320923\n","Epoch: 2263/8000  Traning Loss: 96.25419425964355  Train_Reconstruction: 92.75797748565674  Train_KL: 3.496218055486679  Validation Loss : 95.36649322509766 Val_Reconstruction : 91.90066146850586 Val_KL : 3.4658329486846924\n","Epoch: 2264/8000  Traning Loss: 96.32047653198242  Train_Reconstruction: 92.82206153869629  Train_KL: 3.4984157383441925  Validation Loss : 95.34517669677734 Val_Reconstruction : 91.89446258544922 Val_KL : 3.450712203979492\n","Epoch: 2265/8000  Traning Loss: 96.10885047912598  Train_Reconstruction: 92.61160182952881  Train_KL: 3.497248739004135  Validation Loss : 95.40899276733398 Val_Reconstruction : 91.95637512207031 Val_KL : 3.4526166915893555\n","Epoch: 2266/8000  Traning Loss: 96.10705280303955  Train_Reconstruction: 92.61239528656006  Train_KL: 3.4946570694446564  Validation Loss : 95.07293701171875 Val_Reconstruction : 91.62088775634766 Val_KL : 3.452047824859619\n","Epoch: 2267/8000  Traning Loss: 95.74812030792236  Train_Reconstruction: 92.25714683532715  Train_KL: 3.490972876548767  Validation Loss : 94.77445602416992 Val_Reconstruction : 91.33064651489258 Val_KL : 3.4438074827194214\n","Epoch: 2268/8000  Traning Loss: 95.59899234771729  Train_Reconstruction: 92.10286903381348  Train_KL: 3.496124029159546  Validation Loss : 94.74458694458008 Val_Reconstruction : 91.2919692993164 Val_KL : 3.4526188373565674\n","Epoch: 2269/8000  Traning Loss: 95.73625469207764  Train_Reconstruction: 92.24273300170898  Train_KL: 3.4935214817523956  Validation Loss : 94.67593383789062 Val_Reconstruction : 91.23669052124023 Val_KL : 3.4392400979995728\n","Epoch: 2270/8000  Traning Loss: 95.48069190979004  Train_Reconstruction: 91.99367427825928  Train_KL: 3.4870168268680573  Validation Loss : 94.43450546264648 Val_Reconstruction : 90.98924255371094 Val_KL : 3.4452632665634155\n","Epoch: 2271/8000  Traning Loss: 95.50171852111816  Train_Reconstruction: 92.00595760345459  Train_KL: 3.49576073884964  Validation Loss : 94.85226058959961 Val_Reconstruction : 91.399658203125 Val_KL : 3.4526023864746094\n","Epoch: 2272/8000  Traning Loss: 95.63898086547852  Train_Reconstruction: 92.14354038238525  Train_KL: 3.495441645383835  Validation Loss : 94.70668029785156 Val_Reconstruction : 91.25065612792969 Val_KL : 3.456021308898926\n","Epoch: 2273/8000  Traning Loss: 95.62221431732178  Train_Reconstruction: 92.13000106811523  Train_KL: 3.492212325334549  Validation Loss : 94.912109375 Val_Reconstruction : 91.46727752685547 Val_KL : 3.444832682609558\n","Epoch: 2274/8000  Traning Loss: 95.67887115478516  Train_Reconstruction: 92.18851089477539  Train_KL: 3.4903601109981537  Validation Loss : 95.13553237915039 Val_Reconstruction : 91.69184112548828 Val_KL : 3.4436920881271362\n","Epoch: 2275/8000  Traning Loss: 96.01076412200928  Train_Reconstruction: 92.51031303405762  Train_KL: 3.5004504919052124  Validation Loss : 95.24052047729492 Val_Reconstruction : 91.78394317626953 Val_KL : 3.4565749168395996\n","Epoch: 2276/8000  Traning Loss: 95.88911533355713  Train_Reconstruction: 92.38140964508057  Train_KL: 3.507705181837082  Validation Loss : 94.56603240966797 Val_Reconstruction : 91.10013580322266 Val_KL : 3.4658995866775513\n","Epoch: 2277/8000  Traning Loss: 95.49288845062256  Train_Reconstruction: 91.9933090209961  Train_KL: 3.4995782673358917  Validation Loss : 94.39500045776367 Val_Reconstruction : 90.94119644165039 Val_KL : 3.453804850578308\n","Epoch: 2278/8000  Traning Loss: 95.39977741241455  Train_Reconstruction: 91.90792846679688  Train_KL: 3.4918475449085236  Validation Loss : 94.44845199584961 Val_Reconstruction : 91.00307083129883 Val_KL : 3.445379853248596\n","Epoch: 2279/8000  Traning Loss: 95.97669982910156  Train_Reconstruction: 92.48859691619873  Train_KL: 3.488102048635483  Validation Loss : 95.4586181640625 Val_Reconstruction : 92.01628494262695 Val_KL : 3.4423317909240723\n","Epoch: 2280/8000  Traning Loss: 95.95137119293213  Train_Reconstruction: 92.45928287506104  Train_KL: 3.492087423801422  Validation Loss : 95.16229629516602 Val_Reconstruction : 91.70154571533203 Val_KL : 3.4607502222061157\n","Epoch: 2281/8000  Traning Loss: 95.69002723693848  Train_Reconstruction: 92.19106483459473  Train_KL: 3.4989623725414276  Validation Loss : 94.68179321289062 Val_Reconstruction : 91.22521209716797 Val_KL : 3.4565815925598145\n","Epoch: 2282/8000  Traning Loss: 95.82999992370605  Train_Reconstruction: 92.32689380645752  Train_KL: 3.5031049251556396  Validation Loss : 95.42223358154297 Val_Reconstruction : 91.96944808959961 Val_KL : 3.4527865648269653\n","Epoch: 2283/8000  Traning Loss: 95.81347846984863  Train_Reconstruction: 92.31602096557617  Train_KL: 3.4974581003189087  Validation Loss : 94.95523071289062 Val_Reconstruction : 91.50713729858398 Val_KL : 3.4480931758880615\n","Epoch: 2284/8000  Traning Loss: 95.86291694641113  Train_Reconstruction: 92.37356090545654  Train_KL: 3.4893553256988525  Validation Loss : 95.38553619384766 Val_Reconstruction : 91.94145584106445 Val_KL : 3.444082021713257\n","Epoch: 2285/8000  Traning Loss: 95.80074214935303  Train_Reconstruction: 92.31393241882324  Train_KL: 3.4868091344833374  Validation Loss : 94.90009689331055 Val_Reconstruction : 91.44676971435547 Val_KL : 3.453327775001526\n","Epoch: 2286/8000  Traning Loss: 95.63857460021973  Train_Reconstruction: 92.14371871948242  Train_KL: 3.4948557317256927  Validation Loss : 94.86899948120117 Val_Reconstruction : 91.41461563110352 Val_KL : 3.4543811082839966\n","Epoch: 2287/8000  Traning Loss: 95.58077144622803  Train_Reconstruction: 92.08102321624756  Train_KL: 3.49974861741066  Validation Loss : 94.77619171142578 Val_Reconstruction : 91.33432388305664 Val_KL : 3.441866874694824\n","Epoch: 2288/8000  Traning Loss: 95.45127487182617  Train_Reconstruction: 91.95160102844238  Train_KL: 3.499673694372177  Validation Loss : 94.48524856567383 Val_Reconstruction : 91.03525924682617 Val_KL : 3.4499869346618652\n","Epoch: 2289/8000  Traning Loss: 95.60406303405762  Train_Reconstruction: 92.11461067199707  Train_KL: 3.4894519448280334  Validation Loss : 94.9603271484375 Val_Reconstruction : 91.5152702331543 Val_KL : 3.4450572729110718\n","Epoch: 2290/8000  Traning Loss: 96.24614429473877  Train_Reconstruction: 92.75278568267822  Train_KL: 3.493357241153717  Validation Loss : 95.1070671081543 Val_Reconstruction : 91.64695358276367 Val_KL : 3.4601153135299683\n","Epoch: 2291/8000  Traning Loss: 95.78305149078369  Train_Reconstruction: 92.28568649291992  Train_KL: 3.4973637759685516  Validation Loss : 94.5611457824707 Val_Reconstruction : 91.11386489868164 Val_KL : 3.4472800493240356\n","Epoch: 2292/8000  Traning Loss: 95.48789405822754  Train_Reconstruction: 91.99376392364502  Train_KL: 3.4941299855709076  Validation Loss : 94.75869750976562 Val_Reconstruction : 91.29953384399414 Val_KL : 3.4591630697250366\n","Epoch: 2293/8000  Traning Loss: 95.72945404052734  Train_Reconstruction: 92.23407173156738  Train_KL: 3.4953812658786774  Validation Loss : 94.6465072631836 Val_Reconstruction : 91.20655059814453 Val_KL : 3.4399584531784058\n","Epoch: 2294/8000  Traning Loss: 95.9350938796997  Train_Reconstruction: 92.43889236450195  Train_KL: 3.4962016940116882  Validation Loss : 95.3649673461914 Val_Reconstruction : 91.91095352172852 Val_KL : 3.4540138244628906\n","Epoch: 2295/8000  Traning Loss: 95.98384761810303  Train_Reconstruction: 92.48284816741943  Train_KL: 3.501000016927719  Validation Loss : 94.97773742675781 Val_Reconstruction : 91.52950668334961 Val_KL : 3.448232054710388\n","Epoch: 2296/8000  Traning Loss: 95.80641746520996  Train_Reconstruction: 92.3177843093872  Train_KL: 3.4886314272880554  Validation Loss : 94.78564071655273 Val_Reconstruction : 91.34364318847656 Val_KL : 3.4419991970062256\n","Epoch: 2297/8000  Traning Loss: 95.56907749176025  Train_Reconstruction: 92.08324813842773  Train_KL: 3.4858302772045135  Validation Loss : 94.63074493408203 Val_Reconstruction : 91.18484497070312 Val_KL : 3.445897340774536\n","Epoch: 2298/8000  Traning Loss: 95.53350639343262  Train_Reconstruction: 92.03555583953857  Train_KL: 3.4979505836963654  Validation Loss : 94.77917861938477 Val_Reconstruction : 91.32337951660156 Val_KL : 3.4557992219924927\n","Epoch: 2299/8000  Traning Loss: 95.57074069976807  Train_Reconstruction: 92.07526683807373  Train_KL: 3.495473027229309  Validation Loss : 95.16963958740234 Val_Reconstruction : 91.71600723266602 Val_KL : 3.453630566596985\n","Epoch: 2300/8000  Traning Loss: 96.21023464202881  Train_Reconstruction: 92.70760345458984  Train_KL: 3.5026314854621887  Validation Loss : 95.7870979309082 Val_Reconstruction : 92.3276481628418 Val_KL : 3.4594461917877197\n","Epoch: 2301/8000  Traning Loss: 96.17074871063232  Train_Reconstruction: 92.66860389709473  Train_KL: 3.5021457374095917  Validation Loss : 94.71250534057617 Val_Reconstruction : 91.2600212097168 Val_KL : 3.4524840116500854\n","Epoch: 2302/8000  Traning Loss: 95.38140392303467  Train_Reconstruction: 91.88523960113525  Train_KL: 3.4961642920970917  Validation Loss : 94.41212844848633 Val_Reconstruction : 90.96559143066406 Val_KL : 3.446536421775818\n","Epoch: 2303/8000  Traning Loss: 95.57695198059082  Train_Reconstruction: 92.0887279510498  Train_KL: 3.488223433494568  Validation Loss : 94.65126037597656 Val_Reconstruction : 91.1988754272461 Val_KL : 3.452382802963257\n","Epoch: 2304/8000  Traning Loss: 95.90474224090576  Train_Reconstruction: 92.40568828582764  Train_KL: 3.4990541338920593  Validation Loss : 94.8443832397461 Val_Reconstruction : 91.37871551513672 Val_KL : 3.4656656980514526\n","Epoch: 2305/8000  Traning Loss: 95.71938610076904  Train_Reconstruction: 92.22196674346924  Train_KL: 3.497420459985733  Validation Loss : 94.81030654907227 Val_Reconstruction : 91.3526725769043 Val_KL : 3.4576363563537598\n","Epoch: 2306/8000  Traning Loss: 95.5113525390625  Train_Reconstruction: 92.01962089538574  Train_KL: 3.4917314052581787  Validation Loss : 94.42586135864258 Val_Reconstruction : 90.97970199584961 Val_KL : 3.4461580514907837\n","Epoch: 2307/8000  Traning Loss: 95.41396522521973  Train_Reconstruction: 91.91970825195312  Train_KL: 3.4942581951618195  Validation Loss : 94.49301528930664 Val_Reconstruction : 91.03417205810547 Val_KL : 3.4588414430618286\n","Epoch: 2308/8000  Traning Loss: 95.52454948425293  Train_Reconstruction: 92.01358795166016  Train_KL: 3.510963052511215  Validation Loss : 94.68989944458008 Val_Reconstruction : 91.22982406616211 Val_KL : 3.4600772857666016\n","Epoch: 2309/8000  Traning Loss: 95.77019309997559  Train_Reconstruction: 92.27410125732422  Train_KL: 3.496091604232788  Validation Loss : 94.88934326171875 Val_Reconstruction : 91.45365142822266 Val_KL : 3.435689330101013\n","Epoch: 2310/8000  Traning Loss: 95.40921878814697  Train_Reconstruction: 91.93083667755127  Train_KL: 3.478383034467697  Validation Loss : 94.5258674621582 Val_Reconstruction : 91.08479309082031 Val_KL : 3.441071629524231\n","Epoch: 2311/8000  Traning Loss: 95.4024887084961  Train_Reconstruction: 91.90957355499268  Train_KL: 3.4929155111312866  Validation Loss : 94.41414642333984 Val_Reconstruction : 90.96258163452148 Val_KL : 3.4515682458877563\n","Epoch: 2312/8000  Traning Loss: 95.77983093261719  Train_Reconstruction: 92.2821273803711  Train_KL: 3.497703403234482  Validation Loss : 95.35793685913086 Val_Reconstruction : 91.90336990356445 Val_KL : 3.454566717147827\n","Epoch: 2313/8000  Traning Loss: 96.02160167694092  Train_Reconstruction: 92.53030395507812  Train_KL: 3.4912966191768646  Validation Loss : 95.07333374023438 Val_Reconstruction : 91.63175582885742 Val_KL : 3.441578507423401\n","Epoch: 2314/8000  Traning Loss: 95.85055637359619  Train_Reconstruction: 92.3625020980835  Train_KL: 3.488053470849991  Validation Loss : 94.80693054199219 Val_Reconstruction : 91.35591888427734 Val_KL : 3.4510107040405273\n","Epoch: 2315/8000  Traning Loss: 95.56065368652344  Train_Reconstruction: 92.06655883789062  Train_KL: 3.494093656539917  Validation Loss : 94.84850311279297 Val_Reconstruction : 91.3963851928711 Val_KL : 3.4521162509918213\n","Epoch: 2316/8000  Traning Loss: 95.61115646362305  Train_Reconstruction: 92.11111259460449  Train_KL: 3.500044882297516  Validation Loss : 94.65495300292969 Val_Reconstruction : 91.19071197509766 Val_KL : 3.4642409086227417\n","Epoch: 2317/8000  Traning Loss: 95.52004623413086  Train_Reconstruction: 92.02443313598633  Train_KL: 3.495612323284149  Validation Loss : 94.50361251831055 Val_Reconstruction : 91.06116104125977 Val_KL : 3.4424500465393066\n","Epoch: 2318/8000  Traning Loss: 95.27688598632812  Train_Reconstruction: 91.78460025787354  Train_KL: 3.4922857880592346  Validation Loss : 94.60334396362305 Val_Reconstruction : 91.1530532836914 Val_KL : 3.4502928256988525\n","Epoch: 2319/8000  Traning Loss: 95.39198112487793  Train_Reconstruction: 91.90110301971436  Train_KL: 3.490878254175186  Validation Loss : 94.9189224243164 Val_Reconstruction : 91.47274780273438 Val_KL : 3.4461746215820312\n","Epoch: 2320/8000  Traning Loss: 95.31662940979004  Train_Reconstruction: 91.82446670532227  Train_KL: 3.4921617209911346  Validation Loss : 94.49288558959961 Val_Reconstruction : 91.03735733032227 Val_KL : 3.4555294513702393\n","Epoch: 2321/8000  Traning Loss: 95.14182472229004  Train_Reconstruction: 91.63151454925537  Train_KL: 3.510310113430023  Validation Loss : 94.25104141235352 Val_Reconstruction : 90.78400802612305 Val_KL : 3.4670329093933105\n","Epoch: 2322/8000  Traning Loss: 95.42279815673828  Train_Reconstruction: 91.91997623443604  Train_KL: 3.5028212666511536  Validation Loss : 94.43709182739258 Val_Reconstruction : 90.98186111450195 Val_KL : 3.4552284479141235\n","Epoch: 2323/8000  Traning Loss: 95.28102779388428  Train_Reconstruction: 91.78663635253906  Train_KL: 3.4943920373916626  Validation Loss : 94.22989654541016 Val_Reconstruction : 90.77988815307617 Val_KL : 3.450005054473877\n","Epoch: 2324/8000  Traning Loss: 95.3869400024414  Train_Reconstruction: 91.8892011642456  Train_KL: 3.4977405071258545  Validation Loss : 94.45241165161133 Val_Reconstruction : 91.0031509399414 Val_KL : 3.449259638786316\n","Epoch: 2325/8000  Traning Loss: 95.44263458251953  Train_Reconstruction: 91.93654251098633  Train_KL: 3.506091594696045  Validation Loss : 94.75429153442383 Val_Reconstruction : 91.29283142089844 Val_KL : 3.461458206176758\n","Epoch: 2326/8000  Traning Loss: 95.58793544769287  Train_Reconstruction: 92.09285640716553  Train_KL: 3.495079815387726  Validation Loss : 94.681884765625 Val_Reconstruction : 91.24063110351562 Val_KL : 3.441251516342163\n","Epoch: 2327/8000  Traning Loss: 95.89278030395508  Train_Reconstruction: 92.3957109451294  Train_KL: 3.497069537639618  Validation Loss : 94.88224792480469 Val_Reconstruction : 91.43595123291016 Val_KL : 3.4462960958480835\n","Epoch: 2328/8000  Traning Loss: 95.27991676330566  Train_Reconstruction: 91.78076267242432  Train_KL: 3.4991544485092163  Validation Loss : 94.16299057006836 Val_Reconstruction : 90.70022964477539 Val_KL : 3.4627599716186523\n","Epoch: 2329/8000  Traning Loss: 95.09029293060303  Train_Reconstruction: 91.58678817749023  Train_KL: 3.5035053193569183  Validation Loss : 94.21321868896484 Val_Reconstruction : 90.7602767944336 Val_KL : 3.4529412984848022\n","Epoch: 2330/8000  Traning Loss: 95.21016788482666  Train_Reconstruction: 91.72147560119629  Train_KL: 3.4886921644210815  Validation Loss : 94.31095123291016 Val_Reconstruction : 90.85901260375977 Val_KL : 3.4519392251968384\n","Epoch: 2331/8000  Traning Loss: 95.56183815002441  Train_Reconstruction: 92.05279159545898  Train_KL: 3.5090459883213043  Validation Loss : 94.84175109863281 Val_Reconstruction : 91.36962890625 Val_KL : 3.4721206426620483\n","Epoch: 2332/8000  Traning Loss: 96.31079292297363  Train_Reconstruction: 92.81183910369873  Train_KL: 3.498954087495804  Validation Loss : 95.66370391845703 Val_Reconstruction : 92.22524642944336 Val_KL : 3.4384591579437256\n","Epoch: 2333/8000  Traning Loss: 96.85590648651123  Train_Reconstruction: 93.37011909484863  Train_KL: 3.485789030790329  Validation Loss : 95.51309967041016 Val_Reconstruction : 92.05865859985352 Val_KL : 3.4544416666030884\n","Epoch: 2334/8000  Traning Loss: 95.93869209289551  Train_Reconstruction: 92.4266185760498  Train_KL: 3.5120742321014404  Validation Loss : 94.56024169921875 Val_Reconstruction : 91.08816146850586 Val_KL : 3.4720818996429443\n","Epoch: 2335/8000  Traning Loss: 95.60160732269287  Train_Reconstruction: 92.0966100692749  Train_KL: 3.5049982368946075  Validation Loss : 94.75951766967773 Val_Reconstruction : 91.30960845947266 Val_KL : 3.449909806251526\n","Epoch: 2336/8000  Traning Loss: 95.88877773284912  Train_Reconstruction: 92.3934268951416  Train_KL: 3.4953514337539673  Validation Loss : 94.99189376831055 Val_Reconstruction : 91.5415267944336 Val_KL : 3.45036780834198\n","Epoch: 2337/8000  Traning Loss: 95.5782470703125  Train_Reconstruction: 92.076003074646  Train_KL: 3.502244621515274  Validation Loss : 94.42235565185547 Val_Reconstruction : 90.96574783325195 Val_KL : 3.4566060304641724\n","Epoch: 2338/8000  Traning Loss: 95.25264072418213  Train_Reconstruction: 91.74799919128418  Train_KL: 3.5046410262584686  Validation Loss : 94.55559921264648 Val_Reconstruction : 91.09540557861328 Val_KL : 3.4601962566375732\n","Epoch: 2339/8000  Traning Loss: 95.47550964355469  Train_Reconstruction: 91.98718643188477  Train_KL: 3.488324522972107  Validation Loss : 94.65838623046875 Val_Reconstruction : 91.21921920776367 Val_KL : 3.439167618751526\n","Epoch: 2340/8000  Traning Loss: 95.5641679763794  Train_Reconstruction: 92.07800579071045  Train_KL: 3.4861618876457214  Validation Loss : 94.61408233642578 Val_Reconstruction : 91.17203521728516 Val_KL : 3.44204580783844\n","Epoch: 2341/8000  Traning Loss: 95.39029693603516  Train_Reconstruction: 91.89431571960449  Train_KL: 3.4959805607795715  Validation Loss : 94.34907913208008 Val_Reconstruction : 90.88427734375 Val_KL : 3.4647998809814453\n","Epoch: 2342/8000  Traning Loss: 95.36527156829834  Train_Reconstruction: 91.8532943725586  Train_KL: 3.5119765400886536  Validation Loss : 94.66162490844727 Val_Reconstruction : 91.20281600952148 Val_KL : 3.4588085412979126\n","Epoch: 2343/8000  Traning Loss: 95.5817060470581  Train_Reconstruction: 92.08666515350342  Train_KL: 3.4950405955314636  Validation Loss : 94.82717514038086 Val_Reconstruction : 91.38938903808594 Val_KL : 3.4377843141555786\n","Epoch: 2344/8000  Traning Loss: 95.59351539611816  Train_Reconstruction: 92.10020160675049  Train_KL: 3.4933139085769653  Validation Loss : 94.49026489257812 Val_Reconstruction : 91.03338241577148 Val_KL : 3.456883192062378\n","Epoch: 2345/8000  Traning Loss: 95.32271194458008  Train_Reconstruction: 91.81983280181885  Train_KL: 3.5028796792030334  Validation Loss : 94.33965682983398 Val_Reconstruction : 90.87073516845703 Val_KL : 3.468924880027771\n","Epoch: 2346/8000  Traning Loss: 95.23576641082764  Train_Reconstruction: 91.72626495361328  Train_KL: 3.5095010697841644  Validation Loss : 94.36794662475586 Val_Reconstruction : 90.9076042175293 Val_KL : 3.4603430032730103\n","Epoch: 2347/8000  Traning Loss: 95.35566520690918  Train_Reconstruction: 91.85824871063232  Train_KL: 3.497417062520981  Validation Loss : 94.61969375610352 Val_Reconstruction : 91.16419219970703 Val_KL : 3.4555037021636963\n","Epoch: 2348/8000  Traning Loss: 95.21896553039551  Train_Reconstruction: 91.72006511688232  Train_KL: 3.4989001750946045  Validation Loss : 94.29515075683594 Val_Reconstruction : 90.83881759643555 Val_KL : 3.45633327960968\n","Epoch: 2349/8000  Traning Loss: 95.37208461761475  Train_Reconstruction: 91.87058067321777  Train_KL: 3.5015053749084473  Validation Loss : 94.67649459838867 Val_Reconstruction : 91.22705459594727 Val_KL : 3.4494396448135376\n","Epoch: 2350/8000  Traning Loss: 95.45859909057617  Train_Reconstruction: 91.95526695251465  Train_KL: 3.5033326745033264  Validation Loss : 94.53939819335938 Val_Reconstruction : 91.07889938354492 Val_KL : 3.4605000019073486\n","Epoch: 2351/8000  Traning Loss: 95.25716972351074  Train_Reconstruction: 91.75663089752197  Train_KL: 3.500538617372513  Validation Loss : 94.6295394897461 Val_Reconstruction : 91.18013763427734 Val_KL : 3.4494006633758545\n","Epoch: 2352/8000  Traning Loss: 95.71726036071777  Train_Reconstruction: 92.22523593902588  Train_KL: 3.4920226335525513  Validation Loss : 95.22146224975586 Val_Reconstruction : 91.78154754638672 Val_KL : 3.439916253089905\n","Epoch: 2353/8000  Traning Loss: 95.5715503692627  Train_Reconstruction: 92.0743989944458  Train_KL: 3.4971509873867035  Validation Loss : 94.42314147949219 Val_Reconstruction : 90.97030258178711 Val_KL : 3.45284104347229\n","Epoch: 2354/8000  Traning Loss: 95.43917274475098  Train_Reconstruction: 91.93404960632324  Train_KL: 3.505123198032379  Validation Loss : 94.73518371582031 Val_Reconstruction : 91.27429580688477 Val_KL : 3.4608877897262573\n","Epoch: 2355/8000  Traning Loss: 95.33402729034424  Train_Reconstruction: 91.83342361450195  Train_KL: 3.5006030797958374  Validation Loss : 94.39166641235352 Val_Reconstruction : 90.93584442138672 Val_KL : 3.455822229385376\n","Epoch: 2356/8000  Traning Loss: 95.14584636688232  Train_Reconstruction: 91.64639472961426  Train_KL: 3.4994515776634216  Validation Loss : 94.52477264404297 Val_Reconstruction : 91.06885147094727 Val_KL : 3.4559191465377808\n","Epoch: 2357/8000  Traning Loss: 95.07464694976807  Train_Reconstruction: 91.57747077941895  Train_KL: 3.4971780478954315  Validation Loss : 94.17458724975586 Val_Reconstruction : 90.72882080078125 Val_KL : 3.4457653760910034\n","Epoch: 2358/8000  Traning Loss: 95.1736707687378  Train_Reconstruction: 91.68898010253906  Train_KL: 3.484690636396408  Validation Loss : 94.26325988769531 Val_Reconstruction : 90.82239151000977 Val_KL : 3.4408702850341797\n","Epoch: 2359/8000  Traning Loss: 95.28765392303467  Train_Reconstruction: 91.7984733581543  Train_KL: 3.489179879426956  Validation Loss : 94.3099365234375 Val_Reconstruction : 90.85580444335938 Val_KL : 3.454132080078125\n","Epoch: 2360/8000  Traning Loss: 95.3471508026123  Train_Reconstruction: 91.84309101104736  Train_KL: 3.5040596425533295  Validation Loss : 94.91193008422852 Val_Reconstruction : 91.44573974609375 Val_KL : 3.466191053390503\n","Epoch: 2361/8000  Traning Loss: 95.59876823425293  Train_Reconstruction: 92.09697437286377  Train_KL: 3.50179386138916  Validation Loss : 94.72296142578125 Val_Reconstruction : 91.2701416015625 Val_KL : 3.452820301055908\n","Epoch: 2362/8000  Traning Loss: 95.66220378875732  Train_Reconstruction: 92.15805244445801  Train_KL: 3.5041506588459015  Validation Loss : 94.52156829833984 Val_Reconstruction : 91.0616340637207 Val_KL : 3.4599342346191406\n","Epoch: 2363/8000  Traning Loss: 95.55564975738525  Train_Reconstruction: 92.05344581604004  Train_KL: 3.5022029280662537  Validation Loss : 94.51417541503906 Val_Reconstruction : 91.0662841796875 Val_KL : 3.447889804840088\n","Epoch: 2364/8000  Traning Loss: 95.56405639648438  Train_Reconstruction: 92.07719898223877  Train_KL: 3.4868572652339935  Validation Loss : 94.73366165161133 Val_Reconstruction : 91.29069900512695 Val_KL : 3.4429619312286377\n","Epoch: 2365/8000  Traning Loss: 95.17637538909912  Train_Reconstruction: 91.68745517730713  Train_KL: 3.4889198541641235  Validation Loss : 94.19362258911133 Val_Reconstruction : 90.73779296875 Val_KL : 3.4558297395706177\n","Epoch: 2366/8000  Traning Loss: 95.33277034759521  Train_Reconstruction: 91.82573699951172  Train_KL: 3.5070334672927856  Validation Loss : 94.36279296875 Val_Reconstruction : 90.90355682373047 Val_KL : 3.4592368602752686\n","Epoch: 2367/8000  Traning Loss: 95.30297470092773  Train_Reconstruction: 91.81174373626709  Train_KL: 3.4912306368350983  Validation Loss : 94.62498474121094 Val_Reconstruction : 91.18290710449219 Val_KL : 3.442077398300171\n","Epoch: 2368/8000  Traning Loss: 95.32961082458496  Train_Reconstruction: 91.8412675857544  Train_KL: 3.4883430898189545  Validation Loss : 94.32855987548828 Val_Reconstruction : 90.88706588745117 Val_KL : 3.4414944648742676\n","Epoch: 2369/8000  Traning Loss: 95.19894218444824  Train_Reconstruction: 91.70401096343994  Train_KL: 3.494931638240814  Validation Loss : 94.38103103637695 Val_Reconstruction : 90.92644119262695 Val_KL : 3.4545894861221313\n","Epoch: 2370/8000  Traning Loss: 95.18496799468994  Train_Reconstruction: 91.6844129562378  Train_KL: 3.500555455684662  Validation Loss : 94.50841903686523 Val_Reconstruction : 91.05198287963867 Val_KL : 3.4564335346221924\n","Epoch: 2371/8000  Traning Loss: 95.45459270477295  Train_Reconstruction: 91.96086025238037  Train_KL: 3.4937316179275513  Validation Loss : 94.75764083862305 Val_Reconstruction : 91.3095474243164 Val_KL : 3.448094367980957\n","Epoch: 2372/8000  Traning Loss: 95.43083572387695  Train_Reconstruction: 91.93707942962646  Train_KL: 3.493755340576172  Validation Loss : 94.55557632446289 Val_Reconstruction : 91.10752868652344 Val_KL : 3.4480483531951904\n","Epoch: 2373/8000  Traning Loss: 95.24698543548584  Train_Reconstruction: 91.75274276733398  Train_KL: 3.4942419826984406  Validation Loss : 94.22967147827148 Val_Reconstruction : 90.7838363647461 Val_KL : 3.445835590362549\n","Epoch: 2374/8000  Traning Loss: 95.28011417388916  Train_Reconstruction: 91.77938842773438  Train_KL: 3.5007259249687195  Validation Loss : 94.55195617675781 Val_Reconstruction : 91.08736419677734 Val_KL : 3.4645917415618896\n","Epoch: 2375/8000  Traning Loss: 95.38580513000488  Train_Reconstruction: 91.87174129486084  Train_KL: 3.5140625536441803  Validation Loss : 94.47222900390625 Val_Reconstruction : 91.00478744506836 Val_KL : 3.467440128326416\n","Epoch: 2376/8000  Traning Loss: 95.1889476776123  Train_Reconstruction: 91.6877794265747  Train_KL: 3.5011675655841827  Validation Loss : 94.21471405029297 Val_Reconstruction : 90.77179718017578 Val_KL : 3.442917585372925\n","Epoch: 2377/8000  Traning Loss: 95.1182508468628  Train_Reconstruction: 91.63888549804688  Train_KL: 3.47936549782753  Validation Loss : 94.10382461547852 Val_Reconstruction : 90.67174911499023 Val_KL : 3.4320762157440186\n","Epoch: 2378/8000  Traning Loss: 95.38083076477051  Train_Reconstruction: 91.89525032043457  Train_KL: 3.485580623149872  Validation Loss : 94.47499084472656 Val_Reconstruction : 91.01917266845703 Val_KL : 3.4558180570602417\n","Epoch: 2379/8000  Traning Loss: 95.65734577178955  Train_Reconstruction: 92.15188789367676  Train_KL: 3.5054589211940765  Validation Loss : 94.89707565307617 Val_Reconstruction : 91.4366340637207 Val_KL : 3.4604415893554688\n","Epoch: 2380/8000  Traning Loss: 96.03708553314209  Train_Reconstruction: 92.53871822357178  Train_KL: 3.498367488384247  Validation Loss : 95.12865829467773 Val_Reconstruction : 91.67257690429688 Val_KL : 3.456080198287964\n","Epoch: 2381/8000  Traning Loss: 95.99585342407227  Train_Reconstruction: 92.5048246383667  Train_KL: 3.4910287857055664  Validation Loss : 94.88911056518555 Val_Reconstruction : 91.43888092041016 Val_KL : 3.4502307176589966\n","Epoch: 2382/8000  Traning Loss: 95.38660621643066  Train_Reconstruction: 91.8924036026001  Train_KL: 3.4942017197608948  Validation Loss : 94.4648208618164 Val_Reconstruction : 91.00670623779297 Val_KL : 3.4581120014190674\n","Epoch: 2383/8000  Traning Loss: 95.32356357574463  Train_Reconstruction: 91.81293678283691  Train_KL: 3.5106259882450104  Validation Loss : 94.54415893554688 Val_Reconstruction : 91.08402252197266 Val_KL : 3.460135817527771\n","Epoch: 2384/8000  Traning Loss: 95.38718223571777  Train_Reconstruction: 91.8892126083374  Train_KL: 3.497969627380371  Validation Loss : 94.74299240112305 Val_Reconstruction : 91.28415298461914 Val_KL : 3.458841919898987\n","Epoch: 2385/8000  Traning Loss: 95.61719989776611  Train_Reconstruction: 92.11218070983887  Train_KL: 3.5050188601017  Validation Loss : 94.5908203125 Val_Reconstruction : 91.12904357910156 Val_KL : 3.4617761373519897\n","Epoch: 2386/8000  Traning Loss: 95.34311389923096  Train_Reconstruction: 91.85626125335693  Train_KL: 3.486851781606674  Validation Loss : 94.61260986328125 Val_Reconstruction : 91.18588256835938 Val_KL : 3.4267263412475586\n","Epoch: 2387/8000  Traning Loss: 95.23173141479492  Train_Reconstruction: 91.75438976287842  Train_KL: 3.477341949939728  Validation Loss : 94.17275619506836 Val_Reconstruction : 90.7331428527832 Val_KL : 3.4396114349365234\n","Epoch: 2388/8000  Traning Loss: 95.21269226074219  Train_Reconstruction: 91.7148494720459  Train_KL: 3.4978432059288025  Validation Loss : 94.09674072265625 Val_Reconstruction : 90.63142013549805 Val_KL : 3.465320110321045\n","Epoch: 2389/8000  Traning Loss: 95.06418132781982  Train_Reconstruction: 91.5578556060791  Train_KL: 3.5063267946243286  Validation Loss : 94.04597854614258 Val_Reconstruction : 90.58613967895508 Val_KL : 3.459842085838318\n","Epoch: 2390/8000  Traning Loss: 95.03948783874512  Train_Reconstruction: 91.53705883026123  Train_KL: 3.5024293959140778  Validation Loss : 94.17263793945312 Val_Reconstruction : 90.7183837890625 Val_KL : 3.45425283908844\n","Epoch: 2391/8000  Traning Loss: 95.09797859191895  Train_Reconstruction: 91.60473728179932  Train_KL: 3.493240565061569  Validation Loss : 94.30558013916016 Val_Reconstruction : 90.85308456420898 Val_KL : 3.4524953365325928\n","Epoch: 2392/8000  Traning Loss: 95.12039184570312  Train_Reconstruction: 91.61796188354492  Train_KL: 3.5024293065071106  Validation Loss : 94.18997192382812 Val_Reconstruction : 90.72418212890625 Val_KL : 3.465790271759033\n","Epoch: 2393/8000  Traning Loss: 95.16743278503418  Train_Reconstruction: 91.65714836120605  Train_KL: 3.510284572839737  Validation Loss : 94.33980941772461 Val_Reconstruction : 90.88190841674805 Val_KL : 3.4579018354415894\n","Epoch: 2394/8000  Traning Loss: 95.0964584350586  Train_Reconstruction: 91.59391784667969  Train_KL: 3.502540946006775  Validation Loss : 94.12068557739258 Val_Reconstruction : 90.67702102661133 Val_KL : 3.4436635971069336\n","Epoch: 2395/8000  Traning Loss: 95.1133337020874  Train_Reconstruction: 91.61560726165771  Train_KL: 3.497725695371628  Validation Loss : 94.2104721069336 Val_Reconstruction : 90.75320434570312 Val_KL : 3.457269072532654\n","Epoch: 2396/8000  Traning Loss: 95.29626941680908  Train_Reconstruction: 91.7905445098877  Train_KL: 3.5057241916656494  Validation Loss : 94.37439727783203 Val_Reconstruction : 90.91337585449219 Val_KL : 3.461021661758423\n","Epoch: 2397/8000  Traning Loss: 95.68639278411865  Train_Reconstruction: 92.18995094299316  Train_KL: 3.4964417815208435  Validation Loss : 95.34779739379883 Val_Reconstruction : 91.89925003051758 Val_KL : 3.4485470056533813\n","Epoch: 2398/8000  Traning Loss: 96.12939929962158  Train_Reconstruction: 92.6235580444336  Train_KL: 3.5058401823043823  Validation Loss : 94.82079315185547 Val_Reconstruction : 91.34962844848633 Val_KL : 3.4711629152297974\n","Epoch: 2399/8000  Traning Loss: 95.54648399353027  Train_Reconstruction: 92.04089450836182  Train_KL: 3.50558865070343  Validation Loss : 94.29706954956055 Val_Reconstruction : 90.85022354125977 Val_KL : 3.446848750114441\n","Epoch: 2400/8000  Traning Loss: 95.13276767730713  Train_Reconstruction: 91.63759326934814  Train_KL: 3.495173007249832  Validation Loss : 94.2197380065918 Val_Reconstruction : 90.7737045288086 Val_KL : 3.446035146713257\n","Epoch: 2401/8000  Traning Loss: 95.39518547058105  Train_Reconstruction: 91.9016170501709  Train_KL: 3.493568331003189  Validation Loss : 95.03769302368164 Val_Reconstruction : 91.58981323242188 Val_KL : 3.4478793144226074\n","Epoch: 2402/8000  Traning Loss: 95.84936714172363  Train_Reconstruction: 92.34691619873047  Train_KL: 3.5024517476558685  Validation Loss : 95.11271667480469 Val_Reconstruction : 91.65413665771484 Val_KL : 3.4585787057876587\n","Epoch: 2403/8000  Traning Loss: 95.66452312469482  Train_Reconstruction: 92.16763687133789  Train_KL: 3.4968866109848022  Validation Loss : 94.87659454345703 Val_Reconstruction : 91.4350700378418 Val_KL : 3.441522240638733\n","Epoch: 2404/8000  Traning Loss: 95.35372066497803  Train_Reconstruction: 91.85885429382324  Train_KL: 3.494865357875824  Validation Loss : 94.58066177368164 Val_Reconstruction : 91.1246223449707 Val_KL : 3.4560412168502808\n","Epoch: 2405/8000  Traning Loss: 95.33832836151123  Train_Reconstruction: 91.83641338348389  Train_KL: 3.5019151270389557  Validation Loss : 94.6073226928711 Val_Reconstruction : 91.15370559692383 Val_KL : 3.4536144733428955\n","Epoch: 2406/8000  Traning Loss: 95.34532833099365  Train_Reconstruction: 91.84918880462646  Train_KL: 3.496140092611313  Validation Loss : 94.28240966796875 Val_Reconstruction : 90.82594299316406 Val_KL : 3.456467866897583\n","Epoch: 2407/8000  Traning Loss: 95.11127662658691  Train_Reconstruction: 91.60720729827881  Train_KL: 3.5040689408779144  Validation Loss : 94.2326774597168 Val_Reconstruction : 90.76802062988281 Val_KL : 3.4646583795547485\n","Epoch: 2408/8000  Traning Loss: 94.98785972595215  Train_Reconstruction: 91.49118137359619  Train_KL: 3.4966779947280884  Validation Loss : 94.3157730102539 Val_Reconstruction : 90.88542938232422 Val_KL : 3.430344343185425\n","Epoch: 2409/8000  Traning Loss: 95.3286304473877  Train_Reconstruction: 91.83971309661865  Train_KL: 3.488918423652649  Validation Loss : 94.41593933105469 Val_Reconstruction : 90.9664192199707 Val_KL : 3.4495192766189575\n","Epoch: 2410/8000  Traning Loss: 95.52366256713867  Train_Reconstruction: 92.01622772216797  Train_KL: 3.507435768842697  Validation Loss : 94.76720809936523 Val_Reconstruction : 91.30376815795898 Val_KL : 3.463441252708435\n","Epoch: 2411/8000  Traning Loss: 95.81964015960693  Train_Reconstruction: 92.31832122802734  Train_KL: 3.5013202130794525  Validation Loss : 94.71218490600586 Val_Reconstruction : 91.2636489868164 Val_KL : 3.448539137840271\n","Epoch: 2412/8000  Traning Loss: 95.28122901916504  Train_Reconstruction: 91.781569480896  Train_KL: 3.4996581375598907  Validation Loss : 94.50363159179688 Val_Reconstruction : 91.04608917236328 Val_KL : 3.4575434923171997\n","Epoch: 2413/8000  Traning Loss: 95.59077072143555  Train_Reconstruction: 92.08301734924316  Train_KL: 3.5077535212039948  Validation Loss : 95.53281784057617 Val_Reconstruction : 92.0664291381836 Val_KL : 3.466388702392578\n","Epoch: 2414/8000  Traning Loss: 96.19799900054932  Train_Reconstruction: 92.69289970397949  Train_KL: 3.5051001012325287  Validation Loss : 95.33206558227539 Val_Reconstruction : 91.87366104125977 Val_KL : 3.4584027528762817\n","Epoch: 2415/8000  Traning Loss: 95.88209342956543  Train_Reconstruction: 92.39319324493408  Train_KL: 3.4889005720615387  Validation Loss : 94.99974822998047 Val_Reconstruction : 91.56754302978516 Val_KL : 3.432205319404602\n","Epoch: 2416/8000  Traning Loss: 95.49244976043701  Train_Reconstruction: 92.00928497314453  Train_KL: 3.4831632375717163  Validation Loss : 94.3106918334961 Val_Reconstruction : 90.8765640258789 Val_KL : 3.4341291189193726\n","Epoch: 2417/8000  Traning Loss: 94.93559265136719  Train_Reconstruction: 91.44282722473145  Train_KL: 3.4927653670310974  Validation Loss : 94.02746200561523 Val_Reconstruction : 90.58932113647461 Val_KL : 3.438139319419861\n","Epoch: 2418/8000  Traning Loss: 95.05117321014404  Train_Reconstruction: 91.55168533325195  Train_KL: 3.4994879364967346  Validation Loss : 94.41464233398438 Val_Reconstruction : 90.94517517089844 Val_KL : 3.4694637060165405\n","Epoch: 2419/8000  Traning Loss: 95.14989757537842  Train_Reconstruction: 91.64284992218018  Train_KL: 3.5070464611053467  Validation Loss : 94.5050048828125 Val_Reconstruction : 91.05345153808594 Val_KL : 3.451553463935852\n","Epoch: 2420/8000  Traning Loss: 95.26465892791748  Train_Reconstruction: 91.76538562774658  Train_KL: 3.499271720647812  Validation Loss : 94.89187622070312 Val_Reconstruction : 91.43169784545898 Val_KL : 3.4601773023605347\n","Epoch: 2421/8000  Traning Loss: 95.26642322540283  Train_Reconstruction: 91.76059627532959  Train_KL: 3.505826383829117  Validation Loss : 94.2877197265625 Val_Reconstruction : 90.82088851928711 Val_KL : 3.4668309688568115\n","Epoch: 2422/8000  Traning Loss: 95.13640022277832  Train_Reconstruction: 91.6330041885376  Train_KL: 3.503396689891815  Validation Loss : 94.38429260253906 Val_Reconstruction : 90.9224624633789 Val_KL : 3.461831212043762\n","Epoch: 2423/8000  Traning Loss: 95.30288219451904  Train_Reconstruction: 91.80494976043701  Train_KL: 3.4979332089424133  Validation Loss : 94.70086288452148 Val_Reconstruction : 91.25094985961914 Val_KL : 3.4499125480651855\n","Epoch: 2424/8000  Traning Loss: 95.0053482055664  Train_Reconstruction: 91.5149040222168  Train_KL: 3.4904431104660034  Validation Loss : 94.16884231567383 Val_Reconstruction : 90.71541595458984 Val_KL : 3.453427314758301\n","Epoch: 2425/8000  Traning Loss: 95.02091598510742  Train_Reconstruction: 91.51051616668701  Train_KL: 3.510399669408798  Validation Loss : 94.39557266235352 Val_Reconstruction : 90.92215347290039 Val_KL : 3.473419427871704\n","Epoch: 2426/8000  Traning Loss: 95.13249683380127  Train_Reconstruction: 91.62047386169434  Train_KL: 3.5120221376419067  Validation Loss : 94.44091033935547 Val_Reconstruction : 90.98221969604492 Val_KL : 3.458687663078308\n","Epoch: 2427/8000  Traning Loss: 95.15304374694824  Train_Reconstruction: 91.65535736083984  Train_KL: 3.4976867139339447  Validation Loss : 94.21643829345703 Val_Reconstruction : 90.76081848144531 Val_KL : 3.455619692802429\n","Epoch: 2428/8000  Traning Loss: 95.02233505249023  Train_Reconstruction: 91.52185821533203  Train_KL: 3.5004763305187225  Validation Loss : 94.31337356567383 Val_Reconstruction : 90.85122299194336 Val_KL : 3.4621522426605225\n","Epoch: 2429/8000  Traning Loss: 95.29277992248535  Train_Reconstruction: 91.7854642868042  Train_KL: 3.5073164105415344  Validation Loss : 94.21572875976562 Val_Reconstruction : 90.75100326538086 Val_KL : 3.4647258520126343\n","Epoch: 2430/8000  Traning Loss: 95.47117137908936  Train_Reconstruction: 91.95792293548584  Train_KL: 3.5132478177547455  Validation Loss : 95.01636505126953 Val_Reconstruction : 91.54771041870117 Val_KL : 3.4686533212661743\n","Epoch: 2431/8000  Traning Loss: 95.62942218780518  Train_Reconstruction: 92.12078189849854  Train_KL: 3.5086406767368317  Validation Loss : 94.8711166381836 Val_Reconstruction : 91.39334869384766 Val_KL : 3.477768659591675\n","Epoch: 2432/8000  Traning Loss: 95.22108840942383  Train_Reconstruction: 91.71040153503418  Train_KL: 3.5106885731220245  Validation Loss : 94.00038528442383 Val_Reconstruction : 90.53060913085938 Val_KL : 3.4697794914245605\n","Epoch: 2433/8000  Traning Loss: 94.90908145904541  Train_Reconstruction: 91.41692638397217  Train_KL: 3.4921551644802094  Validation Loss : 94.01291275024414 Val_Reconstruction : 90.56937026977539 Val_KL : 3.4435431957244873\n","Epoch: 2434/8000  Traning Loss: 95.03449630737305  Train_Reconstruction: 91.54399299621582  Train_KL: 3.4905039370059967  Validation Loss : 94.33855438232422 Val_Reconstruction : 90.89083099365234 Val_KL : 3.4477208852767944\n","Epoch: 2435/8000  Traning Loss: 95.39280033111572  Train_Reconstruction: 91.88643646240234  Train_KL: 3.5063638985157013  Validation Loss : 94.85709381103516 Val_Reconstruction : 91.38358306884766 Val_KL : 3.4735103845596313\n","Epoch: 2436/8000  Traning Loss: 96.01490688323975  Train_Reconstruction: 92.50452995300293  Train_KL: 3.510375499725342  Validation Loss : 95.57522201538086 Val_Reconstruction : 92.12438583374023 Val_KL : 3.450837731361389\n","Epoch: 2437/8000  Traning Loss: 95.80727291107178  Train_Reconstruction: 92.31210041046143  Train_KL: 3.4951725900173187  Validation Loss : 94.58613586425781 Val_Reconstruction : 91.1229019165039 Val_KL : 3.463233709335327\n","Epoch: 2438/8000  Traning Loss: 95.52015495300293  Train_Reconstruction: 92.01167869567871  Train_KL: 3.50847464799881  Validation Loss : 94.80948638916016 Val_Reconstruction : 91.3557243347168 Val_KL : 3.4537607431411743\n","Epoch: 2439/8000  Traning Loss: 95.35745334625244  Train_Reconstruction: 91.85712337493896  Train_KL: 3.500330150127411  Validation Loss : 94.27024459838867 Val_Reconstruction : 90.81415176391602 Val_KL : 3.456091284751892\n","Epoch: 2440/8000  Traning Loss: 95.15434837341309  Train_Reconstruction: 91.65855503082275  Train_KL: 3.4957954585552216  Validation Loss : 94.09283828735352 Val_Reconstruction : 90.6423225402832 Val_KL : 3.4505152702331543\n","Epoch: 2441/8000  Traning Loss: 95.06175231933594  Train_Reconstruction: 91.56295013427734  Train_KL: 3.4988024830818176  Validation Loss : 94.15753173828125 Val_Reconstruction : 90.69845962524414 Val_KL : 3.4590706825256348\n","Epoch: 2442/8000  Traning Loss: 95.04832458496094  Train_Reconstruction: 91.54433155059814  Train_KL: 3.5039928257465363  Validation Loss : 94.59119033813477 Val_Reconstruction : 91.12780380249023 Val_KL : 3.4633872509002686\n","Epoch: 2443/8000  Traning Loss: 95.73356342315674  Train_Reconstruction: 92.23198127746582  Train_KL: 3.501581132411957  Validation Loss : 95.0084114074707 Val_Reconstruction : 91.55669403076172 Val_KL : 3.451717972755432\n","Epoch: 2444/8000  Traning Loss: 96.13856887817383  Train_Reconstruction: 92.63613033294678  Train_KL: 3.502438634634018  Validation Loss : 95.2894401550293 Val_Reconstruction : 91.84040451049805 Val_KL : 3.4490350484848022\n","Epoch: 2445/8000  Traning Loss: 95.65747165679932  Train_Reconstruction: 92.15988636016846  Train_KL: 3.4975850582122803  Validation Loss : 94.48526763916016 Val_Reconstruction : 91.0297622680664 Val_KL : 3.455502986907959\n","Epoch: 2446/8000  Traning Loss: 95.20253658294678  Train_Reconstruction: 91.70731925964355  Train_KL: 3.4952169954776764  Validation Loss : 93.95869445800781 Val_Reconstruction : 90.50444412231445 Val_KL : 3.454249620437622\n","Epoch: 2447/8000  Traning Loss: 94.93012714385986  Train_Reconstruction: 91.43589973449707  Train_KL: 3.494227260351181  Validation Loss : 94.16824722290039 Val_Reconstruction : 90.71324157714844 Val_KL : 3.4550055265426636\n","Epoch: 2448/8000  Traning Loss: 94.88170433044434  Train_Reconstruction: 91.37888622283936  Train_KL: 3.5028164386749268  Validation Loss : 94.0376091003418 Val_Reconstruction : 90.58316802978516 Val_KL : 3.4544410705566406\n","Epoch: 2449/8000  Traning Loss: 95.14469623565674  Train_Reconstruction: 91.64390468597412  Train_KL: 3.5007925629615784  Validation Loss : 94.68542098999023 Val_Reconstruction : 91.21775817871094 Val_KL : 3.4676607847213745\n","Epoch: 2450/8000  Traning Loss: 95.30949115753174  Train_Reconstruction: 91.7888536453247  Train_KL: 3.5206377804279327  Validation Loss : 94.23151397705078 Val_Reconstruction : 90.74381637573242 Val_KL : 3.4876983165740967\n","Epoch: 2451/8000  Traning Loss: 95.27062034606934  Train_Reconstruction: 91.76095771789551  Train_KL: 3.5096627175807953  Validation Loss : 94.51160049438477 Val_Reconstruction : 91.05905151367188 Val_KL : 3.4525506496429443\n","Epoch: 2452/8000  Traning Loss: 95.30847835540771  Train_Reconstruction: 91.81411075592041  Train_KL: 3.4943675100803375  Validation Loss : 94.22543716430664 Val_Reconstruction : 90.77074432373047 Val_KL : 3.454694986343384\n","Epoch: 2453/8000  Traning Loss: 95.1671667098999  Train_Reconstruction: 91.66156482696533  Train_KL: 3.5056022107601166  Validation Loss : 94.27322387695312 Val_Reconstruction : 90.80962753295898 Val_KL : 3.463595986366272\n","Epoch: 2454/8000  Traning Loss: 95.30462741851807  Train_Reconstruction: 91.79160213470459  Train_KL: 3.513024538755417  Validation Loss : 94.44108581542969 Val_Reconstruction : 90.97863388061523 Val_KL : 3.462449550628662\n","Epoch: 2455/8000  Traning Loss: 95.0237684249878  Train_Reconstruction: 91.51823997497559  Train_KL: 3.5055281817913055  Validation Loss : 94.25292205810547 Val_Reconstruction : 90.79726028442383 Val_KL : 3.4556615352630615\n","Epoch: 2456/8000  Traning Loss: 95.29691219329834  Train_Reconstruction: 91.79412841796875  Train_KL: 3.5027829110622406  Validation Loss : 94.53490829467773 Val_Reconstruction : 91.08161544799805 Val_KL : 3.453293561935425\n","Epoch: 2457/8000  Traning Loss: 95.74712371826172  Train_Reconstruction: 92.24325847625732  Train_KL: 3.503865122795105  Validation Loss : 95.0190658569336 Val_Reconstruction : 91.54420471191406 Val_KL : 3.474859595298767\n","Epoch: 2458/8000  Traning Loss: 96.00346755981445  Train_Reconstruction: 92.50137042999268  Train_KL: 3.502096712589264  Validation Loss : 95.37981414794922 Val_Reconstruction : 91.93132400512695 Val_KL : 3.4484893083572388\n","Epoch: 2459/8000  Traning Loss: 95.32942581176758  Train_Reconstruction: 91.83375549316406  Train_KL: 3.4956694841384888  Validation Loss : 94.87578201293945 Val_Reconstruction : 91.42080688476562 Val_KL : 3.454974889755249\n","Epoch: 2460/8000  Traning Loss: 95.31744384765625  Train_Reconstruction: 91.80651760101318  Train_KL: 3.5109252333641052  Validation Loss : 94.37490463256836 Val_Reconstruction : 90.9135513305664 Val_KL : 3.4613531827926636\n","Epoch: 2461/8000  Traning Loss: 95.33922386169434  Train_Reconstruction: 91.83778953552246  Train_KL: 3.5014339983463287  Validation Loss : 94.95853424072266 Val_Reconstruction : 91.51134490966797 Val_KL : 3.447190046310425\n","Epoch: 2462/8000  Traning Loss: 95.39624404907227  Train_Reconstruction: 91.90886116027832  Train_KL: 3.487383484840393  Validation Loss : 94.63912582397461 Val_Reconstruction : 91.20108032226562 Val_KL : 3.438044548034668\n","Epoch: 2463/8000  Traning Loss: 95.49490737915039  Train_Reconstruction: 92.00076961517334  Train_KL: 3.494137614965439  Validation Loss : 94.76020431518555 Val_Reconstruction : 91.29743194580078 Val_KL : 3.462769627571106\n","Epoch: 2464/8000  Traning Loss: 95.1558666229248  Train_Reconstruction: 91.63891315460205  Train_KL: 3.5169525146484375  Validation Loss : 94.11612701416016 Val_Reconstruction : 90.64929962158203 Val_KL : 3.466825485229492\n","Epoch: 2465/8000  Traning Loss: 95.24547386169434  Train_Reconstruction: 91.74513816833496  Train_KL: 3.5003357529640198  Validation Loss : 94.28357315063477 Val_Reconstruction : 90.84134674072266 Val_KL : 3.4422260522842407\n","Epoch: 2466/8000  Traning Loss: 95.15618228912354  Train_Reconstruction: 91.6777114868164  Train_KL: 3.478470265865326  Validation Loss : 94.20825958251953 Val_Reconstruction : 90.76349258422852 Val_KL : 3.4447693824768066\n","Epoch: 2467/8000  Traning Loss: 94.88569450378418  Train_Reconstruction: 91.38405990600586  Train_KL: 3.50163471698761  Validation Loss : 93.78366088867188 Val_Reconstruction : 90.31314086914062 Val_KL : 3.470517873764038\n","Epoch: 2468/8000  Traning Loss: 94.87315464019775  Train_Reconstruction: 91.37191009521484  Train_KL: 3.501244068145752  Validation Loss : 94.23315811157227 Val_Reconstruction : 90.77605819702148 Val_KL : 3.4571025371551514\n","Epoch: 2469/8000  Traning Loss: 95.05024433135986  Train_Reconstruction: 91.54416084289551  Train_KL: 3.5060828030109406  Validation Loss : 94.65907669067383 Val_Reconstruction : 91.20405197143555 Val_KL : 3.455024242401123\n","Epoch: 2470/8000  Traning Loss: 95.25842761993408  Train_Reconstruction: 91.75401306152344  Train_KL: 3.5044135749340057  Validation Loss : 94.49528884887695 Val_Reconstruction : 91.03635787963867 Val_KL : 3.458929181098938\n","Epoch: 2471/8000  Traning Loss: 94.83897972106934  Train_Reconstruction: 91.33371829986572  Train_KL: 3.505261719226837  Validation Loss : 93.9772720336914 Val_Reconstruction : 90.51169967651367 Val_KL : 3.465572953224182\n","Epoch: 2472/8000  Traning Loss: 94.99227523803711  Train_Reconstruction: 91.48958969116211  Train_KL: 3.502685010433197  Validation Loss : 94.52117156982422 Val_Reconstruction : 91.06224060058594 Val_KL : 3.4589338302612305\n","Epoch: 2473/8000  Traning Loss: 95.39919471740723  Train_Reconstruction: 91.9019136428833  Train_KL: 3.4972804188728333  Validation Loss : 94.71217346191406 Val_Reconstruction : 91.2626724243164 Val_KL : 3.4495004415512085\n","Epoch: 2474/8000  Traning Loss: 95.65189361572266  Train_Reconstruction: 92.15397834777832  Train_KL: 3.4979155361652374  Validation Loss : 95.51421356201172 Val_Reconstruction : 92.0623779296875 Val_KL : 3.451833724975586\n","Epoch: 2475/8000  Traning Loss: 96.53335285186768  Train_Reconstruction: 93.02535152435303  Train_KL: 3.508001059293747  Validation Loss : 95.3512077331543 Val_Reconstruction : 91.88742446899414 Val_KL : 3.4637826681137085\n","Epoch: 2476/8000  Traning Loss: 96.03215217590332  Train_Reconstruction: 92.5346908569336  Train_KL: 3.497461438179016  Validation Loss : 94.93856430053711 Val_Reconstruction : 91.48970413208008 Val_KL : 3.448858141899109\n","Epoch: 2477/8000  Traning Loss: 95.39929580688477  Train_Reconstruction: 91.90023231506348  Train_KL: 3.499063551425934  Validation Loss : 94.33329391479492 Val_Reconstruction : 90.87147521972656 Val_KL : 3.4618178606033325\n","Epoch: 2478/8000  Traning Loss: 95.51939296722412  Train_Reconstruction: 92.01736831665039  Train_KL: 3.5020237267017365  Validation Loss : 94.80041885375977 Val_Reconstruction : 91.33870315551758 Val_KL : 3.461715817451477\n","Epoch: 2479/8000  Traning Loss: 95.23099994659424  Train_Reconstruction: 91.7352237701416  Train_KL: 3.4957765340805054  Validation Loss : 94.5416488647461 Val_Reconstruction : 91.09218215942383 Val_KL : 3.449469566345215\n","Epoch: 2480/8000  Traning Loss: 95.03802013397217  Train_Reconstruction: 91.53855419158936  Train_KL: 3.499465376138687  Validation Loss : 93.95694351196289 Val_Reconstruction : 90.4940185546875 Val_KL : 3.4629228115081787\n","Epoch: 2481/8000  Traning Loss: 94.89071559906006  Train_Reconstruction: 91.38940525054932  Train_KL: 3.5013096034526825  Validation Loss : 93.94018936157227 Val_Reconstruction : 90.48611831665039 Val_KL : 3.4540719985961914\n","Epoch: 2482/8000  Traning Loss: 94.77949333190918  Train_Reconstruction: 91.28075408935547  Train_KL: 3.498739629983902  Validation Loss : 93.91058349609375 Val_Reconstruction : 90.45568084716797 Val_KL : 3.4549014568328857\n","Epoch: 2483/8000  Traning Loss: 95.1445894241333  Train_Reconstruction: 91.64881134033203  Train_KL: 3.4957783222198486  Validation Loss : 94.19769668579102 Val_Reconstruction : 90.74348831176758 Val_KL : 3.454209089279175\n","Epoch: 2484/8000  Traning Loss: 94.94762706756592  Train_Reconstruction: 91.4499340057373  Train_KL: 3.49769389629364  Validation Loss : 94.21521759033203 Val_Reconstruction : 90.75883102416992 Val_KL : 3.4563876390457153\n","Epoch: 2485/8000  Traning Loss: 95.04348850250244  Train_Reconstruction: 91.54092025756836  Train_KL: 3.5025670528411865  Validation Loss : 94.11573028564453 Val_Reconstruction : 90.64995956420898 Val_KL : 3.4657695293426514\n","Epoch: 2486/8000  Traning Loss: 95.0181074142456  Train_Reconstruction: 91.51218128204346  Train_KL: 3.505925476551056  Validation Loss : 94.06170654296875 Val_Reconstruction : 90.60641479492188 Val_KL : 3.455290913581848\n","Epoch: 2487/8000  Traning Loss: 94.9839038848877  Train_Reconstruction: 91.4857702255249  Train_KL: 3.498135656118393  Validation Loss : 94.3011474609375 Val_Reconstruction : 90.84266662597656 Val_KL : 3.4584800004959106\n","Epoch: 2488/8000  Traning Loss: 94.98771572113037  Train_Reconstruction: 91.48462104797363  Train_KL: 3.5030954480171204  Validation Loss : 94.32410430908203 Val_Reconstruction : 90.86958312988281 Val_KL : 3.4545217752456665\n","Epoch: 2489/8000  Traning Loss: 95.1262559890747  Train_Reconstruction: 91.63202285766602  Train_KL: 3.494232475757599  Validation Loss : 94.49234390258789 Val_Reconstruction : 91.0434341430664 Val_KL : 3.448907732963562\n","Epoch: 2490/8000  Traning Loss: 94.91873931884766  Train_Reconstruction: 91.41437339782715  Train_KL: 3.5043667256832123  Validation Loss : 94.1918830871582 Val_Reconstruction : 90.72540283203125 Val_KL : 3.4664807319641113\n","Epoch: 2491/8000  Traning Loss: 94.84565925598145  Train_Reconstruction: 91.32849597930908  Train_KL: 3.5171629786491394  Validation Loss : 94.34447479248047 Val_Reconstruction : 90.87361526489258 Val_KL : 3.470862030982971\n","Epoch: 2492/8000  Traning Loss: 95.21199798583984  Train_Reconstruction: 91.71179008483887  Train_KL: 3.5002071261405945  Validation Loss : 94.4588851928711 Val_Reconstruction : 91.01704025268555 Val_KL : 3.44184410572052\n","Epoch: 2493/8000  Traning Loss: 95.19285583496094  Train_Reconstruction: 91.70746612548828  Train_KL: 3.4853902459144592  Validation Loss : 94.48526000976562 Val_Reconstruction : 91.04634475708008 Val_KL : 3.4389140605926514\n","Epoch: 2494/8000  Traning Loss: 95.0802755355835  Train_Reconstruction: 91.57982730865479  Train_KL: 3.500446707010269  Validation Loss : 94.16834259033203 Val_Reconstruction : 90.70181274414062 Val_KL : 3.466529369354248\n","Epoch: 2495/8000  Traning Loss: 94.99233436584473  Train_Reconstruction: 91.48554801940918  Train_KL: 3.506786286830902  Validation Loss : 94.31304550170898 Val_Reconstruction : 90.85120391845703 Val_KL : 3.4618419408798218\n","Epoch: 2496/8000  Traning Loss: 95.28820991516113  Train_Reconstruction: 91.77703380584717  Train_KL: 3.5111751556396484  Validation Loss : 94.40288162231445 Val_Reconstruction : 90.93342971801758 Val_KL : 3.469449043273926\n","Epoch: 2497/8000  Traning Loss: 95.33351230621338  Train_Reconstruction: 91.82767486572266  Train_KL: 3.5058389604091644  Validation Loss : 94.36322784423828 Val_Reconstruction : 90.90096282958984 Val_KL : 3.4622642993927\n","Epoch: 2498/8000  Traning Loss: 94.86974143981934  Train_Reconstruction: 91.36752128601074  Train_KL: 3.5022192299365997  Validation Loss : 93.76419448852539 Val_Reconstruction : 90.30558776855469 Val_KL : 3.45860493183136\n","Epoch: 2499/8000  Traning Loss: 94.81859111785889  Train_Reconstruction: 91.31785011291504  Train_KL: 3.50074103474617  Validation Loss : 93.92723083496094 Val_Reconstruction : 90.4737548828125 Val_KL : 3.453476667404175\n","Epoch: 2500/8000  Traning Loss: 94.81030464172363  Train_Reconstruction: 91.30859184265137  Train_KL: 3.501713275909424  Validation Loss : 93.94744110107422 Val_Reconstruction : 90.48248672485352 Val_KL : 3.464954137802124\n","Epoch: 2501/8000  Traning Loss: 94.89747619628906  Train_Reconstruction: 91.40023612976074  Train_KL: 3.4972402453422546  Validation Loss : 93.86412811279297 Val_Reconstruction : 90.40800857543945 Val_KL : 3.456118583679199\n","Epoch: 2502/8000  Traning Loss: 95.02439022064209  Train_Reconstruction: 91.52716255187988  Train_KL: 3.497228354215622  Validation Loss : 94.59403610229492 Val_Reconstruction : 91.1407241821289 Val_KL : 3.4533138275146484\n","Epoch: 2503/8000  Traning Loss: 95.05262756347656  Train_Reconstruction: 91.55238246917725  Train_KL: 3.5002448558807373  Validation Loss : 94.20451736450195 Val_Reconstruction : 90.75028991699219 Val_KL : 3.454229235649109\n","Epoch: 2504/8000  Traning Loss: 95.00029563903809  Train_Reconstruction: 91.50147914886475  Train_KL: 3.4988159239292145  Validation Loss : 94.14693069458008 Val_Reconstruction : 90.68573379516602 Val_KL : 3.4611977338790894\n","Epoch: 2505/8000  Traning Loss: 95.04409885406494  Train_Reconstruction: 91.5365629196167  Train_KL: 3.5075376331806183  Validation Loss : 94.02557754516602 Val_Reconstruction : 90.5554084777832 Val_KL : 3.4701662063598633\n","Epoch: 2506/8000  Traning Loss: 94.98517799377441  Train_Reconstruction: 91.48602390289307  Train_KL: 3.4991539418697357  Validation Loss : 93.86376571655273 Val_Reconstruction : 90.41732025146484 Val_KL : 3.4464457035064697\n","Epoch: 2507/8000  Traning Loss: 94.88019561767578  Train_Reconstruction: 91.38944721221924  Train_KL: 3.490748018026352  Validation Loss : 94.32102966308594 Val_Reconstruction : 90.88052368164062 Val_KL : 3.440503478050232\n","Epoch: 2508/8000  Traning Loss: 94.96599864959717  Train_Reconstruction: 91.46944999694824  Train_KL: 3.496549129486084  Validation Loss : 94.39117813110352 Val_Reconstruction : 90.93952560424805 Val_KL : 3.451654076576233\n","Epoch: 2509/8000  Traning Loss: 95.00027179718018  Train_Reconstruction: 91.49368190765381  Train_KL: 3.506588578224182  Validation Loss : 94.33417510986328 Val_Reconstruction : 90.87388229370117 Val_KL : 3.4602932929992676\n","Epoch: 2510/8000  Traning Loss: 95.01056480407715  Train_Reconstruction: 91.51919174194336  Train_KL: 3.4913738667964935  Validation Loss : 94.19127655029297 Val_Reconstruction : 90.74518203735352 Val_KL : 3.446096658706665\n","Epoch: 2511/8000  Traning Loss: 94.92542839050293  Train_Reconstruction: 91.42620754241943  Train_KL: 3.4992199540138245  Validation Loss : 94.0152816772461 Val_Reconstruction : 90.55068588256836 Val_KL : 3.464595675468445\n","Epoch: 2512/8000  Traning Loss: 94.76334476470947  Train_Reconstruction: 91.25765228271484  Train_KL: 3.505692631006241  Validation Loss : 93.7986068725586 Val_Reconstruction : 90.34148025512695 Val_KL : 3.457124352455139\n","Epoch: 2513/8000  Traning Loss: 94.68880462646484  Train_Reconstruction: 91.1968765258789  Train_KL: 3.491928219795227  Validation Loss : 93.6982650756836 Val_Reconstruction : 90.24539184570312 Val_KL : 3.452874779701233\n","Epoch: 2514/8000  Traning Loss: 94.4409122467041  Train_Reconstruction: 90.94216442108154  Train_KL: 3.4987470507621765  Validation Loss : 93.6290512084961 Val_Reconstruction : 90.16686248779297 Val_KL : 3.4621881246566772\n","Epoch: 2515/8000  Traning Loss: 94.95474624633789  Train_Reconstruction: 91.44952297210693  Train_KL: 3.5052221417427063  Validation Loss : 94.22909927368164 Val_Reconstruction : 90.76263809204102 Val_KL : 3.4664628505706787\n","Epoch: 2516/8000  Traning Loss: 94.92593574523926  Train_Reconstruction: 91.42218685150146  Train_KL: 3.503748893737793  Validation Loss : 94.10177230834961 Val_Reconstruction : 90.64900588989258 Val_KL : 3.452765703201294\n","Epoch: 2517/8000  Traning Loss: 94.77596473693848  Train_Reconstruction: 91.28133773803711  Train_KL: 3.494626998901367  Validation Loss : 94.03181076049805 Val_Reconstruction : 90.58160018920898 Val_KL : 3.4502108097076416\n","Epoch: 2518/8000  Traning Loss: 94.81843852996826  Train_Reconstruction: 91.31630516052246  Train_KL: 3.50213360786438  Validation Loss : 94.28690719604492 Val_Reconstruction : 90.82545471191406 Val_KL : 3.461452603340149\n","Epoch: 2519/8000  Traning Loss: 95.00986576080322  Train_Reconstruction: 91.51084041595459  Train_KL: 3.4990250766277313  Validation Loss : 94.48169708251953 Val_Reconstruction : 91.01730728149414 Val_KL : 3.464387893676758\n","Epoch: 2520/8000  Traning Loss: 94.80744361877441  Train_Reconstruction: 91.30464935302734  Train_KL: 3.5027944445610046  Validation Loss : 93.95553970336914 Val_Reconstruction : 90.4950065612793 Val_KL : 3.4605331420898438\n","Epoch: 2521/8000  Traning Loss: 95.00260829925537  Train_Reconstruction: 91.49067115783691  Train_KL: 3.511937230825424  Validation Loss : 94.19081115722656 Val_Reconstruction : 90.7213363647461 Val_KL : 3.469475507736206\n","Epoch: 2522/8000  Traning Loss: 95.02340412139893  Train_Reconstruction: 91.52973461151123  Train_KL: 3.493669271469116  Validation Loss : 94.07366561889648 Val_Reconstruction : 90.62297058105469 Val_KL : 3.450693726539612\n","Epoch: 2523/8000  Traning Loss: 94.8011531829834  Train_Reconstruction: 91.30581474304199  Train_KL: 3.4953371584415436  Validation Loss : 93.84296035766602 Val_Reconstruction : 90.38969802856445 Val_KL : 3.453260898590088\n","Epoch: 2524/8000  Traning Loss: 94.75939178466797  Train_Reconstruction: 91.25453281402588  Train_KL: 3.5048580169677734  Validation Loss : 93.70866775512695 Val_Reconstruction : 90.24678802490234 Val_KL : 3.461877703666687\n","Epoch: 2525/8000  Traning Loss: 94.925705909729  Train_Reconstruction: 91.42140197753906  Train_KL: 3.5043044984340668  Validation Loss : 94.2179946899414 Val_Reconstruction : 90.75576400756836 Val_KL : 3.4622294902801514\n","Epoch: 2526/8000  Traning Loss: 95.30811977386475  Train_Reconstruction: 91.81430053710938  Train_KL: 3.493819624185562  Validation Loss : 94.42161178588867 Val_Reconstruction : 90.97644424438477 Val_KL : 3.4451690912246704\n","Epoch: 2527/8000  Traning Loss: 95.19169521331787  Train_Reconstruction: 91.69916152954102  Train_KL: 3.492532730102539  Validation Loss : 94.69292831420898 Val_Reconstruction : 91.24454879760742 Val_KL : 3.448380470275879\n","Epoch: 2528/8000  Traning Loss: 95.76608085632324  Train_Reconstruction: 92.27027320861816  Train_KL: 3.4958081543445587  Validation Loss : 95.08216857910156 Val_Reconstruction : 91.61793899536133 Val_KL : 3.464229464530945\n","Epoch: 2529/8000  Traning Loss: 96.11647033691406  Train_Reconstruction: 92.60698509216309  Train_KL: 3.5094848573207855  Validation Loss : 95.11493301391602 Val_Reconstruction : 91.65176773071289 Val_KL : 3.463164448738098\n","Epoch: 2530/8000  Traning Loss: 96.3380298614502  Train_Reconstruction: 92.83893203735352  Train_KL: 3.499097913503647  Validation Loss : 95.96632766723633 Val_Reconstruction : 92.51216125488281 Val_KL : 3.4541643857955933\n","Epoch: 2531/8000  Traning Loss: 96.54145622253418  Train_Reconstruction: 93.03566074371338  Train_KL: 3.505796790122986  Validation Loss : 95.5301284790039 Val_Reconstruction : 92.07147979736328 Val_KL : 3.458647131919861\n","Epoch: 2532/8000  Traning Loss: 95.93229675292969  Train_Reconstruction: 92.4298038482666  Train_KL: 3.502492278814316  Validation Loss : 94.3541030883789 Val_Reconstruction : 90.8994369506836 Val_KL : 3.454667806625366\n","Epoch: 2533/8000  Traning Loss: 95.30144214630127  Train_Reconstruction: 91.81013107299805  Train_KL: 3.4913113713264465  Validation Loss : 95.29012298583984 Val_Reconstruction : 91.84529495239258 Val_KL : 3.4448293447494507\n","Epoch: 2534/8000  Traning Loss: 95.3390884399414  Train_Reconstruction: 91.84979248046875  Train_KL: 3.489294797182083  Validation Loss : 94.30277633666992 Val_Reconstruction : 90.85017395019531 Val_KL : 3.4526047706604004\n","Epoch: 2535/8000  Traning Loss: 94.8337574005127  Train_Reconstruction: 91.3388319015503  Train_KL: 3.494925230741501  Validation Loss : 94.22000503540039 Val_Reconstruction : 90.77224731445312 Val_KL : 3.4477566480636597\n","Epoch: 2536/8000  Traning Loss: 94.89906311035156  Train_Reconstruction: 91.40661430358887  Train_KL: 3.4924489557743073  Validation Loss : 94.06317520141602 Val_Reconstruction : 90.61791610717773 Val_KL : 3.445258617401123\n","Epoch: 2537/8000  Traning Loss: 94.88518524169922  Train_Reconstruction: 91.38782787322998  Train_KL: 3.497357577085495  Validation Loss : 94.1529312133789 Val_Reconstruction : 90.7009506225586 Val_KL : 3.4519824981689453\n","Epoch: 2538/8000  Traning Loss: 94.85910415649414  Train_Reconstruction: 91.36412620544434  Train_KL: 3.4949783980846405  Validation Loss : 94.04448318481445 Val_Reconstruction : 90.59336471557617 Val_KL : 3.4511168003082275\n","Epoch: 2539/8000  Traning Loss: 95.04112243652344  Train_Reconstruction: 91.53741550445557  Train_KL: 3.503705859184265  Validation Loss : 94.39763259887695 Val_Reconstruction : 90.92962646484375 Val_KL : 3.468003988265991\n","Epoch: 2540/8000  Traning Loss: 94.90490818023682  Train_Reconstruction: 91.40259838104248  Train_KL: 3.50231009721756  Validation Loss : 93.85275650024414 Val_Reconstruction : 90.40068435668945 Val_KL : 3.4520719051361084\n","Epoch: 2541/8000  Traning Loss: 94.52035808563232  Train_Reconstruction: 91.02212715148926  Train_KL: 3.498230367898941  Validation Loss : 93.77555084228516 Val_Reconstruction : 90.31292724609375 Val_KL : 3.46262264251709\n","Epoch: 2542/8000  Traning Loss: 94.59486770629883  Train_Reconstruction: 91.08906078338623  Train_KL: 3.5058058202266693  Validation Loss : 93.9455451965332 Val_Reconstruction : 90.48193359375 Val_KL : 3.4636127948760986\n","Epoch: 2543/8000  Traning Loss: 94.79603672027588  Train_Reconstruction: 91.29372787475586  Train_KL: 3.5023091435432434  Validation Loss : 94.33158874511719 Val_Reconstruction : 90.87117767333984 Val_KL : 3.4604135751724243\n","Epoch: 2544/8000  Traning Loss: 95.30801677703857  Train_Reconstruction: 91.8114128112793  Train_KL: 3.4966049790382385  Validation Loss : 95.07070922851562 Val_Reconstruction : 91.61639785766602 Val_KL : 3.4543097019195557\n","Epoch: 2545/8000  Traning Loss: 95.85952758789062  Train_Reconstruction: 92.35667133331299  Train_KL: 3.5028559267520905  Validation Loss : 95.1954345703125 Val_Reconstruction : 91.73664474487305 Val_KL : 3.45879065990448\n","Epoch: 2546/8000  Traning Loss: 95.40431499481201  Train_Reconstruction: 91.89860153198242  Train_KL: 3.505713850259781  Validation Loss : 94.34174728393555 Val_Reconstruction : 90.8857192993164 Val_KL : 3.4560306072235107\n","Epoch: 2547/8000  Traning Loss: 95.01208782196045  Train_Reconstruction: 91.50835132598877  Train_KL: 3.5037381649017334  Validation Loss : 94.01349639892578 Val_Reconstruction : 90.55173110961914 Val_KL : 3.461766004562378\n","Epoch: 2548/8000  Traning Loss: 94.71062278747559  Train_Reconstruction: 91.2157096862793  Train_KL: 3.4949137568473816  Validation Loss : 93.82685089111328 Val_Reconstruction : 90.37224578857422 Val_KL : 3.4546051025390625\n","Epoch: 2549/8000  Traning Loss: 94.64155673980713  Train_Reconstruction: 91.14122009277344  Train_KL: 3.5003368854522705  Validation Loss : 93.75382232666016 Val_Reconstruction : 90.29064559936523 Val_KL : 3.463175415992737\n","Epoch: 2550/8000  Traning Loss: 94.96299648284912  Train_Reconstruction: 91.46363067626953  Train_KL: 3.499365657567978  Validation Loss : 94.55308151245117 Val_Reconstruction : 91.1065788269043 Val_KL : 3.4465036392211914\n","Epoch: 2551/8000  Traning Loss: 95.77678108215332  Train_Reconstruction: 92.27803134918213  Train_KL: 3.498748868703842  Validation Loss : 95.11709213256836 Val_Reconstruction : 91.66008758544922 Val_KL : 3.457003355026245\n","Epoch: 2552/8000  Traning Loss: 95.93940830230713  Train_Reconstruction: 92.43243885040283  Train_KL: 3.5069691836833954  Validation Loss : 94.91171646118164 Val_Reconstruction : 91.44678115844727 Val_KL : 3.464935779571533\n","Epoch: 2553/8000  Traning Loss: 95.66694068908691  Train_Reconstruction: 92.16701126098633  Train_KL: 3.499929428100586  Validation Loss : 94.55035018920898 Val_Reconstruction : 91.09812545776367 Val_KL : 3.452221989631653\n","Epoch: 2554/8000  Traning Loss: 95.02809143066406  Train_Reconstruction: 91.53348159790039  Train_KL: 3.494608849287033  Validation Loss : 94.32845306396484 Val_Reconstruction : 90.86882019042969 Val_KL : 3.459633946418762\n","Epoch: 2555/8000  Traning Loss: 94.92778873443604  Train_Reconstruction: 91.42343997955322  Train_KL: 3.5043483674526215  Validation Loss : 94.34127807617188 Val_Reconstruction : 90.87837600708008 Val_KL : 3.4628994464874268\n","Epoch: 2556/8000  Traning Loss: 95.08699417114258  Train_Reconstruction: 91.58302211761475  Train_KL: 3.5039726197719574  Validation Loss : 94.37631225585938 Val_Reconstruction : 90.92002487182617 Val_KL : 3.456284761428833\n","Epoch: 2557/8000  Traning Loss: 94.89996147155762  Train_Reconstruction: 91.40507793426514  Train_KL: 3.494883745908737  Validation Loss : 93.86079406738281 Val_Reconstruction : 90.3953971862793 Val_KL : 3.465395927429199\n","Epoch: 2558/8000  Traning Loss: 94.97074222564697  Train_Reconstruction: 91.45956039428711  Train_KL: 3.511180132627487  Validation Loss : 94.58331680297852 Val_Reconstruction : 91.11093521118164 Val_KL : 3.4723790884017944\n","Epoch: 2559/8000  Traning Loss: 95.1619520187378  Train_Reconstruction: 91.6576566696167  Train_KL: 3.5042949318885803  Validation Loss : 94.59061431884766 Val_Reconstruction : 91.12743377685547 Val_KL : 3.4631799459457397\n","Epoch: 2560/8000  Traning Loss: 95.36639308929443  Train_Reconstruction: 91.86138153076172  Train_KL: 3.5050108730793  Validation Loss : 94.9613037109375 Val_Reconstruction : 91.49214553833008 Val_KL : 3.4691561460494995\n","Epoch: 2561/8000  Traning Loss: 95.87411308288574  Train_Reconstruction: 92.36849594116211  Train_KL: 3.5056174993515015  Validation Loss : 95.27730560302734 Val_Reconstruction : 91.80357360839844 Val_KL : 3.4737335443496704\n","Epoch: 2562/8000  Traning Loss: 95.541428565979  Train_Reconstruction: 92.02834892272949  Train_KL: 3.513080358505249  Validation Loss : 94.72906875610352 Val_Reconstruction : 91.26641464233398 Val_KL : 3.462655544281006\n","Epoch: 2563/8000  Traning Loss: 95.47754383087158  Train_Reconstruction: 91.97491264343262  Train_KL: 3.502631962299347  Validation Loss : 94.52631759643555 Val_Reconstruction : 91.07184219360352 Val_KL : 3.454474925994873\n","Epoch: 2564/8000  Traning Loss: 95.00427532196045  Train_Reconstruction: 91.50130558013916  Train_KL: 3.5029694736003876  Validation Loss : 94.12266540527344 Val_Reconstruction : 90.66651153564453 Val_KL : 3.4561535120010376\n","Epoch: 2565/8000  Traning Loss: 94.68063735961914  Train_Reconstruction: 91.174560546875  Train_KL: 3.506077319383621  Validation Loss : 93.9471321105957 Val_Reconstruction : 90.48255157470703 Val_KL : 3.4645793437957764\n","Epoch: 2566/8000  Traning Loss: 94.89751434326172  Train_Reconstruction: 91.39949131011963  Train_KL: 3.4980233311653137  Validation Loss : 93.88813018798828 Val_Reconstruction : 90.4425163269043 Val_KL : 3.4456125497817993\n","Epoch: 2567/8000  Traning Loss: 94.6150255203247  Train_Reconstruction: 91.1177339553833  Train_KL: 3.497292220592499  Validation Loss : 93.68736267089844 Val_Reconstruction : 90.2362060546875 Val_KL : 3.4511555433273315\n","Epoch: 2568/8000  Traning Loss: 94.63730812072754  Train_Reconstruction: 91.13598918914795  Train_KL: 3.501319497823715  Validation Loss : 94.04434585571289 Val_Reconstruction : 90.58541870117188 Val_KL : 3.458927035331726\n","Epoch: 2569/8000  Traning Loss: 94.61390018463135  Train_Reconstruction: 91.1022596359253  Train_KL: 3.511640429496765  Validation Loss : 93.75963592529297 Val_Reconstruction : 90.28605270385742 Val_KL : 3.4735864400863647\n","Epoch: 2570/8000  Traning Loss: 94.47061252593994  Train_Reconstruction: 90.95735931396484  Train_KL: 3.5132531821727753  Validation Loss : 93.6641731262207 Val_Reconstruction : 90.19304275512695 Val_KL : 3.4711318016052246\n","Epoch: 2571/8000  Traning Loss: 94.38739490509033  Train_Reconstruction: 90.88432312011719  Train_KL: 3.503071278333664  Validation Loss : 93.66291046142578 Val_Reconstruction : 90.21491241455078 Val_KL : 3.4479974508285522\n","Epoch: 2572/8000  Traning Loss: 94.5085334777832  Train_Reconstruction: 91.00767040252686  Train_KL: 3.500863164663315  Validation Loss : 93.88447189331055 Val_Reconstruction : 90.42071533203125 Val_KL : 3.463753581047058\n","Epoch: 2573/8000  Traning Loss: 94.8777666091919  Train_Reconstruction: 91.36510753631592  Train_KL: 3.5126598179340363  Validation Loss : 94.23077011108398 Val_Reconstruction : 90.75617218017578 Val_KL : 3.474596858024597\n","Epoch: 2574/8000  Traning Loss: 95.08548641204834  Train_Reconstruction: 91.57630443572998  Train_KL: 3.5091826617717743  Validation Loss : 94.36825180053711 Val_Reconstruction : 90.91215133666992 Val_KL : 3.456100344657898\n","Epoch: 2575/8000  Traning Loss: 95.29116821289062  Train_Reconstruction: 91.80145645141602  Train_KL: 3.4897115528583527  Validation Loss : 94.84989929199219 Val_Reconstruction : 91.40096664428711 Val_KL : 3.4489328861236572\n","Epoch: 2576/8000  Traning Loss: 95.27677154541016  Train_Reconstruction: 91.77199649810791  Train_KL: 3.5047741532325745  Validation Loss : 94.28070449829102 Val_Reconstruction : 90.8152084350586 Val_KL : 3.465493321418762\n","Epoch: 2577/8000  Traning Loss: 94.83520793914795  Train_Reconstruction: 91.3250207901001  Train_KL: 3.5101875364780426  Validation Loss : 94.16511917114258 Val_Reconstruction : 90.70249938964844 Val_KL : 3.4626219272613525\n","Epoch: 2578/8000  Traning Loss: 94.797926902771  Train_Reconstruction: 91.29881954193115  Train_KL: 3.4991058111190796  Validation Loss : 94.25213623046875 Val_Reconstruction : 90.80689239501953 Val_KL : 3.445243000984192\n","Epoch: 2579/8000  Traning Loss: 94.91408157348633  Train_Reconstruction: 91.41461944580078  Train_KL: 3.4994617998600006  Validation Loss : 94.20124053955078 Val_Reconstruction : 90.73402404785156 Val_KL : 3.467217206954956\n","Epoch: 2580/8000  Traning Loss: 94.69802951812744  Train_Reconstruction: 91.19662189483643  Train_KL: 3.501407027244568  Validation Loss : 94.03991317749023 Val_Reconstruction : 90.58087158203125 Val_KL : 3.4590389728546143\n","Epoch: 2581/8000  Traning Loss: 95.05459022521973  Train_Reconstruction: 91.55092811584473  Train_KL: 3.5036623775959015  Validation Loss : 94.18633651733398 Val_Reconstruction : 90.72934341430664 Val_KL : 3.4569942951202393\n","Epoch: 2582/8000  Traning Loss: 94.88023662567139  Train_Reconstruction: 91.37691497802734  Train_KL: 3.503321647644043  Validation Loss : 94.4190788269043 Val_Reconstruction : 90.9592056274414 Val_KL : 3.4598710536956787\n","Epoch: 2583/8000  Traning Loss: 94.99131393432617  Train_Reconstruction: 91.49412059783936  Train_KL: 3.4971939623355865  Validation Loss : 94.27978134155273 Val_Reconstruction : 90.83377456665039 Val_KL : 3.4460076093673706\n","Epoch: 2584/8000  Traning Loss: 94.71036529541016  Train_Reconstruction: 91.21808242797852  Train_KL: 3.492284119129181  Validation Loss : 94.01227188110352 Val_Reconstruction : 90.57096099853516 Val_KL : 3.4413106441497803\n","Epoch: 2585/8000  Traning Loss: 94.66751956939697  Train_Reconstruction: 91.17166137695312  Train_KL: 3.4958572685718536  Validation Loss : 93.74867630004883 Val_Reconstruction : 90.30970764160156 Val_KL : 3.4389678239822388\n","Epoch: 2586/8000  Traning Loss: 94.62696933746338  Train_Reconstruction: 91.1357421875  Train_KL: 3.491227477788925  Validation Loss : 94.01649856567383 Val_Reconstruction : 90.56766128540039 Val_KL : 3.4488385915756226\n","Epoch: 2587/8000  Traning Loss: 94.72966194152832  Train_Reconstruction: 91.22576999664307  Train_KL: 3.5038923621177673  Validation Loss : 93.90893173217773 Val_Reconstruction : 90.4401626586914 Val_KL : 3.4687689542770386\n","Epoch: 2588/8000  Traning Loss: 95.14389610290527  Train_Reconstruction: 91.63496685028076  Train_KL: 3.5089299082756042  Validation Loss : 94.43759155273438 Val_Reconstruction : 90.97848510742188 Val_KL : 3.4591063261032104\n","Epoch: 2589/8000  Traning Loss: 94.80205059051514  Train_Reconstruction: 91.3041582107544  Train_KL: 3.4978927671909332  Validation Loss : 93.88328170776367 Val_Reconstruction : 90.43719482421875 Val_KL : 3.4460872411727905\n","Epoch: 2590/8000  Traning Loss: 94.86952495574951  Train_Reconstruction: 91.36654472351074  Train_KL: 3.502979964017868  Validation Loss : 94.53474426269531 Val_Reconstruction : 91.07527160644531 Val_KL : 3.4594740867614746\n","Epoch: 2591/8000  Traning Loss: 94.97135639190674  Train_Reconstruction: 91.46908569335938  Train_KL: 3.50227090716362  Validation Loss : 94.43390655517578 Val_Reconstruction : 90.99702072143555 Val_KL : 3.4368873834609985\n","Epoch: 2592/8000  Traning Loss: 94.9296178817749  Train_Reconstruction: 91.4437370300293  Train_KL: 3.4858819246292114  Validation Loss : 94.3147964477539 Val_Reconstruction : 90.87223434448242 Val_KL : 3.4425623416900635\n","Epoch: 2593/8000  Traning Loss: 95.0635814666748  Train_Reconstruction: 91.55311965942383  Train_KL: 3.510460525751114  Validation Loss : 94.18506240844727 Val_Reconstruction : 90.70598220825195 Val_KL : 3.4790780544281006\n","Epoch: 2594/8000  Traning Loss: 94.88517475128174  Train_Reconstruction: 91.37776279449463  Train_KL: 3.507412165403366  Validation Loss : 94.60515594482422 Val_Reconstruction : 91.15307235717773 Val_KL : 3.452080488204956\n","Epoch: 2595/8000  Traning Loss: 95.15713405609131  Train_Reconstruction: 91.66772651672363  Train_KL: 3.489408701658249  Validation Loss : 94.3287353515625 Val_Reconstruction : 90.88855361938477 Val_KL : 3.4401798248291016\n","Epoch: 2596/8000  Traning Loss: 95.16036319732666  Train_Reconstruction: 91.6717882156372  Train_KL: 3.4885751008987427  Validation Loss : 94.4534683227539 Val_Reconstruction : 91.00103378295898 Val_KL : 3.4524341821670532\n","Epoch: 2597/8000  Traning Loss: 95.25501537322998  Train_Reconstruction: 91.74799251556396  Train_KL: 3.507023513317108  Validation Loss : 94.94126892089844 Val_Reconstruction : 91.4736557006836 Val_KL : 3.4676146507263184\n","Epoch: 2598/8000  Traning Loss: 95.53709030151367  Train_Reconstruction: 92.02174186706543  Train_KL: 3.5153492093086243  Validation Loss : 94.39538955688477 Val_Reconstruction : 90.92462921142578 Val_KL : 3.4707624912261963\n","Epoch: 2599/8000  Traning Loss: 95.27211666107178  Train_Reconstruction: 91.77136707305908  Train_KL: 3.500750720500946  Validation Loss : 94.4528694152832 Val_Reconstruction : 91.00377655029297 Val_KL : 3.4490935802459717\n","Epoch: 2600/8000  Traning Loss: 95.28107738494873  Train_Reconstruction: 91.78713607788086  Train_KL: 3.4939412772655487  Validation Loss : 94.11623001098633 Val_Reconstruction : 90.6727180480957 Val_KL : 3.4435126781463623\n","Epoch: 2601/8000  Traning Loss: 95.00170516967773  Train_Reconstruction: 91.50530815124512  Train_KL: 3.496397078037262  Validation Loss : 94.29372787475586 Val_Reconstruction : 90.83587646484375 Val_KL : 3.4578521251678467\n","Epoch: 2602/8000  Traning Loss: 94.64759635925293  Train_Reconstruction: 91.15006923675537  Train_KL: 3.4975273609161377  Validation Loss : 93.91460037231445 Val_Reconstruction : 90.47074890136719 Val_KL : 3.4438503980636597\n","Epoch: 2603/8000  Traning Loss: 95.17710399627686  Train_Reconstruction: 91.68153953552246  Train_KL: 3.4955649077892303  Validation Loss : 94.62725448608398 Val_Reconstruction : 91.17884063720703 Val_KL : 3.4484134912490845\n","Epoch: 2604/8000  Traning Loss: 95.17790412902832  Train_Reconstruction: 91.67955589294434  Train_KL: 3.498348504304886  Validation Loss : 94.2467269897461 Val_Reconstruction : 90.79937362670898 Val_KL : 3.4473531246185303\n","Epoch: 2605/8000  Traning Loss: 94.816481590271  Train_Reconstruction: 91.32819557189941  Train_KL: 3.488284945487976  Validation Loss : 94.02642822265625 Val_Reconstruction : 90.58184432983398 Val_KL : 3.4445847272872925\n","Epoch: 2606/8000  Traning Loss: 94.64196014404297  Train_Reconstruction: 91.14506149291992  Train_KL: 3.4968983232975006  Validation Loss : 93.98069763183594 Val_Reconstruction : 90.51891326904297 Val_KL : 3.4617855548858643\n","Epoch: 2607/8000  Traning Loss: 94.54804420471191  Train_Reconstruction: 91.03779888153076  Train_KL: 3.5102448761463165  Validation Loss : 93.67681884765625 Val_Reconstruction : 90.21411895751953 Val_KL : 3.462701678276062\n","Epoch: 2608/8000  Traning Loss: 94.59770107269287  Train_Reconstruction: 91.09919452667236  Train_KL: 3.498505622148514  Validation Loss : 94.09296798706055 Val_Reconstruction : 90.6470718383789 Val_KL : 3.4458959102630615\n","Epoch: 2609/8000  Traning Loss: 95.27569580078125  Train_Reconstruction: 91.79109382629395  Train_KL: 3.4846023321151733  Validation Loss : 94.45336532592773 Val_Reconstruction : 91.00479507446289 Val_KL : 3.448568105697632\n","Epoch: 2610/8000  Traning Loss: 95.04873561859131  Train_Reconstruction: 91.53340148925781  Train_KL: 3.5153342187404633  Validation Loss : 94.53182983398438 Val_Reconstruction : 91.05482482910156 Val_KL : 3.477003812789917\n","Epoch: 2611/8000  Traning Loss: 94.91097640991211  Train_Reconstruction: 91.4008264541626  Train_KL: 3.5101506114006042  Validation Loss : 94.35451126098633 Val_Reconstruction : 90.90311050415039 Val_KL : 3.4514025449752808\n","Epoch: 2612/8000  Traning Loss: 95.02230834960938  Train_Reconstruction: 91.53621578216553  Train_KL: 3.486092835664749  Validation Loss : 94.47544479370117 Val_Reconstruction : 91.03625106811523 Val_KL : 3.4391908645629883\n","Epoch: 2613/8000  Traning Loss: 94.86418533325195  Train_Reconstruction: 91.37273979187012  Train_KL: 3.491446375846863  Validation Loss : 94.24701309204102 Val_Reconstruction : 90.79326629638672 Val_KL : 3.453747510910034\n","Epoch: 2614/8000  Traning Loss: 94.80185794830322  Train_Reconstruction: 91.29960918426514  Train_KL: 3.5022489428520203  Validation Loss : 93.90855026245117 Val_Reconstruction : 90.45588684082031 Val_KL : 3.452664375305176\n","Epoch: 2615/8000  Traning Loss: 94.67638111114502  Train_Reconstruction: 91.1728162765503  Train_KL: 3.5035646557807922  Validation Loss : 93.86673355102539 Val_Reconstruction : 90.411376953125 Val_KL : 3.4553550481796265\n","Epoch: 2616/8000  Traning Loss: 94.57956027984619  Train_Reconstruction: 91.07363986968994  Train_KL: 3.5059214532375336  Validation Loss : 93.9672622680664 Val_Reconstruction : 90.5090446472168 Val_KL : 3.458216428756714\n","Epoch: 2617/8000  Traning Loss: 95.17816638946533  Train_Reconstruction: 91.68450927734375  Train_KL: 3.493657737970352  Validation Loss : 94.77815628051758 Val_Reconstruction : 91.3428726196289 Val_KL : 3.43528413772583\n","Epoch: 2618/8000  Traning Loss: 95.57383155822754  Train_Reconstruction: 92.0888500213623  Train_KL: 3.484980493783951  Validation Loss : 94.88716888427734 Val_Reconstruction : 91.43623733520508 Val_KL : 3.450932025909424\n","Epoch: 2619/8000  Traning Loss: 95.94277381896973  Train_Reconstruction: 92.43855094909668  Train_KL: 3.504223555326462  Validation Loss : 94.77659225463867 Val_Reconstruction : 91.3155517578125 Val_KL : 3.4610402584075928\n","Epoch: 2620/8000  Traning Loss: 95.63879680633545  Train_Reconstruction: 92.13347911834717  Train_KL: 3.5053188502788544  Validation Loss : 95.12607192993164 Val_Reconstruction : 91.67469787597656 Val_KL : 3.4513739347457886\n","Epoch: 2621/8000  Traning Loss: 95.58069896697998  Train_Reconstruction: 92.07692241668701  Train_KL: 3.503777325153351  Validation Loss : 94.91912841796875 Val_Reconstruction : 91.46293258666992 Val_KL : 3.4561978578567505\n","Epoch: 2622/8000  Traning Loss: 95.58360385894775  Train_Reconstruction: 92.08503723144531  Train_KL: 3.4985657930374146  Validation Loss : 95.14860916137695 Val_Reconstruction : 91.68937683105469 Val_KL : 3.459230899810791\n","Epoch: 2623/8000  Traning Loss: 96.12505054473877  Train_Reconstruction: 92.62627124786377  Train_KL: 3.4987801909446716  Validation Loss : 95.13153457641602 Val_Reconstruction : 91.67756652832031 Val_KL : 3.453969359397888\n","Epoch: 2624/8000  Traning Loss: 96.13304138183594  Train_Reconstruction: 92.62556648254395  Train_KL: 3.507474958896637  Validation Loss : 95.6197738647461 Val_Reconstruction : 92.14919662475586 Val_KL : 3.470578193664551\n","Epoch: 2625/8000  Traning Loss: 95.5808973312378  Train_Reconstruction: 92.07014560699463  Train_KL: 3.510751038789749  Validation Loss : 94.17512130737305 Val_Reconstruction : 90.70875930786133 Val_KL : 3.466360569000244\n","Epoch: 2626/8000  Traning Loss: 94.73241424560547  Train_Reconstruction: 91.21816921234131  Train_KL: 3.514245092868805  Validation Loss : 93.85561752319336 Val_Reconstruction : 90.3828353881836 Val_KL : 3.4727792739868164\n","Epoch: 2627/8000  Traning Loss: 94.69247245788574  Train_Reconstruction: 91.17872524261475  Train_KL: 3.5137478411197662  Validation Loss : 93.83926010131836 Val_Reconstruction : 90.37709426879883 Val_KL : 3.4621673822402954\n","Epoch: 2628/8000  Traning Loss: 94.56475257873535  Train_Reconstruction: 91.06532764434814  Train_KL: 3.4994238018989563  Validation Loss : 94.01016998291016 Val_Reconstruction : 90.55915069580078 Val_KL : 3.4510213136672974\n","Epoch: 2629/8000  Traning Loss: 94.546630859375  Train_Reconstruction: 91.04846382141113  Train_KL: 3.498166471719742  Validation Loss : 94.31073760986328 Val_Reconstruction : 90.85478591918945 Val_KL : 3.4559537172317505\n","Epoch: 2630/8000  Traning Loss: 94.94079875946045  Train_Reconstruction: 91.43484497070312  Train_KL: 3.5059551894664764  Validation Loss : 94.10043334960938 Val_Reconstruction : 90.63521575927734 Val_KL : 3.465219497680664\n","Epoch: 2631/8000  Traning Loss: 94.48774242401123  Train_Reconstruction: 90.99125099182129  Train_KL: 3.496491938829422  Validation Loss : 93.6966323852539 Val_Reconstruction : 90.24987030029297 Val_KL : 3.446760654449463\n","Epoch: 2632/8000  Traning Loss: 94.80827236175537  Train_Reconstruction: 91.31855964660645  Train_KL: 3.489712029695511  Validation Loss : 94.38412857055664 Val_Reconstruction : 90.94517517089844 Val_KL : 3.438953399658203\n","Epoch: 2633/8000  Traning Loss: 94.99194431304932  Train_Reconstruction: 91.49135303497314  Train_KL: 3.500591278076172  Validation Loss : 94.2940902709961 Val_Reconstruction : 90.82890701293945 Val_KL : 3.4651843309402466\n","Epoch: 2634/8000  Traning Loss: 94.85345649719238  Train_Reconstruction: 91.3327693939209  Train_KL: 3.52068755030632  Validation Loss : 94.00800323486328 Val_Reconstruction : 90.52400970458984 Val_KL : 3.4839937686920166\n","Epoch: 2635/8000  Traning Loss: 94.73374366760254  Train_Reconstruction: 91.21779441833496  Train_KL: 3.515949457883835  Validation Loss : 93.95036697387695 Val_Reconstruction : 90.48641967773438 Val_KL : 3.463945746421814\n","Epoch: 2636/8000  Traning Loss: 94.64105606079102  Train_Reconstruction: 91.13309001922607  Train_KL: 3.5079662203788757  Validation Loss : 93.8635025024414 Val_Reconstruction : 90.39660263061523 Val_KL : 3.4668973684310913\n","Epoch: 2637/8000  Traning Loss: 94.76912784576416  Train_Reconstruction: 91.26685237884521  Train_KL: 3.502276748418808  Validation Loss : 94.24396133422852 Val_Reconstruction : 90.79751586914062 Val_KL : 3.4464447498321533\n","Epoch: 2638/8000  Traning Loss: 94.68784141540527  Train_Reconstruction: 91.20652294158936  Train_KL: 3.4813171327114105  Validation Loss : 94.19669342041016 Val_Reconstruction : 90.76377487182617 Val_KL : 3.4329177141189575\n","Epoch: 2639/8000  Traning Loss: 94.73775005340576  Train_Reconstruction: 91.24731159210205  Train_KL: 3.4904376566410065  Validation Loss : 93.72147750854492 Val_Reconstruction : 90.26598739624023 Val_KL : 3.455490469932556\n","Epoch: 2640/8000  Traning Loss: 94.66928291320801  Train_Reconstruction: 91.16890048980713  Train_KL: 3.5003822445869446  Validation Loss : 94.12817764282227 Val_Reconstruction : 90.66719436645508 Val_KL : 3.460986614227295\n","Epoch: 2641/8000  Traning Loss: 95.29721736907959  Train_Reconstruction: 91.78939247131348  Train_KL: 3.5078248381614685  Validation Loss : 94.65269088745117 Val_Reconstruction : 91.18865966796875 Val_KL : 3.4640331268310547\n","Epoch: 2642/8000  Traning Loss: 95.09081172943115  Train_Reconstruction: 91.59691143035889  Train_KL: 3.4939009249210358  Validation Loss : 94.35015869140625 Val_Reconstruction : 90.90454483032227 Val_KL : 3.445613741874695\n","Epoch: 2643/8000  Traning Loss: 95.35206699371338  Train_Reconstruction: 91.8605432510376  Train_KL: 3.4915234744548798  Validation Loss : 94.70047760009766 Val_Reconstruction : 91.23761749267578 Val_KL : 3.4628636837005615\n","Epoch: 2644/8000  Traning Loss: 95.04284286499023  Train_Reconstruction: 91.52886962890625  Train_KL: 3.5139742493629456  Validation Loss : 94.23659133911133 Val_Reconstruction : 90.76483917236328 Val_KL : 3.471752166748047\n","Epoch: 2645/8000  Traning Loss: 94.80308723449707  Train_Reconstruction: 91.29996299743652  Train_KL: 3.5031240582466125  Validation Loss : 94.21292877197266 Val_Reconstruction : 90.76421737670898 Val_KL : 3.4487096071243286\n","Epoch: 2646/8000  Traning Loss: 94.80473709106445  Train_Reconstruction: 91.30777168273926  Train_KL: 3.4969645142555237  Validation Loss : 94.02395629882812 Val_Reconstruction : 90.5665512084961 Val_KL : 3.457404136657715\n","Epoch: 2647/8000  Traning Loss: 94.61149311065674  Train_Reconstruction: 91.10425567626953  Train_KL: 3.507237285375595  Validation Loss : 93.75181579589844 Val_Reconstruction : 90.27781677246094 Val_KL : 3.473995804786682\n","Epoch: 2648/8000  Traning Loss: 94.48491287231445  Train_Reconstruction: 90.97046852111816  Train_KL: 3.514444351196289  Validation Loss : 94.11071014404297 Val_Reconstruction : 90.63875579833984 Val_KL : 3.471954345703125\n","Epoch: 2649/8000  Traning Loss: 94.26999187469482  Train_Reconstruction: 90.76449775695801  Train_KL: 3.5054939091205597  Validation Loss : 93.43930435180664 Val_Reconstruction : 89.97768783569336 Val_KL : 3.461617588996887\n","Epoch: 2650/8000  Traning Loss: 94.28000259399414  Train_Reconstruction: 90.7790174484253  Train_KL: 3.5009858310222626  Validation Loss : 93.77072143554688 Val_Reconstruction : 90.31704711914062 Val_KL : 3.4536744356155396\n","Epoch: 2651/8000  Traning Loss: 94.67975902557373  Train_Reconstruction: 91.1698408126831  Train_KL: 3.5099185407161713  Validation Loss : 94.27132797241211 Val_Reconstruction : 90.79888153076172 Val_KL : 3.47244393825531\n","Epoch: 2652/8000  Traning Loss: 94.84978103637695  Train_Reconstruction: 91.34281539916992  Train_KL: 3.5069668889045715  Validation Loss : 94.21429061889648 Val_Reconstruction : 90.7467269897461 Val_KL : 3.4675642251968384\n","Epoch: 2653/8000  Traning Loss: 95.00909996032715  Train_Reconstruction: 91.50544834136963  Train_KL: 3.503651261329651  Validation Loss : 94.53223419189453 Val_Reconstruction : 91.0579948425293 Val_KL : 3.4742408990859985\n","Epoch: 2654/8000  Traning Loss: 95.2986707687378  Train_Reconstruction: 91.78133487701416  Train_KL: 3.517334908246994  Validation Loss : 94.44146728515625 Val_Reconstruction : 90.98180770874023 Val_KL : 3.4596608877182007\n","Epoch: 2655/8000  Traning Loss: 94.91097736358643  Train_Reconstruction: 91.4146614074707  Train_KL: 3.496316224336624  Validation Loss : 94.22408676147461 Val_Reconstruction : 90.77550888061523 Val_KL : 3.4485803842544556\n","Epoch: 2656/8000  Traning Loss: 94.68410015106201  Train_Reconstruction: 91.19308185577393  Train_KL: 3.491019457578659  Validation Loss : 94.08433151245117 Val_Reconstruction : 90.63928604125977 Val_KL : 3.4450457096099854\n","Epoch: 2657/8000  Traning Loss: 94.83311653137207  Train_Reconstruction: 91.3408784866333  Train_KL: 3.492237836122513  Validation Loss : 94.1284065246582 Val_Reconstruction : 90.68668365478516 Val_KL : 3.4417243003845215\n","Epoch: 2658/8000  Traning Loss: 94.491286277771  Train_Reconstruction: 90.99962520599365  Train_KL: 3.4916606843471527  Validation Loss : 93.83870315551758 Val_Reconstruction : 90.38426208496094 Val_KL : 3.454440712928772\n","Epoch: 2659/8000  Traning Loss: 94.46826171875  Train_Reconstruction: 90.96674919128418  Train_KL: 3.501513361930847  Validation Loss : 93.8462142944336 Val_Reconstruction : 90.37454605102539 Val_KL : 3.4716691970825195\n","Epoch: 2660/8000  Traning Loss: 95.06475067138672  Train_Reconstruction: 91.55300235748291  Train_KL: 3.51174795627594  Validation Loss : 94.48534393310547 Val_Reconstruction : 91.02166366577148 Val_KL : 3.4636796712875366\n","Epoch: 2661/8000  Traning Loss: 96.06662845611572  Train_Reconstruction: 92.56974792480469  Train_KL: 3.4968807697296143  Validation Loss : 95.59397888183594 Val_Reconstruction : 92.14286041259766 Val_KL : 3.4511210918426514\n","Epoch: 2662/8000  Traning Loss: 96.47660541534424  Train_Reconstruction: 92.98329544067383  Train_KL: 3.4933087825775146  Validation Loss : 95.59712600708008 Val_Reconstruction : 92.14483642578125 Val_KL : 3.4522889852523804\n","Epoch: 2663/8000  Traning Loss: 95.0604305267334  Train_Reconstruction: 91.56182384490967  Train_KL: 3.4986066222190857  Validation Loss : 93.87714004516602 Val_Reconstruction : 90.4271011352539 Val_KL : 3.4500380754470825\n","Epoch: 2664/8000  Traning Loss: 94.54529571533203  Train_Reconstruction: 91.05146884918213  Train_KL: 3.4938277900218964  Validation Loss : 94.07436752319336 Val_Reconstruction : 90.62063980102539 Val_KL : 3.453726291656494\n","Epoch: 2665/8000  Traning Loss: 94.52093982696533  Train_Reconstruction: 91.0260066986084  Train_KL: 3.4949330389499664  Validation Loss : 93.7131118774414 Val_Reconstruction : 90.2593002319336 Val_KL : 3.453811287879944\n","Epoch: 2666/8000  Traning Loss: 94.45394229888916  Train_Reconstruction: 90.95288562774658  Train_KL: 3.5010570883750916  Validation Loss : 93.61058044433594 Val_Reconstruction : 90.14866638183594 Val_KL : 3.4619168043136597\n","Epoch: 2667/8000  Traning Loss: 94.90142631530762  Train_Reconstruction: 91.39938640594482  Train_KL: 3.502040386199951  Validation Loss : 94.0759391784668 Val_Reconstruction : 90.6145248413086 Val_KL : 3.4614171981811523\n","Epoch: 2668/8000  Traning Loss: 94.7498664855957  Train_Reconstruction: 91.24926662445068  Train_KL: 3.500599831342697  Validation Loss : 93.83623123168945 Val_Reconstruction : 90.38539505004883 Val_KL : 3.450836420059204\n","Epoch: 2669/8000  Traning Loss: 95.12697315216064  Train_Reconstruction: 91.62306785583496  Train_KL: 3.5039051175117493  Validation Loss : 94.71332550048828 Val_Reconstruction : 91.25688552856445 Val_KL : 3.4564409255981445\n","Epoch: 2670/8000  Traning Loss: 95.19592666625977  Train_Reconstruction: 91.69482326507568  Train_KL: 3.5011022686958313  Validation Loss : 94.3840103149414 Val_Reconstruction : 90.92384719848633 Val_KL : 3.460164427757263\n","Epoch: 2671/8000  Traning Loss: 94.78739929199219  Train_Reconstruction: 91.29152584075928  Train_KL: 3.4958740770816803  Validation Loss : 93.8884162902832 Val_Reconstruction : 90.42584609985352 Val_KL : 3.462570905685425\n","Epoch: 2672/8000  Traning Loss: 94.5927906036377  Train_Reconstruction: 91.09560585021973  Train_KL: 3.4971842765808105  Validation Loss : 94.1436538696289 Val_Reconstruction : 90.6881332397461 Val_KL : 3.4555225372314453\n","Epoch: 2673/8000  Traning Loss: 94.4972333908081  Train_Reconstruction: 90.9957160949707  Train_KL: 3.501518040895462  Validation Loss : 93.96095275878906 Val_Reconstruction : 90.50026321411133 Val_KL : 3.46068799495697\n","Epoch: 2674/8000  Traning Loss: 94.5285177230835  Train_Reconstruction: 91.01876926422119  Train_KL: 3.509747862815857  Validation Loss : 93.73740768432617 Val_Reconstruction : 90.275390625 Val_KL : 3.4620152711868286\n","Epoch: 2675/8000  Traning Loss: 94.64947509765625  Train_Reconstruction: 91.1425428390503  Train_KL: 3.5069321990013123  Validation Loss : 94.09474563598633 Val_Reconstruction : 90.64314651489258 Val_KL : 3.4515997171401978\n","Epoch: 2676/8000  Traning Loss: 94.51247501373291  Train_Reconstruction: 91.02298927307129  Train_KL: 3.489486485719681  Validation Loss : 93.83808517456055 Val_Reconstruction : 90.38784790039062 Val_KL : 3.450239658355713\n","Epoch: 2677/8000  Traning Loss: 94.5072078704834  Train_Reconstruction: 91.0079698562622  Train_KL: 3.4992381632328033  Validation Loss : 93.82698822021484 Val_Reconstruction : 90.36217498779297 Val_KL : 3.4648122787475586\n","Epoch: 2678/8000  Traning Loss: 95.00576496124268  Train_Reconstruction: 91.49759006500244  Train_KL: 3.508174568414688  Validation Loss : 94.73332214355469 Val_Reconstruction : 91.26228332519531 Val_KL : 3.4710376262664795\n","Epoch: 2679/8000  Traning Loss: 95.63101100921631  Train_Reconstruction: 92.12899017333984  Train_KL: 3.502020448446274  Validation Loss : 95.04739761352539 Val_Reconstruction : 91.59662246704102 Val_KL : 3.4507745504379272\n","Epoch: 2680/8000  Traning Loss: 95.1418046951294  Train_Reconstruction: 91.64951133728027  Train_KL: 3.492293506860733  Validation Loss : 94.01460647583008 Val_Reconstruction : 90.57046508789062 Val_KL : 3.4441442489624023\n","Epoch: 2681/8000  Traning Loss: 94.34383392333984  Train_Reconstruction: 90.84872055053711  Train_KL: 3.4951124489307404  Validation Loss : 93.77765655517578 Val_Reconstruction : 90.32512664794922 Val_KL : 3.4525303840637207\n","Epoch: 2682/8000  Traning Loss: 94.44636535644531  Train_Reconstruction: 90.95038604736328  Train_KL: 3.495980352163315  Validation Loss : 93.73756408691406 Val_Reconstruction : 90.27783203125 Val_KL : 3.459731340408325\n","Epoch: 2683/8000  Traning Loss: 94.34549903869629  Train_Reconstruction: 90.84158134460449  Train_KL: 3.5039175748825073  Validation Loss : 93.69641494750977 Val_Reconstruction : 90.21664810180664 Val_KL : 3.47976553440094\n","Epoch: 2684/8000  Traning Loss: 94.48159694671631  Train_Reconstruction: 90.95910930633545  Train_KL: 3.522488087415695  Validation Loss : 93.95051193237305 Val_Reconstruction : 90.46644592285156 Val_KL : 3.4840691089630127\n","Epoch: 2685/8000  Traning Loss: 94.67782211303711  Train_Reconstruction: 91.16977787017822  Train_KL: 3.508044272661209  Validation Loss : 94.2068977355957 Val_Reconstruction : 90.74818420410156 Val_KL : 3.4587162733078003\n","Epoch: 2686/8000  Traning Loss: 95.2747745513916  Train_Reconstruction: 91.77609443664551  Train_KL: 3.498680353164673  Validation Loss : 94.75378036499023 Val_Reconstruction : 91.30030059814453 Val_KL : 3.453479290008545\n","Epoch: 2687/8000  Traning Loss: 94.88911247253418  Train_Reconstruction: 91.38581657409668  Train_KL: 3.5032947659492493  Validation Loss : 93.92463302612305 Val_Reconstruction : 90.45547485351562 Val_KL : 3.4691582918167114\n","Epoch: 2688/8000  Traning Loss: 94.85957527160645  Train_Reconstruction: 91.34408569335938  Train_KL: 3.51549032330513  Validation Loss : 94.29657363891602 Val_Reconstruction : 90.82940673828125 Val_KL : 3.4671688079833984\n","Epoch: 2689/8000  Traning Loss: 94.72230911254883  Train_Reconstruction: 91.21552181243896  Train_KL: 3.506786733865738  Validation Loss : 94.0416145324707 Val_Reconstruction : 90.57959747314453 Val_KL : 3.4620176553726196\n","Epoch: 2690/8000  Traning Loss: 94.42477607727051  Train_Reconstruction: 90.92054462432861  Train_KL: 3.504230350255966  Validation Loss : 93.89108657836914 Val_Reconstruction : 90.43375015258789 Val_KL : 3.457335352897644\n","Epoch: 2691/8000  Traning Loss: 94.52336502075195  Train_Reconstruction: 91.0174503326416  Train_KL: 3.505916029214859  Validation Loss : 94.06460571289062 Val_Reconstruction : 90.60815811157227 Val_KL : 3.4564472436904907\n","Epoch: 2692/8000  Traning Loss: 94.68348598480225  Train_Reconstruction: 91.1791000366211  Train_KL: 3.504387229681015  Validation Loss : 93.97920989990234 Val_Reconstruction : 90.52103805541992 Val_KL : 3.4581717252731323\n","Epoch: 2693/8000  Traning Loss: 94.593186378479  Train_Reconstruction: 91.0898847579956  Train_KL: 3.5033015310764313  Validation Loss : 93.6558837890625 Val_Reconstruction : 90.1981201171875 Val_KL : 3.457766890525818\n","Epoch: 2694/8000  Traning Loss: 94.54898071289062  Train_Reconstruction: 91.05224418640137  Train_KL: 3.496735602617264  Validation Loss : 94.03912353515625 Val_Reconstruction : 90.5992202758789 Val_KL : 3.439903736114502\n","Epoch: 2695/8000  Traning Loss: 94.5785140991211  Train_Reconstruction: 91.08972072601318  Train_KL: 3.4887924790382385  Validation Loss : 93.71988677978516 Val_Reconstruction : 90.27571487426758 Val_KL : 3.444172978401184\n","Epoch: 2696/8000  Traning Loss: 94.86373329162598  Train_Reconstruction: 91.36547565460205  Train_KL: 3.498257100582123  Validation Loss : 94.50563430786133 Val_Reconstruction : 91.05008697509766 Val_KL : 3.4555450677871704\n","Epoch: 2697/8000  Traning Loss: 95.02637100219727  Train_Reconstruction: 91.52580261230469  Train_KL: 3.5005703270435333  Validation Loss : 94.12914657592773 Val_Reconstruction : 90.68243408203125 Val_KL : 3.4467121362686157\n","Epoch: 2698/8000  Traning Loss: 94.45185947418213  Train_Reconstruction: 90.95852279663086  Train_KL: 3.493338108062744  Validation Loss : 93.65864562988281 Val_Reconstruction : 90.20652389526367 Val_KL : 3.45212185382843\n","Epoch: 2699/8000  Traning Loss: 94.43387794494629  Train_Reconstruction: 90.93329906463623  Train_KL: 3.500579744577408  Validation Loss : 93.67687225341797 Val_Reconstruction : 90.22291946411133 Val_KL : 3.4539538621902466\n","Epoch: 2700/8000  Traning Loss: 94.4783525466919  Train_Reconstruction: 90.97419166564941  Train_KL: 3.5041599571704865  Validation Loss : 93.84766006469727 Val_Reconstruction : 90.38851165771484 Val_KL : 3.4591493606567383\n","Epoch: 2701/8000  Traning Loss: 94.5233268737793  Train_Reconstruction: 91.00925731658936  Train_KL: 3.5140689313411713  Validation Loss : 93.96662902832031 Val_Reconstruction : 90.50108337402344 Val_KL : 3.465547561645508\n","Epoch: 2702/8000  Traning Loss: 94.55234146118164  Train_Reconstruction: 91.04246616363525  Train_KL: 3.5098759829998016  Validation Loss : 94.07514190673828 Val_Reconstruction : 90.60951614379883 Val_KL : 3.465627431869507\n","Epoch: 2703/8000  Traning Loss: 94.60061168670654  Train_Reconstruction: 91.09474182128906  Train_KL: 3.5058701634407043  Validation Loss : 93.81442260742188 Val_Reconstruction : 90.35320281982422 Val_KL : 3.4612221717834473\n","Epoch: 2704/8000  Traning Loss: 94.82479286193848  Train_Reconstruction: 91.31988716125488  Train_KL: 3.5049051344394684  Validation Loss : 94.15553665161133 Val_Reconstruction : 90.69292831420898 Val_KL : 3.4626089334487915\n","Epoch: 2705/8000  Traning Loss: 95.24857997894287  Train_Reconstruction: 91.75152778625488  Train_KL: 3.497053384780884  Validation Loss : 94.01203536987305 Val_Reconstruction : 90.55348587036133 Val_KL : 3.4585506916046143\n","Epoch: 2706/8000  Traning Loss: 94.83083724975586  Train_Reconstruction: 91.3272762298584  Train_KL: 3.5035606622695923  Validation Loss : 93.98023986816406 Val_Reconstruction : 90.52176666259766 Val_KL : 3.458469867706299\n","Epoch: 2707/8000  Traning Loss: 94.912184715271  Train_Reconstruction: 91.41108417510986  Train_KL: 3.5011001229286194  Validation Loss : 94.25152587890625 Val_Reconstruction : 90.7999496459961 Val_KL : 3.451579451560974\n","Epoch: 2708/8000  Traning Loss: 94.76959991455078  Train_Reconstruction: 91.28104972839355  Train_KL: 3.488550901412964  Validation Loss : 93.88706588745117 Val_Reconstruction : 90.44263458251953 Val_KL : 3.444432020187378\n","Epoch: 2709/8000  Traning Loss: 94.62353229522705  Train_Reconstruction: 91.14196014404297  Train_KL: 3.481571227312088  Validation Loss : 94.25626754760742 Val_Reconstruction : 90.81706619262695 Val_KL : 3.4392021894454956\n","Epoch: 2710/8000  Traning Loss: 94.2425708770752  Train_Reconstruction: 90.74605369567871  Train_KL: 3.4965178072452545  Validation Loss : 93.39933776855469 Val_Reconstruction : 89.94135284423828 Val_KL : 3.4579848051071167\n","Epoch: 2711/8000  Traning Loss: 94.34361553192139  Train_Reconstruction: 90.83313465118408  Train_KL: 3.5104799270629883  Validation Loss : 93.87834167480469 Val_Reconstruction : 90.40646743774414 Val_KL : 3.4718722105026245\n","Epoch: 2712/8000  Traning Loss: 94.47433471679688  Train_Reconstruction: 90.96668434143066  Train_KL: 3.5076498985290527  Validation Loss : 94.17441940307617 Val_Reconstruction : 90.70841217041016 Val_KL : 3.466005325317383\n","Epoch: 2713/8000  Traning Loss: 94.58809566497803  Train_Reconstruction: 91.08171558380127  Train_KL: 3.506380796432495  Validation Loss : 94.21763610839844 Val_Reconstruction : 90.74301528930664 Val_KL : 3.4746233224868774\n","Epoch: 2714/8000  Traning Loss: 94.38035297393799  Train_Reconstruction: 90.8605260848999  Train_KL: 3.519827038049698  Validation Loss : 94.31256484985352 Val_Reconstruction : 90.83814239501953 Val_KL : 3.474423050880432\n","Epoch: 2715/8000  Traning Loss: 94.41954135894775  Train_Reconstruction: 90.90974712371826  Train_KL: 3.5097933411598206  Validation Loss : 93.59500503540039 Val_Reconstruction : 90.13537979125977 Val_KL : 3.4596227407455444\n","Epoch: 2716/8000  Traning Loss: 94.10771083831787  Train_Reconstruction: 90.5951156616211  Train_KL: 3.512596905231476  Validation Loss : 93.47120666503906 Val_Reconstruction : 90.00137710571289 Val_KL : 3.4698259830474854\n","Epoch: 2717/8000  Traning Loss: 94.39413261413574  Train_Reconstruction: 90.88009929656982  Train_KL: 3.514033854007721  Validation Loss : 94.1128921508789 Val_Reconstruction : 90.64581298828125 Val_KL : 3.467078685760498\n","Epoch: 2718/8000  Traning Loss: 94.59831237792969  Train_Reconstruction: 91.10314464569092  Train_KL: 3.4951659440994263  Validation Loss : 93.771240234375 Val_Reconstruction : 90.32806396484375 Val_KL : 3.4431756734848022\n","Epoch: 2719/8000  Traning Loss: 94.64380931854248  Train_Reconstruction: 91.15561008453369  Train_KL: 3.4881991147994995  Validation Loss : 94.015869140625 Val_Reconstruction : 90.56450271606445 Val_KL : 3.451366901397705\n","Epoch: 2720/8000  Traning Loss: 94.86438083648682  Train_Reconstruction: 91.36588191986084  Train_KL: 3.498498857021332  Validation Loss : 94.00905990600586 Val_Reconstruction : 90.5518798828125 Val_KL : 3.457179546356201\n","Epoch: 2721/8000  Traning Loss: 94.72815990447998  Train_Reconstruction: 91.22661399841309  Train_KL: 3.5015468895435333  Validation Loss : 93.90731430053711 Val_Reconstruction : 90.4437255859375 Val_KL : 3.463586688041687\n","Epoch: 2722/8000  Traning Loss: 94.24843215942383  Train_Reconstruction: 90.73146438598633  Train_KL: 3.5169678032398224  Validation Loss : 93.43260192871094 Val_Reconstruction : 89.94754409790039 Val_KL : 3.4850603342056274\n","Epoch: 2723/8000  Traning Loss: 94.29138278961182  Train_Reconstruction: 90.77254390716553  Train_KL: 3.518840938806534  Validation Loss : 93.93788146972656 Val_Reconstruction : 90.46795272827148 Val_KL : 3.4699286222457886\n","Epoch: 2724/8000  Traning Loss: 94.5243272781372  Train_Reconstruction: 91.0191650390625  Train_KL: 3.505161792039871  Validation Loss : 93.70947647094727 Val_Reconstruction : 90.24958419799805 Val_KL : 3.459892153739929\n","Epoch: 2725/8000  Traning Loss: 94.65275764465332  Train_Reconstruction: 91.16181468963623  Train_KL: 3.4909427165985107  Validation Loss : 93.94258117675781 Val_Reconstruction : 90.50379943847656 Val_KL : 3.438782572746277\n","Epoch: 2726/8000  Traning Loss: 95.21098327636719  Train_Reconstruction: 91.72023391723633  Train_KL: 3.4907485246658325  Validation Loss : 94.73203659057617 Val_Reconstruction : 91.2748794555664 Val_KL : 3.457157254219055\n","Epoch: 2727/8000  Traning Loss: 95.47729969024658  Train_Reconstruction: 91.97253322601318  Train_KL: 3.5047678649425507  Validation Loss : 94.23958587646484 Val_Reconstruction : 90.77428436279297 Val_KL : 3.465303659439087\n","Epoch: 2728/8000  Traning Loss: 94.58520030975342  Train_Reconstruction: 91.07743835449219  Train_KL: 3.5077624917030334  Validation Loss : 93.8724594116211 Val_Reconstruction : 90.41879653930664 Val_KL : 3.453665256500244\n","Epoch: 2729/8000  Traning Loss: 94.43905067443848  Train_Reconstruction: 90.94663619995117  Train_KL: 3.492415279150009  Validation Loss : 93.6389045715332 Val_Reconstruction : 90.19456100463867 Val_KL : 3.444345712661743\n","Epoch: 2730/8000  Traning Loss: 94.18063163757324  Train_Reconstruction: 90.68756294250488  Train_KL: 3.4930692315101624  Validation Loss : 93.35658645629883 Val_Reconstruction : 89.90057754516602 Val_KL : 3.456008553504944\n","Epoch: 2731/8000  Traning Loss: 94.3212661743164  Train_Reconstruction: 90.82690715789795  Train_KL: 3.4943579137325287  Validation Loss : 93.56705856323242 Val_Reconstruction : 90.11507034301758 Val_KL : 3.4519859552383423\n","Epoch: 2732/8000  Traning Loss: 94.62843418121338  Train_Reconstruction: 91.12858963012695  Train_KL: 3.499844402074814  Validation Loss : 94.24084854125977 Val_Reconstruction : 90.7861442565918 Val_KL : 3.4547057151794434\n","Epoch: 2733/8000  Traning Loss: 94.85651588439941  Train_Reconstruction: 91.3516321182251  Train_KL: 3.5048833787441254  Validation Loss : 94.33035278320312 Val_Reconstruction : 90.86338424682617 Val_KL : 3.4669711589813232\n","Epoch: 2734/8000  Traning Loss: 94.54806804656982  Train_Reconstruction: 91.04367733001709  Train_KL: 3.504392057657242  Validation Loss : 93.5799674987793 Val_Reconstruction : 90.12418746948242 Val_KL : 3.455778956413269\n","Epoch: 2735/8000  Traning Loss: 94.5744276046753  Train_Reconstruction: 91.0780258178711  Train_KL: 3.496401935815811  Validation Loss : 94.1038818359375 Val_Reconstruction : 90.65403747558594 Val_KL : 3.4498454332351685\n","Epoch: 2736/8000  Traning Loss: 94.79332256317139  Train_Reconstruction: 91.29842853546143  Train_KL: 3.494894176721573  Validation Loss : 93.93445587158203 Val_Reconstruction : 90.48050689697266 Val_KL : 3.453947067260742\n","Epoch: 2737/8000  Traning Loss: 94.40835571289062  Train_Reconstruction: 90.91078758239746  Train_KL: 3.497568666934967  Validation Loss : 94.09920501708984 Val_Reconstruction : 90.63417053222656 Val_KL : 3.4650325775146484\n","Epoch: 2738/8000  Traning Loss: 94.56675243377686  Train_Reconstruction: 91.05550384521484  Train_KL: 3.5112484097480774  Validation Loss : 94.34164810180664 Val_Reconstruction : 90.87685012817383 Val_KL : 3.4647958278656006\n","Epoch: 2739/8000  Traning Loss: 94.64426803588867  Train_Reconstruction: 91.14270782470703  Train_KL: 3.5015602707862854  Validation Loss : 94.00728225708008 Val_Reconstruction : 90.55349731445312 Val_KL : 3.453781485557556\n","Epoch: 2740/8000  Traning Loss: 94.75646877288818  Train_Reconstruction: 91.25912761688232  Train_KL: 3.497340440750122  Validation Loss : 93.6467399597168 Val_Reconstruction : 90.1922378540039 Val_KL : 3.454501509666443\n","Epoch: 2741/8000  Traning Loss: 94.45744228363037  Train_Reconstruction: 90.95798110961914  Train_KL: 3.499461382627487  Validation Loss : 93.44506454467773 Val_Reconstruction : 89.99304962158203 Val_KL : 3.4520158767700195\n","Epoch: 2742/8000  Traning Loss: 94.21601009368896  Train_Reconstruction: 90.72132301330566  Train_KL: 3.4946868419647217  Validation Loss : 93.53239822387695 Val_Reconstruction : 90.08891296386719 Val_KL : 3.4434845447540283\n","Epoch: 2743/8000  Traning Loss: 94.15275859832764  Train_Reconstruction: 90.65555095672607  Train_KL: 3.497207671403885  Validation Loss : 93.59567260742188 Val_Reconstruction : 90.13339614868164 Val_KL : 3.462277889251709\n","Epoch: 2744/8000  Traning Loss: 94.53684043884277  Train_Reconstruction: 91.02957344055176  Train_KL: 3.507266879081726  Validation Loss : 93.87997436523438 Val_Reconstruction : 90.42170715332031 Val_KL : 3.4582690000534058\n","Epoch: 2745/8000  Traning Loss: 95.19456386566162  Train_Reconstruction: 91.69508361816406  Train_KL: 3.499480217695236  Validation Loss : 94.40829086303711 Val_Reconstruction : 90.96125411987305 Val_KL : 3.447036385536194\n","Epoch: 2746/8000  Traning Loss: 95.27695846557617  Train_Reconstruction: 91.78780364990234  Train_KL: 3.489155739545822  Validation Loss : 94.20392227172852 Val_Reconstruction : 90.75184631347656 Val_KL : 3.4520769119262695\n","Epoch: 2747/8000  Traning Loss: 94.74240970611572  Train_Reconstruction: 91.22602844238281  Train_KL: 3.5163823068141937  Validation Loss : 93.89436340332031 Val_Reconstruction : 90.4222526550293 Val_KL : 3.472109317779541\n","Epoch: 2748/8000  Traning Loss: 94.30877304077148  Train_Reconstruction: 90.79005813598633  Train_KL: 3.518715351819992  Validation Loss : 93.69161987304688 Val_Reconstruction : 90.22186660766602 Val_KL : 3.4697561264038086\n","Epoch: 2749/8000  Traning Loss: 94.24908447265625  Train_Reconstruction: 90.75181770324707  Train_KL: 3.497267425060272  Validation Loss : 93.86592102050781 Val_Reconstruction : 90.41619873046875 Val_KL : 3.449722409248352\n","Epoch: 2750/8000  Traning Loss: 94.31672477722168  Train_Reconstruction: 90.8203239440918  Train_KL: 3.4964009821414948  Validation Loss : 93.8122329711914 Val_Reconstruction : 90.34821701049805 Val_KL : 3.464017391204834\n","Epoch: 2751/8000  Traning Loss: 94.72894859313965  Train_Reconstruction: 91.21655178070068  Train_KL: 3.512397140264511  Validation Loss : 94.32722091674805 Val_Reconstruction : 90.85771942138672 Val_KL : 3.4695013761520386\n","Epoch: 2752/8000  Traning Loss: 94.87732791900635  Train_Reconstruction: 91.38089179992676  Train_KL: 3.4964350759983063  Validation Loss : 94.42501449584961 Val_Reconstruction : 90.98270797729492 Val_KL : 3.4423052072525024\n","Epoch: 2753/8000  Traning Loss: 95.11582088470459  Train_Reconstruction: 91.61578178405762  Train_KL: 3.5000393986701965  Validation Loss : 94.51297760009766 Val_Reconstruction : 91.04289627075195 Val_KL : 3.470078945159912\n","Epoch: 2754/8000  Traning Loss: 95.25353145599365  Train_Reconstruction: 91.73789501190186  Train_KL: 3.5156368017196655  Validation Loss : 94.54364776611328 Val_Reconstruction : 91.066650390625 Val_KL : 3.4769985675811768\n","Epoch: 2755/8000  Traning Loss: 94.84757232666016  Train_Reconstruction: 91.33885383605957  Train_KL: 3.5087192058563232  Validation Loss : 93.777587890625 Val_Reconstruction : 90.3165168762207 Val_KL : 3.461069703102112\n","Epoch: 2756/8000  Traning Loss: 94.51146125793457  Train_Reconstruction: 91.00843906402588  Train_KL: 3.5030221045017242  Validation Loss : 94.14649200439453 Val_Reconstruction : 90.68073272705078 Val_KL : 3.465761184692383\n","Epoch: 2757/8000  Traning Loss: 94.72426509857178  Train_Reconstruction: 91.21838283538818  Train_KL: 3.505880504846573  Validation Loss : 94.02186584472656 Val_Reconstruction : 90.56800079345703 Val_KL : 3.4538662433624268\n","Epoch: 2758/8000  Traning Loss: 94.8633165359497  Train_Reconstruction: 91.36592102050781  Train_KL: 3.49739533662796  Validation Loss : 94.39634323120117 Val_Reconstruction : 90.93401718139648 Val_KL : 3.4623262882232666\n","Epoch: 2759/8000  Traning Loss: 94.51620101928711  Train_Reconstruction: 91.00824069976807  Train_KL: 3.507959395647049  Validation Loss : 93.91296005249023 Val_Reconstruction : 90.45206451416016 Val_KL : 3.4608951807022095\n","Epoch: 2760/8000  Traning Loss: 94.5799207687378  Train_Reconstruction: 91.0689172744751  Train_KL: 3.5110023617744446  Validation Loss : 94.38430404663086 Val_Reconstruction : 90.91997146606445 Val_KL : 3.4643346071243286\n","Epoch: 2761/8000  Traning Loss: 94.71148490905762  Train_Reconstruction: 91.20417881011963  Train_KL: 3.5073061287403107  Validation Loss : 93.75476455688477 Val_Reconstruction : 90.286865234375 Val_KL : 3.467897057533264\n","Epoch: 2762/8000  Traning Loss: 94.40267562866211  Train_Reconstruction: 90.90145206451416  Train_KL: 3.5012238025665283  Validation Loss : 93.40771102905273 Val_Reconstruction : 89.94971084594727 Val_KL : 3.458000898361206\n","Epoch: 2763/8000  Traning Loss: 94.13693809509277  Train_Reconstruction: 90.6384048461914  Train_KL: 3.498533308506012  Validation Loss : 93.68790435791016 Val_Reconstruction : 90.22428131103516 Val_KL : 3.463623285293579\n","Epoch: 2764/8000  Traning Loss: 94.7671537399292  Train_Reconstruction: 91.2678575515747  Train_KL: 3.4992959201335907  Validation Loss : 94.33342742919922 Val_Reconstruction : 90.8828010559082 Val_KL : 3.450628399848938\n","Epoch: 2765/8000  Traning Loss: 94.54005432128906  Train_Reconstruction: 91.04243850708008  Train_KL: 3.4976175725460052  Validation Loss : 93.603515625 Val_Reconstruction : 90.14128494262695 Val_KL : 3.4622284173965454\n","Epoch: 2766/8000  Traning Loss: 94.17958354949951  Train_Reconstruction: 90.67744159698486  Train_KL: 3.5021420419216156  Validation Loss : 93.62465286254883 Val_Reconstruction : 90.1587905883789 Val_KL : 3.465860366821289\n","Epoch: 2767/8000  Traning Loss: 94.18823719024658  Train_Reconstruction: 90.68375301361084  Train_KL: 3.5044829547405243  Validation Loss : 93.37154388427734 Val_Reconstruction : 89.9079360961914 Val_KL : 3.4636054039001465\n","Epoch: 2768/8000  Traning Loss: 94.3325719833374  Train_Reconstruction: 90.8286657333374  Train_KL: 3.5039049983024597  Validation Loss : 93.66102600097656 Val_Reconstruction : 90.2099609375 Val_KL : 3.4510650634765625\n","Epoch: 2769/8000  Traning Loss: 94.79357814788818  Train_Reconstruction: 91.28684520721436  Train_KL: 3.5067329704761505  Validation Loss : 94.5616340637207 Val_Reconstruction : 91.10073471069336 Val_KL : 3.460901975631714\n","Epoch: 2770/8000  Traning Loss: 95.39339351654053  Train_Reconstruction: 91.88882732391357  Train_KL: 3.504565715789795  Validation Loss : 94.57676696777344 Val_Reconstruction : 91.11287689208984 Val_KL : 3.463892936706543\n","Epoch: 2771/8000  Traning Loss: 95.25043487548828  Train_Reconstruction: 91.74792957305908  Train_KL: 3.5025050938129425  Validation Loss : 94.60041809082031 Val_Reconstruction : 91.15226745605469 Val_KL : 3.448150634765625\n","Epoch: 2772/8000  Traning Loss: 94.8166618347168  Train_Reconstruction: 91.32078075408936  Train_KL: 3.4958823323249817  Validation Loss : 93.82524108886719 Val_Reconstruction : 90.36725616455078 Val_KL : 3.4579838514328003\n","Epoch: 2773/8000  Traning Loss: 94.40446281433105  Train_Reconstruction: 90.89650630950928  Train_KL: 3.507955551147461  Validation Loss : 94.1456527709961 Val_Reconstruction : 90.68283462524414 Val_KL : 3.462818145751953\n","Epoch: 2774/8000  Traning Loss: 95.12251663208008  Train_Reconstruction: 91.62463283538818  Train_KL: 3.4978828132152557  Validation Loss : 95.35646057128906 Val_Reconstruction : 91.91195678710938 Val_KL : 3.444504499435425\n","Epoch: 2775/8000  Traning Loss: 95.5992078781128  Train_Reconstruction: 92.10688495635986  Train_KL: 3.492323637008667  Validation Loss : 94.57422256469727 Val_Reconstruction : 91.12957763671875 Val_KL : 3.444643259048462\n","Epoch: 2776/8000  Traning Loss: 95.0923900604248  Train_Reconstruction: 91.5975751876831  Train_KL: 3.4948145151138306  Validation Loss : 94.41209030151367 Val_Reconstruction : 90.94911193847656 Val_KL : 3.4629807472229004\n","Epoch: 2777/8000  Traning Loss: 94.74512004852295  Train_Reconstruction: 91.24551105499268  Train_KL: 3.4996097087860107  Validation Loss : 94.2188720703125 Val_Reconstruction : 90.76144027709961 Val_KL : 3.4574332237243652\n","Epoch: 2778/8000  Traning Loss: 94.48738956451416  Train_Reconstruction: 90.98898506164551  Train_KL: 3.4984036087989807  Validation Loss : 93.88770294189453 Val_Reconstruction : 90.43361282348633 Val_KL : 3.4540880918502808\n","Epoch: 2779/8000  Traning Loss: 94.27761459350586  Train_Reconstruction: 90.77717018127441  Train_KL: 3.500442922115326  Validation Loss : 93.44173049926758 Val_Reconstruction : 89.98944091796875 Val_KL : 3.4522899389266968\n","Epoch: 2780/8000  Traning Loss: 94.03829574584961  Train_Reconstruction: 90.54141807556152  Train_KL: 3.4968784153461456  Validation Loss : 93.2935905456543 Val_Reconstruction : 89.83568572998047 Val_KL : 3.4579073190689087\n","Epoch: 2781/8000  Traning Loss: 94.1637487411499  Train_Reconstruction: 90.65923881530762  Train_KL: 3.5045113265514374  Validation Loss : 93.83002471923828 Val_Reconstruction : 90.36842727661133 Val_KL : 3.461598515510559\n","Epoch: 2782/8000  Traning Loss: 94.17057704925537  Train_Reconstruction: 90.67312622070312  Train_KL: 3.4974495470523834  Validation Loss : 93.54073715209961 Val_Reconstruction : 90.08205032348633 Val_KL : 3.4586886167526245\n","Epoch: 2783/8000  Traning Loss: 94.23925590515137  Train_Reconstruction: 90.73716354370117  Train_KL: 3.50209379196167  Validation Loss : 93.89471435546875 Val_Reconstruction : 90.43816375732422 Val_KL : 3.4565471410751343\n","Epoch: 2784/8000  Traning Loss: 94.37920475006104  Train_Reconstruction: 90.87522315979004  Train_KL: 3.503982424736023  Validation Loss : 94.00695037841797 Val_Reconstruction : 90.55438995361328 Val_KL : 3.4525588750839233\n","Epoch: 2785/8000  Traning Loss: 94.46504402160645  Train_Reconstruction: 90.96073246002197  Train_KL: 3.5043118000030518  Validation Loss : 93.84396743774414 Val_Reconstruction : 90.37954330444336 Val_KL : 3.4644246101379395\n","Epoch: 2786/8000  Traning Loss: 94.41226291656494  Train_Reconstruction: 90.90719509124756  Train_KL: 3.505068063735962  Validation Loss : 93.85060119628906 Val_Reconstruction : 90.38275146484375 Val_KL : 3.4678492546081543\n","Epoch: 2787/8000  Traning Loss: 94.56264877319336  Train_Reconstruction: 91.05380058288574  Train_KL: 3.508849173784256  Validation Loss : 93.86598205566406 Val_Reconstruction : 90.3941879272461 Val_KL : 3.471795678138733\n","Epoch: 2788/8000  Traning Loss: 95.06564712524414  Train_Reconstruction: 91.55143451690674  Train_KL: 3.514214813709259  Validation Loss : 94.303955078125 Val_Reconstruction : 90.84032821655273 Val_KL : 3.4636270999908447\n","Epoch: 2789/8000  Traning Loss: 94.83575057983398  Train_Reconstruction: 91.33403491973877  Train_KL: 3.501716762781143  Validation Loss : 94.1982650756836 Val_Reconstruction : 90.74607467651367 Val_KL : 3.452191114425659\n","Epoch: 2790/8000  Traning Loss: 94.79216289520264  Train_Reconstruction: 91.28196239471436  Train_KL: 3.5102009773254395  Validation Loss : 93.89804458618164 Val_Reconstruction : 90.42754364013672 Val_KL : 3.4705029726028442\n","Epoch: 2791/8000  Traning Loss: 94.04174327850342  Train_Reconstruction: 90.53261470794678  Train_KL: 3.509129047393799  Validation Loss : 93.46878814697266 Val_Reconstruction : 90.00342559814453 Val_KL : 3.4653615951538086\n","Epoch: 2792/8000  Traning Loss: 94.07500743865967  Train_Reconstruction: 90.57879734039307  Train_KL: 3.496210366487503  Validation Loss : 93.46089553833008 Val_Reconstruction : 90.0141372680664 Val_KL : 3.4467567205429077\n","Epoch: 2793/8000  Traning Loss: 94.28864002227783  Train_Reconstruction: 90.78734970092773  Train_KL: 3.501290738582611  Validation Loss : 94.04561233520508 Val_Reconstruction : 90.57635879516602 Val_KL : 3.4692553281784058\n","Epoch: 2794/8000  Traning Loss: 94.9637975692749  Train_Reconstruction: 91.44900035858154  Train_KL: 3.5147978365421295  Validation Loss : 94.1780776977539 Val_Reconstruction : 90.72188949584961 Val_KL : 3.4561861753463745\n","Epoch: 2795/8000  Traning Loss: 94.88209438323975  Train_Reconstruction: 91.38239765167236  Train_KL: 3.499696433544159  Validation Loss : 94.23312377929688 Val_Reconstruction : 90.77594375610352 Val_KL : 3.457180380821228\n","Epoch: 2796/8000  Traning Loss: 94.52498722076416  Train_Reconstruction: 91.02132034301758  Train_KL: 3.503666251897812  Validation Loss : 93.46552658081055 Val_Reconstruction : 89.9975357055664 Val_KL : 3.467989444732666\n","Epoch: 2797/8000  Traning Loss: 94.25837516784668  Train_Reconstruction: 90.75483417510986  Train_KL: 3.503540873527527  Validation Loss : 93.74097061157227 Val_Reconstruction : 90.27640914916992 Val_KL : 3.464560389518738\n","Epoch: 2798/8000  Traning Loss: 94.24994564056396  Train_Reconstruction: 90.7429313659668  Train_KL: 3.5070144534111023  Validation Loss : 93.3929672241211 Val_Reconstruction : 89.92267608642578 Val_KL : 3.470290184020996\n","Epoch: 2799/8000  Traning Loss: 94.32776832580566  Train_Reconstruction: 90.81565475463867  Train_KL: 3.512113481760025  Validation Loss : 94.07967758178711 Val_Reconstruction : 90.62194442749023 Val_KL : 3.457730293273926\n","Epoch: 2800/8000  Traning Loss: 94.81467723846436  Train_Reconstruction: 91.31599044799805  Train_KL: 3.498688131570816  Validation Loss : 94.37525177001953 Val_Reconstruction : 90.93008804321289 Val_KL : 3.4451630115509033\n","Epoch: 2801/8000  Traning Loss: 95.17460823059082  Train_Reconstruction: 91.67413711547852  Train_KL: 3.5004710853099823  Validation Loss : 94.8383674621582 Val_Reconstruction : 91.36442565917969 Val_KL : 3.47394335269928\n","Epoch: 2802/8000  Traning Loss: 94.79188537597656  Train_Reconstruction: 91.27805709838867  Train_KL: 3.5138277411460876  Validation Loss : 93.4872055053711 Val_Reconstruction : 90.0167007446289 Val_KL : 3.470505475997925\n","Epoch: 2803/8000  Traning Loss: 93.99228000640869  Train_Reconstruction: 90.4932508468628  Train_KL: 3.4990281462669373  Validation Loss : 93.21637344360352 Val_Reconstruction : 89.7679557800293 Val_KL : 3.448417544364929\n","Epoch: 2804/8000  Traning Loss: 93.88468074798584  Train_Reconstruction: 90.38070678710938  Train_KL: 3.5039739310741425  Validation Loss : 93.6073112487793 Val_Reconstruction : 90.13954162597656 Val_KL : 3.467769980430603\n","Epoch: 2805/8000  Traning Loss: 93.98727130889893  Train_Reconstruction: 90.46692657470703  Train_KL: 3.520344853401184  Validation Loss : 93.41265106201172 Val_Reconstruction : 89.9272346496582 Val_KL : 3.485416531562805\n","Epoch: 2806/8000  Traning Loss: 94.33358383178711  Train_Reconstruction: 90.81654453277588  Train_KL: 3.517040252685547  Validation Loss : 93.69152450561523 Val_Reconstruction : 90.2311019897461 Val_KL : 3.4604233503341675\n","Epoch: 2807/8000  Traning Loss: 94.33561611175537  Train_Reconstruction: 90.83567237854004  Train_KL: 3.49994295835495  Validation Loss : 93.58794784545898 Val_Reconstruction : 90.11898040771484 Val_KL : 3.4689677953720093\n","Epoch: 2808/8000  Traning Loss: 94.20235252380371  Train_Reconstruction: 90.68663120269775  Train_KL: 3.51572185754776  Validation Loss : 93.7525405883789 Val_Reconstruction : 90.27297592163086 Val_KL : 3.4795639514923096\n","Epoch: 2809/8000  Traning Loss: 94.24315071105957  Train_Reconstruction: 90.72525215148926  Train_KL: 3.517897456884384  Validation Loss : 93.6602554321289 Val_Reconstruction : 90.1913070678711 Val_KL : 3.46894633769989\n","Epoch: 2810/8000  Traning Loss: 94.41617774963379  Train_Reconstruction: 90.92005825042725  Train_KL: 3.496118664741516  Validation Loss : 93.82847213745117 Val_Reconstruction : 90.38404846191406 Val_KL : 3.444421648979187\n","Epoch: 2811/8000  Traning Loss: 94.6181697845459  Train_Reconstruction: 91.11912155151367  Train_KL: 3.4990483820438385  Validation Loss : 93.50264358520508 Val_Reconstruction : 90.0418930053711 Val_KL : 3.4607491493225098\n","Epoch: 2812/8000  Traning Loss: 94.1279706954956  Train_Reconstruction: 90.62571048736572  Train_KL: 3.502261161804199  Validation Loss : 93.49606323242188 Val_Reconstruction : 90.04589462280273 Val_KL : 3.450169324874878\n","Epoch: 2813/8000  Traning Loss: 94.2527904510498  Train_Reconstruction: 90.75381565093994  Train_KL: 3.498973697423935  Validation Loss : 93.39114761352539 Val_Reconstruction : 89.93377304077148 Val_KL : 3.4573758840560913\n","Epoch: 2814/8000  Traning Loss: 94.21536350250244  Train_Reconstruction: 90.70936679840088  Train_KL: 3.5059967041015625  Validation Loss : 93.65317153930664 Val_Reconstruction : 90.18850326538086 Val_KL : 3.46466863155365\n","Epoch: 2815/8000  Traning Loss: 94.0915355682373  Train_Reconstruction: 90.58881664276123  Train_KL: 3.502718359231949  Validation Loss : 93.46139526367188 Val_Reconstruction : 90.0073356628418 Val_KL : 3.4540568590164185\n","Epoch: 2816/8000  Traning Loss: 94.31963062286377  Train_Reconstruction: 90.8241548538208  Train_KL: 3.49547615647316  Validation Loss : 93.97455596923828 Val_Reconstruction : 90.51179122924805 Val_KL : 3.462767720222473\n","Epoch: 2817/8000  Traning Loss: 94.54221248626709  Train_Reconstruction: 91.02723217010498  Train_KL: 3.5149805545806885  Validation Loss : 93.95903396606445 Val_Reconstruction : 90.48141479492188 Val_KL : 3.4776222705841064\n","Epoch: 2818/8000  Traning Loss: 94.50432109832764  Train_Reconstruction: 90.98779106140137  Train_KL: 3.5165298879146576  Validation Loss : 93.71009063720703 Val_Reconstruction : 90.2438850402832 Val_KL : 3.4662084579467773\n","Epoch: 2819/8000  Traning Loss: 94.49493026733398  Train_Reconstruction: 90.99359703063965  Train_KL: 3.501332938671112  Validation Loss : 93.87939071655273 Val_Reconstruction : 90.42035293579102 Val_KL : 3.4590377807617188\n","Epoch: 2820/8000  Traning Loss: 94.4132194519043  Train_Reconstruction: 90.90796184539795  Train_KL: 3.505257785320282  Validation Loss : 93.52800369262695 Val_Reconstruction : 90.05340576171875 Val_KL : 3.47459614276886\n","Epoch: 2821/8000  Traning Loss: 94.3023738861084  Train_Reconstruction: 90.78528213500977  Train_KL: 3.517091929912567  Validation Loss : 93.63857650756836 Val_Reconstruction : 90.16500854492188 Val_KL : 3.4735668897628784\n","Epoch: 2822/8000  Traning Loss: 94.02231311798096  Train_Reconstruction: 90.51670265197754  Train_KL: 3.5056101381778717  Validation Loss : 93.3344955444336 Val_Reconstruction : 89.88249588012695 Val_KL : 3.451997399330139\n","Epoch: 2823/8000  Traning Loss: 94.22685813903809  Train_Reconstruction: 90.73186683654785  Train_KL: 3.494991958141327  Validation Loss : 93.61942672729492 Val_Reconstruction : 90.1573715209961 Val_KL : 3.4620567560195923\n","Epoch: 2824/8000  Traning Loss: 94.09210586547852  Train_Reconstruction: 90.57522296905518  Train_KL: 3.516883611679077  Validation Loss : 93.31327438354492 Val_Reconstruction : 89.83288955688477 Val_KL : 3.480382561683655\n","Epoch: 2825/8000  Traning Loss: 94.17087459564209  Train_Reconstruction: 90.65471649169922  Train_KL: 3.5161590576171875  Validation Loss : 93.64942932128906 Val_Reconstruction : 90.18112564086914 Val_KL : 3.468304395675659\n","Epoch: 2826/8000  Traning Loss: 94.65897464752197  Train_Reconstruction: 91.15768146514893  Train_KL: 3.501294583082199  Validation Loss : 94.36441040039062 Val_Reconstruction : 90.91286087036133 Val_KL : 3.451549768447876\n","Epoch: 2827/8000  Traning Loss: 95.36246109008789  Train_Reconstruction: 91.86067390441895  Train_KL: 3.5017865002155304  Validation Loss : 95.14675903320312 Val_Reconstruction : 91.6871337890625 Val_KL : 3.459623098373413\n","Epoch: 2828/8000  Traning Loss: 94.90750503540039  Train_Reconstruction: 91.39122867584229  Train_KL: 3.516276866197586  Validation Loss : 94.1545295715332 Val_Reconstruction : 90.66878890991211 Val_KL : 3.4857407808303833\n","Epoch: 2829/8000  Traning Loss: 94.74650573730469  Train_Reconstruction: 91.22838401794434  Train_KL: 3.5181232392787933  Validation Loss : 93.47127151489258 Val_Reconstruction : 90.00252914428711 Val_KL : 3.4687435626983643\n","Epoch: 2830/8000  Traning Loss: 94.38649463653564  Train_Reconstruction: 90.8919038772583  Train_KL: 3.494590938091278  Validation Loss : 93.90026092529297 Val_Reconstruction : 90.45734024047852 Val_KL : 3.442923903465271\n","Epoch: 2831/8000  Traning Loss: 94.0623025894165  Train_Reconstruction: 90.56124973297119  Train_KL: 3.501053214073181  Validation Loss : 93.40146255493164 Val_Reconstruction : 89.9346809387207 Val_KL : 3.4667820930480957\n","Epoch: 2832/8000  Traning Loss: 94.10784244537354  Train_Reconstruction: 90.5934066772461  Train_KL: 3.5144351422786713  Validation Loss : 93.69815063476562 Val_Reconstruction : 90.23217010498047 Val_KL : 3.4659807682037354\n","Epoch: 2833/8000  Traning Loss: 94.37648010253906  Train_Reconstruction: 90.87423801422119  Train_KL: 3.5022424161434174  Validation Loss : 93.97554016113281 Val_Reconstruction : 90.51798629760742 Val_KL : 3.4575555324554443\n","Epoch: 2834/8000  Traning Loss: 94.57823657989502  Train_Reconstruction: 91.07992839813232  Train_KL: 3.4983089566230774  Validation Loss : 94.0025520324707 Val_Reconstruction : 90.54098129272461 Val_KL : 3.4615737199783325\n","Epoch: 2835/8000  Traning Loss: 94.8795166015625  Train_Reconstruction: 91.36502075195312  Train_KL: 3.514494478702545  Validation Loss : 94.1730842590332 Val_Reconstruction : 90.69305419921875 Val_KL : 3.48003089427948\n","Epoch: 2836/8000  Traning Loss: 94.28826141357422  Train_Reconstruction: 90.77925205230713  Train_KL: 3.5090097784996033  Validation Loss : 93.43758010864258 Val_Reconstruction : 89.97906112670898 Val_KL : 3.4585185050964355\n","Epoch: 2837/8000  Traning Loss: 93.78924942016602  Train_Reconstruction: 90.28868007659912  Train_KL: 3.5005690455436707  Validation Loss : 93.2585334777832 Val_Reconstruction : 89.80231857299805 Val_KL : 3.4562143087387085\n","Epoch: 2838/8000  Traning Loss: 94.00533962249756  Train_Reconstruction: 90.51020908355713  Train_KL: 3.4951292276382446  Validation Loss : 93.3804702758789 Val_Reconstruction : 89.92314910888672 Val_KL : 3.457322597503662\n","Epoch: 2839/8000  Traning Loss: 94.40779495239258  Train_Reconstruction: 90.89486789703369  Train_KL: 3.512928158044815  Validation Loss : 94.11331558227539 Val_Reconstruction : 90.63991928100586 Val_KL : 3.4733951091766357\n","Epoch: 2840/8000  Traning Loss: 94.71006965637207  Train_Reconstruction: 91.19087028503418  Train_KL: 3.519199311733246  Validation Loss : 94.37189102172852 Val_Reconstruction : 90.91043472290039 Val_KL : 3.4614561796188354\n","Epoch: 2841/8000  Traning Loss: 94.71820831298828  Train_Reconstruction: 91.22059917449951  Train_KL: 3.4976096153259277  Validation Loss : 94.1741943359375 Val_Reconstruction : 90.72558975219727 Val_KL : 3.448604702949524\n","Epoch: 2842/8000  Traning Loss: 94.60716342926025  Train_Reconstruction: 91.11506748199463  Train_KL: 3.4920970797538757  Validation Loss : 94.26761245727539 Val_Reconstruction : 90.81182861328125 Val_KL : 3.4557830095291138\n","Epoch: 2843/8000  Traning Loss: 94.47852516174316  Train_Reconstruction: 90.96020793914795  Train_KL: 3.5183171033859253  Validation Loss : 94.46462631225586 Val_Reconstruction : 90.98278427124023 Val_KL : 3.4818423986434937\n","Epoch: 2844/8000  Traning Loss: 94.18919658660889  Train_Reconstruction: 90.66932582855225  Train_KL: 3.5198701322078705  Validation Loss : 93.45770263671875 Val_Reconstruction : 89.9854621887207 Val_KL : 3.4722403287887573\n","Epoch: 2845/8000  Traning Loss: 94.21200752258301  Train_Reconstruction: 90.70707035064697  Train_KL: 3.5049364864826202  Validation Loss : 93.621337890625 Val_Reconstruction : 90.16789627075195 Val_KL : 3.453443765640259\n","Epoch: 2846/8000  Traning Loss: 94.23459529876709  Train_Reconstruction: 90.74068546295166  Train_KL: 3.4939105808734894  Validation Loss : 93.62682342529297 Val_Reconstruction : 90.18050384521484 Val_KL : 3.446322441101074\n","Epoch: 2847/8000  Traning Loss: 94.3372573852539  Train_Reconstruction: 90.83641147613525  Train_KL: 3.5008446872234344  Validation Loss : 93.47269821166992 Val_Reconstruction : 90.01383590698242 Val_KL : 3.4588629007339478\n","Epoch: 2848/8000  Traning Loss: 94.22686672210693  Train_Reconstruction: 90.71651077270508  Train_KL: 3.510355532169342  Validation Loss : 93.5306167602539 Val_Reconstruction : 90.05979919433594 Val_KL : 3.470817446708679\n","Epoch: 2849/8000  Traning Loss: 94.26082706451416  Train_Reconstruction: 90.75008392333984  Train_KL: 3.51074481010437  Validation Loss : 93.55965423583984 Val_Reconstruction : 90.0921745300293 Val_KL : 3.4674826860427856\n","Epoch: 2850/8000  Traning Loss: 94.30965518951416  Train_Reconstruction: 90.80521774291992  Train_KL: 3.5044380128383636  Validation Loss : 93.63817977905273 Val_Reconstruction : 90.17486953735352 Val_KL : 3.46330726146698\n","Epoch: 2851/8000  Traning Loss: 94.30265712738037  Train_Reconstruction: 90.79342269897461  Train_KL: 3.5092343986034393  Validation Loss : 93.57275772094727 Val_Reconstruction : 90.11410140991211 Val_KL : 3.4586594104766846\n","Epoch: 2852/8000  Traning Loss: 94.498291015625  Train_Reconstruction: 90.99646282196045  Train_KL: 3.501827120780945  Validation Loss : 93.6786880493164 Val_Reconstruction : 90.2204360961914 Val_KL : 3.4582509994506836\n","Epoch: 2853/8000  Traning Loss: 94.29622173309326  Train_Reconstruction: 90.79848766326904  Train_KL: 3.4977355301380157  Validation Loss : 93.54819869995117 Val_Reconstruction : 90.09489822387695 Val_KL : 3.453298568725586\n","Epoch: 2854/8000  Traning Loss: 94.47447204589844  Train_Reconstruction: 90.97591876983643  Train_KL: 3.498553991317749  Validation Loss : 93.63628387451172 Val_Reconstruction : 90.18092727661133 Val_KL : 3.4553550481796265\n","Epoch: 2855/8000  Traning Loss: 94.51637744903564  Train_Reconstruction: 91.01195240020752  Train_KL: 3.504424601793289  Validation Loss : 94.34303283691406 Val_Reconstruction : 90.88516235351562 Val_KL : 3.457872986793518\n","Epoch: 2856/8000  Traning Loss: 94.56171131134033  Train_Reconstruction: 91.05976676940918  Train_KL: 3.5019449591636658  Validation Loss : 93.99466705322266 Val_Reconstruction : 90.54296875 Val_KL : 3.4516972303390503\n","Epoch: 2857/8000  Traning Loss: 94.11349487304688  Train_Reconstruction: 90.62428379058838  Train_KL: 3.489211618900299  Validation Loss : 93.35291290283203 Val_Reconstruction : 89.89950180053711 Val_KL : 3.453409790992737\n","Epoch: 2858/8000  Traning Loss: 94.0747766494751  Train_Reconstruction: 90.56271362304688  Train_KL: 3.5120630860328674  Validation Loss : 93.5186653137207 Val_Reconstruction : 90.03229141235352 Val_KL : 3.4863734245300293\n","Epoch: 2859/8000  Traning Loss: 94.23270511627197  Train_Reconstruction: 90.71185493469238  Train_KL: 3.5208497643470764  Validation Loss : 93.67610931396484 Val_Reconstruction : 90.20500183105469 Val_KL : 3.471107006072998\n","Epoch: 2860/8000  Traning Loss: 94.65453147888184  Train_Reconstruction: 91.14720153808594  Train_KL: 3.5073297321796417  Validation Loss : 93.89472579956055 Val_Reconstruction : 90.43458938598633 Val_KL : 3.4601356983184814\n","Epoch: 2861/8000  Traning Loss: 94.89894485473633  Train_Reconstruction: 91.39314365386963  Train_KL: 3.5058006942272186  Validation Loss : 94.62656784057617 Val_Reconstruction : 91.166748046875 Val_KL : 3.45982027053833\n","Epoch: 2862/8000  Traning Loss: 95.50995445251465  Train_Reconstruction: 92.00871562957764  Train_KL: 3.501238763332367  Validation Loss : 95.17464065551758 Val_Reconstruction : 91.718994140625 Val_KL : 3.4556463956832886\n","Epoch: 2863/8000  Traning Loss: 95.40784168243408  Train_Reconstruction: 91.89907264709473  Train_KL: 3.5087685585021973  Validation Loss : 93.92456817626953 Val_Reconstruction : 90.46086883544922 Val_KL : 3.463697910308838\n","Epoch: 2864/8000  Traning Loss: 94.52759265899658  Train_Reconstruction: 91.01376628875732  Train_KL: 3.51382839679718  Validation Loss : 93.5946273803711 Val_Reconstruction : 90.1196517944336 Val_KL : 3.474976897239685\n","Epoch: 2865/8000  Traning Loss: 94.14241123199463  Train_Reconstruction: 90.6234245300293  Train_KL: 3.518985241651535  Validation Loss : 93.66841888427734 Val_Reconstruction : 90.19388580322266 Val_KL : 3.474531650543213\n","Epoch: 2866/8000  Traning Loss: 94.17269611358643  Train_Reconstruction: 90.66201877593994  Train_KL: 3.510676473379135  Validation Loss : 93.5311279296875 Val_Reconstruction : 90.07090377807617 Val_KL : 3.460224151611328\n","Epoch: 2867/8000  Traning Loss: 94.22569751739502  Train_Reconstruction: 90.71533107757568  Train_KL: 3.510365128517151  Validation Loss : 93.56466674804688 Val_Reconstruction : 90.10189437866211 Val_KL : 3.462772488594055\n","Epoch: 2868/8000  Traning Loss: 93.9576063156128  Train_Reconstruction: 90.44814872741699  Train_KL: 3.50945907831192  Validation Loss : 93.06540298461914 Val_Reconstruction : 89.60330200195312 Val_KL : 3.462102174758911\n","Epoch: 2869/8000  Traning Loss: 93.89881420135498  Train_Reconstruction: 90.39248943328857  Train_KL: 3.5063253045082092  Validation Loss : 93.40349197387695 Val_Reconstruction : 89.93890762329102 Val_KL : 3.464583396911621\n","Epoch: 2870/8000  Traning Loss: 94.11596012115479  Train_Reconstruction: 90.60896110534668  Train_KL: 3.5069998502731323  Validation Loss : 93.85671997070312 Val_Reconstruction : 90.39153671264648 Val_KL : 3.4651830196380615\n","Epoch: 2871/8000  Traning Loss: 94.33964538574219  Train_Reconstruction: 90.83202266693115  Train_KL: 3.507623314857483  Validation Loss : 93.60099029541016 Val_Reconstruction : 90.14051055908203 Val_KL : 3.46047842502594\n","Epoch: 2872/8000  Traning Loss: 94.43608283996582  Train_Reconstruction: 90.93620014190674  Train_KL: 3.4998825192451477  Validation Loss : 93.94237518310547 Val_Reconstruction : 90.49263000488281 Val_KL : 3.4497441053390503\n","Epoch: 2873/8000  Traning Loss: 94.33193588256836  Train_Reconstruction: 90.82493877410889  Train_KL: 3.506995916366577  Validation Loss : 93.65716934204102 Val_Reconstruction : 90.18828201293945 Val_KL : 3.4688868522644043\n","Epoch: 2874/8000  Traning Loss: 94.47008228302002  Train_Reconstruction: 90.95792198181152  Train_KL: 3.5121607780456543  Validation Loss : 94.29370498657227 Val_Reconstruction : 90.82812118530273 Val_KL : 3.465583562850952\n","Epoch: 2875/8000  Traning Loss: 94.20101165771484  Train_Reconstruction: 90.69754028320312  Train_KL: 3.5034721195697784  Validation Loss : 93.32647705078125 Val_Reconstruction : 89.87533950805664 Val_KL : 3.4511375427246094\n","Epoch: 2876/8000  Traning Loss: 94.20872497558594  Train_Reconstruction: 90.71306133270264  Train_KL: 3.4956647157669067  Validation Loss : 93.9149398803711 Val_Reconstruction : 90.4666748046875 Val_KL : 3.448264241218567\n","Epoch: 2877/8000  Traning Loss: 94.35581684112549  Train_Reconstruction: 90.85646915435791  Train_KL: 3.499346911907196  Validation Loss : 93.66421127319336 Val_Reconstruction : 90.21360778808594 Val_KL : 3.4506036043167114\n","Epoch: 2878/8000  Traning Loss: 94.9150743484497  Train_Reconstruction: 91.41054916381836  Train_KL: 3.5045251548290253  Validation Loss : 94.04759979248047 Val_Reconstruction : 90.57901382446289 Val_KL : 3.468585968017578\n","Epoch: 2879/8000  Traning Loss: 94.76841640472412  Train_Reconstruction: 91.25968551635742  Train_KL: 3.508729189634323  Validation Loss : 93.80320358276367 Val_Reconstruction : 90.33278274536133 Val_KL : 3.470421314239502\n","Epoch: 2880/8000  Traning Loss: 94.79590606689453  Train_Reconstruction: 91.28442764282227  Train_KL: 3.5114773213863373  Validation Loss : 93.88174819946289 Val_Reconstruction : 90.41890335083008 Val_KL : 3.462846279144287\n","Epoch: 2881/8000  Traning Loss: 94.42372131347656  Train_Reconstruction: 90.91927909851074  Train_KL: 3.5044417083263397  Validation Loss : 93.72317504882812 Val_Reconstruction : 90.26451873779297 Val_KL : 3.4586539268493652\n","Epoch: 2882/8000  Traning Loss: 94.00172424316406  Train_Reconstruction: 90.49812889099121  Train_KL: 3.5035951733589172  Validation Loss : 93.39732360839844 Val_Reconstruction : 89.94106674194336 Val_KL : 3.4562571048736572\n","Epoch: 2883/8000  Traning Loss: 93.99346351623535  Train_Reconstruction: 90.50136089324951  Train_KL: 3.4921011328697205  Validation Loss : 93.34067916870117 Val_Reconstruction : 89.89163208007812 Val_KL : 3.4490487575531006\n","Epoch: 2884/8000  Traning Loss: 93.91311359405518  Train_Reconstruction: 90.41847801208496  Train_KL: 3.494634449481964  Validation Loss : 93.5534553527832 Val_Reconstruction : 90.09032821655273 Val_KL : 3.4631279706954956\n","Epoch: 2885/8000  Traning Loss: 94.28043460845947  Train_Reconstruction: 90.76888751983643  Train_KL: 3.5115471482276917  Validation Loss : 93.54784774780273 Val_Reconstruction : 90.07118606567383 Val_KL : 3.476662278175354\n","Epoch: 2886/8000  Traning Loss: 94.12859153747559  Train_Reconstruction: 90.62368869781494  Train_KL: 3.504904568195343  Validation Loss : 93.58322143554688 Val_Reconstruction : 90.12311553955078 Val_KL : 3.460107445716858\n","Epoch: 2887/8000  Traning Loss: 94.09278774261475  Train_Reconstruction: 90.59363079071045  Train_KL: 3.4991559386253357  Validation Loss : 93.57822036743164 Val_Reconstruction : 90.11178588867188 Val_KL : 3.4664371013641357\n","Epoch: 2888/8000  Traning Loss: 94.26767063140869  Train_Reconstruction: 90.75497055053711  Train_KL: 3.5126990973949432  Validation Loss : 93.83238220214844 Val_Reconstruction : 90.3621826171875 Val_KL : 3.470198631286621\n","Epoch: 2889/8000  Traning Loss: 94.50338172912598  Train_Reconstruction: 90.99507999420166  Train_KL: 3.5083008408546448  Validation Loss : 93.63712310791016 Val_Reconstruction : 90.17042922973633 Val_KL : 3.466694474220276\n","Epoch: 2890/8000  Traning Loss: 94.16734600067139  Train_Reconstruction: 90.66133689880371  Train_KL: 3.506007820367813  Validation Loss : 93.21765518188477 Val_Reconstruction : 89.7569808959961 Val_KL : 3.4606722593307495\n","Epoch: 2891/8000  Traning Loss: 94.1280574798584  Train_Reconstruction: 90.62629222869873  Train_KL: 3.5017642080783844  Validation Loss : 93.55171203613281 Val_Reconstruction : 90.09288787841797 Val_KL : 3.4588241577148438\n","Epoch: 2892/8000  Traning Loss: 93.98878383636475  Train_Reconstruction: 90.48446559906006  Train_KL: 3.5043179094791412  Validation Loss : 93.86979675292969 Val_Reconstruction : 90.4066047668457 Val_KL : 3.4631930589675903\n","Epoch: 2893/8000  Traning Loss: 94.07289505004883  Train_Reconstruction: 90.55842113494873  Train_KL: 3.5144755244255066  Validation Loss : 93.22994995117188 Val_Reconstruction : 89.75108337402344 Val_KL : 3.4788658618927\n","Epoch: 2894/8000  Traning Loss: 94.03187847137451  Train_Reconstruction: 90.52757167816162  Train_KL: 3.504306346178055  Validation Loss : 93.35660934448242 Val_Reconstruction : 89.90067672729492 Val_KL : 3.4559340476989746\n","Epoch: 2895/8000  Traning Loss: 93.91361808776855  Train_Reconstruction: 90.41722774505615  Train_KL: 3.4963910579681396  Validation Loss : 93.34496307373047 Val_Reconstruction : 89.88621139526367 Val_KL : 3.4587548971176147\n","Epoch: 2896/8000  Traning Loss: 93.78532886505127  Train_Reconstruction: 90.27361106872559  Train_KL: 3.5117176473140717  Validation Loss : 93.2648696899414 Val_Reconstruction : 89.79297637939453 Val_KL : 3.471895217895508\n","Epoch: 2897/8000  Traning Loss: 93.8407793045044  Train_Reconstruction: 90.32592296600342  Train_KL: 3.5148565471172333  Validation Loss : 93.29800033569336 Val_Reconstruction : 89.82804489135742 Val_KL : 3.4699536561965942\n","Epoch: 2898/8000  Traning Loss: 94.16925430297852  Train_Reconstruction: 90.6570291519165  Train_KL: 3.512224555015564  Validation Loss : 93.91054916381836 Val_Reconstruction : 90.43733978271484 Val_KL : 3.4732096195220947\n","Epoch: 2899/8000  Traning Loss: 94.24049472808838  Train_Reconstruction: 90.73610782623291  Train_KL: 3.5043868124485016  Validation Loss : 93.33706283569336 Val_Reconstruction : 89.88268661499023 Val_KL : 3.4543747901916504\n","Epoch: 2900/8000  Traning Loss: 93.8887767791748  Train_Reconstruction: 90.3858995437622  Train_KL: 3.502876967191696  Validation Loss : 93.37759017944336 Val_Reconstruction : 89.90684509277344 Val_KL : 3.4707430601119995\n","Epoch: 2901/8000  Traning Loss: 94.05884552001953  Train_Reconstruction: 90.5502405166626  Train_KL: 3.5086054503917694  Validation Loss : 93.56376266479492 Val_Reconstruction : 90.10355377197266 Val_KL : 3.4602078199386597\n","Epoch: 2902/8000  Traning Loss: 94.3939847946167  Train_Reconstruction: 90.8997573852539  Train_KL: 3.4942272305488586  Validation Loss : 94.11916732788086 Val_Reconstruction : 90.66608428955078 Val_KL : 3.4530829191207886\n","Epoch: 2903/8000  Traning Loss: 94.2518367767334  Train_Reconstruction: 90.74449348449707  Train_KL: 3.507343143224716  Validation Loss : 93.6209716796875 Val_Reconstruction : 90.14311218261719 Val_KL : 3.477857232093811\n","Epoch: 2904/8000  Traning Loss: 94.06274604797363  Train_Reconstruction: 90.54094886779785  Train_KL: 3.521796941757202  Validation Loss : 93.20294570922852 Val_Reconstruction : 89.73035430908203 Val_KL : 3.4725886583328247\n","Epoch: 2905/8000  Traning Loss: 94.45318508148193  Train_Reconstruction: 90.94599056243896  Train_KL: 3.5071938037872314  Validation Loss : 93.90703201293945 Val_Reconstruction : 90.4490966796875 Val_KL : 3.457935094833374\n","Epoch: 2906/8000  Traning Loss: 94.56816291809082  Train_Reconstruction: 91.07060623168945  Train_KL: 3.4975562691688538  Validation Loss : 94.10914611816406 Val_Reconstruction : 90.6596794128418 Val_KL : 3.4494688510894775\n","Epoch: 2907/8000  Traning Loss: 94.26226615905762  Train_Reconstruction: 90.77012062072754  Train_KL: 3.4921445548534393  Validation Loss : 93.62309646606445 Val_Reconstruction : 90.1711540222168 Val_KL : 3.451942563056946\n","Epoch: 2908/8000  Traning Loss: 93.76022720336914  Train_Reconstruction: 90.26072788238525  Train_KL: 3.499498635530472  Validation Loss : 93.32390975952148 Val_Reconstruction : 89.86470413208008 Val_KL : 3.4592032432556152\n","Epoch: 2909/8000  Traning Loss: 93.83694267272949  Train_Reconstruction: 90.3320665359497  Train_KL: 3.504876971244812  Validation Loss : 93.2052230834961 Val_Reconstruction : 89.7397232055664 Val_KL : 3.4655022621154785\n","Epoch: 2910/8000  Traning Loss: 93.95865058898926  Train_Reconstruction: 90.4540548324585  Train_KL: 3.504595845937729  Validation Loss : 94.00747299194336 Val_Reconstruction : 90.53967666625977 Val_KL : 3.467796802520752\n","Epoch: 2911/8000  Traning Loss: 94.66948699951172  Train_Reconstruction: 91.1610918045044  Train_KL: 3.5083954632282257  Validation Loss : 94.5958480834961 Val_Reconstruction : 91.1309814453125 Val_KL : 3.464865207672119\n","Epoch: 2912/8000  Traning Loss: 94.65485668182373  Train_Reconstruction: 91.15064716339111  Train_KL: 3.5042077004909515  Validation Loss : 93.68415451049805 Val_Reconstruction : 90.21537017822266 Val_KL : 3.46878445148468\n","Epoch: 2913/8000  Traning Loss: 94.15954780578613  Train_Reconstruction: 90.64050674438477  Train_KL: 3.5190407037734985  Validation Loss : 93.58342742919922 Val_Reconstruction : 90.10895156860352 Val_KL : 3.4744749069213867\n","Epoch: 2914/8000  Traning Loss: 94.05164813995361  Train_Reconstruction: 90.54513931274414  Train_KL: 3.506509840488434  Validation Loss : 93.44737243652344 Val_Reconstruction : 89.99182510375977 Val_KL : 3.4555476903915405\n","Epoch: 2915/8000  Traning Loss: 94.13193893432617  Train_Reconstruction: 90.63197898864746  Train_KL: 3.49995818734169  Validation Loss : 93.53147506713867 Val_Reconstruction : 90.07052993774414 Val_KL : 3.4609450101852417\n","Epoch: 2916/8000  Traning Loss: 93.87372779846191  Train_Reconstruction: 90.37064361572266  Train_KL: 3.5030838549137115  Validation Loss : 93.14047622680664 Val_Reconstruction : 89.6835823059082 Val_KL : 3.4568965435028076\n","Epoch: 2917/8000  Traning Loss: 93.7211046218872  Train_Reconstruction: 90.21855640411377  Train_KL: 3.5025480687618256  Validation Loss : 93.3494644165039 Val_Reconstruction : 89.88375854492188 Val_KL : 3.4657037258148193\n","Epoch: 2918/8000  Traning Loss: 93.69415664672852  Train_Reconstruction: 90.17461681365967  Train_KL: 3.5195397436618805  Validation Loss : 93.3608627319336 Val_Reconstruction : 89.88688659667969 Val_KL : 3.4739736318588257\n","Epoch: 2919/8000  Traning Loss: 93.93733215332031  Train_Reconstruction: 90.43483924865723  Train_KL: 3.5024935007095337  Validation Loss : 93.3400764465332 Val_Reconstruction : 89.88508224487305 Val_KL : 3.454992175102234\n","Epoch: 2920/8000  Traning Loss: 94.76281929016113  Train_Reconstruction: 91.2622184753418  Train_KL: 3.5006017088890076  Validation Loss : 94.67462921142578 Val_Reconstruction : 91.21018981933594 Val_KL : 3.4644418954849243\n","Epoch: 2921/8000  Traning Loss: 94.68284606933594  Train_Reconstruction: 91.17570209503174  Train_KL: 3.5071429014205933  Validation Loss : 94.24600219726562 Val_Reconstruction : 90.77479553222656 Val_KL : 3.471208930015564\n","Epoch: 2922/8000  Traning Loss: 94.25532817840576  Train_Reconstruction: 90.75043678283691  Train_KL: 3.5048923194408417  Validation Loss : 93.64292907714844 Val_Reconstruction : 90.18473052978516 Val_KL : 3.4581979513168335\n","Epoch: 2923/8000  Traning Loss: 94.3046464920044  Train_Reconstruction: 90.79617977142334  Train_KL: 3.508467346429825  Validation Loss : 93.77121353149414 Val_Reconstruction : 90.30438232421875 Val_KL : 3.466832160949707\n","Epoch: 2924/8000  Traning Loss: 94.30384731292725  Train_Reconstruction: 90.79589748382568  Train_KL: 3.507950633764267  Validation Loss : 93.58673095703125 Val_Reconstruction : 90.12268829345703 Val_KL : 3.4640432596206665\n","Epoch: 2925/8000  Traning Loss: 93.98831462860107  Train_Reconstruction: 90.48225975036621  Train_KL: 3.506054997444153  Validation Loss : 93.12230682373047 Val_Reconstruction : 89.66242980957031 Val_KL : 3.4598793983459473\n","Epoch: 2926/8000  Traning Loss: 93.9336748123169  Train_Reconstruction: 90.42762851715088  Train_KL: 3.5060459971427917  Validation Loss : 93.9341926574707 Val_Reconstruction : 90.46167755126953 Val_KL : 3.472515344619751\n","Epoch: 2927/8000  Traning Loss: 94.57143020629883  Train_Reconstruction: 91.05042552947998  Train_KL: 3.521004557609558  Validation Loss : 93.7027702331543 Val_Reconstruction : 90.22031021118164 Val_KL : 3.4824624061584473\n","Epoch: 2928/8000  Traning Loss: 95.21316528320312  Train_Reconstruction: 91.69736862182617  Train_KL: 3.515796571969986  Validation Loss : 94.53789901733398 Val_Reconstruction : 91.06974029541016 Val_KL : 3.4681583642959595\n","Epoch: 2929/8000  Traning Loss: 95.29121208190918  Train_Reconstruction: 91.78710651397705  Train_KL: 3.50410395860672  Validation Loss : 94.32942199707031 Val_Reconstruction : 90.8749771118164 Val_KL : 3.4544456005096436\n","Epoch: 2930/8000  Traning Loss: 94.41928386688232  Train_Reconstruction: 90.90832233428955  Train_KL: 3.510961800813675  Validation Loss : 93.53438949584961 Val_Reconstruction : 90.06135177612305 Val_KL : 3.473036289215088\n","Epoch: 2931/8000  Traning Loss: 94.25989055633545  Train_Reconstruction: 90.74128437042236  Train_KL: 3.51860511302948  Validation Loss : 93.59451293945312 Val_Reconstruction : 90.12158584594727 Val_KL : 3.4729262590408325\n","Epoch: 2932/8000  Traning Loss: 94.28068542480469  Train_Reconstruction: 90.7784366607666  Train_KL: 3.502248525619507  Validation Loss : 93.7977180480957 Val_Reconstruction : 90.34556579589844 Val_KL : 3.4521502256393433\n","Epoch: 2933/8000  Traning Loss: 94.90881252288818  Train_Reconstruction: 91.40764236450195  Train_KL: 3.5011705458164215  Validation Loss : 94.06066513061523 Val_Reconstruction : 90.59940719604492 Val_KL : 3.4612563848495483\n","Epoch: 2934/8000  Traning Loss: 94.83849620819092  Train_Reconstruction: 91.32689380645752  Train_KL: 3.511601001024246  Validation Loss : 94.07915496826172 Val_Reconstruction : 90.6090316772461 Val_KL : 3.4701225757598877\n","Epoch: 2935/8000  Traning Loss: 94.56911182403564  Train_Reconstruction: 91.05954456329346  Train_KL: 3.5095676481723785  Validation Loss : 93.99619674682617 Val_Reconstruction : 90.53968048095703 Val_KL : 3.456515073776245\n","Epoch: 2936/8000  Traning Loss: 94.4298791885376  Train_Reconstruction: 90.92007732391357  Train_KL: 3.509801894426346  Validation Loss : 93.64083099365234 Val_Reconstruction : 90.17132949829102 Val_KL : 3.469502806663513\n","Epoch: 2937/8000  Traning Loss: 94.1874885559082  Train_Reconstruction: 90.67943668365479  Train_KL: 3.508050560951233  Validation Loss : 93.71091079711914 Val_Reconstruction : 90.25211334228516 Val_KL : 3.4587976932525635\n","Epoch: 2938/8000  Traning Loss: 94.15827751159668  Train_Reconstruction: 90.65484428405762  Train_KL: 3.5034331679344177  Validation Loss : 93.44628524780273 Val_Reconstruction : 89.9839859008789 Val_KL : 3.462297797203064\n","Epoch: 2939/8000  Traning Loss: 93.96784400939941  Train_Reconstruction: 90.45901489257812  Train_KL: 3.508830487728119  Validation Loss : 93.36557388305664 Val_Reconstruction : 89.88701629638672 Val_KL : 3.4785557985305786\n","Epoch: 2940/8000  Traning Loss: 94.27812767028809  Train_Reconstruction: 90.75771713256836  Train_KL: 3.520410567522049  Validation Loss : 94.11172103881836 Val_Reconstruction : 90.63898468017578 Val_KL : 3.4727373123168945\n","Epoch: 2941/8000  Traning Loss: 94.66723155975342  Train_Reconstruction: 91.14487171173096  Train_KL: 3.522360473871231  Validation Loss : 93.925537109375 Val_Reconstruction : 90.44149017333984 Val_KL : 3.4840468168258667\n","Epoch: 2942/8000  Traning Loss: 94.00610828399658  Train_Reconstruction: 90.48644065856934  Train_KL: 3.519668459892273  Validation Loss : 93.05691528320312 Val_Reconstruction : 89.58441162109375 Val_KL : 3.4725040197372437\n","Epoch: 2943/8000  Traning Loss: 93.98153591156006  Train_Reconstruction: 90.46978378295898  Train_KL: 3.511752277612686  Validation Loss : 93.40169906616211 Val_Reconstruction : 89.92215347290039 Val_KL : 3.4795475006103516\n","Epoch: 2944/8000  Traning Loss: 94.46141338348389  Train_Reconstruction: 90.94287395477295  Train_KL: 3.518538862466812  Validation Loss : 93.83398056030273 Val_Reconstruction : 90.35728073120117 Val_KL : 3.4767005443573\n","Epoch: 2945/8000  Traning Loss: 94.3412218093872  Train_Reconstruction: 90.82512855529785  Train_KL: 3.5160945057868958  Validation Loss : 93.6917953491211 Val_Reconstruction : 90.22483444213867 Val_KL : 3.4669606685638428\n","Epoch: 2946/8000  Traning Loss: 94.53862571716309  Train_Reconstruction: 91.0415906906128  Train_KL: 3.497035264968872  Validation Loss : 93.81403732299805 Val_Reconstruction : 90.36334609985352 Val_KL : 3.45068895816803\n","Epoch: 2947/8000  Traning Loss: 94.39884185791016  Train_Reconstruction: 90.90042114257812  Train_KL: 3.4984207451343536  Validation Loss : 93.3242416381836 Val_Reconstruction : 89.85883331298828 Val_KL : 3.4654077291488647\n","Epoch: 2948/8000  Traning Loss: 93.95756435394287  Train_Reconstruction: 90.44427967071533  Train_KL: 3.513283520936966  Validation Loss : 93.59288787841797 Val_Reconstruction : 90.11885070800781 Val_KL : 3.474037766456604\n","Epoch: 2949/8000  Traning Loss: 94.00700092315674  Train_Reconstruction: 90.49793148040771  Train_KL: 3.5090690553188324  Validation Loss : 93.36289978027344 Val_Reconstruction : 89.90485763549805 Val_KL : 3.4580421447753906\n","Epoch: 2950/8000  Traning Loss: 94.28064346313477  Train_Reconstruction: 90.77924728393555  Train_KL: 3.5013960003852844  Validation Loss : 93.84162139892578 Val_Reconstruction : 90.36826705932617 Val_KL : 3.473351240158081\n","Epoch: 2951/8000  Traning Loss: 94.28478527069092  Train_Reconstruction: 90.77204895019531  Train_KL: 3.5127378702163696  Validation Loss : 94.08905410766602 Val_Reconstruction : 90.6085319519043 Val_KL : 3.480520009994507\n","Epoch: 2952/8000  Traning Loss: 94.25446033477783  Train_Reconstruction: 90.74594593048096  Train_KL: 3.508514881134033  Validation Loss : 93.53306198120117 Val_Reconstruction : 90.07138442993164 Val_KL : 3.4616771936416626\n","Epoch: 2953/8000  Traning Loss: 93.95450592041016  Train_Reconstruction: 90.45775985717773  Train_KL: 3.496746778488159  Validation Loss : 93.15715026855469 Val_Reconstruction : 89.70938873291016 Val_KL : 3.447761297225952\n","Epoch: 2954/8000  Traning Loss: 93.78397846221924  Train_Reconstruction: 90.27014636993408  Train_KL: 3.5138319432735443  Validation Loss : 93.49508285522461 Val_Reconstruction : 90.01020812988281 Val_KL : 3.4848756790161133\n","Epoch: 2955/8000  Traning Loss: 93.82161140441895  Train_Reconstruction: 90.29322624206543  Train_KL: 3.5283861458301544  Validation Loss : 93.01758575439453 Val_Reconstruction : 89.54548263549805 Val_KL : 3.47210431098938\n","Epoch: 2956/8000  Traning Loss: 93.77921867370605  Train_Reconstruction: 90.2783899307251  Train_KL: 3.5008287131786346  Validation Loss : 93.27499771118164 Val_Reconstruction : 89.83056259155273 Val_KL : 3.4444379806518555\n","Epoch: 2957/8000  Traning Loss: 94.3399772644043  Train_Reconstruction: 90.84557628631592  Train_KL: 3.4944014251232147  Validation Loss : 93.68140029907227 Val_Reconstruction : 90.2176399230957 Val_KL : 3.463759183883667\n","Epoch: 2958/8000  Traning Loss: 94.46412086486816  Train_Reconstruction: 90.94763851165771  Train_KL: 3.516481399536133  Validation Loss : 93.7455940246582 Val_Reconstruction : 90.28021621704102 Val_KL : 3.465379238128662\n","Epoch: 2959/8000  Traning Loss: 94.56181907653809  Train_Reconstruction: 91.05956745147705  Train_KL: 3.5022515654563904  Validation Loss : 93.45365524291992 Val_Reconstruction : 90.01052856445312 Val_KL : 3.4431232213974\n","Epoch: 2960/8000  Traning Loss: 94.28545665740967  Train_Reconstruction: 90.77881050109863  Train_KL: 3.506646066904068  Validation Loss : 93.39062881469727 Val_Reconstruction : 89.92265319824219 Val_KL : 3.467974901199341\n","Epoch: 2961/8000  Traning Loss: 94.26404571533203  Train_Reconstruction: 90.75511837005615  Train_KL: 3.5089266300201416  Validation Loss : 93.70623397827148 Val_Reconstruction : 90.25563430786133 Val_KL : 3.4505982398986816\n","Epoch: 2962/8000  Traning Loss: 93.93918800354004  Train_Reconstruction: 90.43903541564941  Train_KL: 3.5001524090766907  Validation Loss : 93.33454132080078 Val_Reconstruction : 89.87957763671875 Val_KL : 3.454965353012085\n","Epoch: 2963/8000  Traning Loss: 93.65461349487305  Train_Reconstruction: 90.15654754638672  Train_KL: 3.4980663061141968  Validation Loss : 93.18796157836914 Val_Reconstruction : 89.72760772705078 Val_KL : 3.4603543281555176\n","Epoch: 2964/8000  Traning Loss: 93.72618579864502  Train_Reconstruction: 90.22045040130615  Train_KL: 3.5057354271411896  Validation Loss : 92.92631530761719 Val_Reconstruction : 89.45984649658203 Val_KL : 3.4664710760116577\n","Epoch: 2965/8000  Traning Loss: 93.62406349182129  Train_Reconstruction: 90.12412929534912  Train_KL: 3.4999334812164307  Validation Loss : 92.90729522705078 Val_Reconstruction : 89.45038223266602 Val_KL : 3.4569114446640015\n","Epoch: 2966/8000  Traning Loss: 93.90536308288574  Train_Reconstruction: 90.39651203155518  Train_KL: 3.5088528096675873  Validation Loss : 93.28937149047852 Val_Reconstruction : 89.81233978271484 Val_KL : 3.4770315885543823\n","Epoch: 2967/8000  Traning Loss: 93.95141696929932  Train_Reconstruction: 90.42882919311523  Train_KL: 3.5225888192653656  Validation Loss : 93.37477493286133 Val_Reconstruction : 89.90253067016602 Val_KL : 3.472246527671814\n","Epoch: 2968/8000  Traning Loss: 94.0136308670044  Train_Reconstruction: 90.49877166748047  Train_KL: 3.5148580968379974  Validation Loss : 93.42426681518555 Val_Reconstruction : 89.95452499389648 Val_KL : 3.4697418212890625\n","Epoch: 2969/8000  Traning Loss: 94.82155418395996  Train_Reconstruction: 91.31557559967041  Train_KL: 3.505978137254715  Validation Loss : 94.93706893920898 Val_Reconstruction : 91.46361541748047 Val_KL : 3.47345507144928\n","Epoch: 2970/8000  Traning Loss: 94.86136531829834  Train_Reconstruction: 91.34474086761475  Train_KL: 3.5166239738464355  Validation Loss : 94.42575073242188 Val_Reconstruction : 90.94234466552734 Val_KL : 3.4834063053131104\n","Epoch: 2971/8000  Traning Loss: 94.38059711456299  Train_Reconstruction: 90.86112022399902  Train_KL: 3.519476741552353  Validation Loss : 93.88761520385742 Val_Reconstruction : 90.42639923095703 Val_KL : 3.4612146615982056\n","Epoch: 2972/8000  Traning Loss: 94.24639225006104  Train_Reconstruction: 90.74658393859863  Train_KL: 3.499808043241501  Validation Loss : 93.35784530639648 Val_Reconstruction : 89.89860916137695 Val_KL : 3.459233522415161\n","Epoch: 2973/8000  Traning Loss: 94.1963415145874  Train_Reconstruction: 90.69061851501465  Train_KL: 3.5057215988636017  Validation Loss : 94.03097915649414 Val_Reconstruction : 90.56731414794922 Val_KL : 3.4636658430099487\n","Epoch: 2974/8000  Traning Loss: 94.27253341674805  Train_Reconstruction: 90.76397800445557  Train_KL: 3.5085549652576447  Validation Loss : 93.49277114868164 Val_Reconstruction : 90.02405166625977 Val_KL : 3.4687217473983765\n","Epoch: 2975/8000  Traning Loss: 93.84495258331299  Train_Reconstruction: 90.33782958984375  Train_KL: 3.5071233212947845  Validation Loss : 93.57488632202148 Val_Reconstruction : 90.11046981811523 Val_KL : 3.464416027069092\n","Epoch: 2976/8000  Traning Loss: 94.05036354064941  Train_Reconstruction: 90.54022598266602  Train_KL: 3.510136991739273  Validation Loss : 93.18034362792969 Val_Reconstruction : 89.69987106323242 Val_KL : 3.480470895767212\n","Epoch: 2977/8000  Traning Loss: 93.67761516571045  Train_Reconstruction: 90.16001224517822  Train_KL: 3.5176027715206146  Validation Loss : 93.11542129516602 Val_Reconstruction : 89.63203048706055 Val_KL : 3.4833909273147583\n","Epoch: 2978/8000  Traning Loss: 93.66922378540039  Train_Reconstruction: 90.15939140319824  Train_KL: 3.509832054376602  Validation Loss : 93.19685363769531 Val_Reconstruction : 89.73910140991211 Val_KL : 3.4577534198760986\n","Epoch: 2979/8000  Traning Loss: 93.80891036987305  Train_Reconstruction: 90.30878257751465  Train_KL: 3.5001265704631805  Validation Loss : 93.30244445800781 Val_Reconstruction : 89.84354400634766 Val_KL : 3.4589016437530518\n","Epoch: 2980/8000  Traning Loss: 93.99165439605713  Train_Reconstruction: 90.48391342163086  Train_KL: 3.5077421367168427  Validation Loss : 93.82792663574219 Val_Reconstruction : 90.3708724975586 Val_KL : 3.4570544958114624\n","Epoch: 2981/8000  Traning Loss: 93.98052883148193  Train_Reconstruction: 90.4718132019043  Train_KL: 3.5087150633335114  Validation Loss : 93.3595085144043 Val_Reconstruction : 89.90272521972656 Val_KL : 3.4567803144454956\n","Epoch: 2982/8000  Traning Loss: 93.8207540512085  Train_Reconstruction: 90.31781101226807  Train_KL: 3.502944141626358  Validation Loss : 93.62543106079102 Val_Reconstruction : 90.16694259643555 Val_KL : 3.458487868309021\n","Epoch: 2983/8000  Traning Loss: 93.8694658279419  Train_Reconstruction: 90.36584758758545  Train_KL: 3.5036179423332214  Validation Loss : 93.33675765991211 Val_Reconstruction : 89.8696403503418 Val_KL : 3.467117428779602\n","Epoch: 2984/8000  Traning Loss: 93.89766693115234  Train_Reconstruction: 90.3824110031128  Train_KL: 3.5152556598186493  Validation Loss : 93.61820983886719 Val_Reconstruction : 90.14752960205078 Val_KL : 3.47067928314209\n","Epoch: 2985/8000  Traning Loss: 94.22558975219727  Train_Reconstruction: 90.70917892456055  Train_KL: 3.516411304473877  Validation Loss : 93.67536163330078 Val_Reconstruction : 90.20806121826172 Val_KL : 3.467300057411194\n","Epoch: 2986/8000  Traning Loss: 94.59510803222656  Train_Reconstruction: 91.09134101867676  Train_KL: 3.5037675201892853  Validation Loss : 93.98221206665039 Val_Reconstruction : 90.52363204956055 Val_KL : 3.458580732345581\n","Epoch: 2987/8000  Traning Loss: 94.44557762145996  Train_Reconstruction: 90.93912029266357  Train_KL: 3.5064571499824524  Validation Loss : 93.41874694824219 Val_Reconstruction : 89.9408073425293 Val_KL : 3.477940320968628\n","Epoch: 2988/8000  Traning Loss: 94.20535850524902  Train_Reconstruction: 90.69282245635986  Train_KL: 3.5125369131565094  Validation Loss : 93.59991836547852 Val_Reconstruction : 90.13475799560547 Val_KL : 3.4651607275009155\n","Epoch: 2989/8000  Traning Loss: 93.79493999481201  Train_Reconstruction: 90.29030513763428  Train_KL: 3.5046356916427612  Validation Loss : 93.13760757446289 Val_Reconstruction : 89.68094253540039 Val_KL : 3.4566656351089478\n","Epoch: 2990/8000  Traning Loss: 93.70949935913086  Train_Reconstruction: 90.2059268951416  Train_KL: 3.503571331501007  Validation Loss : 93.34325790405273 Val_Reconstruction : 89.87874984741211 Val_KL : 3.4645081758499146\n","Epoch: 2991/8000  Traning Loss: 93.86370277404785  Train_Reconstruction: 90.35397148132324  Train_KL: 3.5097304582595825  Validation Loss : 93.70579147338867 Val_Reconstruction : 90.23965454101562 Val_KL : 3.466139078140259\n","Epoch: 2992/8000  Traning Loss: 94.51837062835693  Train_Reconstruction: 91.01020812988281  Train_KL: 3.508161574602127  Validation Loss : 94.17231750488281 Val_Reconstruction : 90.70624923706055 Val_KL : 3.466066837310791\n","Epoch: 2993/8000  Traning Loss: 95.02023983001709  Train_Reconstruction: 91.51129722595215  Train_KL: 3.5089422166347504  Validation Loss : 94.6433334350586 Val_Reconstruction : 91.16825485229492 Val_KL : 3.4750806093215942\n","Epoch: 2994/8000  Traning Loss: 94.49371910095215  Train_Reconstruction: 90.98298740386963  Train_KL: 3.510732889175415  Validation Loss : 93.79998779296875 Val_Reconstruction : 90.33369064331055 Val_KL : 3.4662975072860718\n","Epoch: 2995/8000  Traning Loss: 94.20039081573486  Train_Reconstruction: 90.69162559509277  Train_KL: 3.508765786886215  Validation Loss : 93.92803192138672 Val_Reconstruction : 90.46222305297852 Val_KL : 3.4658089876174927\n","Epoch: 2996/8000  Traning Loss: 94.4406099319458  Train_Reconstruction: 90.92750835418701  Train_KL: 3.5131006240844727  Validation Loss : 93.65405654907227 Val_Reconstruction : 90.18490982055664 Val_KL : 3.4691503047943115\n","Epoch: 2997/8000  Traning Loss: 94.32179355621338  Train_Reconstruction: 90.80742168426514  Train_KL: 3.514371156692505  Validation Loss : 93.4084587097168 Val_Reconstruction : 89.93912124633789 Val_KL : 3.469338536262512\n","Epoch: 2998/8000  Traning Loss: 93.95888233184814  Train_Reconstruction: 90.45244026184082  Train_KL: 3.506441742181778  Validation Loss : 93.32627868652344 Val_Reconstruction : 89.86683654785156 Val_KL : 3.4594441652297974\n","Epoch: 2999/8000  Traning Loss: 93.99549961090088  Train_Reconstruction: 90.49407768249512  Train_KL: 3.501420885324478  Validation Loss : 93.84747314453125 Val_Reconstruction : 90.38652801513672 Val_KL : 3.4609475135803223\n","Epoch: 3000/8000  Traning Loss: 94.34934616088867  Train_Reconstruction: 90.84404850006104  Train_KL: 3.5052990317344666  Validation Loss : 93.79591369628906 Val_Reconstruction : 90.33015823364258 Val_KL : 3.4657561779022217\n","Epoch: 3001/8000  Traning Loss: 94.06078815460205  Train_Reconstruction: 90.55227756500244  Train_KL: 3.5085102319717407  Validation Loss : 93.35600662231445 Val_Reconstruction : 89.89122009277344 Val_KL : 3.464787721633911\n","Epoch: 3002/8000  Traning Loss: 94.02904033660889  Train_Reconstruction: 90.52301597595215  Train_KL: 3.5060245394706726  Validation Loss : 93.61505508422852 Val_Reconstruction : 90.15046310424805 Val_KL : 3.464590072631836\n","Epoch: 3003/8000  Traning Loss: 94.08958053588867  Train_Reconstruction: 90.58131694793701  Train_KL: 3.5082629919052124  Validation Loss : 93.67714309692383 Val_Reconstruction : 90.2100944519043 Val_KL : 3.467048764228821\n","Epoch: 3004/8000  Traning Loss: 94.31508159637451  Train_Reconstruction: 90.80358219146729  Train_KL: 3.511498987674713  Validation Loss : 93.36548233032227 Val_Reconstruction : 89.9000358581543 Val_KL : 3.4654446840286255\n","Epoch: 3005/8000  Traning Loss: 93.91322612762451  Train_Reconstruction: 90.4104471206665  Train_KL: 3.502778500318527  Validation Loss : 93.17179870605469 Val_Reconstruction : 89.71566009521484 Val_KL : 3.4561398029327393\n","Epoch: 3006/8000  Traning Loss: 93.64572525024414  Train_Reconstruction: 90.14385509490967  Train_KL: 3.5018690824508667  Validation Loss : 93.02510070800781 Val_Reconstruction : 89.56143188476562 Val_KL : 3.46367084980011\n","Epoch: 3007/8000  Traning Loss: 93.69790077209473  Train_Reconstruction: 90.18686580657959  Train_KL: 3.511035054922104  Validation Loss : 93.20927429199219 Val_Reconstruction : 89.7315788269043 Val_KL : 3.477694869041443\n","Epoch: 3008/8000  Traning Loss: 94.02340888977051  Train_Reconstruction: 90.50499629974365  Train_KL: 3.5184120535850525  Validation Loss : 93.83427047729492 Val_Reconstruction : 90.36433029174805 Val_KL : 3.4699398279190063\n","Epoch: 3009/8000  Traning Loss: 93.88786888122559  Train_Reconstruction: 90.38410663604736  Train_KL: 3.503760904073715  Validation Loss : 93.21015930175781 Val_Reconstruction : 89.74673461914062 Val_KL : 3.463426947593689\n","Epoch: 3010/8000  Traning Loss: 93.85012340545654  Train_Reconstruction: 90.34856510162354  Train_KL: 3.5015580654144287  Validation Loss : 93.92063903808594 Val_Reconstruction : 90.46119689941406 Val_KL : 3.4594403505325317\n","Epoch: 3011/8000  Traning Loss: 94.16344356536865  Train_Reconstruction: 90.6665267944336  Train_KL: 3.4969154000282288  Validation Loss : 93.62016677856445 Val_Reconstruction : 90.16193008422852 Val_KL : 3.458239793777466\n","Epoch: 3012/8000  Traning Loss: 94.24367713928223  Train_Reconstruction: 90.73585796356201  Train_KL: 3.507818967103958  Validation Loss : 93.43949127197266 Val_Reconstruction : 89.98222351074219 Val_KL : 3.457269072532654\n","Epoch: 3013/8000  Traning Loss: 94.10266590118408  Train_Reconstruction: 90.59451198577881  Train_KL: 3.508153736591339  Validation Loss : 93.3110580444336 Val_Reconstruction : 89.85172271728516 Val_KL : 3.4593353271484375\n","Epoch: 3014/8000  Traning Loss: 93.51913547515869  Train_Reconstruction: 90.01840496063232  Train_KL: 3.5007300972938538  Validation Loss : 93.16144561767578 Val_Reconstruction : 89.69686889648438 Val_KL : 3.4645789861679077\n","Epoch: 3015/8000  Traning Loss: 94.24736881256104  Train_Reconstruction: 90.74079322814941  Train_KL: 3.506575286388397  Validation Loss : 94.5219497680664 Val_Reconstruction : 91.05793380737305 Val_KL : 3.4640140533447266\n","Epoch: 3016/8000  Traning Loss: 94.95128726959229  Train_Reconstruction: 91.44523334503174  Train_KL: 3.5060527622699738  Validation Loss : 94.50263595581055 Val_Reconstruction : 91.03804397583008 Val_KL : 3.4645936489105225\n","Epoch: 3017/8000  Traning Loss: 95.9023551940918  Train_Reconstruction: 92.39178276062012  Train_KL: 3.5105725526809692  Validation Loss : 95.36447143554688 Val_Reconstruction : 91.89603424072266 Val_KL : 3.4684362411499023\n","Epoch: 3018/8000  Traning Loss: 96.3191089630127  Train_Reconstruction: 92.81874179840088  Train_KL: 3.5003664791584015  Validation Loss : 95.54727172851562 Val_Reconstruction : 92.08903503417969 Val_KL : 3.45823872089386\n","Epoch: 3019/8000  Traning Loss: 94.77761840820312  Train_Reconstruction: 91.27435874938965  Train_KL: 3.503259688615799  Validation Loss : 94.14995193481445 Val_Reconstruction : 90.68025588989258 Val_KL : 3.469694495201111\n","Epoch: 3020/8000  Traning Loss: 94.45627498626709  Train_Reconstruction: 90.94838333129883  Train_KL: 3.507891684770584  Validation Loss : 93.785888671875 Val_Reconstruction : 90.32168960571289 Val_KL : 3.464200496673584\n","Epoch: 3021/8000  Traning Loss: 94.06208896636963  Train_Reconstruction: 90.55669689178467  Train_KL: 3.5053915083408356  Validation Loss : 93.40417098999023 Val_Reconstruction : 89.94667434692383 Val_KL : 3.457497239112854\n","Epoch: 3022/8000  Traning Loss: 93.9058609008789  Train_Reconstruction: 90.40527725219727  Train_KL: 3.500583916902542  Validation Loss : 93.55384826660156 Val_Reconstruction : 90.09091186523438 Val_KL : 3.462934136390686\n","Epoch: 3023/8000  Traning Loss: 93.95285415649414  Train_Reconstruction: 90.44198322296143  Train_KL: 3.510870933532715  Validation Loss : 93.27482604980469 Val_Reconstruction : 89.80021667480469 Val_KL : 3.4746094942092896\n","Epoch: 3024/8000  Traning Loss: 93.72641181945801  Train_Reconstruction: 90.21076583862305  Train_KL: 3.5156464874744415  Validation Loss : 93.18167877197266 Val_Reconstruction : 89.71353149414062 Val_KL : 3.4681466817855835\n","Epoch: 3025/8000  Traning Loss: 93.47350597381592  Train_Reconstruction: 89.9691572189331  Train_KL: 3.5043486654758453  Validation Loss : 92.90547943115234 Val_Reconstruction : 89.44579315185547 Val_KL : 3.459685802459717\n","Epoch: 3026/8000  Traning Loss: 93.76015090942383  Train_Reconstruction: 90.25428199768066  Train_KL: 3.505868762731552  Validation Loss : 93.29801177978516 Val_Reconstruction : 89.83460998535156 Val_KL : 3.463404655456543\n","Epoch: 3027/8000  Traning Loss: 93.75757694244385  Train_Reconstruction: 90.2489881515503  Train_KL: 3.508588343858719  Validation Loss : 93.40292358398438 Val_Reconstruction : 89.93053436279297 Val_KL : 3.4723891019821167\n","Epoch: 3028/8000  Traning Loss: 93.73316764831543  Train_Reconstruction: 90.22258472442627  Train_KL: 3.5105824172496796  Validation Loss : 93.31574630737305 Val_Reconstruction : 89.84440231323242 Val_KL : 3.4713445901870728\n","Epoch: 3029/8000  Traning Loss: 93.97314071655273  Train_Reconstruction: 90.45840072631836  Train_KL: 3.514739364385605  Validation Loss : 93.53655624389648 Val_Reconstruction : 90.05915832519531 Val_KL : 3.477400064468384\n","Epoch: 3030/8000  Traning Loss: 94.0674695968628  Train_Reconstruction: 90.5599889755249  Train_KL: 3.50748074054718  Validation Loss : 93.21639251708984 Val_Reconstruction : 89.75486373901367 Val_KL : 3.4615280628204346\n","Epoch: 3031/8000  Traning Loss: 93.76454830169678  Train_Reconstruction: 90.25223064422607  Train_KL: 3.5123180747032166  Validation Loss : 93.24623107910156 Val_Reconstruction : 89.76925277709961 Val_KL : 3.47697913646698\n","Epoch: 3032/8000  Traning Loss: 93.77388191223145  Train_Reconstruction: 90.26064586639404  Train_KL: 3.5132355093955994  Validation Loss : 93.4981918334961 Val_Reconstruction : 90.03699493408203 Val_KL : 3.461198091506958\n","Epoch: 3033/8000  Traning Loss: 94.12368392944336  Train_Reconstruction: 90.61089992523193  Train_KL: 3.5127850770950317  Validation Loss : 93.57875442504883 Val_Reconstruction : 90.10233688354492 Val_KL : 3.476420760154724\n","Epoch: 3034/8000  Traning Loss: 94.75143241882324  Train_Reconstruction: 91.23561668395996  Train_KL: 3.515815705060959  Validation Loss : 94.59872436523438 Val_Reconstruction : 91.12249755859375 Val_KL : 3.4762264490127563\n","Epoch: 3035/8000  Traning Loss: 94.7050666809082  Train_Reconstruction: 91.19344806671143  Train_KL: 3.5116180777549744  Validation Loss : 94.00271224975586 Val_Reconstruction : 90.53791427612305 Val_KL : 3.464799165725708\n","Epoch: 3036/8000  Traning Loss: 93.97962379455566  Train_Reconstruction: 90.48237037658691  Train_KL: 3.4972535371780396  Validation Loss : 93.4914436340332 Val_Reconstruction : 90.03409957885742 Val_KL : 3.457343101501465\n","Epoch: 3037/8000  Traning Loss: 93.92534446716309  Train_Reconstruction: 90.41515731811523  Train_KL: 3.5101875364780426  Validation Loss : 93.05231857299805 Val_Reconstruction : 89.57729721069336 Val_KL : 3.4750218391418457\n","Epoch: 3038/8000  Traning Loss: 93.73967361450195  Train_Reconstruction: 90.2173547744751  Train_KL: 3.522320508956909  Validation Loss : 93.28047180175781 Val_Reconstruction : 89.80716705322266 Val_KL : 3.4733057022094727\n","Epoch: 3039/8000  Traning Loss: 93.96839046478271  Train_Reconstruction: 90.4631929397583  Train_KL: 3.5051957070827484  Validation Loss : 93.27630996704102 Val_Reconstruction : 89.81403732299805 Val_KL : 3.462275743484497\n","Epoch: 3040/8000  Traning Loss: 93.99841690063477  Train_Reconstruction: 90.49961566925049  Train_KL: 3.4987996220588684  Validation Loss : 93.35567092895508 Val_Reconstruction : 89.89487838745117 Val_KL : 3.460792899131775\n","Epoch: 3041/8000  Traning Loss: 94.50786399841309  Train_Reconstruction: 90.99028778076172  Train_KL: 3.517576724290848  Validation Loss : 94.57369613647461 Val_Reconstruction : 91.09122467041016 Val_KL : 3.482468605041504\n","Epoch: 3042/8000  Traning Loss: 95.16694927215576  Train_Reconstruction: 91.65308284759521  Train_KL: 3.5138663053512573  Validation Loss : 93.77829360961914 Val_Reconstruction : 90.29425048828125 Val_KL : 3.4840452671051025\n","Epoch: 3043/8000  Traning Loss: 94.3697099685669  Train_Reconstruction: 90.85000896453857  Train_KL: 3.5197002291679382  Validation Loss : 93.69754791259766 Val_Reconstruction : 90.2165641784668 Val_KL : 3.480983853340149\n","Epoch: 3044/8000  Traning Loss: 93.80053901672363  Train_Reconstruction: 90.27804279327393  Train_KL: 3.5224951207637787  Validation Loss : 93.17208480834961 Val_Reconstruction : 89.69091033935547 Val_KL : 3.4811712503433228\n","Epoch: 3045/8000  Traning Loss: 93.87676334381104  Train_Reconstruction: 90.3580904006958  Train_KL: 3.5186737179756165  Validation Loss : 93.32797241210938 Val_Reconstruction : 89.85847854614258 Val_KL : 3.469495415687561\n","Epoch: 3046/8000  Traning Loss: 93.61821174621582  Train_Reconstruction: 90.11251258850098  Train_KL: 3.5057003796100616  Validation Loss : 93.33691024780273 Val_Reconstruction : 89.86959457397461 Val_KL : 3.467315673828125\n","Epoch: 3047/8000  Traning Loss: 93.61203956604004  Train_Reconstruction: 90.10499954223633  Train_KL: 3.5070391297340393  Validation Loss : 93.22272491455078 Val_Reconstruction : 89.75506591796875 Val_KL : 3.4676613807678223\n","Epoch: 3048/8000  Traning Loss: 94.03429222106934  Train_Reconstruction: 90.52349376678467  Train_KL: 3.5107984840869904  Validation Loss : 93.52956771850586 Val_Reconstruction : 90.06346130371094 Val_KL : 3.466107130050659\n","Epoch: 3049/8000  Traning Loss: 93.99543952941895  Train_Reconstruction: 90.48744678497314  Train_KL: 3.507992297410965  Validation Loss : 93.05414962768555 Val_Reconstruction : 89.58328628540039 Val_KL : 3.4708651304244995\n","Epoch: 3050/8000  Traning Loss: 93.84750080108643  Train_Reconstruction: 90.34095287322998  Train_KL: 3.5065478086471558  Validation Loss : 93.2828254699707 Val_Reconstruction : 89.81802368164062 Val_KL : 3.464801788330078\n","Epoch: 3051/8000  Traning Loss: 93.90503597259521  Train_Reconstruction: 90.39912796020508  Train_KL: 3.505908280611038  Validation Loss : 93.18219757080078 Val_Reconstruction : 89.71900177001953 Val_KL : 3.4631950855255127\n","Epoch: 3052/8000  Traning Loss: 93.9264326095581  Train_Reconstruction: 90.41132259368896  Train_KL: 3.5151102542877197  Validation Loss : 93.3058853149414 Val_Reconstruction : 89.83323669433594 Val_KL : 3.4726498126983643\n","Epoch: 3053/8000  Traning Loss: 93.88919162750244  Train_Reconstruction: 90.37220001220703  Train_KL: 3.5169916450977325  Validation Loss : 93.30363845825195 Val_Reconstruction : 89.83056640625 Val_KL : 3.4730732440948486\n","Epoch: 3054/8000  Traning Loss: 93.8556261062622  Train_Reconstruction: 90.3492259979248  Train_KL: 3.50640007853508  Validation Loss : 93.06730270385742 Val_Reconstruction : 89.61240005493164 Val_KL : 3.4549007415771484\n","Epoch: 3055/8000  Traning Loss: 94.18557929992676  Train_Reconstruction: 90.6820478439331  Train_KL: 3.503530263900757  Validation Loss : 93.68018341064453 Val_Reconstruction : 90.20769500732422 Val_KL : 3.4724879264831543\n","Epoch: 3056/8000  Traning Loss: 94.47677040100098  Train_Reconstruction: 90.96185779571533  Train_KL: 3.514912933111191  Validation Loss : 93.82014846801758 Val_Reconstruction : 90.35910415649414 Val_KL : 3.4610430002212524\n","Epoch: 3057/8000  Traning Loss: 94.2157154083252  Train_Reconstruction: 90.70669174194336  Train_KL: 3.5090240240097046  Validation Loss : 93.63802337646484 Val_Reconstruction : 90.17354965209961 Val_KL : 3.464473009109497\n","Epoch: 3058/8000  Traning Loss: 94.08851718902588  Train_Reconstruction: 90.57268142700195  Train_KL: 3.515835613012314  Validation Loss : 93.76433563232422 Val_Reconstruction : 90.29143142700195 Val_KL : 3.472907304763794\n","Epoch: 3059/8000  Traning Loss: 94.33956050872803  Train_Reconstruction: 90.82813358306885  Train_KL: 3.5114269256591797  Validation Loss : 93.63007736206055 Val_Reconstruction : 90.16507339477539 Val_KL : 3.4650055170059204\n","Epoch: 3060/8000  Traning Loss: 94.6262321472168  Train_Reconstruction: 91.12104797363281  Train_KL: 3.5051845014095306  Validation Loss : 93.97494888305664 Val_Reconstruction : 90.50874710083008 Val_KL : 3.4662020206451416\n","Epoch: 3061/8000  Traning Loss: 94.17364501953125  Train_Reconstruction: 90.65574264526367  Train_KL: 3.517902195453644  Validation Loss : 93.14998626708984 Val_Reconstruction : 89.67204284667969 Val_KL : 3.4779443740844727\n","Epoch: 3062/8000  Traning Loss: 93.66828632354736  Train_Reconstruction: 90.14994430541992  Train_KL: 3.518340677022934  Validation Loss : 93.2936897277832 Val_Reconstruction : 89.82260513305664 Val_KL : 3.4710863828659058\n","Epoch: 3063/8000  Traning Loss: 93.63622760772705  Train_Reconstruction: 90.12158679962158  Train_KL: 3.5146404802799225  Validation Loss : 92.99222946166992 Val_Reconstruction : 89.52035903930664 Val_KL : 3.4718692302703857\n","Epoch: 3064/8000  Traning Loss: 93.60878276824951  Train_Reconstruction: 90.10652542114258  Train_KL: 3.502257823944092  Validation Loss : 93.06201171875 Val_Reconstruction : 89.59746170043945 Val_KL : 3.464550495147705\n","Epoch: 3065/8000  Traning Loss: 93.98257160186768  Train_Reconstruction: 90.47493553161621  Train_KL: 3.5076367259025574  Validation Loss : 94.12551498413086 Val_Reconstruction : 90.6563720703125 Val_KL : 3.4691420793533325\n","Epoch: 3066/8000  Traning Loss: 94.61349773406982  Train_Reconstruction: 91.10798835754395  Train_KL: 3.505509525537491  Validation Loss : 94.82344436645508 Val_Reconstruction : 91.36537551879883 Val_KL : 3.458070397377014\n","Epoch: 3067/8000  Traning Loss: 95.10456085205078  Train_Reconstruction: 91.60142707824707  Train_KL: 3.5031341910362244  Validation Loss : 94.1566276550293 Val_Reconstruction : 90.68742370605469 Val_KL : 3.4692022800445557\n","Epoch: 3068/8000  Traning Loss: 94.39354705810547  Train_Reconstruction: 90.88279151916504  Train_KL: 3.5107564628124237  Validation Loss : 94.03401947021484 Val_Reconstruction : 90.56081771850586 Val_KL : 3.4731998443603516\n","Epoch: 3069/8000  Traning Loss: 93.88214302062988  Train_Reconstruction: 90.36762619018555  Train_KL: 3.514515668153763  Validation Loss : 93.06286239624023 Val_Reconstruction : 89.59330368041992 Val_KL : 3.4695568084716797\n","Epoch: 3070/8000  Traning Loss: 93.72809028625488  Train_Reconstruction: 90.21348094940186  Train_KL: 3.5146091282367706  Validation Loss : 93.12695693969727 Val_Reconstruction : 89.65653610229492 Val_KL : 3.470423460006714\n","Epoch: 3071/8000  Traning Loss: 93.72968864440918  Train_Reconstruction: 90.21389770507812  Train_KL: 3.515791267156601  Validation Loss : 93.14403533935547 Val_Reconstruction : 89.67688369750977 Val_KL : 3.4671519994735718\n","Epoch: 3072/8000  Traning Loss: 93.61228466033936  Train_Reconstruction: 90.10297584533691  Train_KL: 3.5093095004558563  Validation Loss : 93.11410522460938 Val_Reconstruction : 89.65131378173828 Val_KL : 3.4627901315689087\n","Epoch: 3073/8000  Traning Loss: 93.46504020690918  Train_Reconstruction: 89.9513349533081  Train_KL: 3.5137053728103638  Validation Loss : 93.1352767944336 Val_Reconstruction : 89.66175079345703 Val_KL : 3.4735257625579834\n","Epoch: 3074/8000  Traning Loss: 93.56075954437256  Train_Reconstruction: 90.0391206741333  Train_KL: 3.521638721227646  Validation Loss : 93.11007690429688 Val_Reconstruction : 89.63798522949219 Val_KL : 3.472093105316162\n","Epoch: 3075/8000  Traning Loss: 93.5552225112915  Train_Reconstruction: 90.04787063598633  Train_KL: 3.507351130247116  Validation Loss : 92.97018051147461 Val_Reconstruction : 89.50090026855469 Val_KL : 3.4692782163619995\n","Epoch: 3076/8000  Traning Loss: 93.49392700195312  Train_Reconstruction: 89.98071956634521  Train_KL: 3.5132074058055878  Validation Loss : 93.03326416015625 Val_Reconstruction : 89.55951309204102 Val_KL : 3.4737493991851807\n","Epoch: 3077/8000  Traning Loss: 93.62685585021973  Train_Reconstruction: 90.10912036895752  Train_KL: 3.517735719680786  Validation Loss : 93.38078308105469 Val_Reconstruction : 89.90782165527344 Val_KL : 3.4729634523391724\n","Epoch: 3078/8000  Traning Loss: 93.82944965362549  Train_Reconstruction: 90.31511116027832  Train_KL: 3.5143392980098724  Validation Loss : 93.22863388061523 Val_Reconstruction : 89.76281356811523 Val_KL : 3.4658203125\n","Epoch: 3079/8000  Traning Loss: 93.68572425842285  Train_Reconstruction: 90.17149353027344  Train_KL: 3.514231115579605  Validation Loss : 92.86651611328125 Val_Reconstruction : 89.39326858520508 Val_KL : 3.473249077796936\n","Epoch: 3080/8000  Traning Loss: 93.64296436309814  Train_Reconstruction: 90.12528324127197  Train_KL: 3.517682433128357  Validation Loss : 93.0566520690918 Val_Reconstruction : 89.58350372314453 Val_KL : 3.4731467962265015\n","Epoch: 3081/8000  Traning Loss: 93.57848072052002  Train_Reconstruction: 90.0715217590332  Train_KL: 3.506958454847336  Validation Loss : 93.03914642333984 Val_Reconstruction : 89.57491302490234 Val_KL : 3.464231252670288\n","Epoch: 3082/8000  Traning Loss: 93.92433166503906  Train_Reconstruction: 90.40718650817871  Train_KL: 3.517145723104477  Validation Loss : 93.86661148071289 Val_Reconstruction : 90.38554763793945 Val_KL : 3.4810662269592285\n","Epoch: 3083/8000  Traning Loss: 94.21683502197266  Train_Reconstruction: 90.69232845306396  Train_KL: 3.524506777524948  Validation Loss : 93.81642150878906 Val_Reconstruction : 90.33760070800781 Val_KL : 3.4788230657577515\n","Epoch: 3084/8000  Traning Loss: 94.3893871307373  Train_Reconstruction: 90.87010288238525  Train_KL: 3.5192841291427612  Validation Loss : 93.8145751953125 Val_Reconstruction : 90.33956909179688 Val_KL : 3.475004196166992\n","Epoch: 3085/8000  Traning Loss: 94.19016361236572  Train_Reconstruction: 90.68900203704834  Train_KL: 3.501162052154541  Validation Loss : 94.11449432373047 Val_Reconstruction : 90.6578254699707 Val_KL : 3.456667184829712\n","Epoch: 3086/8000  Traning Loss: 93.9444408416748  Train_Reconstruction: 90.44837665557861  Train_KL: 3.4960647225379944  Validation Loss : 93.16301345825195 Val_Reconstruction : 89.69393920898438 Val_KL : 3.469075322151184\n","Epoch: 3087/8000  Traning Loss: 93.78523063659668  Train_Reconstruction: 90.26094245910645  Train_KL: 3.5242894887924194  Validation Loss : 93.10812377929688 Val_Reconstruction : 89.62429809570312 Val_KL : 3.4838255643844604\n","Epoch: 3088/8000  Traning Loss: 93.6528205871582  Train_Reconstruction: 90.12634372711182  Train_KL: 3.526477426290512  Validation Loss : 92.77684783935547 Val_Reconstruction : 89.29740524291992 Val_KL : 3.479440450668335\n","Epoch: 3089/8000  Traning Loss: 93.36497497558594  Train_Reconstruction: 89.8516731262207  Train_KL: 3.513302266597748  Validation Loss : 92.66850662231445 Val_Reconstruction : 89.2044563293457 Val_KL : 3.4640499353408813\n","Epoch: 3090/8000  Traning Loss: 93.41578388214111  Train_Reconstruction: 89.90431690216064  Train_KL: 3.5114666223526  Validation Loss : 93.31942749023438 Val_Reconstruction : 89.85901641845703 Val_KL : 3.460412859916687\n","Epoch: 3091/8000  Traning Loss: 93.7158555984497  Train_Reconstruction: 90.20492935180664  Train_KL: 3.5109254717826843  Validation Loss : 93.07696914672852 Val_Reconstruction : 89.6091423034668 Val_KL : 3.4678263664245605\n","Epoch: 3092/8000  Traning Loss: 93.93977069854736  Train_Reconstruction: 90.43496417999268  Train_KL: 3.504806727170944  Validation Loss : 93.96672821044922 Val_Reconstruction : 90.5101547241211 Val_KL : 3.4565727710723877\n","Epoch: 3093/8000  Traning Loss: 94.08434963226318  Train_Reconstruction: 90.58123970031738  Train_KL: 3.5031104385852814  Validation Loss : 93.21452331542969 Val_Reconstruction : 89.75112533569336 Val_KL : 3.463399648666382\n","Epoch: 3094/8000  Traning Loss: 93.76783084869385  Train_Reconstruction: 90.26237297058105  Train_KL: 3.5054579973220825  Validation Loss : 93.39424896240234 Val_Reconstruction : 89.93102645874023 Val_KL : 3.463218927383423\n","Epoch: 3095/8000  Traning Loss: 93.85080528259277  Train_Reconstruction: 90.3331937789917  Train_KL: 3.5176100730895996  Validation Loss : 93.51712036132812 Val_Reconstruction : 90.03765869140625 Val_KL : 3.479460835456848\n","Epoch: 3096/8000  Traning Loss: 93.68526554107666  Train_Reconstruction: 90.17228889465332  Train_KL: 3.5129761695861816  Validation Loss : 93.39880752563477 Val_Reconstruction : 89.92884063720703 Val_KL : 3.4699654579162598\n","Epoch: 3097/8000  Traning Loss: 93.90520668029785  Train_Reconstruction: 90.4023609161377  Train_KL: 3.5028454959392548  Validation Loss : 93.80838775634766 Val_Reconstruction : 90.34771347045898 Val_KL : 3.46067476272583\n","Epoch: 3098/8000  Traning Loss: 93.84973812103271  Train_Reconstruction: 90.34094047546387  Train_KL: 3.5087971687316895  Validation Loss : 93.18512725830078 Val_Reconstruction : 89.71622848510742 Val_KL : 3.4688968658447266\n","Epoch: 3099/8000  Traning Loss: 93.44991683959961  Train_Reconstruction: 89.93029499053955  Train_KL: 3.5196212232112885  Validation Loss : 93.06180953979492 Val_Reconstruction : 89.57559204101562 Val_KL : 3.4862159490585327\n","Epoch: 3100/8000  Traning Loss: 93.83436679840088  Train_Reconstruction: 90.313157081604  Train_KL: 3.5212096869945526  Validation Loss : 93.80014038085938 Val_Reconstruction : 90.31986236572266 Val_KL : 3.4802775382995605\n","Epoch: 3101/8000  Traning Loss: 94.12653541564941  Train_Reconstruction: 90.6193962097168  Train_KL: 3.5071389079093933  Validation Loss : 93.42547988891602 Val_Reconstruction : 89.9630241394043 Val_KL : 3.4624576568603516\n","Epoch: 3102/8000  Traning Loss: 94.16834545135498  Train_Reconstruction: 90.65697765350342  Train_KL: 3.5113677084445953  Validation Loss : 93.6551742553711 Val_Reconstruction : 90.18672943115234 Val_KL : 3.4684447050094604\n","Epoch: 3103/8000  Traning Loss: 94.28913974761963  Train_Reconstruction: 90.78040790557861  Train_KL: 3.508730500936508  Validation Loss : 93.55511093139648 Val_Reconstruction : 90.08678817749023 Val_KL : 3.4683237075805664\n","Epoch: 3104/8000  Traning Loss: 94.23487567901611  Train_Reconstruction: 90.73143100738525  Train_KL: 3.503444164991379  Validation Loss : 93.45575714111328 Val_Reconstruction : 89.99411392211914 Val_KL : 3.4616414308547974\n","Epoch: 3105/8000  Traning Loss: 93.79625988006592  Train_Reconstruction: 90.28899097442627  Train_KL: 3.5072692930698395  Validation Loss : 93.18080139160156 Val_Reconstruction : 89.71988296508789 Val_KL : 3.4609179496765137\n","Epoch: 3106/8000  Traning Loss: 93.630690574646  Train_Reconstruction: 90.11653900146484  Train_KL: 3.5141522884368896  Validation Loss : 93.06815719604492 Val_Reconstruction : 89.5878791809082 Val_KL : 3.4802807569503784\n","Epoch: 3107/8000  Traning Loss: 93.7501745223999  Train_Reconstruction: 90.229079246521  Train_KL: 3.5210950672626495  Validation Loss : 93.29825210571289 Val_Reconstruction : 89.82139205932617 Val_KL : 3.4768600463867188\n","Epoch: 3108/8000  Traning Loss: 93.8816909790039  Train_Reconstruction: 90.3678035736084  Train_KL: 3.5138866007328033  Validation Loss : 93.6384506225586 Val_Reconstruction : 90.16170120239258 Val_KL : 3.476749062538147\n","Epoch: 3109/8000  Traning Loss: 93.7843885421753  Train_Reconstruction: 90.2693099975586  Train_KL: 3.5150796473026276  Validation Loss : 93.2237548828125 Val_Reconstruction : 89.75177764892578 Val_KL : 3.4719772338867188\n","Epoch: 3110/8000  Traning Loss: 93.74538707733154  Train_Reconstruction: 90.24388790130615  Train_KL: 3.501498430967331  Validation Loss : 92.97987365722656 Val_Reconstruction : 89.52339172363281 Val_KL : 3.4564826488494873\n","Epoch: 3111/8000  Traning Loss: 93.71452903747559  Train_Reconstruction: 90.2037239074707  Train_KL: 3.5108053386211395  Validation Loss : 92.99781036376953 Val_Reconstruction : 89.51961517333984 Val_KL : 3.478196620941162\n","Epoch: 3112/8000  Traning Loss: 93.80453586578369  Train_Reconstruction: 90.27883052825928  Train_KL: 3.525704503059387  Validation Loss : 93.17960357666016 Val_Reconstruction : 89.70295715332031 Val_KL : 3.4766440391540527\n","Epoch: 3113/8000  Traning Loss: 93.58110809326172  Train_Reconstruction: 90.06868839263916  Train_KL: 3.51241996884346  Validation Loss : 93.25296401977539 Val_Reconstruction : 89.78709411621094 Val_KL : 3.4658687114715576\n","Epoch: 3114/8000  Traning Loss: 93.53443336486816  Train_Reconstruction: 90.03119850158691  Train_KL: 3.5032348930835724  Validation Loss : 93.13912582397461 Val_Reconstruction : 89.67022705078125 Val_KL : 3.4688992500305176\n","Epoch: 3115/8000  Traning Loss: 93.43512344360352  Train_Reconstruction: 89.91493129730225  Train_KL: 3.5201924443244934  Validation Loss : 92.77905654907227 Val_Reconstruction : 89.2940673828125 Val_KL : 3.4849895238876343\n","Epoch: 3116/8000  Traning Loss: 93.5452127456665  Train_Reconstruction: 90.02704048156738  Train_KL: 3.5181721448898315  Validation Loss : 93.43550491333008 Val_Reconstruction : 89.97339630126953 Val_KL : 3.462108612060547\n","Epoch: 3117/8000  Traning Loss: 93.77229881286621  Train_Reconstruction: 90.27147197723389  Train_KL: 3.500825971364975  Validation Loss : 93.46084594726562 Val_Reconstruction : 90.00072860717773 Val_KL : 3.46011483669281\n","Epoch: 3118/8000  Traning Loss: 93.7461166381836  Train_Reconstruction: 90.23727989196777  Train_KL: 3.508836328983307  Validation Loss : 93.12858963012695 Val_Reconstruction : 89.6521224975586 Val_KL : 3.476468563079834\n","Epoch: 3119/8000  Traning Loss: 93.72531318664551  Train_Reconstruction: 90.19993591308594  Train_KL: 3.5253776609897614  Validation Loss : 93.12749862670898 Val_Reconstruction : 89.6435317993164 Val_KL : 3.4839683771133423\n","Epoch: 3120/8000  Traning Loss: 93.9556074142456  Train_Reconstruction: 90.43684196472168  Train_KL: 3.5187662541866302  Validation Loss : 93.66547393798828 Val_Reconstruction : 90.19790649414062 Val_KL : 3.467566132545471\n","Epoch: 3121/8000  Traning Loss: 93.70358943939209  Train_Reconstruction: 90.19971752166748  Train_KL: 3.5038726031780243  Validation Loss : 92.79718399047852 Val_Reconstruction : 89.33786010742188 Val_KL : 3.4593230485916138\n","Epoch: 3122/8000  Traning Loss: 93.4343032836914  Train_Reconstruction: 89.91933059692383  Train_KL: 3.514972448348999  Validation Loss : 92.93438720703125 Val_Reconstruction : 89.46011734008789 Val_KL : 3.4742684364318848\n","Epoch: 3123/8000  Traning Loss: 93.71779441833496  Train_Reconstruction: 90.20392417907715  Train_KL: 3.5138704776763916  Validation Loss : 93.0709342956543 Val_Reconstruction : 89.59662246704102 Val_KL : 3.4743107557296753\n","Epoch: 3124/8000  Traning Loss: 93.71937847137451  Train_Reconstruction: 90.20914840698242  Train_KL: 3.5102291107177734  Validation Loss : 92.9885025024414 Val_Reconstruction : 89.50977325439453 Val_KL : 3.47873055934906\n","Epoch: 3125/8000  Traning Loss: 93.45832443237305  Train_Reconstruction: 89.93082809448242  Train_KL: 3.5274960100650787  Validation Loss : 93.0663948059082 Val_Reconstruction : 89.57735824584961 Val_KL : 3.489034414291382\n","Epoch: 3126/8000  Traning Loss: 93.61036109924316  Train_Reconstruction: 90.08972549438477  Train_KL: 3.520636707544327  Validation Loss : 93.00299072265625 Val_Reconstruction : 89.53018951416016 Val_KL : 3.4728009700775146\n","Epoch: 3127/8000  Traning Loss: 93.65289211273193  Train_Reconstruction: 90.14360523223877  Train_KL: 3.509287565946579  Validation Loss : 93.44842147827148 Val_Reconstruction : 89.97704696655273 Val_KL : 3.471374034881592\n","Epoch: 3128/8000  Traning Loss: 93.60134315490723  Train_Reconstruction: 90.0850830078125  Train_KL: 3.5162599086761475  Validation Loss : 92.95064163208008 Val_Reconstruction : 89.468994140625 Val_KL : 3.4816486835479736\n","Epoch: 3129/8000  Traning Loss: 93.63372611999512  Train_Reconstruction: 90.11307334899902  Train_KL: 3.5206537544727325  Validation Loss : 93.39663314819336 Val_Reconstruction : 89.91875076293945 Val_KL : 3.477882981300354\n","Epoch: 3130/8000  Traning Loss: 94.01201629638672  Train_Reconstruction: 90.50958633422852  Train_KL: 3.5024290084838867  Validation Loss : 93.77799606323242 Val_Reconstruction : 90.32667922973633 Val_KL : 3.451314091682434\n","Epoch: 3131/8000  Traning Loss: 93.92524814605713  Train_Reconstruction: 90.43592357635498  Train_KL: 3.4893235564231873  Validation Loss : 92.97830963134766 Val_Reconstruction : 89.52217864990234 Val_KL : 3.456127643585205\n","Epoch: 3132/8000  Traning Loss: 93.79794883728027  Train_Reconstruction: 90.28790760040283  Train_KL: 3.510039120912552  Validation Loss : 93.17699813842773 Val_Reconstruction : 89.69099426269531 Val_KL : 3.486006021499634\n","Epoch: 3133/8000  Traning Loss: 93.86643600463867  Train_Reconstruction: 90.35326862335205  Train_KL: 3.5131671726703644  Validation Loss : 93.11320877075195 Val_Reconstruction : 89.64718627929688 Val_KL : 3.4660240411758423\n","Epoch: 3134/8000  Traning Loss: 93.60305118560791  Train_Reconstruction: 90.10179042816162  Train_KL: 3.5012595653533936  Validation Loss : 92.9654312133789 Val_Reconstruction : 89.5025405883789 Val_KL : 3.462891101837158\n","Epoch: 3135/8000  Traning Loss: 93.37817192077637  Train_Reconstruction: 89.86722373962402  Train_KL: 3.5109489262104034  Validation Loss : 92.81765365600586 Val_Reconstruction : 89.34511184692383 Val_KL : 3.4725431203842163\n","Epoch: 3136/8000  Traning Loss: 93.7841682434082  Train_Reconstruction: 90.2673568725586  Train_KL: 3.5168095231056213  Validation Loss : 93.02154541015625 Val_Reconstruction : 89.55178833007812 Val_KL : 3.469754219055176\n","Epoch: 3137/8000  Traning Loss: 94.06247329711914  Train_Reconstruction: 90.55219268798828  Train_KL: 3.5102797746658325  Validation Loss : 93.71356582641602 Val_Reconstruction : 90.2420425415039 Val_KL : 3.4715237617492676\n","Epoch: 3138/8000  Traning Loss: 93.80838108062744  Train_Reconstruction: 90.29012489318848  Train_KL: 3.51825612783432  Validation Loss : 93.00369262695312 Val_Reconstruction : 89.52808380126953 Val_KL : 3.4756053686141968\n","Epoch: 3139/8000  Traning Loss: 93.88801956176758  Train_Reconstruction: 90.37484836578369  Train_KL: 3.513171821832657  Validation Loss : 93.25086212158203 Val_Reconstruction : 89.7818489074707 Val_KL : 3.469011425971985\n","Epoch: 3140/8000  Traning Loss: 93.90485095977783  Train_Reconstruction: 90.40774345397949  Train_KL: 3.497107356786728  Validation Loss : 93.31816101074219 Val_Reconstruction : 89.86647415161133 Val_KL : 3.4516851902008057\n","Epoch: 3141/8000  Traning Loss: 93.72446537017822  Train_Reconstruction: 90.22483348846436  Train_KL: 3.499631851911545  Validation Loss : 93.54072189331055 Val_Reconstruction : 90.07971954345703 Val_KL : 3.4610037803649902\n","Epoch: 3142/8000  Traning Loss: 93.8304033279419  Train_Reconstruction: 90.31686878204346  Train_KL: 3.5135331749916077  Validation Loss : 93.18424224853516 Val_Reconstruction : 89.70593643188477 Val_KL : 3.4783051013946533\n","Epoch: 3143/8000  Traning Loss: 93.87612819671631  Train_Reconstruction: 90.35392665863037  Train_KL: 3.522200971841812  Validation Loss : 93.76530075073242 Val_Reconstruction : 90.28387451171875 Val_KL : 3.4814252853393555\n","Epoch: 3144/8000  Traning Loss: 94.0087890625  Train_Reconstruction: 90.49835109710693  Train_KL: 3.5104385316371918  Validation Loss : 93.52322006225586 Val_Reconstruction : 90.07022857666016 Val_KL : 3.452991247177124\n","Epoch: 3145/8000  Traning Loss: 94.05397510528564  Train_Reconstruction: 90.55574607849121  Train_KL: 3.4982287883758545  Validation Loss : 93.15472412109375 Val_Reconstruction : 89.70379257202148 Val_KL : 3.4509315490722656\n","Epoch: 3146/8000  Traning Loss: 93.57411670684814  Train_Reconstruction: 90.07170295715332  Train_KL: 3.502413034439087  Validation Loss : 92.90228271484375 Val_Reconstruction : 89.43568801879883 Val_KL : 3.4665956497192383\n","Epoch: 3147/8000  Traning Loss: 93.4397201538086  Train_Reconstruction: 89.92305088043213  Train_KL: 3.516669064760208  Validation Loss : 93.03515625 Val_Reconstruction : 89.54827499389648 Val_KL : 3.4868850708007812\n","Epoch: 3148/8000  Traning Loss: 93.6449842453003  Train_Reconstruction: 90.13411903381348  Train_KL: 3.510865658521652  Validation Loss : 92.96585083007812 Val_Reconstruction : 89.50153732299805 Val_KL : 3.464314818382263\n","Epoch: 3149/8000  Traning Loss: 93.52832508087158  Train_Reconstruction: 90.03051090240479  Train_KL: 3.497814029455185  Validation Loss : 93.08686447143555 Val_Reconstruction : 89.62581634521484 Val_KL : 3.461047649383545\n","Epoch: 3150/8000  Traning Loss: 93.4592399597168  Train_Reconstruction: 89.94818782806396  Train_KL: 3.5110529363155365  Validation Loss : 92.8771858215332 Val_Reconstruction : 89.40191268920898 Val_KL : 3.4752752780914307\n","Epoch: 3151/8000  Traning Loss: 93.27731418609619  Train_Reconstruction: 89.76850318908691  Train_KL: 3.508810043334961  Validation Loss : 92.93313217163086 Val_Reconstruction : 89.46942138671875 Val_KL : 3.4637125730514526\n","Epoch: 3152/8000  Traning Loss: 93.60534191131592  Train_Reconstruction: 90.09928512573242  Train_KL: 3.506057471036911  Validation Loss : 93.1466064453125 Val_Reconstruction : 89.68104553222656 Val_KL : 3.4655630588531494\n","Epoch: 3153/8000  Traning Loss: 93.54109954833984  Train_Reconstruction: 90.02948951721191  Train_KL: 3.511610746383667  Validation Loss : 93.43895721435547 Val_Reconstruction : 89.96903991699219 Val_KL : 3.4699175357818604\n","Epoch: 3154/8000  Traning Loss: 93.64968585968018  Train_Reconstruction: 90.14469718933105  Train_KL: 3.5049885511398315  Validation Loss : 93.09429550170898 Val_Reconstruction : 89.62705993652344 Val_KL : 3.4672385454177856\n","Epoch: 3155/8000  Traning Loss: 93.4450855255127  Train_Reconstruction: 89.93697929382324  Train_KL: 3.5081076323986053  Validation Loss : 92.9778060913086 Val_Reconstruction : 89.51511764526367 Val_KL : 3.4626880884170532\n","Epoch: 3156/8000  Traning Loss: 93.47968864440918  Train_Reconstruction: 89.97471714019775  Train_KL: 3.5049712359905243  Validation Loss : 93.17671585083008 Val_Reconstruction : 89.71343231201172 Val_KL : 3.46328341960907\n","Epoch: 3157/8000  Traning Loss: 93.60598373413086  Train_Reconstruction: 90.09066867828369  Train_KL: 3.5153169333934784  Validation Loss : 93.36288070678711 Val_Reconstruction : 89.88621139526367 Val_KL : 3.4766677618026733\n","Epoch: 3158/8000  Traning Loss: 94.17731380462646  Train_Reconstruction: 90.65272235870361  Train_KL: 3.524590849876404  Validation Loss : 93.94375610351562 Val_Reconstruction : 90.46004104614258 Val_KL : 3.4837135076522827\n","Epoch: 3159/8000  Traning Loss: 94.09010982513428  Train_Reconstruction: 90.58054065704346  Train_KL: 3.509569674730301  Validation Loss : 93.79391098022461 Val_Reconstruction : 90.32730102539062 Val_KL : 3.466607689857483\n","Epoch: 3160/8000  Traning Loss: 93.79853343963623  Train_Reconstruction: 90.28873062133789  Train_KL: 3.5098035037517548  Validation Loss : 93.45960998535156 Val_Reconstruction : 89.98881149291992 Val_KL : 3.4707980155944824\n","Epoch: 3161/8000  Traning Loss: 93.88868427276611  Train_Reconstruction: 90.36994457244873  Train_KL: 3.5187420547008514  Validation Loss : 94.06771469116211 Val_Reconstruction : 90.58388900756836 Val_KL : 3.4838249683380127\n","Epoch: 3162/8000  Traning Loss: 94.07728004455566  Train_Reconstruction: 90.5606803894043  Train_KL: 3.5166000723838806  Validation Loss : 93.20810317993164 Val_Reconstruction : 89.73028564453125 Val_KL : 3.4778167009353638\n","Epoch: 3163/8000  Traning Loss: 93.45803546905518  Train_Reconstruction: 89.94756507873535  Train_KL: 3.510469615459442  Validation Loss : 92.90019989013672 Val_Reconstruction : 89.43632888793945 Val_KL : 3.4638718366622925\n","Epoch: 3164/8000  Traning Loss: 93.36913204193115  Train_Reconstruction: 89.85479164123535  Train_KL: 3.5143397748470306  Validation Loss : 92.96139144897461 Val_Reconstruction : 89.47676086425781 Val_KL : 3.4846303462982178\n","Epoch: 3165/8000  Traning Loss: 93.3769884109497  Train_Reconstruction: 89.85374927520752  Train_KL: 3.523239105939865  Validation Loss : 93.0130844116211 Val_Reconstruction : 89.53699111938477 Val_KL : 3.4760936498641968\n","Epoch: 3166/8000  Traning Loss: 93.50016117095947  Train_Reconstruction: 89.98532009124756  Train_KL: 3.514840930700302  Validation Loss : 92.86563491821289 Val_Reconstruction : 89.38764190673828 Val_KL : 3.4779930114746094\n","Epoch: 3167/8000  Traning Loss: 93.50531387329102  Train_Reconstruction: 89.99193572998047  Train_KL: 3.5133785903453827  Validation Loss : 92.9803581237793 Val_Reconstruction : 89.50238800048828 Val_KL : 3.4779701232910156\n","Epoch: 3168/8000  Traning Loss: 93.32263565063477  Train_Reconstruction: 89.81109809875488  Train_KL: 3.5115375220775604  Validation Loss : 92.95780181884766 Val_Reconstruction : 89.48796463012695 Val_KL : 3.469837784767151\n","Epoch: 3169/8000  Traning Loss: 93.40506172180176  Train_Reconstruction: 89.89865016937256  Train_KL: 3.5064101219177246  Validation Loss : 92.82002258300781 Val_Reconstruction : 89.358154296875 Val_KL : 3.46186900138855\n","Epoch: 3170/8000  Traning Loss: 93.47424602508545  Train_Reconstruction: 89.95338439941406  Train_KL: 3.5208609104156494  Validation Loss : 93.03934860229492 Val_Reconstruction : 89.55490112304688 Val_KL : 3.4844446182250977\n","Epoch: 3171/8000  Traning Loss: 93.49034404754639  Train_Reconstruction: 89.9664249420166  Train_KL: 3.5239196717739105  Validation Loss : 92.83550262451172 Val_Reconstruction : 89.36504364013672 Val_KL : 3.4704595804214478\n","Epoch: 3172/8000  Traning Loss: 93.56667423248291  Train_Reconstruction: 90.06933498382568  Train_KL: 3.4973392486572266  Validation Loss : 92.93082046508789 Val_Reconstruction : 89.48264694213867 Val_KL : 3.4481736421585083\n","Epoch: 3173/8000  Traning Loss: 93.64533424377441  Train_Reconstruction: 90.14360046386719  Train_KL: 3.5017336308956146  Validation Loss : 93.21197891235352 Val_Reconstruction : 89.73769760131836 Val_KL : 3.4742826223373413\n","Epoch: 3174/8000  Traning Loss: 93.86926460266113  Train_Reconstruction: 90.35525703430176  Train_KL: 3.5140075981616974  Validation Loss : 93.66414642333984 Val_Reconstruction : 90.1911849975586 Val_KL : 3.472961902618408\n","Epoch: 3175/8000  Traning Loss: 93.8993091583252  Train_Reconstruction: 90.39926242828369  Train_KL: 3.5000469982624054  Validation Loss : 93.3609733581543 Val_Reconstruction : 89.89667510986328 Val_KL : 3.4642980098724365\n","Epoch: 3176/8000  Traning Loss: 93.86824417114258  Train_Reconstruction: 90.36569404602051  Train_KL: 3.5025502741336823  Validation Loss : 93.33480453491211 Val_Reconstruction : 89.87805938720703 Val_KL : 3.456745743751526\n","Epoch: 3177/8000  Traning Loss: 94.51627731323242  Train_Reconstruction: 91.00668907165527  Train_KL: 3.5095886290073395  Validation Loss : 94.7182502746582 Val_Reconstruction : 91.23804473876953 Val_KL : 3.480204701423645\n","Epoch: 3178/8000  Traning Loss: 94.03334331512451  Train_Reconstruction: 90.51987266540527  Train_KL: 3.5134706795215607  Validation Loss : 93.28805541992188 Val_Reconstruction : 89.82041931152344 Val_KL : 3.4676355123519897\n","Epoch: 3179/8000  Traning Loss: 93.70855808258057  Train_Reconstruction: 90.19255352020264  Train_KL: 3.5160042345523834  Validation Loss : 93.57297897338867 Val_Reconstruction : 90.08907318115234 Val_KL : 3.483903408050537\n","Epoch: 3180/8000  Traning Loss: 93.70236682891846  Train_Reconstruction: 90.18189907073975  Train_KL: 3.5204681754112244  Validation Loss : 93.00472259521484 Val_Reconstruction : 89.5313720703125 Val_KL : 3.4733493328094482\n","Epoch: 3181/8000  Traning Loss: 93.72180366516113  Train_Reconstruction: 90.20785331726074  Train_KL: 3.513950765132904  Validation Loss : 93.79448699951172 Val_Reconstruction : 90.32714462280273 Val_KL : 3.467344641685486\n","Epoch: 3182/8000  Traning Loss: 94.01281452178955  Train_Reconstruction: 90.49302005767822  Train_KL: 3.5197933316230774  Validation Loss : 93.39912414550781 Val_Reconstruction : 89.91842651367188 Val_KL : 3.480697989463806\n","Epoch: 3183/8000  Traning Loss: 93.59557247161865  Train_Reconstruction: 90.07377052307129  Train_KL: 3.521801173686981  Validation Loss : 93.18037796020508 Val_Reconstruction : 89.71139526367188 Val_KL : 3.4689844846725464\n","Epoch: 3184/8000  Traning Loss: 93.56254291534424  Train_Reconstruction: 90.05260753631592  Train_KL: 3.5099349915981293  Validation Loss : 93.01850128173828 Val_Reconstruction : 89.55218505859375 Val_KL : 3.4663138389587402\n","Epoch: 3185/8000  Traning Loss: 93.53435516357422  Train_Reconstruction: 90.0221118927002  Train_KL: 3.512242078781128  Validation Loss : 93.0587387084961 Val_Reconstruction : 89.58847045898438 Val_KL : 3.4702694416046143\n","Epoch: 3186/8000  Traning Loss: 93.54520034790039  Train_Reconstruction: 90.04402542114258  Train_KL: 3.501176029443741  Validation Loss : 92.9697265625 Val_Reconstruction : 89.50851821899414 Val_KL : 3.4612075090408325\n","Epoch: 3187/8000  Traning Loss: 93.58262348175049  Train_Reconstruction: 90.0800666809082  Train_KL: 3.5025572180747986  Validation Loss : 92.76680755615234 Val_Reconstruction : 89.29634857177734 Val_KL : 3.470458507537842\n","Epoch: 3188/8000  Traning Loss: 93.4495210647583  Train_Reconstruction: 89.93484783172607  Train_KL: 3.514672636985779  Validation Loss : 93.26723861694336 Val_Reconstruction : 89.78996276855469 Val_KL : 3.4772753715515137\n","Epoch: 3189/8000  Traning Loss: 93.52229595184326  Train_Reconstruction: 90.00439643859863  Train_KL: 3.5178997814655304  Validation Loss : 93.41154098510742 Val_Reconstruction : 89.93778228759766 Val_KL : 3.473760485649109\n","Epoch: 3190/8000  Traning Loss: 93.56025695800781  Train_Reconstruction: 90.05797672271729  Train_KL: 3.5022807717323303  Validation Loss : 93.18545150756836 Val_Reconstruction : 89.71865844726562 Val_KL : 3.4667937755584717\n","Epoch: 3191/8000  Traning Loss: 93.43821716308594  Train_Reconstruction: 89.93323516845703  Train_KL: 3.5049813389778137  Validation Loss : 92.79718399047852 Val_Reconstruction : 89.32526779174805 Val_KL : 3.4719158411026\n","Epoch: 3192/8000  Traning Loss: 93.74817276000977  Train_Reconstruction: 90.23840713500977  Train_KL: 3.5097655951976776  Validation Loss : 93.24569702148438 Val_Reconstruction : 89.77676391601562 Val_KL : 3.4689329862594604\n","Epoch: 3193/8000  Traning Loss: 93.67915534973145  Train_Reconstruction: 90.17513465881348  Train_KL: 3.504021465778351  Validation Loss : 93.0284309387207 Val_Reconstruction : 89.55965423583984 Val_KL : 3.4687743186950684\n","Epoch: 3194/8000  Traning Loss: 93.83205223083496  Train_Reconstruction: 90.31871223449707  Train_KL: 3.513340413570404  Validation Loss : 93.56327819824219 Val_Reconstruction : 90.09052276611328 Val_KL : 3.4727582931518555\n","Epoch: 3195/8000  Traning Loss: 94.00129413604736  Train_Reconstruction: 90.49713611602783  Train_KL: 3.504158765077591  Validation Loss : 93.11838912963867 Val_Reconstruction : 89.65537643432617 Val_KL : 3.463013768196106\n","Epoch: 3196/8000  Traning Loss: 93.63113403320312  Train_Reconstruction: 90.12638473510742  Train_KL: 3.5047488808631897  Validation Loss : 93.14953994750977 Val_Reconstruction : 89.68087005615234 Val_KL : 3.4686723947525024\n","Epoch: 3197/8000  Traning Loss: 93.45880031585693  Train_Reconstruction: 89.95430183410645  Train_KL: 3.5044994056224823  Validation Loss : 93.0409049987793 Val_Reconstruction : 89.57773971557617 Val_KL : 3.46316397190094\n","Epoch: 3198/8000  Traning Loss: 93.85152530670166  Train_Reconstruction: 90.34068012237549  Train_KL: 3.5108452141284943  Validation Loss : 93.54785537719727 Val_Reconstruction : 90.08057022094727 Val_KL : 3.467285394668579\n","Epoch: 3199/8000  Traning Loss: 93.98960494995117  Train_Reconstruction: 90.47303295135498  Train_KL: 3.516571819782257  Validation Loss : 93.37709426879883 Val_Reconstruction : 89.90359497070312 Val_KL : 3.473499298095703\n","Epoch: 3200/8000  Traning Loss: 93.60497856140137  Train_Reconstruction: 90.10317707061768  Train_KL: 3.501800537109375  Validation Loss : 92.83538055419922 Val_Reconstruction : 89.37533569335938 Val_KL : 3.460045576095581\n","Epoch: 3201/8000  Traning Loss: 93.60971069335938  Train_Reconstruction: 90.10161113739014  Train_KL: 3.508100062608719  Validation Loss : 93.4650764465332 Val_Reconstruction : 89.9952278137207 Val_KL : 3.469850182533264\n","Epoch: 3202/8000  Traning Loss: 94.0376091003418  Train_Reconstruction: 90.52120780944824  Train_KL: 3.51640185713768  Validation Loss : 93.83071899414062 Val_Reconstruction : 90.35783767700195 Val_KL : 3.472881317138672\n","Epoch: 3203/8000  Traning Loss: 93.76735019683838  Train_Reconstruction: 90.25938129425049  Train_KL: 3.5079676806926727  Validation Loss : 93.21826934814453 Val_Reconstruction : 89.75611114501953 Val_KL : 3.4621570110321045\n","Epoch: 3204/8000  Traning Loss: 93.56681632995605  Train_Reconstruction: 90.06005668640137  Train_KL: 3.5067595541477203  Validation Loss : 92.90169143676758 Val_Reconstruction : 89.43090057373047 Val_KL : 3.4707889556884766\n","Epoch: 3205/8000  Traning Loss: 93.52960395812988  Train_Reconstruction: 90.01804542541504  Train_KL: 3.511558771133423  Validation Loss : 93.10199356079102 Val_Reconstruction : 89.61861419677734 Val_KL : 3.4833801984786987\n","Epoch: 3206/8000  Traning Loss: 93.23223304748535  Train_Reconstruction: 89.71818447113037  Train_KL: 3.514048457145691  Validation Loss : 92.81509780883789 Val_Reconstruction : 89.35306167602539 Val_KL : 3.462038516998291\n","Epoch: 3207/8000  Traning Loss: 93.23219108581543  Train_Reconstruction: 89.7332706451416  Train_KL: 3.498921275138855  Validation Loss : 92.66249084472656 Val_Reconstruction : 89.20254516601562 Val_KL : 3.459944486618042\n","Epoch: 3208/8000  Traning Loss: 93.21088600158691  Train_Reconstruction: 89.70729732513428  Train_KL: 3.503587245941162  Validation Loss : 92.79562759399414 Val_Reconstruction : 89.32803726196289 Val_KL : 3.4675897359848022\n","Epoch: 3209/8000  Traning Loss: 93.18993473052979  Train_Reconstruction: 89.67646026611328  Train_KL: 3.5134743750095367  Validation Loss : 92.80899810791016 Val_Reconstruction : 89.33390045166016 Val_KL : 3.4750982522964478\n","Epoch: 3210/8000  Traning Loss: 93.26442337036133  Train_Reconstruction: 89.75315284729004  Train_KL: 3.511270433664322  Validation Loss : 92.94242477416992 Val_Reconstruction : 89.47750473022461 Val_KL : 3.4649205207824707\n","Epoch: 3211/8000  Traning Loss: 93.4380407333374  Train_Reconstruction: 89.92546272277832  Train_KL: 3.512578308582306  Validation Loss : 93.14790725708008 Val_Reconstruction : 89.67671585083008 Val_KL : 3.471191644668579\n","Epoch: 3212/8000  Traning Loss: 94.1199369430542  Train_Reconstruction: 90.60805416107178  Train_KL: 3.5118826627731323  Validation Loss : 93.91427993774414 Val_Reconstruction : 90.44388580322266 Val_KL : 3.47039532661438\n","Epoch: 3213/8000  Traning Loss: 94.06217670440674  Train_Reconstruction: 90.55199909210205  Train_KL: 3.5101767480373383  Validation Loss : 93.44902801513672 Val_Reconstruction : 89.97800064086914 Val_KL : 3.4710278511047363\n","Epoch: 3214/8000  Traning Loss: 93.68213844299316  Train_Reconstruction: 90.15318012237549  Train_KL: 3.5289585292339325  Validation Loss : 93.04267883300781 Val_Reconstruction : 89.5504150390625 Val_KL : 3.492263078689575\n","Epoch: 3215/8000  Traning Loss: 93.78370380401611  Train_Reconstruction: 90.26326942443848  Train_KL: 3.5204339921474457  Validation Loss : 93.40227890014648 Val_Reconstruction : 89.9395637512207 Val_KL : 3.462716579437256\n","Epoch: 3216/8000  Traning Loss: 93.85541439056396  Train_Reconstruction: 90.35022926330566  Train_KL: 3.5051838159561157  Validation Loss : 93.31541442871094 Val_Reconstruction : 89.84593200683594 Val_KL : 3.469482421875\n","Epoch: 3217/8000  Traning Loss: 93.35416030883789  Train_Reconstruction: 89.83720588684082  Train_KL: 3.5169540643692017  Validation Loss : 92.84449768066406 Val_Reconstruction : 89.36613464355469 Val_KL : 3.478362202644348\n","Epoch: 3218/8000  Traning Loss: 93.47905349731445  Train_Reconstruction: 89.96345520019531  Train_KL: 3.5155979990959167  Validation Loss : 92.93569564819336 Val_Reconstruction : 89.47111511230469 Val_KL : 3.4645804166793823\n","Epoch: 3219/8000  Traning Loss: 93.56386756896973  Train_Reconstruction: 90.05655479431152  Train_KL: 3.5073136389255524  Validation Loss : 93.10685348510742 Val_Reconstruction : 89.6488037109375 Val_KL : 3.4580495357513428\n","Epoch: 3220/8000  Traning Loss: 93.39841175079346  Train_Reconstruction: 89.89600276947021  Train_KL: 3.5024091601371765  Validation Loss : 93.28472900390625 Val_Reconstruction : 89.81907653808594 Val_KL : 3.4656494855880737\n","Epoch: 3221/8000  Traning Loss: 93.303635597229  Train_Reconstruction: 89.79961681365967  Train_KL: 3.504017472267151  Validation Loss : 93.08790588378906 Val_Reconstruction : 89.6232795715332 Val_KL : 3.4646271467208862\n","Epoch: 3222/8000  Traning Loss: 93.46633434295654  Train_Reconstruction: 89.96296691894531  Train_KL: 3.503365635871887  Validation Loss : 93.04995346069336 Val_Reconstruction : 89.58614349365234 Val_KL : 3.4638129472732544\n","Epoch: 3223/8000  Traning Loss: 93.62728023529053  Train_Reconstruction: 90.11935520172119  Train_KL: 3.5079239308834076  Validation Loss : 93.4519157409668 Val_Reconstruction : 89.9835205078125 Val_KL : 3.4683960676193237\n","Epoch: 3224/8000  Traning Loss: 93.81029891967773  Train_Reconstruction: 90.30583000183105  Train_KL: 3.504469633102417  Validation Loss : 93.16425704956055 Val_Reconstruction : 89.7010726928711 Val_KL : 3.4631863832473755\n","Epoch: 3225/8000  Traning Loss: 93.76133155822754  Train_Reconstruction: 90.24986553192139  Train_KL: 3.5114674866199493  Validation Loss : 93.5492172241211 Val_Reconstruction : 90.07254028320312 Val_KL : 3.4766781330108643\n","Epoch: 3226/8000  Traning Loss: 93.84678077697754  Train_Reconstruction: 90.33487033843994  Train_KL: 3.5119094252586365  Validation Loss : 93.45352935791016 Val_Reconstruction : 89.98223876953125 Val_KL : 3.471290349960327\n","Epoch: 3227/8000  Traning Loss: 93.43448448181152  Train_Reconstruction: 89.92626285552979  Train_KL: 3.5082212388515472  Validation Loss : 92.74984741210938 Val_Reconstruction : 89.28104019165039 Val_KL : 3.46880841255188\n","Epoch: 3228/8000  Traning Loss: 93.33435153961182  Train_Reconstruction: 89.82195377349854  Train_KL: 3.5123974978923798  Validation Loss : 92.82781219482422 Val_Reconstruction : 89.34885025024414 Val_KL : 3.4789596796035767\n","Epoch: 3229/8000  Traning Loss: 93.39870262145996  Train_Reconstruction: 89.88101005554199  Train_KL: 3.5176923274993896  Validation Loss : 93.28697967529297 Val_Reconstruction : 89.81153869628906 Val_KL : 3.4754403829574585\n","Epoch: 3230/8000  Traning Loss: 93.52195835113525  Train_Reconstruction: 90.01297855377197  Train_KL: 3.5089801847934723  Validation Loss : 93.01393127441406 Val_Reconstruction : 89.55427551269531 Val_KL : 3.459656596183777\n","Epoch: 3231/8000  Traning Loss: 93.52521133422852  Train_Reconstruction: 90.01980209350586  Train_KL: 3.5054087042808533  Validation Loss : 93.20589447021484 Val_Reconstruction : 89.7394027709961 Val_KL : 3.4664918184280396\n","Epoch: 3232/8000  Traning Loss: 93.38590335845947  Train_Reconstruction: 89.87242984771729  Train_KL: 3.513473242521286  Validation Loss : 93.2813949584961 Val_Reconstruction : 89.80864334106445 Val_KL : 3.472750186920166\n","Epoch: 3233/8000  Traning Loss: 93.45335102081299  Train_Reconstruction: 89.94335174560547  Train_KL: 3.5100002884864807  Validation Loss : 92.90328216552734 Val_Reconstruction : 89.44575881958008 Val_KL : 3.457523465156555\n","Epoch: 3234/8000  Traning Loss: 93.40896415710449  Train_Reconstruction: 89.90333461761475  Train_KL: 3.5056295096874237  Validation Loss : 92.89458084106445 Val_Reconstruction : 89.42394256591797 Val_KL : 3.4706391096115112\n","Epoch: 3235/8000  Traning Loss: 93.32174777984619  Train_Reconstruction: 89.80584621429443  Train_KL: 3.5159007608890533  Validation Loss : 92.78937911987305 Val_Reconstruction : 89.30808639526367 Val_KL : 3.4812909364700317\n","Epoch: 3236/8000  Traning Loss: 93.21010112762451  Train_Reconstruction: 89.69081497192383  Train_KL: 3.519285351037979  Validation Loss : 92.82341003417969 Val_Reconstruction : 89.3430061340332 Val_KL : 3.480405807495117\n","Epoch: 3237/8000  Traning Loss: 93.38920211791992  Train_Reconstruction: 89.87375068664551  Train_KL: 3.5154514610767365  Validation Loss : 92.85402297973633 Val_Reconstruction : 89.36979675292969 Val_KL : 3.48422634601593\n","Epoch: 3238/8000  Traning Loss: 93.5960283279419  Train_Reconstruction: 90.076003074646  Train_KL: 3.520025819540024  Validation Loss : 93.09546279907227 Val_Reconstruction : 89.62449645996094 Val_KL : 3.4709655046463013\n","Epoch: 3239/8000  Traning Loss: 93.4581470489502  Train_Reconstruction: 89.95285320281982  Train_KL: 3.5052943527698517  Validation Loss : 92.86817932128906 Val_Reconstruction : 89.40432739257812 Val_KL : 3.4638514518737793\n","Epoch: 3240/8000  Traning Loss: 93.39446830749512  Train_Reconstruction: 89.8854923248291  Train_KL: 3.5089752078056335  Validation Loss : 93.22661209106445 Val_Reconstruction : 89.74539566040039 Val_KL : 3.4812140464782715\n","Epoch: 3241/8000  Traning Loss: 93.38940811157227  Train_Reconstruction: 89.86860847473145  Train_KL: 3.5207991898059845  Validation Loss : 92.97797393798828 Val_Reconstruction : 89.4952621459961 Val_KL : 3.482708215713501\n","Epoch: 3242/8000  Traning Loss: 93.45030307769775  Train_Reconstruction: 89.93572521209717  Train_KL: 3.514576733112335  Validation Loss : 93.0607681274414 Val_Reconstruction : 89.58530807495117 Val_KL : 3.4754574298858643\n","Epoch: 3243/8000  Traning Loss: 93.55223846435547  Train_Reconstruction: 90.0349931716919  Train_KL: 3.5172455310821533  Validation Loss : 92.95904922485352 Val_Reconstruction : 89.4781379699707 Val_KL : 3.4809101819992065\n","Epoch: 3244/8000  Traning Loss: 93.51945686340332  Train_Reconstruction: 90.00187969207764  Train_KL: 3.517577201128006  Validation Loss : 92.8885498046875 Val_Reconstruction : 89.40954208374023 Val_KL : 3.4790056943893433\n","Epoch: 3245/8000  Traning Loss: 93.50838375091553  Train_Reconstruction: 89.99869728088379  Train_KL: 3.509686976671219  Validation Loss : 93.02779769897461 Val_Reconstruction : 89.55284118652344 Val_KL : 3.4749562740325928\n","Epoch: 3246/8000  Traning Loss: 93.36972618103027  Train_Reconstruction: 89.85351848602295  Train_KL: 3.5162079334259033  Validation Loss : 92.7247543334961 Val_Reconstruction : 89.24577713012695 Val_KL : 3.4789782762527466\n","Epoch: 3247/8000  Traning Loss: 93.536789894104  Train_Reconstruction: 90.01946830749512  Train_KL: 3.517322689294815  Validation Loss : 93.24748229980469 Val_Reconstruction : 89.77341079711914 Val_KL : 3.4740700721740723\n","Epoch: 3248/8000  Traning Loss: 93.60274696350098  Train_Reconstruction: 90.09143352508545  Train_KL: 3.5113134682178497  Validation Loss : 93.24833297729492 Val_Reconstruction : 89.78080749511719 Val_KL : 3.46752393245697\n","Epoch: 3249/8000  Traning Loss: 93.67483329772949  Train_Reconstruction: 90.16325283050537  Train_KL: 3.5115799605846405  Validation Loss : 92.96379852294922 Val_Reconstruction : 89.49959564208984 Val_KL : 3.4642027616500854\n","Epoch: 3250/8000  Traning Loss: 93.57441139221191  Train_Reconstruction: 90.06656265258789  Train_KL: 3.5078484416007996  Validation Loss : 93.1487922668457 Val_Reconstruction : 89.67960357666016 Val_KL : 3.469190239906311\n","Epoch: 3251/8000  Traning Loss: 93.53414630889893  Train_Reconstruction: 90.01514911651611  Train_KL: 3.5189975202083588  Validation Loss : 93.15630340576172 Val_Reconstruction : 89.67528533935547 Val_KL : 3.481020450592041\n","Epoch: 3252/8000  Traning Loss: 93.51628494262695  Train_Reconstruction: 90.00309371948242  Train_KL: 3.513191819190979  Validation Loss : 92.92698669433594 Val_Reconstruction : 89.4432487487793 Val_KL : 3.4837374687194824\n","Epoch: 3253/8000  Traning Loss: 93.2977352142334  Train_Reconstruction: 89.7769546508789  Train_KL: 3.5207802057266235  Validation Loss : 92.59757232666016 Val_Reconstruction : 89.11786270141602 Val_KL : 3.4797123670578003\n","Epoch: 3254/8000  Traning Loss: 93.23316287994385  Train_Reconstruction: 89.71752548217773  Train_KL: 3.5156383514404297  Validation Loss : 92.95009994506836 Val_Reconstruction : 89.47232818603516 Val_KL : 3.4777729511260986\n","Epoch: 3255/8000  Traning Loss: 93.3227071762085  Train_Reconstruction: 89.8090238571167  Train_KL: 3.5136837661266327  Validation Loss : 93.08232498168945 Val_Reconstruction : 89.61084747314453 Val_KL : 3.471476674079895\n","Epoch: 3256/8000  Traning Loss: 93.40372562408447  Train_Reconstruction: 89.89080238342285  Train_KL: 3.5129234194755554  Validation Loss : 93.31576919555664 Val_Reconstruction : 89.8485336303711 Val_KL : 3.467237114906311\n","Epoch: 3257/8000  Traning Loss: 93.65876483917236  Train_Reconstruction: 90.15137577056885  Train_KL: 3.507389187812805  Validation Loss : 93.3750114440918 Val_Reconstruction : 89.9085693359375 Val_KL : 3.4664403200149536\n","Epoch: 3258/8000  Traning Loss: 93.67181968688965  Train_Reconstruction: 90.15180587768555  Train_KL: 3.520014613866806  Validation Loss : 93.06721878051758 Val_Reconstruction : 89.5818977355957 Val_KL : 3.4853193759918213\n","Epoch: 3259/8000  Traning Loss: 93.97986698150635  Train_Reconstruction: 90.47109413146973  Train_KL: 3.508771985769272  Validation Loss : 93.33546447753906 Val_Reconstruction : 89.87368392944336 Val_KL : 3.46178138256073\n","Epoch: 3260/8000  Traning Loss: 93.66230392456055  Train_Reconstruction: 90.16853141784668  Train_KL: 3.4937720894813538  Validation Loss : 92.99430084228516 Val_Reconstruction : 89.54079818725586 Val_KL : 3.4535019397735596\n","Epoch: 3261/8000  Traning Loss: 93.65772914886475  Train_Reconstruction: 90.16258716583252  Train_KL: 3.4951421320438385  Validation Loss : 93.0269775390625 Val_Reconstruction : 89.56753540039062 Val_KL : 3.4594415426254272\n","Epoch: 3262/8000  Traning Loss: 93.30609798431396  Train_Reconstruction: 89.79570007324219  Train_KL: 3.5103980898857117  Validation Loss : 93.17868041992188 Val_Reconstruction : 89.69557571411133 Val_KL : 3.483107328414917\n","Epoch: 3263/8000  Traning Loss: 93.5729923248291  Train_Reconstruction: 90.05009841918945  Train_KL: 3.5228939056396484  Validation Loss : 93.16458511352539 Val_Reconstruction : 89.68822479248047 Val_KL : 3.476362943649292\n","Epoch: 3264/8000  Traning Loss: 93.31021213531494  Train_Reconstruction: 89.80959510803223  Train_KL: 3.5006158351898193  Validation Loss : 92.82511520385742 Val_Reconstruction : 89.3675537109375 Val_KL : 3.4575594663619995\n","Epoch: 3265/8000  Traning Loss: 93.28065299987793  Train_Reconstruction: 89.77918529510498  Train_KL: 3.5014689564704895  Validation Loss : 92.80640411376953 Val_Reconstruction : 89.3335952758789 Val_KL : 3.472806930541992\n","Epoch: 3266/8000  Traning Loss: 93.4879379272461  Train_Reconstruction: 89.96214866638184  Train_KL: 3.5257892310619354  Validation Loss : 92.97378158569336 Val_Reconstruction : 89.47822570800781 Val_KL : 3.4955579042434692\n","Epoch: 3267/8000  Traning Loss: 93.32037448883057  Train_Reconstruction: 89.79665946960449  Train_KL: 3.5237156450748444  Validation Loss : 92.74044799804688 Val_Reconstruction : 89.27349472045898 Val_KL : 3.4669547080993652\n","Epoch: 3268/8000  Traning Loss: 93.22710990905762  Train_Reconstruction: 89.72448348999023  Train_KL: 3.5026252567768097  Validation Loss : 92.72492599487305 Val_Reconstruction : 89.26814270019531 Val_KL : 3.456782102584839\n","Epoch: 3269/8000  Traning Loss: 93.23495101928711  Train_Reconstruction: 89.7265567779541  Train_KL: 3.508392870426178  Validation Loss : 93.08805847167969 Val_Reconstruction : 89.61318969726562 Val_KL : 3.474869728088379\n","Epoch: 3270/8000  Traning Loss: 93.79927825927734  Train_Reconstruction: 90.28349590301514  Train_KL: 3.5157823860645294  Validation Loss : 93.35738754272461 Val_Reconstruction : 89.87786102294922 Val_KL : 3.4795281887054443\n","Epoch: 3271/8000  Traning Loss: 93.56628894805908  Train_Reconstruction: 90.04857540130615  Train_KL: 3.517714500427246  Validation Loss : 92.91232299804688 Val_Reconstruction : 89.43558502197266 Val_KL : 3.476738691329956\n","Epoch: 3272/8000  Traning Loss: 93.64376068115234  Train_Reconstruction: 90.13895988464355  Train_KL: 3.5048011541366577  Validation Loss : 93.58588790893555 Val_Reconstruction : 90.12758255004883 Val_KL : 3.458307385444641\n","Epoch: 3273/8000  Traning Loss: 94.09218692779541  Train_Reconstruction: 90.58970642089844  Train_KL: 3.5024795830249786  Validation Loss : 93.52250671386719 Val_Reconstruction : 90.05489349365234 Val_KL : 3.4676148891448975\n","Epoch: 3274/8000  Traning Loss: 93.848219871521  Train_Reconstruction: 90.34435939788818  Train_KL: 3.5038604140281677  Validation Loss : 93.24146270751953 Val_Reconstruction : 89.77849578857422 Val_KL : 3.4629642963409424\n","Epoch: 3275/8000  Traning Loss: 93.3978796005249  Train_Reconstruction: 89.8920955657959  Train_KL: 3.5057834088802338  Validation Loss : 92.88138961791992 Val_Reconstruction : 89.40924835205078 Val_KL : 3.4721442461013794\n","Epoch: 3276/8000  Traning Loss: 93.5215835571289  Train_Reconstruction: 89.99805068969727  Train_KL: 3.5235317051410675  Validation Loss : 92.83996200561523 Val_Reconstruction : 89.36433029174805 Val_KL : 3.475631833076477\n","Epoch: 3277/8000  Traning Loss: 93.19954776763916  Train_Reconstruction: 89.68435859680176  Train_KL: 3.5151901245117188  Validation Loss : 93.02645874023438 Val_Reconstruction : 89.55686569213867 Val_KL : 3.469594717025757\n","Epoch: 3278/8000  Traning Loss: 93.56088542938232  Train_Reconstruction: 90.04622077941895  Train_KL: 3.514664888381958  Validation Loss : 92.8953857421875 Val_Reconstruction : 89.42524337768555 Val_KL : 3.470145583152771\n","Epoch: 3279/8000  Traning Loss: 94.04097938537598  Train_Reconstruction: 90.52910041809082  Train_KL: 3.511878579854965  Validation Loss : 93.43539428710938 Val_Reconstruction : 89.96532440185547 Val_KL : 3.4700711965560913\n","Epoch: 3280/8000  Traning Loss: 93.98544120788574  Train_Reconstruction: 90.47216320037842  Train_KL: 3.513278305530548  Validation Loss : 93.62691116333008 Val_Reconstruction : 90.15265274047852 Val_KL : 3.4742588996887207\n","Epoch: 3281/8000  Traning Loss: 94.23027420043945  Train_Reconstruction: 90.71337223052979  Train_KL: 3.516902267932892  Validation Loss : 93.46946334838867 Val_Reconstruction : 89.99898910522461 Val_KL : 3.4704763889312744\n","Epoch: 3282/8000  Traning Loss: 93.69611930847168  Train_Reconstruction: 90.1870231628418  Train_KL: 3.5090953707695007  Validation Loss : 92.86350631713867 Val_Reconstruction : 89.39932632446289 Val_KL : 3.4641793966293335\n","Epoch: 3283/8000  Traning Loss: 93.38410949707031  Train_Reconstruction: 89.87270164489746  Train_KL: 3.5114074051380157  Validation Loss : 93.04120254516602 Val_Reconstruction : 89.56939315795898 Val_KL : 3.4718111753463745\n","Epoch: 3284/8000  Traning Loss: 93.51308345794678  Train_Reconstruction: 89.99388885498047  Train_KL: 3.519194096326828  Validation Loss : 93.34394836425781 Val_Reconstruction : 89.86189270019531 Val_KL : 3.4820562601089478\n","Epoch: 3285/8000  Traning Loss: 93.76869010925293  Train_Reconstruction: 90.25007629394531  Train_KL: 3.518614649772644  Validation Loss : 93.11991119384766 Val_Reconstruction : 89.63413619995117 Val_KL : 3.485774278640747\n","Epoch: 3286/8000  Traning Loss: 93.71632862091064  Train_Reconstruction: 90.20352363586426  Train_KL: 3.512805074453354  Validation Loss : 93.26601028442383 Val_Reconstruction : 89.8033218383789 Val_KL : 3.4626853466033936\n","Epoch: 3287/8000  Traning Loss: 93.48946189880371  Train_Reconstruction: 89.97922229766846  Train_KL: 3.5102383494377136  Validation Loss : 92.83062744140625 Val_Reconstruction : 89.36369323730469 Val_KL : 3.46693217754364\n","Epoch: 3288/8000  Traning Loss: 93.35535335540771  Train_Reconstruction: 89.84353828430176  Train_KL: 3.51181498169899  Validation Loss : 93.06420516967773 Val_Reconstruction : 89.60268783569336 Val_KL : 3.4615172147750854\n","Epoch: 3289/8000  Traning Loss: 93.62557888031006  Train_Reconstruction: 90.12395286560059  Train_KL: 3.5016255974769592  Validation Loss : 93.07491302490234 Val_Reconstruction : 89.61458587646484 Val_KL : 3.46032977104187\n","Epoch: 3290/8000  Traning Loss: 93.31355953216553  Train_Reconstruction: 89.80100917816162  Train_KL: 3.512550860643387  Validation Loss : 92.68196868896484 Val_Reconstruction : 89.19644165039062 Val_KL : 3.485525608062744\n","Epoch: 3291/8000  Traning Loss: 93.26803398132324  Train_Reconstruction: 89.74703693389893  Train_KL: 3.520998179912567  Validation Loss : 92.81743621826172 Val_Reconstruction : 89.34106063842773 Val_KL : 3.4763745069503784\n","Epoch: 3292/8000  Traning Loss: 93.33592891693115  Train_Reconstruction: 89.81963729858398  Train_KL: 3.5162919461727142  Validation Loss : 93.34119415283203 Val_Reconstruction : 89.8752212524414 Val_KL : 3.465971827507019\n","Epoch: 3293/8000  Traning Loss: 93.20948505401611  Train_Reconstruction: 89.70358276367188  Train_KL: 3.50590181350708  Validation Loss : 92.65283203125 Val_Reconstruction : 89.18212890625 Val_KL : 3.4707043170928955\n","Epoch: 3294/8000  Traning Loss: 93.26111698150635  Train_Reconstruction: 89.74917221069336  Train_KL: 3.5119452476501465  Validation Loss : 92.7686996459961 Val_Reconstruction : 89.29698944091797 Val_KL : 3.4717084169387817\n","Epoch: 3295/8000  Traning Loss: 93.41427993774414  Train_Reconstruction: 89.90110683441162  Train_KL: 3.513173818588257  Validation Loss : 93.22225189208984 Val_Reconstruction : 89.74643325805664 Val_KL : 3.4758189916610718\n","Epoch: 3296/8000  Traning Loss: 94.14627075195312  Train_Reconstruction: 90.62957000732422  Train_KL: 3.5167016685009003  Validation Loss : 93.64645767211914 Val_Reconstruction : 90.17235946655273 Val_KL : 3.4740982055664062\n","Epoch: 3297/8000  Traning Loss: 94.1139554977417  Train_Reconstruction: 90.61440753936768  Train_KL: 3.499547094106674  Validation Loss : 94.09522247314453 Val_Reconstruction : 90.64547729492188 Val_KL : 3.449744462966919\n","Epoch: 3298/8000  Traning Loss: 93.68440341949463  Train_Reconstruction: 90.17952537536621  Train_KL: 3.5048771500587463  Validation Loss : 92.80188369750977 Val_Reconstruction : 89.34465026855469 Val_KL : 3.4572328329086304\n","Epoch: 3299/8000  Traning Loss: 93.12666130065918  Train_Reconstruction: 89.62128925323486  Train_KL: 3.5053725838661194  Validation Loss : 92.73234939575195 Val_Reconstruction : 89.26343154907227 Val_KL : 3.468917489051819\n","Epoch: 3300/8000  Traning Loss: 93.27725410461426  Train_Reconstruction: 89.75827503204346  Train_KL: 3.5189805030822754  Validation Loss : 92.72231674194336 Val_Reconstruction : 89.2383804321289 Val_KL : 3.4839370250701904\n","Epoch: 3301/8000  Traning Loss: 93.20541095733643  Train_Reconstruction: 89.68847179412842  Train_KL: 3.516939163208008  Validation Loss : 93.14897537231445 Val_Reconstruction : 89.67866897583008 Val_KL : 3.4703058004379272\n","Epoch: 3302/8000  Traning Loss: 93.12538814544678  Train_Reconstruction: 89.61575412750244  Train_KL: 3.5096339881420135  Validation Loss : 92.44158172607422 Val_Reconstruction : 88.97442626953125 Val_KL : 3.4671558141708374\n","Epoch: 3303/8000  Traning Loss: 93.02130603790283  Train_Reconstruction: 89.51143264770508  Train_KL: 3.509873479604721  Validation Loss : 92.63983154296875 Val_Reconstruction : 89.1713981628418 Val_KL : 3.4684334993362427\n","Epoch: 3304/8000  Traning Loss: 93.41338539123535  Train_Reconstruction: 89.90047073364258  Train_KL: 3.512915015220642  Validation Loss : 92.83452224731445 Val_Reconstruction : 89.3752326965332 Val_KL : 3.4592909812927246\n","Epoch: 3305/8000  Traning Loss: 93.47155570983887  Train_Reconstruction: 89.97124290466309  Train_KL: 3.5003127455711365  Validation Loss : 93.19597625732422 Val_Reconstruction : 89.73174667358398 Val_KL : 3.464229106903076\n","Epoch: 3306/8000  Traning Loss: 93.28081703186035  Train_Reconstruction: 89.77273368835449  Train_KL: 3.50808247923851  Validation Loss : 92.54232406616211 Val_Reconstruction : 89.07037353515625 Val_KL : 3.471949815750122\n","Epoch: 3307/8000  Traning Loss: 93.13537216186523  Train_Reconstruction: 89.62115383148193  Train_KL: 3.5142205953598022  Validation Loss : 93.04784393310547 Val_Reconstruction : 89.56868743896484 Val_KL : 3.479156970977783\n","Epoch: 3308/8000  Traning Loss: 93.36761856079102  Train_Reconstruction: 89.84403228759766  Train_KL: 3.523587465286255  Validation Loss : 92.88273620605469 Val_Reconstruction : 89.40067291259766 Val_KL : 3.482064127922058\n","Epoch: 3309/8000  Traning Loss: 93.44970321655273  Train_Reconstruction: 89.93544101715088  Train_KL: 3.5142615735530853  Validation Loss : 93.11934280395508 Val_Reconstruction : 89.65393447875977 Val_KL : 3.465409517288208\n","Epoch: 3310/8000  Traning Loss: 93.69450855255127  Train_Reconstruction: 90.18865489959717  Train_KL: 3.505853772163391  Validation Loss : 93.38972473144531 Val_Reconstruction : 89.91009902954102 Val_KL : 3.479624032974243\n","Epoch: 3311/8000  Traning Loss: 93.64912700653076  Train_Reconstruction: 90.13147449493408  Train_KL: 3.5176524221897125  Validation Loss : 93.21683502197266 Val_Reconstruction : 89.72939682006836 Val_KL : 3.4874396324157715\n","Epoch: 3312/8000  Traning Loss: 93.44947910308838  Train_Reconstruction: 89.93647956848145  Train_KL: 3.5129995346069336  Validation Loss : 92.86787033081055 Val_Reconstruction : 89.3983383178711 Val_KL : 3.469532012939453\n","Epoch: 3313/8000  Traning Loss: 93.47757148742676  Train_Reconstruction: 89.97197341918945  Train_KL: 3.5055981874465942  Validation Loss : 93.1485595703125 Val_Reconstruction : 89.69041061401367 Val_KL : 3.4581499099731445\n","Epoch: 3314/8000  Traning Loss: 93.57808303833008  Train_Reconstruction: 90.06838321685791  Train_KL: 3.509699523448944  Validation Loss : 92.89013671875 Val_Reconstruction : 89.40998077392578 Val_KL : 3.480156421661377\n","Epoch: 3315/8000  Traning Loss: 93.61328983306885  Train_Reconstruction: 90.0978422164917  Train_KL: 3.5154483914375305  Validation Loss : 92.88589859008789 Val_Reconstruction : 89.41514587402344 Val_KL : 3.4707508087158203\n","Epoch: 3316/8000  Traning Loss: 93.28451824188232  Train_Reconstruction: 89.77394485473633  Train_KL: 3.5105732083320618  Validation Loss : 92.98200225830078 Val_Reconstruction : 89.50524520874023 Val_KL : 3.4767563343048096\n","Epoch: 3317/8000  Traning Loss: 93.35974311828613  Train_Reconstruction: 89.84426879882812  Train_KL: 3.5154742300510406  Validation Loss : 93.11149215698242 Val_Reconstruction : 89.64708709716797 Val_KL : 3.464405059814453\n","Epoch: 3318/8000  Traning Loss: 93.45428943634033  Train_Reconstruction: 89.94547176361084  Train_KL: 3.5088185369968414  Validation Loss : 93.18401336669922 Val_Reconstruction : 89.72049713134766 Val_KL : 3.4635136127471924\n","Epoch: 3319/8000  Traning Loss: 93.46888160705566  Train_Reconstruction: 89.95738887786865  Train_KL: 3.5114924013614655  Validation Loss : 93.00652694702148 Val_Reconstruction : 89.51962280273438 Val_KL : 3.4869027137756348\n","Epoch: 3320/8000  Traning Loss: 93.7783374786377  Train_Reconstruction: 90.25278377532959  Train_KL: 3.5255547761917114  Validation Loss : 93.20935821533203 Val_Reconstruction : 89.73192977905273 Val_KL : 3.477429986000061\n","Epoch: 3321/8000  Traning Loss: 93.70148086547852  Train_Reconstruction: 90.20320796966553  Train_KL: 3.4982715845108032  Validation Loss : 93.60731506347656 Val_Reconstruction : 90.14993667602539 Val_KL : 3.4573757648468018\n","Epoch: 3322/8000  Traning Loss: 93.5214786529541  Train_Reconstruction: 90.0082540512085  Train_KL: 3.5132249295711517  Validation Loss : 92.98208999633789 Val_Reconstruction : 89.49853515625 Val_KL : 3.4835554361343384\n","Epoch: 3323/8000  Traning Loss: 93.55114841461182  Train_Reconstruction: 90.02352809906006  Train_KL: 3.52761971950531  Validation Loss : 93.2264289855957 Val_Reconstruction : 89.73955535888672 Val_KL : 3.4868738651275635\n","Epoch: 3324/8000  Traning Loss: 93.34868621826172  Train_Reconstruction: 89.83169269561768  Train_KL: 3.5169944167137146  Validation Loss : 92.89260482788086 Val_Reconstruction : 89.41659164428711 Val_KL : 3.4760115146636963\n","Epoch: 3325/8000  Traning Loss: 93.23863983154297  Train_Reconstruction: 89.73567008972168  Train_KL: 3.502969354391098  Validation Loss : 92.62911605834961 Val_Reconstruction : 89.17106628417969 Val_KL : 3.458049774169922\n","Epoch: 3326/8000  Traning Loss: 93.2629508972168  Train_Reconstruction: 89.75845050811768  Train_KL: 3.5044999718666077  Validation Loss : 93.25483703613281 Val_Reconstruction : 89.78873062133789 Val_KL : 3.4661065340042114\n","Epoch: 3327/8000  Traning Loss: 94.09317874908447  Train_Reconstruction: 90.58327960968018  Train_KL: 3.5098994970321655  Validation Loss : 93.7898178100586 Val_Reconstruction : 90.31858825683594 Val_KL : 3.4712278842926025\n","Epoch: 3328/8000  Traning Loss: 94.42037868499756  Train_Reconstruction: 90.91338634490967  Train_KL: 3.50699320435524  Validation Loss : 93.22726058959961 Val_Reconstruction : 89.75947570800781 Val_KL : 3.467782974243164\n","Epoch: 3329/8000  Traning Loss: 93.5287036895752  Train_Reconstruction: 90.01620292663574  Train_KL: 3.5125002562999725  Validation Loss : 92.73023223876953 Val_Reconstruction : 89.25189208984375 Val_KL : 3.478340983390808\n","Epoch: 3330/8000  Traning Loss: 93.65887451171875  Train_Reconstruction: 90.14486980438232  Train_KL: 3.514004409313202  Validation Loss : 93.66990661621094 Val_Reconstruction : 90.19771194458008 Val_KL : 3.4721932411193848\n","Epoch: 3331/8000  Traning Loss: 93.58457469940186  Train_Reconstruction: 90.07671070098877  Train_KL: 3.5078627467155457  Validation Loss : 93.08308792114258 Val_Reconstruction : 89.61733627319336 Val_KL : 3.465752363204956\n","Epoch: 3332/8000  Traning Loss: 93.34508895874023  Train_Reconstruction: 89.8307695388794  Train_KL: 3.5143182575702667  Validation Loss : 92.76177215576172 Val_Reconstruction : 89.28429412841797 Val_KL : 3.4774789810180664\n","Epoch: 3333/8000  Traning Loss: 93.3100814819336  Train_Reconstruction: 89.79520893096924  Train_KL: 3.5148730874061584  Validation Loss : 92.82696914672852 Val_Reconstruction : 89.34882736206055 Val_KL : 3.4781421422958374\n","Epoch: 3334/8000  Traning Loss: 93.26831245422363  Train_Reconstruction: 89.75364398956299  Train_KL: 3.514668822288513  Validation Loss : 92.97154998779297 Val_Reconstruction : 89.49641418457031 Val_KL : 3.4751356840133667\n","Epoch: 3335/8000  Traning Loss: 93.36115264892578  Train_Reconstruction: 89.85426330566406  Train_KL: 3.5068885385990143  Validation Loss : 93.11053466796875 Val_Reconstruction : 89.64367294311523 Val_KL : 3.4668625593185425\n","Epoch: 3336/8000  Traning Loss: 93.34341239929199  Train_Reconstruction: 89.8270149230957  Train_KL: 3.516396015882492  Validation Loss : 92.9510383605957 Val_Reconstruction : 89.4683723449707 Val_KL : 3.4826643466949463\n","Epoch: 3337/8000  Traning Loss: 93.5551176071167  Train_Reconstruction: 90.03197574615479  Train_KL: 3.523141235113144  Validation Loss : 93.23475646972656 Val_Reconstruction : 89.75484848022461 Val_KL : 3.4799059629440308\n","Epoch: 3338/8000  Traning Loss: 93.72086811065674  Train_Reconstruction: 90.2106761932373  Train_KL: 3.510192573070526  Validation Loss : 93.67242431640625 Val_Reconstruction : 90.21302032470703 Val_KL : 3.4594016075134277\n","Epoch: 3339/8000  Traning Loss: 93.84313011169434  Train_Reconstruction: 90.33960723876953  Train_KL: 3.503522217273712  Validation Loss : 93.24002075195312 Val_Reconstruction : 89.76669692993164 Val_KL : 3.4733248949050903\n","Epoch: 3340/8000  Traning Loss: 93.80637264251709  Train_Reconstruction: 90.28854656219482  Train_KL: 3.5178262293338776  Validation Loss : 93.33016204833984 Val_Reconstruction : 89.85107040405273 Val_KL : 3.4790910482406616\n","Epoch: 3341/8000  Traning Loss: 93.43749809265137  Train_Reconstruction: 89.91797351837158  Train_KL: 3.519524961709976  Validation Loss : 92.9957046508789 Val_Reconstruction : 89.51688766479492 Val_KL : 3.478814482688904\n","Epoch: 3342/8000  Traning Loss: 93.37970924377441  Train_Reconstruction: 89.86924457550049  Train_KL: 3.510464996099472  Validation Loss : 92.95344543457031 Val_Reconstruction : 89.4826889038086 Val_KL : 3.4707573652267456\n","Epoch: 3343/8000  Traning Loss: 93.62120056152344  Train_Reconstruction: 90.1107120513916  Train_KL: 3.5104877650737762  Validation Loss : 93.44496154785156 Val_Reconstruction : 89.96583938598633 Val_KL : 3.4791200160980225\n","Epoch: 3344/8000  Traning Loss: 93.3236780166626  Train_Reconstruction: 89.8088607788086  Train_KL: 3.5148169696331024  Validation Loss : 92.81915283203125 Val_Reconstruction : 89.34347152709961 Val_KL : 3.4756819009780884\n","Epoch: 3345/8000  Traning Loss: 93.18270969390869  Train_Reconstruction: 89.67791175842285  Train_KL: 3.5047990679740906  Validation Loss : 92.93719863891602 Val_Reconstruction : 89.4734115600586 Val_KL : 3.463788628578186\n","Epoch: 3346/8000  Traning Loss: 94.13584995269775  Train_Reconstruction: 90.63631916046143  Train_KL: 3.4995296895504  Validation Loss : 93.73052215576172 Val_Reconstruction : 90.27665710449219 Val_KL : 3.4538636207580566\n","Epoch: 3347/8000  Traning Loss: 94.65533542633057  Train_Reconstruction: 91.14196586608887  Train_KL: 3.513369768857956  Validation Loss : 94.06830978393555 Val_Reconstruction : 90.59166717529297 Val_KL : 3.4766392707824707\n","Epoch: 3348/8000  Traning Loss: 93.9380931854248  Train_Reconstruction: 90.43276405334473  Train_KL: 3.5053296089172363  Validation Loss : 93.36110305786133 Val_Reconstruction : 89.90041732788086 Val_KL : 3.4606857299804688\n","Epoch: 3349/8000  Traning Loss: 93.78979110717773  Train_Reconstruction: 90.29399967193604  Train_KL: 3.49578994512558  Validation Loss : 93.1791763305664 Val_Reconstruction : 89.7104606628418 Val_KL : 3.4687142372131348\n","Epoch: 3350/8000  Traning Loss: 93.58339786529541  Train_Reconstruction: 90.0592851638794  Train_KL: 3.524112194776535  Validation Loss : 93.4996566772461 Val_Reconstruction : 90.01123046875 Val_KL : 3.48842716217041\n","Epoch: 3351/8000  Traning Loss: 93.75772094726562  Train_Reconstruction: 90.2331485748291  Train_KL: 3.52457258105278  Validation Loss : 93.20943832397461 Val_Reconstruction : 89.73746490478516 Val_KL : 3.471971035003662\n","Epoch: 3352/8000  Traning Loss: 93.47997665405273  Train_Reconstruction: 89.96599674224854  Train_KL: 3.513978362083435  Validation Loss : 92.92179489135742 Val_Reconstruction : 89.4521713256836 Val_KL : 3.469622492790222\n","Epoch: 3353/8000  Traning Loss: 93.2858943939209  Train_Reconstruction: 89.76912689208984  Train_KL: 3.5167661607265472  Validation Loss : 93.1501579284668 Val_Reconstruction : 89.67393493652344 Val_KL : 3.4762219190597534\n","Epoch: 3354/8000  Traning Loss: 93.23210430145264  Train_Reconstruction: 89.72471046447754  Train_KL: 3.507395088672638  Validation Loss : 92.9200668334961 Val_Reconstruction : 89.45510864257812 Val_KL : 3.464959144592285\n","Epoch: 3355/8000  Traning Loss: 93.2185525894165  Train_Reconstruction: 89.70760822296143  Train_KL: 3.5109444558620453  Validation Loss : 92.83304595947266 Val_Reconstruction : 89.3519401550293 Val_KL : 3.4811038970947266\n","Epoch: 3356/8000  Traning Loss: 93.32127094268799  Train_Reconstruction: 89.79866600036621  Train_KL: 3.5226047337055206  Validation Loss : 93.48795318603516 Val_Reconstruction : 90.00981140136719 Val_KL : 3.47814404964447\n","Epoch: 3357/8000  Traning Loss: 93.21880435943604  Train_Reconstruction: 89.70643138885498  Train_KL: 3.5123741030693054  Validation Loss : 92.78208923339844 Val_Reconstruction : 89.3154182434082 Val_KL : 3.466672658920288\n","Epoch: 3358/8000  Traning Loss: 93.3566370010376  Train_Reconstruction: 89.84747219085693  Train_KL: 3.509163647890091  Validation Loss : 93.58481979370117 Val_Reconstruction : 90.12142944335938 Val_KL : 3.4633915424346924\n","Epoch: 3359/8000  Traning Loss: 94.2182207107544  Train_Reconstruction: 90.70728015899658  Train_KL: 3.5109399557113647  Validation Loss : 93.3768424987793 Val_Reconstruction : 89.90679168701172 Val_KL : 3.4700509309768677\n","Epoch: 3360/8000  Traning Loss: 93.88858032226562  Train_Reconstruction: 90.37226486206055  Train_KL: 3.51631560921669  Validation Loss : 93.28346252441406 Val_Reconstruction : 89.80443954467773 Val_KL : 3.4790223836898804\n","Epoch: 3361/8000  Traning Loss: 93.37135028839111  Train_Reconstruction: 89.8579969406128  Train_KL: 3.5133527517318726  Validation Loss : 93.18684387207031 Val_Reconstruction : 89.7216796875 Val_KL : 3.4651665687561035\n","Epoch: 3362/8000  Traning Loss: 93.40170574188232  Train_Reconstruction: 89.88563346862793  Train_KL: 3.516071707010269  Validation Loss : 92.83844757080078 Val_Reconstruction : 89.36026382446289 Val_KL : 3.4781848192214966\n","Epoch: 3363/8000  Traning Loss: 93.58763980865479  Train_Reconstruction: 90.06647777557373  Train_KL: 3.5211607217788696  Validation Loss : 92.93947219848633 Val_Reconstruction : 89.46744155883789 Val_KL : 3.4720325469970703\n","Epoch: 3364/8000  Traning Loss: 93.6240873336792  Train_Reconstruction: 90.11654472351074  Train_KL: 3.507540702819824  Validation Loss : 93.09458923339844 Val_Reconstruction : 89.63096237182617 Val_KL : 3.4636292457580566\n","Epoch: 3365/8000  Traning Loss: 93.51634979248047  Train_Reconstruction: 90.01474952697754  Train_KL: 3.501598536968231  Validation Loss : 92.76278686523438 Val_Reconstruction : 89.29654312133789 Val_KL : 3.466245174407959\n","Epoch: 3366/8000  Traning Loss: 93.22217750549316  Train_Reconstruction: 89.71088409423828  Train_KL: 3.511292964220047  Validation Loss : 92.93816375732422 Val_Reconstruction : 89.46270370483398 Val_KL : 3.47545850276947\n","Epoch: 3367/8000  Traning Loss: 93.21519947052002  Train_Reconstruction: 89.7006721496582  Train_KL: 3.5145273208618164  Validation Loss : 93.18292999267578 Val_Reconstruction : 89.7117805480957 Val_KL : 3.4711509943008423\n","Epoch: 3368/8000  Traning Loss: 93.3435230255127  Train_Reconstruction: 89.83622550964355  Train_KL: 3.507297545671463  Validation Loss : 93.69540023803711 Val_Reconstruction : 90.22956085205078 Val_KL : 3.465837240219116\n","Epoch: 3369/8000  Traning Loss: 93.67810916900635  Train_Reconstruction: 90.16826725006104  Train_KL: 3.509841352701187  Validation Loss : 92.74589157104492 Val_Reconstruction : 89.27778244018555 Val_KL : 3.4681081771850586\n","Epoch: 3370/8000  Traning Loss: 93.10003471374512  Train_Reconstruction: 89.59586811065674  Train_KL: 3.504167824983597  Validation Loss : 92.84720611572266 Val_Reconstruction : 89.37789154052734 Val_KL : 3.4693174362182617\n","Epoch: 3371/8000  Traning Loss: 93.05964374542236  Train_Reconstruction: 89.54628276824951  Train_KL: 3.513360917568207  Validation Loss : 92.77467346191406 Val_Reconstruction : 89.30208969116211 Val_KL : 3.472585678100586\n","Epoch: 3372/8000  Traning Loss: 92.97536087036133  Train_Reconstruction: 89.45848274230957  Train_KL: 3.516878515481949  Validation Loss : 92.81729125976562 Val_Reconstruction : 89.34325790405273 Val_KL : 3.474031925201416\n","Epoch: 3373/8000  Traning Loss: 93.19990825653076  Train_Reconstruction: 89.68507099151611  Train_KL: 3.514837622642517  Validation Loss : 92.6371841430664 Val_Reconstruction : 89.17256164550781 Val_KL : 3.4646228551864624\n","Epoch: 3374/8000  Traning Loss: 93.34135150909424  Train_Reconstruction: 89.84095764160156  Train_KL: 3.5003942251205444  Validation Loss : 93.13264083862305 Val_Reconstruction : 89.67081451416016 Val_KL : 3.4618282318115234\n","Epoch: 3375/8000  Traning Loss: 93.29017734527588  Train_Reconstruction: 89.7777624130249  Train_KL: 3.512415438890457  Validation Loss : 92.89740371704102 Val_Reconstruction : 89.41767883300781 Val_KL : 3.4797271490097046\n","Epoch: 3376/8000  Traning Loss: 93.02375984191895  Train_Reconstruction: 89.50788402557373  Train_KL: 3.5158759355545044  Validation Loss : 92.61761474609375 Val_Reconstruction : 89.15066909790039 Val_KL : 3.4669450521469116\n","Epoch: 3377/8000  Traning Loss: 93.34407424926758  Train_Reconstruction: 89.83310317993164  Train_KL: 3.510971248149872  Validation Loss : 92.85888290405273 Val_Reconstruction : 89.39251327514648 Val_KL : 3.466370105743408\n","Epoch: 3378/8000  Traning Loss: 93.16455173492432  Train_Reconstruction: 89.65739631652832  Train_KL: 3.507156580686569  Validation Loss : 92.71178817749023 Val_Reconstruction : 89.24829483032227 Val_KL : 3.463493227958679\n","Epoch: 3379/8000  Traning Loss: 93.54930305480957  Train_Reconstruction: 90.0450496673584  Train_KL: 3.504253625869751  Validation Loss : 93.05608749389648 Val_Reconstruction : 89.58564376831055 Val_KL : 3.4704452753067017\n","Epoch: 3380/8000  Traning Loss: 93.14355945587158  Train_Reconstruction: 89.63651180267334  Train_KL: 3.507047325372696  Validation Loss : 92.75389099121094 Val_Reconstruction : 89.28256607055664 Val_KL : 3.471323013305664\n","Epoch: 3381/8000  Traning Loss: 93.34270095825195  Train_Reconstruction: 89.83440589904785  Train_KL: 3.508294403553009  Validation Loss : 92.95494079589844 Val_Reconstruction : 89.48672103881836 Val_KL : 3.4682180881500244\n","Epoch: 3382/8000  Traning Loss: 93.52383708953857  Train_Reconstruction: 90.00409412384033  Train_KL: 3.5197449028491974  Validation Loss : 93.00905227661133 Val_Reconstruction : 89.53329086303711 Val_KL : 3.4757630825042725\n","Epoch: 3383/8000  Traning Loss: 93.41854858398438  Train_Reconstruction: 89.89489841461182  Train_KL: 3.5236511528491974  Validation Loss : 93.07794570922852 Val_Reconstruction : 89.59773254394531 Val_KL : 3.4802147150039673\n","Epoch: 3384/8000  Traning Loss: 93.1467113494873  Train_Reconstruction: 89.63305377960205  Train_KL: 3.5136575996875763  Validation Loss : 92.79074478149414 Val_Reconstruction : 89.31888961791992 Val_KL : 3.4718544483184814\n","Epoch: 3385/8000  Traning Loss: 93.32075023651123  Train_Reconstruction: 89.81208229064941  Train_KL: 3.5086677968502045  Validation Loss : 92.94143676757812 Val_Reconstruction : 89.47183227539062 Val_KL : 3.4696035385131836\n","Epoch: 3386/8000  Traning Loss: 93.53190040588379  Train_Reconstruction: 90.01490116119385  Train_KL: 3.516999453306198  Validation Loss : 93.16773986816406 Val_Reconstruction : 89.68850326538086 Val_KL : 3.4792368412017822\n","Epoch: 3387/8000  Traning Loss: 93.25813007354736  Train_Reconstruction: 89.75311756134033  Train_KL: 3.5050128698349  Validation Loss : 92.80057525634766 Val_Reconstruction : 89.34120178222656 Val_KL : 3.4593722820281982\n","Epoch: 3388/8000  Traning Loss: 93.19090938568115  Train_Reconstruction: 89.68541145324707  Train_KL: 3.5054979026317596  Validation Loss : 92.84124374389648 Val_Reconstruction : 89.36882781982422 Val_KL : 3.4724135398864746\n","Epoch: 3389/8000  Traning Loss: 93.38091564178467  Train_Reconstruction: 89.87471199035645  Train_KL: 3.5062024891376495  Validation Loss : 92.87588500976562 Val_Reconstruction : 89.41585922241211 Val_KL : 3.4600223302841187\n","Epoch: 3390/8000  Traning Loss: 93.59023571014404  Train_Reconstruction: 90.08727264404297  Train_KL: 3.5029629170894623  Validation Loss : 93.3629264831543 Val_Reconstruction : 89.89512634277344 Val_KL : 3.467802405357361\n","Epoch: 3391/8000  Traning Loss: 94.12428092956543  Train_Reconstruction: 90.61855792999268  Train_KL: 3.5057220458984375  Validation Loss : 94.00819396972656 Val_Reconstruction : 90.54310989379883 Val_KL : 3.465084671974182\n","Epoch: 3392/8000  Traning Loss: 94.07266998291016  Train_Reconstruction: 90.56074714660645  Train_KL: 3.5119228661060333  Validation Loss : 93.45936584472656 Val_Reconstruction : 89.98052978515625 Val_KL : 3.4788365364074707\n","Epoch: 3393/8000  Traning Loss: 93.68251895904541  Train_Reconstruction: 90.17488670349121  Train_KL: 3.5076319873332977  Validation Loss : 92.78661727905273 Val_Reconstruction : 89.3182601928711 Val_KL : 3.468355655670166\n","Epoch: 3394/8000  Traning Loss: 93.49763107299805  Train_Reconstruction: 89.9891996383667  Train_KL: 3.5084304809570312  Validation Loss : 93.1830940246582 Val_Reconstruction : 89.7063217163086 Val_KL : 3.4767736196517944\n","Epoch: 3395/8000  Traning Loss: 93.45171451568604  Train_Reconstruction: 89.9288330078125  Train_KL: 3.522881120443344  Validation Loss : 92.64862060546875 Val_Reconstruction : 89.17421340942383 Val_KL : 3.474408268928528\n","Epoch: 3396/8000  Traning Loss: 93.56065940856934  Train_Reconstruction: 90.05339431762695  Train_KL: 3.5072658360004425  Validation Loss : 92.8684310913086 Val_Reconstruction : 89.40629577636719 Val_KL : 3.46213698387146\n","Epoch: 3397/8000  Traning Loss: 92.92564582824707  Train_Reconstruction: 89.42576503753662  Train_KL: 3.499880611896515  Validation Loss : 92.2287826538086 Val_Reconstruction : 88.77061080932617 Val_KL : 3.458171248435974\n","Epoch: 3398/8000  Traning Loss: 92.83194637298584  Train_Reconstruction: 89.32358455657959  Train_KL: 3.5083633065223694  Validation Loss : 92.56415557861328 Val_Reconstruction : 89.08809661865234 Val_KL : 3.4760591983795166\n","Epoch: 3399/8000  Traning Loss: 93.69357490539551  Train_Reconstruction: 90.17083644866943  Train_KL: 3.522739678621292  Validation Loss : 93.84024810791016 Val_Reconstruction : 90.35275650024414 Val_KL : 3.487489938735962\n","Epoch: 3400/8000  Traning Loss: 94.72263240814209  Train_Reconstruction: 91.20438385009766  Train_KL: 3.5182481706142426  Validation Loss : 95.04546356201172 Val_Reconstruction : 91.56631851196289 Val_KL : 3.4791460037231445\n","Epoch: 3401/8000  Traning Loss: 94.95288562774658  Train_Reconstruction: 91.43696689605713  Train_KL: 3.515917330980301  Validation Loss : 95.24156951904297 Val_Reconstruction : 91.75887680053711 Val_KL : 3.4826927185058594\n","Epoch: 3402/8000  Traning Loss: 95.30740165710449  Train_Reconstruction: 91.7821569442749  Train_KL: 3.5252438485622406  Validation Loss : 94.68379592895508 Val_Reconstruction : 91.19513702392578 Val_KL : 3.4886616468429565\n","Epoch: 3403/8000  Traning Loss: 94.29744148254395  Train_Reconstruction: 90.76960277557373  Train_KL: 3.527839332818985  Validation Loss : 93.64706802368164 Val_Reconstruction : 90.16355895996094 Val_KL : 3.483510971069336\n","Epoch: 3404/8000  Traning Loss: 93.63140487670898  Train_Reconstruction: 90.10575485229492  Train_KL: 3.5256505608558655  Validation Loss : 92.87052154541016 Val_Reconstruction : 89.39097595214844 Val_KL : 3.4795457124710083\n","Epoch: 3405/8000  Traning Loss: 93.27608585357666  Train_Reconstruction: 89.75967407226562  Train_KL: 3.516411602497101  Validation Loss : 92.6722412109375 Val_Reconstruction : 89.19206237792969 Val_KL : 3.4801775217056274\n","Epoch: 3406/8000  Traning Loss: 93.42442417144775  Train_Reconstruction: 89.9119291305542  Train_KL: 3.51249498128891  Validation Loss : 93.22068786621094 Val_Reconstruction : 89.75247192382812 Val_KL : 3.468217372894287\n","Epoch: 3407/8000  Traning Loss: 93.39556217193604  Train_Reconstruction: 89.88712120056152  Train_KL: 3.508439213037491  Validation Loss : 93.0104751586914 Val_Reconstruction : 89.5430793762207 Val_KL : 3.4673928022384644\n","Epoch: 3408/8000  Traning Loss: 93.1410140991211  Train_Reconstruction: 89.63363552093506  Train_KL: 3.5073785185813904  Validation Loss : 92.59412384033203 Val_Reconstruction : 89.1314468383789 Val_KL : 3.462679862976074\n","Epoch: 3409/8000  Traning Loss: 93.04174709320068  Train_Reconstruction: 89.53390693664551  Train_KL: 3.507840931415558  Validation Loss : 92.8228759765625 Val_Reconstruction : 89.3572883605957 Val_KL : 3.4655903577804565\n","Epoch: 3410/8000  Traning Loss: 93.08746337890625  Train_Reconstruction: 89.5692491531372  Train_KL: 3.5182142853736877  Validation Loss : 92.9640884399414 Val_Reconstruction : 89.47609329223633 Val_KL : 3.4879977703094482\n","Epoch: 3411/8000  Traning Loss: 93.19248008728027  Train_Reconstruction: 89.67051124572754  Train_KL: 3.5219696760177612  Validation Loss : 92.7138900756836 Val_Reconstruction : 89.23330307006836 Val_KL : 3.480587601661682\n","Epoch: 3412/8000  Traning Loss: 93.58543014526367  Train_Reconstruction: 90.07338905334473  Train_KL: 3.5120405554771423  Validation Loss : 93.31887817382812 Val_Reconstruction : 89.8413200378418 Val_KL : 3.4775595664978027\n","Epoch: 3413/8000  Traning Loss: 93.57685375213623  Train_Reconstruction: 90.06209087371826  Train_KL: 3.5147638022899628  Validation Loss : 92.75101852416992 Val_Reconstruction : 89.27598571777344 Val_KL : 3.4750349521636963\n","Epoch: 3414/8000  Traning Loss: 93.14156723022461  Train_Reconstruction: 89.62226104736328  Train_KL: 3.519306480884552  Validation Loss : 92.78777694702148 Val_Reconstruction : 89.31559371948242 Val_KL : 3.4721797704696655\n","Epoch: 3415/8000  Traning Loss: 93.42745304107666  Train_Reconstruction: 89.9206953048706  Train_KL: 3.5067580938339233  Validation Loss : 92.80794525146484 Val_Reconstruction : 89.35477828979492 Val_KL : 3.453167200088501\n","Epoch: 3416/8000  Traning Loss: 93.7307243347168  Train_Reconstruction: 90.23434448242188  Train_KL: 3.4963818192481995  Validation Loss : 93.30149459838867 Val_Reconstruction : 89.83698272705078 Val_KL : 3.4645107984542847\n","Epoch: 3417/8000  Traning Loss: 93.78561210632324  Train_Reconstruction: 90.27183818817139  Train_KL: 3.5137737691402435  Validation Loss : 93.42675399780273 Val_Reconstruction : 89.94681930541992 Val_KL : 3.479935646057129\n","Epoch: 3418/8000  Traning Loss: 93.71654891967773  Train_Reconstruction: 90.20291042327881  Train_KL: 3.513638198375702  Validation Loss : 93.08842086791992 Val_Reconstruction : 89.6140022277832 Val_KL : 3.474416732788086\n","Epoch: 3419/8000  Traning Loss: 93.49710750579834  Train_Reconstruction: 89.98592948913574  Train_KL: 3.511178284883499  Validation Loss : 93.28164672851562 Val_Reconstruction : 89.80792236328125 Val_KL : 3.4737274646759033\n","Epoch: 3420/8000  Traning Loss: 93.4468412399292  Train_Reconstruction: 89.9420804977417  Train_KL: 3.5047606229782104  Validation Loss : 92.99172973632812 Val_Reconstruction : 89.52505874633789 Val_KL : 3.466671586036682\n","Epoch: 3421/8000  Traning Loss: 93.3812313079834  Train_Reconstruction: 89.86708164215088  Train_KL: 3.514148771762848  Validation Loss : 93.01817321777344 Val_Reconstruction : 89.53911209106445 Val_KL : 3.47905957698822\n","Epoch: 3422/8000  Traning Loss: 93.11008358001709  Train_Reconstruction: 89.59375858306885  Train_KL: 3.516323536634445  Validation Loss : 93.02279663085938 Val_Reconstruction : 89.55231475830078 Val_KL : 3.4704846143722534\n","Epoch: 3423/8000  Traning Loss: 93.10406112670898  Train_Reconstruction: 89.59436702728271  Train_KL: 3.5096945762634277  Validation Loss : 92.8802261352539 Val_Reconstruction : 89.41102981567383 Val_KL : 3.469199538230896\n","Epoch: 3424/8000  Traning Loss: 93.03397369384766  Train_Reconstruction: 89.52135181427002  Train_KL: 3.5126222670078278  Validation Loss : 92.50010299682617 Val_Reconstruction : 89.02602005004883 Val_KL : 3.4740793704986572\n","Epoch: 3425/8000  Traning Loss: 92.88921546936035  Train_Reconstruction: 89.37740707397461  Train_KL: 3.5118083357810974  Validation Loss : 92.64913177490234 Val_Reconstruction : 89.17846298217773 Val_KL : 3.470666766166687\n","Epoch: 3426/8000  Traning Loss: 93.03118991851807  Train_Reconstruction: 89.51750087738037  Train_KL: 3.5136890411376953  Validation Loss : 92.80690002441406 Val_Reconstruction : 89.32924270629883 Val_KL : 3.4776570796966553\n","Epoch: 3427/8000  Traning Loss: 93.05649089813232  Train_Reconstruction: 89.53846263885498  Train_KL: 3.518028497695923  Validation Loss : 92.65901947021484 Val_Reconstruction : 89.17251968383789 Val_KL : 3.486501455307007\n","Epoch: 3428/8000  Traning Loss: 93.01891708374023  Train_Reconstruction: 89.49734497070312  Train_KL: 3.52157261967659  Validation Loss : 92.82033157348633 Val_Reconstruction : 89.33992004394531 Val_KL : 3.4804131984710693\n","Epoch: 3429/8000  Traning Loss: 93.35468769073486  Train_Reconstruction: 89.84340190887451  Train_KL: 3.5112866163253784  Validation Loss : 92.85822296142578 Val_Reconstruction : 89.39178085327148 Val_KL : 3.4664405584335327\n","Epoch: 3430/8000  Traning Loss: 93.23269557952881  Train_Reconstruction: 89.72306156158447  Train_KL: 3.509634703397751  Validation Loss : 92.89202880859375 Val_Reconstruction : 89.42657470703125 Val_KL : 3.4654526710510254\n","Epoch: 3431/8000  Traning Loss: 93.3368034362793  Train_Reconstruction: 89.82610893249512  Train_KL: 3.510693073272705  Validation Loss : 92.76626205444336 Val_Reconstruction : 89.28828811645508 Val_KL : 3.4779725074768066\n","Epoch: 3432/8000  Traning Loss: 93.05815410614014  Train_Reconstruction: 89.54480648040771  Train_KL: 3.5133466720581055  Validation Loss : 92.55978012084961 Val_Reconstruction : 89.07972717285156 Val_KL : 3.4800541400909424\n","Epoch: 3433/8000  Traning Loss: 93.00782012939453  Train_Reconstruction: 89.48952770233154  Train_KL: 3.5182929635047913  Validation Loss : 92.44458389282227 Val_Reconstruction : 88.96010208129883 Val_KL : 3.4844800233840942\n","Epoch: 3434/8000  Traning Loss: 92.85555934906006  Train_Reconstruction: 89.33541870117188  Train_KL: 3.5201388895511627  Validation Loss : 92.52720260620117 Val_Reconstruction : 89.05746078491211 Val_KL : 3.469741702079773\n","Epoch: 3435/8000  Traning Loss: 93.02719116210938  Train_Reconstruction: 89.51782989501953  Train_KL: 3.509361982345581  Validation Loss : 92.79613876342773 Val_Reconstruction : 89.3279914855957 Val_KL : 3.4681488275527954\n","Epoch: 3436/8000  Traning Loss: 93.17547225952148  Train_Reconstruction: 89.66027164459229  Train_KL: 3.5151998102664948  Validation Loss : 92.74673080444336 Val_Reconstruction : 89.27802276611328 Val_KL : 3.4687066078186035\n","Epoch: 3437/8000  Traning Loss: 93.46927452087402  Train_Reconstruction: 89.95099925994873  Train_KL: 3.5182750523090363  Validation Loss : 92.97582244873047 Val_Reconstruction : 89.49943542480469 Val_KL : 3.476386785507202\n","Epoch: 3438/8000  Traning Loss: 93.37521839141846  Train_Reconstruction: 89.86071872711182  Train_KL: 3.514498859643936  Validation Loss : 92.77421951293945 Val_Reconstruction : 89.30331420898438 Val_KL : 3.470906138420105\n","Epoch: 3439/8000  Traning Loss: 93.04834270477295  Train_Reconstruction: 89.53843593597412  Train_KL: 3.509907364845276  Validation Loss : 92.91716003417969 Val_Reconstruction : 89.45184707641602 Val_KL : 3.4653143882751465\n","Epoch: 3440/8000  Traning Loss: 93.17673301696777  Train_Reconstruction: 89.66971969604492  Train_KL: 3.5070152580738068  Validation Loss : 92.93432998657227 Val_Reconstruction : 89.46778869628906 Val_KL : 3.4665406942367554\n","Epoch: 3441/8000  Traning Loss: 93.20742893218994  Train_Reconstruction: 89.68798446655273  Train_KL: 3.519444316625595  Validation Loss : 92.62571716308594 Val_Reconstruction : 89.1395492553711 Val_KL : 3.4861695766448975\n","Epoch: 3442/8000  Traning Loss: 92.96276950836182  Train_Reconstruction: 89.44464206695557  Train_KL: 3.5181272625923157  Validation Loss : 92.63820266723633 Val_Reconstruction : 89.16874694824219 Val_KL : 3.469456434249878\n","Epoch: 3443/8000  Traning Loss: 92.8830394744873  Train_Reconstruction: 89.37289810180664  Train_KL: 3.510141044855118  Validation Loss : 92.4579086303711 Val_Reconstruction : 88.98637008666992 Val_KL : 3.4715389013290405\n","Epoch: 3444/8000  Traning Loss: 93.04287052154541  Train_Reconstruction: 89.52912521362305  Train_KL: 3.5137450098991394  Validation Loss : 93.22964859008789 Val_Reconstruction : 89.75271606445312 Val_KL : 3.4769351482391357\n","Epoch: 3445/8000  Traning Loss: 93.3647575378418  Train_Reconstruction: 89.84757423400879  Train_KL: 3.517182767391205  Validation Loss : 92.94345474243164 Val_Reconstruction : 89.46657180786133 Val_KL : 3.476882576942444\n","Epoch: 3446/8000  Traning Loss: 93.52553272247314  Train_Reconstruction: 90.01546955108643  Train_KL: 3.510063797235489  Validation Loss : 92.92955780029297 Val_Reconstruction : 89.46040344238281 Val_KL : 3.4691548347473145\n","Epoch: 3447/8000  Traning Loss: 93.15691757202148  Train_Reconstruction: 89.64338874816895  Train_KL: 3.5135273039340973  Validation Loss : 92.68106842041016 Val_Reconstruction : 89.2064094543457 Val_KL : 3.4746605157852173\n","Epoch: 3448/8000  Traning Loss: 93.2587890625  Train_Reconstruction: 89.73759460449219  Train_KL: 3.521195948123932  Validation Loss : 93.05119705200195 Val_Reconstruction : 89.56966018676758 Val_KL : 3.4815372228622437\n","Epoch: 3449/8000  Traning Loss: 93.27466201782227  Train_Reconstruction: 89.76121044158936  Train_KL: 3.513451099395752  Validation Loss : 92.81740188598633 Val_Reconstruction : 89.35430145263672 Val_KL : 3.4631012678146362\n","Epoch: 3450/8000  Traning Loss: 93.04314708709717  Train_Reconstruction: 89.54393863677979  Train_KL: 3.4992088079452515  Validation Loss : 92.6009750366211 Val_Reconstruction : 89.13388061523438 Val_KL : 3.467095136642456\n","Epoch: 3451/8000  Traning Loss: 93.12794494628906  Train_Reconstruction: 89.62381935119629  Train_KL: 3.50412455201149  Validation Loss : 92.85574722290039 Val_Reconstruction : 89.38619613647461 Val_KL : 3.469552159309387\n","Epoch: 3452/8000  Traning Loss: 93.08940410614014  Train_Reconstruction: 89.57450771331787  Train_KL: 3.514896720647812  Validation Loss : 92.65508651733398 Val_Reconstruction : 89.17780685424805 Val_KL : 3.4772785902023315\n","Epoch: 3453/8000  Traning Loss: 93.23944854736328  Train_Reconstruction: 89.71615982055664  Train_KL: 3.52328759431839  Validation Loss : 92.95190811157227 Val_Reconstruction : 89.46641159057617 Val_KL : 3.4854975938796997\n","Epoch: 3454/8000  Traning Loss: 93.07414436340332  Train_Reconstruction: 89.54943752288818  Train_KL: 3.524707615375519  Validation Loss : 92.68181610107422 Val_Reconstruction : 89.20236587524414 Val_KL : 3.4794477224349976\n","Epoch: 3455/8000  Traning Loss: 93.09248161315918  Train_Reconstruction: 89.58524513244629  Train_KL: 3.5072373151779175  Validation Loss : 92.75217056274414 Val_Reconstruction : 89.28818130493164 Val_KL : 3.463990092277527\n","Epoch: 3456/8000  Traning Loss: 93.0916223526001  Train_Reconstruction: 89.58514595031738  Train_KL: 3.5064764618873596  Validation Loss : 92.70136260986328 Val_Reconstruction : 89.23173522949219 Val_KL : 3.4696260690689087\n","Epoch: 3457/8000  Traning Loss: 93.33535766601562  Train_Reconstruction: 89.81361293792725  Train_KL: 3.5217444896698  Validation Loss : 93.3472785949707 Val_Reconstruction : 89.86542510986328 Val_KL : 3.4818530082702637\n","Epoch: 3458/8000  Traning Loss: 93.96247386932373  Train_Reconstruction: 90.44762802124023  Train_KL: 3.514844834804535  Validation Loss : 93.88619613647461 Val_Reconstruction : 90.41842651367188 Val_KL : 3.467768669128418\n","Epoch: 3459/8000  Traning Loss: 94.45541286468506  Train_Reconstruction: 90.95380592346191  Train_KL: 3.501606523990631  Validation Loss : 94.27151489257812 Val_Reconstruction : 90.80045318603516 Val_KL : 3.471062183380127\n","Epoch: 3460/8000  Traning Loss: 94.15823554992676  Train_Reconstruction: 90.64440822601318  Train_KL: 3.5138270258903503  Validation Loss : 93.2778549194336 Val_Reconstruction : 89.79425048828125 Val_KL : 3.4836078882217407\n","Epoch: 3461/8000  Traning Loss: 93.75809955596924  Train_Reconstruction: 90.23775863647461  Train_KL: 3.520341604948044  Validation Loss : 92.92863464355469 Val_Reconstruction : 89.45071411132812 Val_KL : 3.477922797203064\n","Epoch: 3462/8000  Traning Loss: 93.87698745727539  Train_Reconstruction: 90.37198734283447  Train_KL: 3.5049991607666016  Validation Loss : 93.11480331420898 Val_Reconstruction : 89.64680862426758 Val_KL : 3.467995285987854\n","Epoch: 3463/8000  Traning Loss: 93.9097785949707  Train_Reconstruction: 90.39273452758789  Train_KL: 3.517043173313141  Validation Loss : 93.45291137695312 Val_Reconstruction : 89.96471786499023 Val_KL : 3.4881932735443115\n","Epoch: 3464/8000  Traning Loss: 93.6192216873169  Train_Reconstruction: 90.09617328643799  Train_KL: 3.5230488777160645  Validation Loss : 93.1264533996582 Val_Reconstruction : 89.63667297363281 Val_KL : 3.489780902862549\n","Epoch: 3465/8000  Traning Loss: 94.11495018005371  Train_Reconstruction: 90.5994291305542  Train_KL: 3.5155207216739655  Validation Loss : 94.31476593017578 Val_Reconstruction : 90.84332656860352 Val_KL : 3.4714425802230835\n","Epoch: 3466/8000  Traning Loss: 94.4602689743042  Train_Reconstruction: 90.95132064819336  Train_KL: 3.508947193622589  Validation Loss : 94.32207870483398 Val_Reconstruction : 90.86181259155273 Val_KL : 3.460268259048462\n","Epoch: 3467/8000  Traning Loss: 94.29521369934082  Train_Reconstruction: 90.78449058532715  Train_KL: 3.5107227861881256  Validation Loss : 93.08869934082031 Val_Reconstruction : 89.61893081665039 Val_KL : 3.4697704315185547\n","Epoch: 3468/8000  Traning Loss: 93.22437191009521  Train_Reconstruction: 89.71441745758057  Train_KL: 3.5099540650844574  Validation Loss : 92.48258209228516 Val_Reconstruction : 89.0146598815918 Val_KL : 3.4679200649261475\n","Epoch: 3469/8000  Traning Loss: 92.8903694152832  Train_Reconstruction: 89.37773513793945  Train_KL: 3.512633442878723  Validation Loss : 92.66678619384766 Val_Reconstruction : 89.1911392211914 Val_KL : 3.4756462574005127\n","Epoch: 3470/8000  Traning Loss: 93.32733631134033  Train_Reconstruction: 89.81144332885742  Train_KL: 3.515892893075943  Validation Loss : 92.91751480102539 Val_Reconstruction : 89.44000625610352 Val_KL : 3.4775081872940063\n","Epoch: 3471/8000  Traning Loss: 93.20556163787842  Train_Reconstruction: 89.68623638153076  Train_KL: 3.519324868917465  Validation Loss : 92.74517059326172 Val_Reconstruction : 89.27764892578125 Val_KL : 3.4675203561782837\n","Epoch: 3472/8000  Traning Loss: 93.19251823425293  Train_Reconstruction: 89.67226886749268  Train_KL: 3.5202490389347076  Validation Loss : 92.97945404052734 Val_Reconstruction : 89.50090026855469 Val_KL : 3.478556275367737\n","Epoch: 3473/8000  Traning Loss: 93.50363636016846  Train_Reconstruction: 89.99081230163574  Train_KL: 3.512823522090912  Validation Loss : 93.11872863769531 Val_Reconstruction : 89.65084457397461 Val_KL : 3.4678828716278076\n","Epoch: 3474/8000  Traning Loss: 93.95318412780762  Train_Reconstruction: 90.44868659973145  Train_KL: 3.5044966340065002  Validation Loss : 93.2834701538086 Val_Reconstruction : 89.81666564941406 Val_KL : 3.4668058156967163\n","Epoch: 3475/8000  Traning Loss: 93.81977844238281  Train_Reconstruction: 90.3121976852417  Train_KL: 3.507580578327179  Validation Loss : 93.0264663696289 Val_Reconstruction : 89.55683898925781 Val_KL : 3.4696273803710938\n","Epoch: 3476/8000  Traning Loss: 93.73212242126465  Train_Reconstruction: 90.22300720214844  Train_KL: 3.509114235639572  Validation Loss : 92.99519348144531 Val_Reconstruction : 89.5291519165039 Val_KL : 3.4660427570343018\n","Epoch: 3477/8000  Traning Loss: 93.1905927658081  Train_Reconstruction: 89.67827033996582  Train_KL: 3.512322813272476  Validation Loss : 92.67546844482422 Val_Reconstruction : 89.19745635986328 Val_KL : 3.4780129194259644\n","Epoch: 3478/8000  Traning Loss: 93.35099697113037  Train_Reconstruction: 89.8294677734375  Train_KL: 3.521529048681259  Validation Loss : 92.96483612060547 Val_Reconstruction : 89.48530578613281 Val_KL : 3.4795295000076294\n","Epoch: 3479/8000  Traning Loss: 93.47724533081055  Train_Reconstruction: 89.95307731628418  Train_KL: 3.524167150259018  Validation Loss : 92.87727737426758 Val_Reconstruction : 89.39745712280273 Val_KL : 3.479822516441345\n","Epoch: 3480/8000  Traning Loss: 92.9513750076294  Train_Reconstruction: 89.44708633422852  Train_KL: 3.5042887926101685  Validation Loss : 92.53230667114258 Val_Reconstruction : 89.08238983154297 Val_KL : 3.449913740158081\n","Epoch: 3481/8000  Traning Loss: 92.8680534362793  Train_Reconstruction: 89.36712455749512  Train_KL: 3.5009302496910095  Validation Loss : 92.66668701171875 Val_Reconstruction : 89.19548034667969 Val_KL : 3.4712072610855103\n","Epoch: 3482/8000  Traning Loss: 93.01350784301758  Train_Reconstruction: 89.49790668487549  Train_KL: 3.5156011283397675  Validation Loss : 92.79321670532227 Val_Reconstruction : 89.31270217895508 Val_KL : 3.480515241622925\n","Epoch: 3483/8000  Traning Loss: 93.0374984741211  Train_Reconstruction: 89.5086669921875  Train_KL: 3.528831958770752  Validation Loss : 92.94733810424805 Val_Reconstruction : 89.45445251464844 Val_KL : 3.4928828477859497\n","Epoch: 3484/8000  Traning Loss: 93.43894958496094  Train_Reconstruction: 89.92637062072754  Train_KL: 3.512578308582306  Validation Loss : 93.2162094116211 Val_Reconstruction : 89.74889755249023 Val_KL : 3.4673112630844116\n","Epoch: 3485/8000  Traning Loss: 93.35199642181396  Train_Reconstruction: 89.8418436050415  Train_KL: 3.510152578353882  Validation Loss : 93.15256118774414 Val_Reconstruction : 89.67898559570312 Val_KL : 3.4735755920410156\n","Epoch: 3486/8000  Traning Loss: 93.94691562652588  Train_Reconstruction: 90.43330955505371  Train_KL: 3.513605237007141  Validation Loss : 93.33197402954102 Val_Reconstruction : 89.86518096923828 Val_KL : 3.4667919874191284\n","Epoch: 3487/8000  Traning Loss: 93.57486248016357  Train_Reconstruction: 90.0658826828003  Train_KL: 3.508979469537735  Validation Loss : 92.80280303955078 Val_Reconstruction : 89.32941818237305 Val_KL : 3.47338330745697\n","Epoch: 3488/8000  Traning Loss: 93.53049087524414  Train_Reconstruction: 90.01029586791992  Train_KL: 3.5201932191848755  Validation Loss : 93.03803253173828 Val_Reconstruction : 89.55469512939453 Val_KL : 3.48333740234375\n","Epoch: 3489/8000  Traning Loss: 93.53666400909424  Train_Reconstruction: 90.00906658172607  Train_KL: 3.5275976061820984  Validation Loss : 92.87598037719727 Val_Reconstruction : 89.39396667480469 Val_KL : 3.482014298439026\n","Epoch: 3490/8000  Traning Loss: 93.51022052764893  Train_Reconstruction: 89.99327754974365  Train_KL: 3.5169432759284973  Validation Loss : 92.92414474487305 Val_Reconstruction : 89.44775772094727 Val_KL : 3.4763885736465454\n","Epoch: 3491/8000  Traning Loss: 93.17955875396729  Train_Reconstruction: 89.6684513092041  Train_KL: 3.5111086666584015  Validation Loss : 92.79446029663086 Val_Reconstruction : 89.33105087280273 Val_KL : 3.463410496711731\n","Epoch: 3492/8000  Traning Loss: 93.1954984664917  Train_Reconstruction: 89.68382835388184  Train_KL: 3.51167094707489  Validation Loss : 92.8711051940918 Val_Reconstruction : 89.39068603515625 Val_KL : 3.4804192781448364\n","Epoch: 3493/8000  Traning Loss: 93.64933681488037  Train_Reconstruction: 90.12902164459229  Train_KL: 3.520316243171692  Validation Loss : 92.81352996826172 Val_Reconstruction : 89.33726119995117 Val_KL : 3.476266384124756\n","Epoch: 3494/8000  Traning Loss: 93.38109588623047  Train_Reconstruction: 89.86913776397705  Train_KL: 3.51195827126503  Validation Loss : 93.08015441894531 Val_Reconstruction : 89.60640335083008 Val_KL : 3.473749279975891\n","Epoch: 3495/8000  Traning Loss: 93.37915325164795  Train_Reconstruction: 89.87018966674805  Train_KL: 3.50896492600441  Validation Loss : 92.99932861328125 Val_Reconstruction : 89.5273666381836 Val_KL : 3.4719632863998413\n","Epoch: 3496/8000  Traning Loss: 93.24281597137451  Train_Reconstruction: 89.72486209869385  Train_KL: 3.51795357465744  Validation Loss : 92.6506462097168 Val_Reconstruction : 89.16353988647461 Val_KL : 3.4871076345443726\n","Epoch: 3497/8000  Traning Loss: 93.03029823303223  Train_Reconstruction: 89.5129222869873  Train_KL: 3.517375260591507  Validation Loss : 92.40795135498047 Val_Reconstruction : 88.9371109008789 Val_KL : 3.470844030380249\n","Epoch: 3498/8000  Traning Loss: 93.1656665802002  Train_Reconstruction: 89.65549755096436  Train_KL: 3.510170429944992  Validation Loss : 92.98040008544922 Val_Reconstruction : 89.50798797607422 Val_KL : 3.4724135398864746\n","Epoch: 3499/8000  Traning Loss: 93.4666166305542  Train_Reconstruction: 89.95320415496826  Train_KL: 3.513413041830063  Validation Loss : 93.08433532714844 Val_Reconstruction : 89.60193252563477 Val_KL : 3.4824013710021973\n","Epoch: 3500/8000  Traning Loss: 93.40785789489746  Train_Reconstruction: 89.89065456390381  Train_KL: 3.5172035098075867  Validation Loss : 92.93701171875 Val_Reconstruction : 89.46099090576172 Val_KL : 3.4760226011276245\n","Epoch: 3501/8000  Traning Loss: 92.9357500076294  Train_Reconstruction: 89.4210786819458  Train_KL: 3.514670968055725  Validation Loss : 92.34304809570312 Val_Reconstruction : 88.87274932861328 Val_KL : 3.4702982902526855\n","Epoch: 3502/8000  Traning Loss: 92.92935752868652  Train_Reconstruction: 89.41702842712402  Train_KL: 3.512328177690506  Validation Loss : 92.6273307800293 Val_Reconstruction : 89.15112686157227 Val_KL : 3.4762027263641357\n","Epoch: 3503/8000  Traning Loss: 93.65818500518799  Train_Reconstruction: 90.1392822265625  Train_KL: 3.5189031064510345  Validation Loss : 93.36077499389648 Val_Reconstruction : 89.8851203918457 Val_KL : 3.47565758228302\n","Epoch: 3504/8000  Traning Loss: 94.19751262664795  Train_Reconstruction: 90.68303394317627  Train_KL: 3.514478385448456  Validation Loss : 93.55667114257812 Val_Reconstruction : 90.07838439941406 Val_KL : 3.4782849550247192\n","Epoch: 3505/8000  Traning Loss: 93.80462741851807  Train_Reconstruction: 90.29129123687744  Train_KL: 3.513335704803467  Validation Loss : 93.29150009155273 Val_Reconstruction : 89.81502532958984 Val_KL : 3.476475954055786\n","Epoch: 3506/8000  Traning Loss: 93.34779071807861  Train_Reconstruction: 89.83030891418457  Train_KL: 3.5174830555915833  Validation Loss : 93.15380477905273 Val_Reconstruction : 89.6739273071289 Val_KL : 3.4798799753189087\n","Epoch: 3507/8000  Traning Loss: 94.07973575592041  Train_Reconstruction: 90.56232929229736  Train_KL: 3.517407387495041  Validation Loss : 94.6632080078125 Val_Reconstruction : 91.1854248046875 Val_KL : 3.47778058052063\n","Epoch: 3508/8000  Traning Loss: 95.04493713378906  Train_Reconstruction: 91.53677940368652  Train_KL: 3.5081558525562286  Validation Loss : 94.08498001098633 Val_Reconstruction : 90.61595916748047 Val_KL : 3.469021201133728\n","Epoch: 3509/8000  Traning Loss: 94.43310070037842  Train_Reconstruction: 90.92111492156982  Train_KL: 3.511984944343567  Validation Loss : 93.98909378051758 Val_Reconstruction : 90.52057266235352 Val_KL : 3.468521475791931\n","Epoch: 3510/8000  Traning Loss: 93.74696922302246  Train_Reconstruction: 90.24030208587646  Train_KL: 3.5066665410995483  Validation Loss : 93.23571014404297 Val_Reconstruction : 89.77680206298828 Val_KL : 3.458909273147583\n","Epoch: 3511/8000  Traning Loss: 93.65927600860596  Train_Reconstruction: 90.15706539154053  Train_KL: 3.502210110425949  Validation Loss : 93.2728500366211 Val_Reconstruction : 89.80066299438477 Val_KL : 3.472185730934143\n","Epoch: 3512/8000  Traning Loss: 93.46659183502197  Train_Reconstruction: 89.94680976867676  Train_KL: 3.519781082868576  Validation Loss : 93.10688781738281 Val_Reconstruction : 89.62406539916992 Val_KL : 3.4828202724456787\n","Epoch: 3513/8000  Traning Loss: 93.07626819610596  Train_Reconstruction: 89.56275081634521  Train_KL: 3.51351734995842  Validation Loss : 92.66219329833984 Val_Reconstruction : 89.18785095214844 Val_KL : 3.4743441343307495\n","Epoch: 3514/8000  Traning Loss: 92.92791175842285  Train_Reconstruction: 89.41902923583984  Train_KL: 3.5088827311992645  Validation Loss : 92.62842559814453 Val_Reconstruction : 89.16706466674805 Val_KL : 3.461359143257141\n","Epoch: 3515/8000  Traning Loss: 93.09238624572754  Train_Reconstruction: 89.58487510681152  Train_KL: 3.507510632276535  Validation Loss : 92.44135665893555 Val_Reconstruction : 88.96676635742188 Val_KL : 3.4745895862579346\n","Epoch: 3516/8000  Traning Loss: 92.84781169891357  Train_Reconstruction: 89.33546733856201  Train_KL: 3.5123440623283386  Validation Loss : 92.65251541137695 Val_Reconstruction : 89.18046951293945 Val_KL : 3.472046732902527\n","Epoch: 3517/8000  Traning Loss: 92.9075984954834  Train_Reconstruction: 89.39807319641113  Train_KL: 3.5095255076885223  Validation Loss : 92.8084716796875 Val_Reconstruction : 89.33672332763672 Val_KL : 3.4717501401901245\n","Epoch: 3518/8000  Traning Loss: 93.34387493133545  Train_Reconstruction: 89.82440090179443  Train_KL: 3.51947483420372  Validation Loss : 93.18986129760742 Val_Reconstruction : 89.70868682861328 Val_KL : 3.481174945831299\n","Epoch: 3519/8000  Traning Loss: 93.70492172241211  Train_Reconstruction: 90.18366527557373  Train_KL: 3.521255314350128  Validation Loss : 93.45758819580078 Val_Reconstruction : 89.97987365722656 Val_KL : 3.477714419364929\n","Epoch: 3520/8000  Traning Loss: 93.05069065093994  Train_Reconstruction: 89.53410339355469  Train_KL: 3.5165864527225494  Validation Loss : 92.73611831665039 Val_Reconstruction : 89.25603103637695 Val_KL : 3.480087399482727\n","Epoch: 3521/8000  Traning Loss: 93.25205993652344  Train_Reconstruction: 89.74005222320557  Train_KL: 3.512009233236313  Validation Loss : 93.00077056884766 Val_Reconstruction : 89.53664779663086 Val_KL : 3.4641215801239014\n","Epoch: 3522/8000  Traning Loss: 93.49116611480713  Train_Reconstruction: 89.97429084777832  Train_KL: 3.516873985528946  Validation Loss : 93.0914192199707 Val_Reconstruction : 89.61408233642578 Val_KL : 3.477335214614868\n","Epoch: 3523/8000  Traning Loss: 93.42701244354248  Train_Reconstruction: 89.90457153320312  Train_KL: 3.52243971824646  Validation Loss : 93.04509353637695 Val_Reconstruction : 89.5650520324707 Val_KL : 3.4800398349761963\n","Epoch: 3524/8000  Traning Loss: 92.95738220214844  Train_Reconstruction: 89.44361114501953  Train_KL: 3.5137704610824585  Validation Loss : 92.59550857543945 Val_Reconstruction : 89.13150024414062 Val_KL : 3.464009642601013\n","Epoch: 3525/8000  Traning Loss: 92.77651500701904  Train_Reconstruction: 89.2637300491333  Train_KL: 3.512784957885742  Validation Loss : 92.40585708618164 Val_Reconstruction : 88.93239974975586 Val_KL : 3.4734573364257812\n","Epoch: 3526/8000  Traning Loss: 92.82451820373535  Train_Reconstruction: 89.30913352966309  Train_KL: 3.515384942293167  Validation Loss : 92.21521759033203 Val_Reconstruction : 88.74148941040039 Val_KL : 3.4737266302108765\n","Epoch: 3527/8000  Traning Loss: 92.96341133117676  Train_Reconstruction: 89.45030212402344  Train_KL: 3.5131096243858337  Validation Loss : 92.4918212890625 Val_Reconstruction : 89.01803207397461 Val_KL : 3.4737868309020996\n","Epoch: 3528/8000  Traning Loss: 92.91553592681885  Train_Reconstruction: 89.40105628967285  Train_KL: 3.5144797265529633  Validation Loss : 92.3471450805664 Val_Reconstruction : 88.87346649169922 Val_KL : 3.473677158355713\n","Epoch: 3529/8000  Traning Loss: 92.85018825531006  Train_Reconstruction: 89.32756328582764  Train_KL: 3.5226262509822845  Validation Loss : 92.48271560668945 Val_Reconstruction : 89.00729751586914 Val_KL : 3.4754186868667603\n","Epoch: 3530/8000  Traning Loss: 92.993971824646  Train_Reconstruction: 89.47531509399414  Train_KL: 3.5186569690704346  Validation Loss : 92.56162643432617 Val_Reconstruction : 89.09513854980469 Val_KL : 3.46648633480072\n","Epoch: 3531/8000  Traning Loss: 92.9442138671875  Train_Reconstruction: 89.43464756011963  Train_KL: 3.5095676481723785  Validation Loss : 92.55775451660156 Val_Reconstruction : 89.08996200561523 Val_KL : 3.467793345451355\n","Epoch: 3532/8000  Traning Loss: 92.95160293579102  Train_Reconstruction: 89.43193817138672  Train_KL: 3.5196645855903625  Validation Loss : 92.58549499511719 Val_Reconstruction : 89.10458374023438 Val_KL : 3.480910062789917\n","Epoch: 3533/8000  Traning Loss: 93.29363441467285  Train_Reconstruction: 89.77267742156982  Train_KL: 3.5209555625915527  Validation Loss : 93.15277099609375 Val_Reconstruction : 89.69012451171875 Val_KL : 3.462644934654236\n","Epoch: 3534/8000  Traning Loss: 93.54228782653809  Train_Reconstruction: 90.03415107727051  Train_KL: 3.508135974407196  Validation Loss : 92.72422790527344 Val_Reconstruction : 89.25320434570312 Val_KL : 3.4710235595703125\n","Epoch: 3535/8000  Traning Loss: 93.84934711456299  Train_Reconstruction: 90.33009147644043  Train_KL: 3.5192560851573944  Validation Loss : 94.03730773925781 Val_Reconstruction : 90.55976486206055 Val_KL : 3.4775439500808716\n","Epoch: 3536/8000  Traning Loss: 94.19663047790527  Train_Reconstruction: 90.68510818481445  Train_KL: 3.5115216970443726  Validation Loss : 93.72870635986328 Val_Reconstruction : 90.26712417602539 Val_KL : 3.4615817070007324\n","Epoch: 3537/8000  Traning Loss: 93.72971630096436  Train_Reconstruction: 90.21922302246094  Train_KL: 3.5104929208755493  Validation Loss : 93.03292083740234 Val_Reconstruction : 89.55689239501953 Val_KL : 3.476027488708496\n","Epoch: 3538/8000  Traning Loss: 93.32755661010742  Train_Reconstruction: 89.81068515777588  Train_KL: 3.5168721079826355  Validation Loss : 93.03318786621094 Val_Reconstruction : 89.55819702148438 Val_KL : 3.4749923944473267\n","Epoch: 3539/8000  Traning Loss: 92.8775463104248  Train_Reconstruction: 89.35265827178955  Train_KL: 3.5248879492282867  Validation Loss : 92.61907196044922 Val_Reconstruction : 89.13145065307617 Val_KL : 3.487622857093811\n","Epoch: 3540/8000  Traning Loss: 93.03236293792725  Train_Reconstruction: 89.50888919830322  Train_KL: 3.523472845554352  Validation Loss : 92.63001251220703 Val_Reconstruction : 89.15777206420898 Val_KL : 3.4722381830215454\n","Epoch: 3541/8000  Traning Loss: 93.01053524017334  Train_Reconstruction: 89.49987602233887  Train_KL: 3.5106584429740906  Validation Loss : 92.46408081054688 Val_Reconstruction : 88.99164199829102 Val_KL : 3.4724414348602295\n","Epoch: 3542/8000  Traning Loss: 92.7930555343628  Train_Reconstruction: 89.26947975158691  Train_KL: 3.523575931787491  Validation Loss : 92.13123321533203 Val_Reconstruction : 88.6495246887207 Val_KL : 3.4817111492156982\n","Epoch: 3543/8000  Traning Loss: 92.86135578155518  Train_Reconstruction: 89.34174346923828  Train_KL: 3.519612818956375  Validation Loss : 92.48371887207031 Val_Reconstruction : 88.99876403808594 Val_KL : 3.4849536418914795\n","Epoch: 3544/8000  Traning Loss: 92.84974384307861  Train_Reconstruction: 89.33214664459229  Train_KL: 3.5175982117652893  Validation Loss : 92.36822128295898 Val_Reconstruction : 88.89097213745117 Val_KL : 3.477250337600708\n","Epoch: 3545/8000  Traning Loss: 93.04367446899414  Train_Reconstruction: 89.53053569793701  Train_KL: 3.5131402909755707  Validation Loss : 92.95755767822266 Val_Reconstruction : 89.48394012451172 Val_KL : 3.4736180305480957\n","Epoch: 3546/8000  Traning Loss: 93.15173053741455  Train_Reconstruction: 89.6334753036499  Train_KL: 3.5182546973228455  Validation Loss : 93.1715087890625 Val_Reconstruction : 89.6881103515625 Val_KL : 3.483399748802185\n","Epoch: 3547/8000  Traning Loss: 93.22190856933594  Train_Reconstruction: 89.7030439376831  Train_KL: 3.51886385679245  Validation Loss : 92.7056770324707 Val_Reconstruction : 89.23065185546875 Val_KL : 3.475026488304138\n","Epoch: 3548/8000  Traning Loss: 93.26662349700928  Train_Reconstruction: 89.75781631469727  Train_KL: 3.5088073313236237  Validation Loss : 93.44652938842773 Val_Reconstruction : 89.982177734375 Val_KL : 3.4643497467041016\n","Epoch: 3549/8000  Traning Loss: 93.8852128982544  Train_Reconstruction: 90.38058948516846  Train_KL: 3.5046237111091614  Validation Loss : 93.42594909667969 Val_Reconstruction : 89.9659652709961 Val_KL : 3.459982395172119\n","Epoch: 3550/8000  Traning Loss: 93.78248691558838  Train_Reconstruction: 90.27849674224854  Train_KL: 3.50398987531662  Validation Loss : 93.32533645629883 Val_Reconstruction : 89.85965347290039 Val_KL : 3.4656842947006226\n","Epoch: 3551/8000  Traning Loss: 93.49380683898926  Train_Reconstruction: 89.98047351837158  Train_KL: 3.5133330523967743  Validation Loss : 92.95999908447266 Val_Reconstruction : 89.48437118530273 Val_KL : 3.4756295680999756\n","Epoch: 3552/8000  Traning Loss: 93.03256225585938  Train_Reconstruction: 89.51566123962402  Train_KL: 3.516902208328247  Validation Loss : 92.8583755493164 Val_Reconstruction : 89.3760986328125 Val_KL : 3.4822793006896973\n","Epoch: 3553/8000  Traning Loss: 93.15098762512207  Train_Reconstruction: 89.62928581237793  Train_KL: 3.5217022597789764  Validation Loss : 92.8066291809082 Val_Reconstruction : 89.32732391357422 Val_KL : 3.4793031215667725\n","Epoch: 3554/8000  Traning Loss: 93.18804359436035  Train_Reconstruction: 89.66691970825195  Train_KL: 3.521124452352524  Validation Loss : 92.91341781616211 Val_Reconstruction : 89.43768310546875 Val_KL : 3.4757357835769653\n","Epoch: 3555/8000  Traning Loss: 93.23518180847168  Train_Reconstruction: 89.72334003448486  Train_KL: 3.5118421614170074  Validation Loss : 92.89777755737305 Val_Reconstruction : 89.43325805664062 Val_KL : 3.4645177125930786\n","Epoch: 3556/8000  Traning Loss: 93.01791667938232  Train_Reconstruction: 89.50318050384521  Train_KL: 3.5147352814674377  Validation Loss : 92.52837371826172 Val_Reconstruction : 89.05047225952148 Val_KL : 3.4779016971588135\n","Epoch: 3557/8000  Traning Loss: 92.81044292449951  Train_Reconstruction: 89.28578281402588  Train_KL: 3.5246587693691254  Validation Loss : 92.2207145690918 Val_Reconstruction : 88.7277603149414 Val_KL : 3.492954730987549\n","Epoch: 3558/8000  Traning Loss: 92.91498947143555  Train_Reconstruction: 89.39414691925049  Train_KL: 3.5208434760570526  Validation Loss : 92.6819839477539 Val_Reconstruction : 89.19995880126953 Val_KL : 3.482021689414978\n","Epoch: 3559/8000  Traning Loss: 93.0116024017334  Train_Reconstruction: 89.49429893493652  Train_KL: 3.517303854227066  Validation Loss : 92.64645004272461 Val_Reconstruction : 89.17395782470703 Val_KL : 3.472493052482605\n","Epoch: 3560/8000  Traning Loss: 92.72784996032715  Train_Reconstruction: 89.21081829071045  Train_KL: 3.517031192779541  Validation Loss : 92.43546676635742 Val_Reconstruction : 88.96211624145508 Val_KL : 3.473350167274475\n","Epoch: 3561/8000  Traning Loss: 92.77826976776123  Train_Reconstruction: 89.26619815826416  Train_KL: 3.512070953845978  Validation Loss : 92.41740798950195 Val_Reconstruction : 88.95532989501953 Val_KL : 3.4620805978775024\n","Epoch: 3562/8000  Traning Loss: 93.28483772277832  Train_Reconstruction: 89.78284740447998  Train_KL: 3.5019913017749786  Validation Loss : 93.12214279174805 Val_Reconstruction : 89.65852737426758 Val_KL : 3.4636142253875732\n","Epoch: 3563/8000  Traning Loss: 93.94431018829346  Train_Reconstruction: 90.43706607818604  Train_KL: 3.50724458694458  Validation Loss : 93.67151641845703 Val_Reconstruction : 90.19888687133789 Val_KL : 3.4726299047470093\n","Epoch: 3564/8000  Traning Loss: 93.85217189788818  Train_Reconstruction: 90.32762432098389  Train_KL: 3.5245483815670013  Validation Loss : 93.72855758666992 Val_Reconstruction : 90.23744583129883 Val_KL : 3.491113543510437\n","Epoch: 3565/8000  Traning Loss: 93.72622680664062  Train_Reconstruction: 90.20146560668945  Train_KL: 3.5247605741024017  Validation Loss : 92.72158813476562 Val_Reconstruction : 89.24223327636719 Val_KL : 3.4793542623519897\n","Epoch: 3566/8000  Traning Loss: 93.13302612304688  Train_Reconstruction: 89.61530017852783  Train_KL: 3.5177256762981415  Validation Loss : 92.89351654052734 Val_Reconstruction : 89.42755508422852 Val_KL : 3.465964436531067\n","Epoch: 3567/8000  Traning Loss: 93.03376579284668  Train_Reconstruction: 89.51028823852539  Train_KL: 3.5234784185886383  Validation Loss : 92.36909103393555 Val_Reconstruction : 88.87857818603516 Val_KL : 3.4905132055282593\n","Epoch: 3568/8000  Traning Loss: 93.1907730102539  Train_Reconstruction: 89.65876293182373  Train_KL: 3.5320107340812683  Validation Loss : 92.72338104248047 Val_Reconstruction : 89.23433685302734 Val_KL : 3.4890425205230713\n","Epoch: 3569/8000  Traning Loss: 93.22743511199951  Train_Reconstruction: 89.70854759216309  Train_KL: 3.5188882648944855  Validation Loss : 92.85603332519531 Val_Reconstruction : 89.38215637207031 Val_KL : 3.473875880241394\n","Epoch: 3570/8000  Traning Loss: 93.17183208465576  Train_Reconstruction: 89.66004371643066  Train_KL: 3.5117891430854797  Validation Loss : 92.5593147277832 Val_Reconstruction : 89.08557510375977 Val_KL : 3.4737383127212524\n","Epoch: 3571/8000  Traning Loss: 92.78119659423828  Train_Reconstruction: 89.27276229858398  Train_KL: 3.508433073759079  Validation Loss : 92.61205291748047 Val_Reconstruction : 89.13505554199219 Val_KL : 3.4769943952560425\n","Epoch: 3572/8000  Traning Loss: 93.23882484436035  Train_Reconstruction: 89.71605682373047  Train_KL: 3.522768199443817  Validation Loss : 93.5204963684082 Val_Reconstruction : 90.02753448486328 Val_KL : 3.4929637908935547\n","Epoch: 3573/8000  Traning Loss: 93.24811172485352  Train_Reconstruction: 89.72704982757568  Train_KL: 3.5210622549057007  Validation Loss : 92.75296401977539 Val_Reconstruction : 89.28153991699219 Val_KL : 3.4714266061782837\n","Epoch: 3574/8000  Traning Loss: 93.04393196105957  Train_Reconstruction: 89.54160594940186  Train_KL: 3.502326160669327  Validation Loss : 92.73836898803711 Val_Reconstruction : 89.27007675170898 Val_KL : 3.468295693397522\n","Epoch: 3575/8000  Traning Loss: 93.24047470092773  Train_Reconstruction: 89.73138618469238  Train_KL: 3.5090882778167725  Validation Loss : 93.07263946533203 Val_Reconstruction : 89.59976959228516 Val_KL : 3.4728699922561646\n","Epoch: 3576/8000  Traning Loss: 93.1573257446289  Train_Reconstruction: 89.64509868621826  Train_KL: 3.512227863073349  Validation Loss : 92.547607421875 Val_Reconstruction : 89.07577514648438 Val_KL : 3.471831798553467\n","Epoch: 3577/8000  Traning Loss: 92.94167900085449  Train_Reconstruction: 89.42364883422852  Train_KL: 3.518029600381851  Validation Loss : 92.57174682617188 Val_Reconstruction : 89.09409713745117 Val_KL : 3.4776490926742554\n","Epoch: 3578/8000  Traning Loss: 93.44286251068115  Train_Reconstruction: 89.92643070220947  Train_KL: 3.5164295732975006  Validation Loss : 93.65474700927734 Val_Reconstruction : 90.18090057373047 Val_KL : 3.473847508430481\n","Epoch: 3579/8000  Traning Loss: 93.71983051300049  Train_Reconstruction: 90.2028980255127  Train_KL: 3.5169323682785034  Validation Loss : 93.42343521118164 Val_Reconstruction : 89.94366836547852 Val_KL : 3.4797667264938354\n","Epoch: 3580/8000  Traning Loss: 93.79002571105957  Train_Reconstruction: 90.27638816833496  Train_KL: 3.5136370956897736  Validation Loss : 93.12662506103516 Val_Reconstruction : 89.65633773803711 Val_KL : 3.470286011695862\n","Epoch: 3581/8000  Traning Loss: 93.66603469848633  Train_Reconstruction: 90.15050983428955  Train_KL: 3.515524744987488  Validation Loss : 93.42502212524414 Val_Reconstruction : 89.94775772094727 Val_KL : 3.4772640466690063\n","Epoch: 3582/8000  Traning Loss: 93.4893045425415  Train_Reconstruction: 89.97207450866699  Train_KL: 3.5172300040721893  Validation Loss : 92.49946594238281 Val_Reconstruction : 89.02431106567383 Val_KL : 3.47515332698822\n","Epoch: 3583/8000  Traning Loss: 92.97151374816895  Train_Reconstruction: 89.46028423309326  Train_KL: 3.511228919029236  Validation Loss : 92.87847900390625 Val_Reconstruction : 89.40618133544922 Val_KL : 3.4722957611083984\n","Epoch: 3584/8000  Traning Loss: 93.16362476348877  Train_Reconstruction: 89.65110778808594  Train_KL: 3.5125180780887604  Validation Loss : 93.2729263305664 Val_Reconstruction : 89.79745864868164 Val_KL : 3.4754669666290283\n","Epoch: 3585/8000  Traning Loss: 93.3513126373291  Train_Reconstruction: 89.8402910232544  Train_KL: 3.511021763086319  Validation Loss : 92.67882537841797 Val_Reconstruction : 89.20793914794922 Val_KL : 3.4708868265151978\n","Epoch: 3586/8000  Traning Loss: 92.9844970703125  Train_Reconstruction: 89.47303009033203  Train_KL: 3.5114667117595673  Validation Loss : 92.62694549560547 Val_Reconstruction : 89.15469360351562 Val_KL : 3.4722522497177124\n","Epoch: 3587/8000  Traning Loss: 93.09140682220459  Train_Reconstruction: 89.57845401763916  Train_KL: 3.512951284646988  Validation Loss : 93.31851196289062 Val_Reconstruction : 89.84625244140625 Val_KL : 3.472259521484375\n","Epoch: 3588/8000  Traning Loss: 93.07498741149902  Train_Reconstruction: 89.55939960479736  Train_KL: 3.5155879259109497  Validation Loss : 92.9367561340332 Val_Reconstruction : 89.46175765991211 Val_KL : 3.475000500679016\n","Epoch: 3589/8000  Traning Loss: 93.01995944976807  Train_Reconstruction: 89.50451278686523  Train_KL: 3.5154463052749634  Validation Loss : 92.73098754882812 Val_Reconstruction : 89.25027847290039 Val_KL : 3.480708360671997\n","Epoch: 3590/8000  Traning Loss: 92.95207118988037  Train_Reconstruction: 89.43493175506592  Train_KL: 3.517139256000519  Validation Loss : 92.4798812866211 Val_Reconstruction : 89.0075569152832 Val_KL : 3.472324252128601\n","Epoch: 3591/8000  Traning Loss: 93.0683822631836  Train_Reconstruction: 89.5500135421753  Train_KL: 3.518369048833847  Validation Loss : 92.65755081176758 Val_Reconstruction : 89.17760848999023 Val_KL : 3.4799450635910034\n","Epoch: 3592/8000  Traning Loss: 93.3831901550293  Train_Reconstruction: 89.87093925476074  Train_KL: 3.5122510194778442  Validation Loss : 92.80635452270508 Val_Reconstruction : 89.33579635620117 Val_KL : 3.470559597015381\n","Epoch: 3593/8000  Traning Loss: 93.0439453125  Train_Reconstruction: 89.52719402313232  Train_KL: 3.5167511999607086  Validation Loss : 92.29900741577148 Val_Reconstruction : 88.80874252319336 Val_KL : 3.490264654159546\n","Epoch: 3594/8000  Traning Loss: 92.80680751800537  Train_Reconstruction: 89.28134441375732  Train_KL: 3.5254638493061066  Validation Loss : 92.50241470336914 Val_Reconstruction : 89.01995468139648 Val_KL : 3.482457160949707\n","Epoch: 3595/8000  Traning Loss: 92.87807846069336  Train_Reconstruction: 89.36922359466553  Train_KL: 3.508853554725647  Validation Loss : 92.45880889892578 Val_Reconstruction : 89.00830459594727 Val_KL : 3.450505495071411\n","Epoch: 3596/8000  Traning Loss: 92.77159690856934  Train_Reconstruction: 89.27636051177979  Train_KL: 3.4952372908592224  Validation Loss : 92.5783805847168 Val_Reconstruction : 89.12001037597656 Val_KL : 3.458369016647339\n","Epoch: 3597/8000  Traning Loss: 92.92923736572266  Train_Reconstruction: 89.41258430480957  Train_KL: 3.5166531205177307  Validation Loss : 92.33145904541016 Val_Reconstruction : 88.85061264038086 Val_KL : 3.480846405029297\n","Epoch: 3598/8000  Traning Loss: 92.83137893676758  Train_Reconstruction: 89.30925846099854  Train_KL: 3.5221208930015564  Validation Loss : 92.5792236328125 Val_Reconstruction : 89.10077667236328 Val_KL : 3.478447437286377\n","Epoch: 3599/8000  Traning Loss: 92.98286724090576  Train_Reconstruction: 89.47110748291016  Train_KL: 3.511759430170059  Validation Loss : 92.90899658203125 Val_Reconstruction : 89.44069290161133 Val_KL : 3.4683037996292114\n","Epoch: 3600/8000  Traning Loss: 93.17413520812988  Train_Reconstruction: 89.65529441833496  Train_KL: 3.5188402831554413  Validation Loss : 92.58693313598633 Val_Reconstruction : 89.10651779174805 Val_KL : 3.4804139137268066\n","Epoch: 3601/8000  Traning Loss: 92.96016216278076  Train_Reconstruction: 89.44207096099854  Train_KL: 3.5180917382240295  Validation Loss : 92.55722427368164 Val_Reconstruction : 89.08218002319336 Val_KL : 3.4750475883483887\n","Epoch: 3602/8000  Traning Loss: 93.30265235900879  Train_Reconstruction: 89.79107475280762  Train_KL: 3.5115779042243958  Validation Loss : 93.13679504394531 Val_Reconstruction : 89.66617965698242 Val_KL : 3.4706151485443115\n","Epoch: 3603/8000  Traning Loss: 93.22953605651855  Train_Reconstruction: 89.7161979675293  Train_KL: 3.513339251279831  Validation Loss : 92.89295959472656 Val_Reconstruction : 89.42850875854492 Val_KL : 3.4644479751586914\n","Epoch: 3604/8000  Traning Loss: 92.93637466430664  Train_Reconstruction: 89.4234790802002  Train_KL: 3.5128964483737946  Validation Loss : 92.4012222290039 Val_Reconstruction : 88.92586898803711 Val_KL : 3.4753525257110596\n","Epoch: 3605/8000  Traning Loss: 92.76904106140137  Train_Reconstruction: 89.24084949493408  Train_KL: 3.5281920731067657  Validation Loss : 92.66034698486328 Val_Reconstruction : 89.16872024536133 Val_KL : 3.491626739501953\n","Epoch: 3606/8000  Traning Loss: 93.0313777923584  Train_Reconstruction: 89.51071357727051  Train_KL: 3.520665943622589  Validation Loss : 92.80392074584961 Val_Reconstruction : 89.32857894897461 Val_KL : 3.475342273712158\n","Epoch: 3607/8000  Traning Loss: 92.9417142868042  Train_Reconstruction: 89.42654800415039  Train_KL: 3.5151663422584534  Validation Loss : 92.44775772094727 Val_Reconstruction : 88.97005081176758 Val_KL : 3.477706551551819\n","Epoch: 3608/8000  Traning Loss: 93.09189987182617  Train_Reconstruction: 89.5747184753418  Train_KL: 3.517181634902954  Validation Loss : 93.07598114013672 Val_Reconstruction : 89.60602569580078 Val_KL : 3.469955325126648\n","Epoch: 3609/8000  Traning Loss: 93.86787986755371  Train_Reconstruction: 90.35495376586914  Train_KL: 3.512925863265991  Validation Loss : 93.33016967773438 Val_Reconstruction : 89.8552017211914 Val_KL : 3.4749653339385986\n","Epoch: 3610/8000  Traning Loss: 93.38770580291748  Train_Reconstruction: 89.87056159973145  Train_KL: 3.5171453058719635  Validation Loss : 92.4852294921875 Val_Reconstruction : 89.0025634765625 Val_KL : 3.482666254043579\n","Epoch: 3611/8000  Traning Loss: 92.77001667022705  Train_Reconstruction: 89.2493257522583  Train_KL: 3.5206898152828217  Validation Loss : 92.18704605102539 Val_Reconstruction : 88.70907974243164 Val_KL : 3.477966785430908\n","Epoch: 3612/8000  Traning Loss: 92.63198852539062  Train_Reconstruction: 89.12167739868164  Train_KL: 3.510312020778656  Validation Loss : 92.47768020629883 Val_Reconstruction : 89.00804901123047 Val_KL : 3.4696319103240967\n","Epoch: 3613/8000  Traning Loss: 92.87600421905518  Train_Reconstruction: 89.36204242706299  Train_KL: 3.5139616429805756  Validation Loss : 92.76655578613281 Val_Reconstruction : 89.28694152832031 Val_KL : 3.479614734649658\n","Epoch: 3614/8000  Traning Loss: 93.0799388885498  Train_Reconstruction: 89.55669498443604  Train_KL: 3.5232438445091248  Validation Loss : 92.76174926757812 Val_Reconstruction : 89.2740249633789 Val_KL : 3.4877235889434814\n","Epoch: 3615/8000  Traning Loss: 93.01316738128662  Train_Reconstruction: 89.48244380950928  Train_KL: 3.5307240784168243  Validation Loss : 93.05426406860352 Val_Reconstruction : 89.56953048706055 Val_KL : 3.4847322702407837\n","Epoch: 3616/8000  Traning Loss: 93.01441669464111  Train_Reconstruction: 89.4996690750122  Train_KL: 3.5147477090358734  Validation Loss : 92.61803817749023 Val_Reconstruction : 89.1497917175293 Val_KL : 3.4682462215423584\n","Epoch: 3617/8000  Traning Loss: 92.98714637756348  Train_Reconstruction: 89.47677516937256  Train_KL: 3.5103717744350433  Validation Loss : 92.6821060180664 Val_Reconstruction : 89.21335220336914 Val_KL : 3.468752861022949\n","Epoch: 3618/8000  Traning Loss: 93.28743934631348  Train_Reconstruction: 89.77228927612305  Train_KL: 3.5151497423648834  Validation Loss : 92.7805404663086 Val_Reconstruction : 89.2942886352539 Val_KL : 3.486254096031189\n","Epoch: 3619/8000  Traning Loss: 92.82864189147949  Train_Reconstruction: 89.3038501739502  Train_KL: 3.524792641401291  Validation Loss : 92.29005813598633 Val_Reconstruction : 88.81113815307617 Val_KL : 3.4789183139801025\n","Epoch: 3620/8000  Traning Loss: 92.75268173217773  Train_Reconstruction: 89.23793601989746  Train_KL: 3.514744609594345  Validation Loss : 92.4342269897461 Val_Reconstruction : 88.96996307373047 Val_KL : 3.464262366294861\n","Epoch: 3621/8000  Traning Loss: 92.89029884338379  Train_Reconstruction: 89.38067054748535  Train_KL: 3.5096297562122345  Validation Loss : 92.39116287231445 Val_Reconstruction : 88.91424942016602 Val_KL : 3.476914644241333\n","Epoch: 3622/8000  Traning Loss: 93.03603076934814  Train_Reconstruction: 89.51691436767578  Train_KL: 3.51911661028862  Validation Loss : 92.68289947509766 Val_Reconstruction : 89.19799041748047 Val_KL : 3.4849058389663696\n","Epoch: 3623/8000  Traning Loss: 92.91195487976074  Train_Reconstruction: 89.39127731323242  Train_KL: 3.5206768810749054  Validation Loss : 92.54663467407227 Val_Reconstruction : 89.07011413574219 Val_KL : 3.4765223264694214\n","Epoch: 3624/8000  Traning Loss: 92.94332885742188  Train_Reconstruction: 89.42715072631836  Train_KL: 3.516178250312805  Validation Loss : 92.77333068847656 Val_Reconstruction : 89.29722213745117 Val_KL : 3.4761104583740234\n","Epoch: 3625/8000  Traning Loss: 93.01292133331299  Train_Reconstruction: 89.50882339477539  Train_KL: 3.5040983259677887  Validation Loss : 92.82374954223633 Val_Reconstruction : 89.36342239379883 Val_KL : 3.4603277444839478\n","Epoch: 3626/8000  Traning Loss: 92.92607307434082  Train_Reconstruction: 89.4274263381958  Train_KL: 3.4986475706100464  Validation Loss : 92.58284759521484 Val_Reconstruction : 89.1229248046875 Val_KL : 3.4599236249923706\n","Epoch: 3627/8000  Traning Loss: 93.06127071380615  Train_Reconstruction: 89.54973411560059  Train_KL: 3.511537045240402  Validation Loss : 92.79182052612305 Val_Reconstruction : 89.31631088256836 Val_KL : 3.475507616996765\n","Epoch: 3628/8000  Traning Loss: 93.38721656799316  Train_Reconstruction: 89.87509536743164  Train_KL: 3.5121211111545563  Validation Loss : 93.26241683959961 Val_Reconstruction : 89.78512191772461 Val_KL : 3.4772924184799194\n","Epoch: 3629/8000  Traning Loss: 93.10649013519287  Train_Reconstruction: 89.60567855834961  Train_KL: 3.5008120834827423  Validation Loss : 92.74089813232422 Val_Reconstruction : 89.27967071533203 Val_KL : 3.461228609085083\n","Epoch: 3630/8000  Traning Loss: 92.92830467224121  Train_Reconstruction: 89.4162015914917  Train_KL: 3.5121035873889923  Validation Loss : 92.24975204467773 Val_Reconstruction : 88.76798248291016 Val_KL : 3.481770873069763\n","Epoch: 3631/8000  Traning Loss: 92.9412202835083  Train_Reconstruction: 89.4220199584961  Train_KL: 3.5191995203495026  Validation Loss : 92.31949996948242 Val_Reconstruction : 88.83335494995117 Val_KL : 3.4861464500427246\n","Epoch: 3632/8000  Traning Loss: 92.94594860076904  Train_Reconstruction: 89.43308353424072  Train_KL: 3.5128650665283203  Validation Loss : 93.01482772827148 Val_Reconstruction : 89.54672622680664 Val_KL : 3.4681012630462646\n","Epoch: 3633/8000  Traning Loss: 93.54946231842041  Train_Reconstruction: 90.0355863571167  Train_KL: 3.5138759911060333  Validation Loss : 92.99287796020508 Val_Reconstruction : 89.51756286621094 Val_KL : 3.475313425064087\n","Epoch: 3634/8000  Traning Loss: 93.73096084594727  Train_Reconstruction: 90.22664165496826  Train_KL: 3.504319489002228  Validation Loss : 93.70965194702148 Val_Reconstruction : 90.23770523071289 Val_KL : 3.4719468355178833\n","Epoch: 3635/8000  Traning Loss: 93.29177761077881  Train_Reconstruction: 89.77892971038818  Train_KL: 3.5128470361232758  Validation Loss : 92.6590347290039 Val_Reconstruction : 89.18133544921875 Val_KL : 3.4776978492736816\n","Epoch: 3636/8000  Traning Loss: 92.95716285705566  Train_Reconstruction: 89.43815231323242  Train_KL: 3.5190094709396362  Validation Loss : 92.68083953857422 Val_Reconstruction : 89.2109260559082 Val_KL : 3.469912528991699\n","Epoch: 3637/8000  Traning Loss: 92.89356422424316  Train_Reconstruction: 89.38178253173828  Train_KL: 3.511781245470047  Validation Loss : 92.49147415161133 Val_Reconstruction : 89.01927185058594 Val_KL : 3.4722015857696533\n","Epoch: 3638/8000  Traning Loss: 92.7959451675415  Train_Reconstruction: 89.28777313232422  Train_KL: 3.5081715881824493  Validation Loss : 92.75100326538086 Val_Reconstruction : 89.27632522583008 Val_KL : 3.4746789932250977\n","Epoch: 3639/8000  Traning Loss: 92.99032974243164  Train_Reconstruction: 89.46631622314453  Train_KL: 3.524013012647629  Validation Loss : 93.08358001708984 Val_Reconstruction : 89.5941276550293 Val_KL : 3.489451766014099\n","Epoch: 3640/8000  Traning Loss: 93.36452770233154  Train_Reconstruction: 89.84864711761475  Train_KL: 3.51588037610054  Validation Loss : 92.79496383666992 Val_Reconstruction : 89.33259963989258 Val_KL : 3.462365984916687\n","Epoch: 3641/8000  Traning Loss: 93.26224040985107  Train_Reconstruction: 89.75841808319092  Train_KL: 3.5038227140903473  Validation Loss : 92.94357681274414 Val_Reconstruction : 89.47661972045898 Val_KL : 3.4669554233551025\n","Epoch: 3642/8000  Traning Loss: 92.95963287353516  Train_Reconstruction: 89.43674659729004  Train_KL: 3.5228860080242157  Validation Loss : 92.5097541809082 Val_Reconstruction : 89.02075576782227 Val_KL : 3.4889981746673584\n","Epoch: 3643/8000  Traning Loss: 92.52831268310547  Train_Reconstruction: 89.0090742111206  Train_KL: 3.519238919019699  Validation Loss : 92.17500305175781 Val_Reconstruction : 88.6967658996582 Val_KL : 3.478238344192505\n","Epoch: 3644/8000  Traning Loss: 92.65133857727051  Train_Reconstruction: 89.13582611083984  Train_KL: 3.515513598918915  Validation Loss : 92.36930847167969 Val_Reconstruction : 88.8916244506836 Val_KL : 3.4776819944381714\n","Epoch: 3645/8000  Traning Loss: 92.88807582855225  Train_Reconstruction: 89.3720703125  Train_KL: 3.5160053074359894  Validation Loss : 92.86548233032227 Val_Reconstruction : 89.38443756103516 Val_KL : 3.4810447692871094\n","Epoch: 3646/8000  Traning Loss: 93.33827686309814  Train_Reconstruction: 89.82155799865723  Train_KL: 3.5167186558246613  Validation Loss : 93.51067733764648 Val_Reconstruction : 90.03705978393555 Val_KL : 3.473617911338806\n","Epoch: 3647/8000  Traning Loss: 93.28119087219238  Train_Reconstruction: 89.76729488372803  Train_KL: 3.5138954520225525  Validation Loss : 92.73540878295898 Val_Reconstruction : 89.26151657104492 Val_KL : 3.4738940000534058\n","Epoch: 3648/8000  Traning Loss: 93.26228141784668  Train_Reconstruction: 89.74411678314209  Train_KL: 3.5181648433208466  Validation Loss : 93.00287246704102 Val_Reconstruction : 89.52710342407227 Val_KL : 3.475770592689514\n","Epoch: 3649/8000  Traning Loss: 92.89165878295898  Train_Reconstruction: 89.3791618347168  Train_KL: 3.512497514486313  Validation Loss : 92.5645980834961 Val_Reconstruction : 89.08681106567383 Val_KL : 3.4777883291244507\n","Epoch: 3650/8000  Traning Loss: 92.95011806488037  Train_Reconstruction: 89.43414497375488  Train_KL: 3.515971690416336  Validation Loss : 92.99937057495117 Val_Reconstruction : 89.51465606689453 Val_KL : 3.4847142696380615\n","Epoch: 3651/8000  Traning Loss: 93.26435947418213  Train_Reconstruction: 89.74588298797607  Train_KL: 3.518477499485016  Validation Loss : 93.11904907226562 Val_Reconstruction : 89.6442642211914 Val_KL : 3.4747854471206665\n","Epoch: 3652/8000  Traning Loss: 93.27039241790771  Train_Reconstruction: 89.7617073059082  Train_KL: 3.508683532476425  Validation Loss : 92.54875183105469 Val_Reconstruction : 89.0731430053711 Val_KL : 3.4756115674972534\n","Epoch: 3653/8000  Traning Loss: 93.07118034362793  Train_Reconstruction: 89.5590648651123  Train_KL: 3.5121161341667175  Validation Loss : 93.02909088134766 Val_Reconstruction : 89.55731582641602 Val_KL : 3.4717745780944824\n","Epoch: 3654/8000  Traning Loss: 93.3777265548706  Train_Reconstruction: 89.8568000793457  Train_KL: 3.5209275782108307  Validation Loss : 93.04740524291992 Val_Reconstruction : 89.55692672729492 Val_KL : 3.490480065345764\n","Epoch: 3655/8000  Traning Loss: 93.30655860900879  Train_Reconstruction: 89.78607749938965  Train_KL: 3.520480841398239  Validation Loss : 92.71850204467773 Val_Reconstruction : 89.24231338500977 Val_KL : 3.476190447807312\n","Epoch: 3656/8000  Traning Loss: 92.99929904937744  Train_Reconstruction: 89.49948120117188  Train_KL: 3.4998170137405396  Validation Loss : 92.66223907470703 Val_Reconstruction : 89.20963287353516 Val_KL : 3.4526054859161377\n","Epoch: 3657/8000  Traning Loss: 93.43741130828857  Train_Reconstruction: 89.92974853515625  Train_KL: 3.507663279771805  Validation Loss : 93.33871078491211 Val_Reconstruction : 89.85565567016602 Val_KL : 3.4830546379089355\n","Epoch: 3658/8000  Traning Loss: 93.80381774902344  Train_Reconstruction: 90.27365112304688  Train_KL: 3.53016597032547  Validation Loss : 93.74471282958984 Val_Reconstruction : 90.2533073425293 Val_KL : 3.491405487060547\n","Epoch: 3659/8000  Traning Loss: 93.5334358215332  Train_Reconstruction: 90.01369380950928  Train_KL: 3.5197409987449646  Validation Loss : 92.73487091064453 Val_Reconstruction : 89.2583999633789 Val_KL : 3.4764719009399414\n","Epoch: 3660/8000  Traning Loss: 93.46773529052734  Train_Reconstruction: 89.94612503051758  Train_KL: 3.521612048149109  Validation Loss : 93.32255172729492 Val_Reconstruction : 89.83459854125977 Val_KL : 3.4879517555236816\n","Epoch: 3661/8000  Traning Loss: 93.08878517150879  Train_Reconstruction: 89.56127166748047  Train_KL: 3.527513086795807  Validation Loss : 92.34889602661133 Val_Reconstruction : 88.86396026611328 Val_KL : 3.4849361181259155\n","Epoch: 3662/8000  Traning Loss: 92.76697444915771  Train_Reconstruction: 89.25217151641846  Train_KL: 3.514803886413574  Validation Loss : 92.54417037963867 Val_Reconstruction : 89.07002639770508 Val_KL : 3.474146246910095\n","Epoch: 3663/8000  Traning Loss: 92.64693450927734  Train_Reconstruction: 89.13942527770996  Train_KL: 3.507510393857956  Validation Loss : 92.22286605834961 Val_Reconstruction : 88.75233840942383 Val_KL : 3.4705296754837036\n","Epoch: 3664/8000  Traning Loss: 92.65070056915283  Train_Reconstruction: 89.13590240478516  Train_KL: 3.5147983729839325  Validation Loss : 92.26364517211914 Val_Reconstruction : 88.77751159667969 Val_KL : 3.4861347675323486\n","Epoch: 3665/8000  Traning Loss: 92.89361476898193  Train_Reconstruction: 89.3637056350708  Train_KL: 3.529908686876297  Validation Loss : 92.28466415405273 Val_Reconstruction : 88.80052947998047 Val_KL : 3.4841352701187134\n","Epoch: 3666/8000  Traning Loss: 92.97353744506836  Train_Reconstruction: 89.462721824646  Train_KL: 3.5108153223991394  Validation Loss : 92.91666030883789 Val_Reconstruction : 89.45847702026367 Val_KL : 3.458181619644165\n","Epoch: 3667/8000  Traning Loss: 93.23207664489746  Train_Reconstruction: 89.7209005355835  Train_KL: 3.5111769139766693  Validation Loss : 93.07635498046875 Val_Reconstruction : 89.60116958618164 Val_KL : 3.4751847982406616\n","Epoch: 3668/8000  Traning Loss: 93.34710788726807  Train_Reconstruction: 89.83249282836914  Train_KL: 3.514615297317505  Validation Loss : 92.97871398925781 Val_Reconstruction : 89.50742721557617 Val_KL : 3.471286654472351\n","Epoch: 3669/8000  Traning Loss: 94.07394409179688  Train_Reconstruction: 90.56427669525146  Train_KL: 3.5096668004989624  Validation Loss : 93.76863861083984 Val_Reconstruction : 90.29314041137695 Val_KL : 3.475495457649231\n","Epoch: 3670/8000  Traning Loss: 93.44668102264404  Train_Reconstruction: 89.93152523040771  Train_KL: 3.5151555836200714  Validation Loss : 93.0029411315918 Val_Reconstruction : 89.53032684326172 Val_KL : 3.4726147651672363\n","Epoch: 3671/8000  Traning Loss: 93.30420017242432  Train_Reconstruction: 89.7874984741211  Train_KL: 3.5167024433612823  Validation Loss : 93.14001083374023 Val_Reconstruction : 89.66992568969727 Val_KL : 3.4700828790664673\n","Epoch: 3672/8000  Traning Loss: 93.21422481536865  Train_Reconstruction: 89.69699001312256  Train_KL: 3.5172339975833893  Validation Loss : 92.80509567260742 Val_Reconstruction : 89.32773971557617 Val_KL : 3.4773536920547485\n","Epoch: 3673/8000  Traning Loss: 93.11831474304199  Train_Reconstruction: 89.5995864868164  Train_KL: 3.5187296271324158  Validation Loss : 92.80571746826172 Val_Reconstruction : 89.31770324707031 Val_KL : 3.4880176782608032\n","Epoch: 3674/8000  Traning Loss: 92.97330284118652  Train_Reconstruction: 89.45013999938965  Train_KL: 3.5231634974479675  Validation Loss : 92.74679565429688 Val_Reconstruction : 89.25761032104492 Val_KL : 3.489184617996216\n","Epoch: 3675/8000  Traning Loss: 93.05385112762451  Train_Reconstruction: 89.53223705291748  Train_KL: 3.521613746881485  Validation Loss : 92.86746215820312 Val_Reconstruction : 89.3845443725586 Val_KL : 3.4829167127609253\n","Epoch: 3676/8000  Traning Loss: 92.82047653198242  Train_Reconstruction: 89.29992198944092  Train_KL: 3.5205539762973785  Validation Loss : 92.5096321105957 Val_Reconstruction : 89.03775405883789 Val_KL : 3.471879005432129\n","Epoch: 3677/8000  Traning Loss: 92.49165725708008  Train_Reconstruction: 88.98169803619385  Train_KL: 3.509959489107132  Validation Loss : 92.29082870483398 Val_Reconstruction : 88.81254959106445 Val_KL : 3.4782780408859253\n","Epoch: 3678/8000  Traning Loss: 92.69917297363281  Train_Reconstruction: 89.17131805419922  Train_KL: 3.5278553664684296  Validation Loss : 92.38930892944336 Val_Reconstruction : 88.89787292480469 Val_KL : 3.4914363622665405\n","Epoch: 3679/8000  Traning Loss: 92.63900375366211  Train_Reconstruction: 89.12288475036621  Train_KL: 3.5161200761795044  Validation Loss : 92.09715270996094 Val_Reconstruction : 88.62289428710938 Val_KL : 3.474256157875061\n","Epoch: 3680/8000  Traning Loss: 92.44798374176025  Train_Reconstruction: 88.94351863861084  Train_KL: 3.50446417927742  Validation Loss : 92.1560287475586 Val_Reconstruction : 88.68358993530273 Val_KL : 3.4724373817443848\n","Epoch: 3681/8000  Traning Loss: 92.49751377105713  Train_Reconstruction: 88.97656154632568  Train_KL: 3.520951062440872  Validation Loss : 91.93400955200195 Val_Reconstruction : 88.44768142700195 Val_KL : 3.486328125\n","Epoch: 3682/8000  Traning Loss: 92.56552028656006  Train_Reconstruction: 89.0443811416626  Train_KL: 3.521138459444046  Validation Loss : 92.42892456054688 Val_Reconstruction : 88.94975662231445 Val_KL : 3.4791702032089233\n","Epoch: 3683/8000  Traning Loss: 92.78517246246338  Train_Reconstruction: 89.26842308044434  Train_KL: 3.51675021648407  Validation Loss : 92.53886795043945 Val_Reconstruction : 89.06866836547852 Val_KL : 3.4701974391937256\n","Epoch: 3684/8000  Traning Loss: 92.81701755523682  Train_Reconstruction: 89.3030252456665  Train_KL: 3.5139916241168976  Validation Loss : 92.58871841430664 Val_Reconstruction : 89.1098403930664 Val_KL : 3.478878617286682\n","Epoch: 3685/8000  Traning Loss: 92.80465126037598  Train_Reconstruction: 89.28239250183105  Train_KL: 3.5222590565681458  Validation Loss : 92.67598724365234 Val_Reconstruction : 89.19901657104492 Val_KL : 3.4769718647003174\n","Epoch: 3686/8000  Traning Loss: 93.13104629516602  Train_Reconstruction: 89.61409759521484  Train_KL: 3.516948938369751  Validation Loss : 92.59503555297852 Val_Reconstruction : 89.11990356445312 Val_KL : 3.4751288890838623\n","Epoch: 3687/8000  Traning Loss: 93.07504653930664  Train_Reconstruction: 89.55905055999756  Train_KL: 3.5159947276115417  Validation Loss : 92.51647567749023 Val_Reconstruction : 89.0403823852539 Val_KL : 3.4760924577713013\n","Epoch: 3688/8000  Traning Loss: 92.94889259338379  Train_Reconstruction: 89.43919849395752  Train_KL: 3.5096940398216248  Validation Loss : 92.45763397216797 Val_Reconstruction : 88.98817825317383 Val_KL : 3.469457507133484\n","Epoch: 3689/8000  Traning Loss: 92.67201614379883  Train_Reconstruction: 89.15591239929199  Train_KL: 3.5161047875881195  Validation Loss : 92.0905990600586 Val_Reconstruction : 88.61151123046875 Val_KL : 3.479087471961975\n","Epoch: 3690/8000  Traning Loss: 92.67243003845215  Train_Reconstruction: 89.15511894226074  Train_KL: 3.5173107981681824  Validation Loss : 92.37992477416992 Val_Reconstruction : 88.90612030029297 Val_KL : 3.473805785179138\n","Epoch: 3691/8000  Traning Loss: 92.5264139175415  Train_Reconstruction: 89.01224136352539  Train_KL: 3.514173001050949  Validation Loss : 92.2589225769043 Val_Reconstruction : 88.78557968139648 Val_KL : 3.473342537879944\n","Epoch: 3692/8000  Traning Loss: 92.61013317108154  Train_Reconstruction: 89.0985050201416  Train_KL: 3.5116284489631653  Validation Loss : 92.27492904663086 Val_Reconstruction : 88.802734375 Val_KL : 3.472196578979492\n","Epoch: 3693/8000  Traning Loss: 92.54637622833252  Train_Reconstruction: 89.03411674499512  Train_KL: 3.512260317802429  Validation Loss : 92.23294448852539 Val_Reconstruction : 88.75634384155273 Val_KL : 3.4766002893447876\n","Epoch: 3694/8000  Traning Loss: 92.45770263671875  Train_Reconstruction: 88.95115756988525  Train_KL: 3.506546765565872  Validation Loss : 91.98004150390625 Val_Reconstruction : 88.51903533935547 Val_KL : 3.461006999015808\n","Epoch: 3695/8000  Traning Loss: 92.53452014923096  Train_Reconstruction: 89.02076148986816  Train_KL: 3.513758599758148  Validation Loss : 92.20176696777344 Val_Reconstruction : 88.71632385253906 Val_KL : 3.485443115234375\n","Epoch: 3696/8000  Traning Loss: 92.7591667175293  Train_Reconstruction: 89.23151016235352  Train_KL: 3.527657002210617  Validation Loss : 92.24633407592773 Val_Reconstruction : 88.75580596923828 Val_KL : 3.4905256032943726\n","Epoch: 3697/8000  Traning Loss: 93.11400604248047  Train_Reconstruction: 89.58536434173584  Train_KL: 3.528641551733017  Validation Loss : 92.79946899414062 Val_Reconstruction : 89.31765747070312 Val_KL : 3.4818114042282104\n","Epoch: 3698/8000  Traning Loss: 92.89178657531738  Train_Reconstruction: 89.38408851623535  Train_KL: 3.5076974034309387  Validation Loss : 92.73113250732422 Val_Reconstruction : 89.26860046386719 Val_KL : 3.4625308513641357\n","Epoch: 3699/8000  Traning Loss: 92.60746097564697  Train_Reconstruction: 89.10306644439697  Train_KL: 3.504394382238388  Validation Loss : 92.42247772216797 Val_Reconstruction : 88.96357727050781 Val_KL : 3.458897352218628\n","Epoch: 3700/8000  Traning Loss: 92.59670829772949  Train_Reconstruction: 89.08065891265869  Train_KL: 3.5160498321056366  Validation Loss : 92.49324417114258 Val_Reconstruction : 89.01729583740234 Val_KL : 3.4759470224380493\n","Epoch: 3701/8000  Traning Loss: 93.0739517211914  Train_Reconstruction: 89.55560398101807  Train_KL: 3.5183468759059906  Validation Loss : 92.59508514404297 Val_Reconstruction : 89.12450408935547 Val_KL : 3.47058367729187\n","Epoch: 3702/8000  Traning Loss: 92.9452486038208  Train_Reconstruction: 89.43048191070557  Train_KL: 3.5147655606269836  Validation Loss : 92.35342407226562 Val_Reconstruction : 88.87906646728516 Val_KL : 3.474359154701233\n","Epoch: 3703/8000  Traning Loss: 92.51779174804688  Train_Reconstruction: 89.00630855560303  Train_KL: 3.511482000350952  Validation Loss : 92.24294662475586 Val_Reconstruction : 88.77433013916016 Val_KL : 3.468614339828491\n","Epoch: 3704/8000  Traning Loss: 92.60320281982422  Train_Reconstruction: 89.09281158447266  Train_KL: 3.510391026735306  Validation Loss : 92.2057113647461 Val_Reconstruction : 88.73846817016602 Val_KL : 3.4672436714172363\n","Epoch: 3705/8000  Traning Loss: 92.62813091278076  Train_Reconstruction: 89.12039089202881  Train_KL: 3.5077406466007233  Validation Loss : 92.25407791137695 Val_Reconstruction : 88.78818893432617 Val_KL : 3.46589195728302\n","Epoch: 3706/8000  Traning Loss: 92.71753978729248  Train_Reconstruction: 89.20254421234131  Train_KL: 3.514996439218521  Validation Loss : 92.23052978515625 Val_Reconstruction : 88.7477035522461 Val_KL : 3.4828299283981323\n","Epoch: 3707/8000  Traning Loss: 92.66155815124512  Train_Reconstruction: 89.13386726379395  Train_KL: 3.5276904106140137  Validation Loss : 92.39785385131836 Val_Reconstruction : 88.90554809570312 Val_KL : 3.49230420589447\n","Epoch: 3708/8000  Traning Loss: 92.80217456817627  Train_Reconstruction: 89.27202606201172  Train_KL: 3.530148059129715  Validation Loss : 92.24186325073242 Val_Reconstruction : 88.76481628417969 Val_KL : 3.4770463705062866\n","Epoch: 3709/8000  Traning Loss: 92.78120708465576  Train_Reconstruction: 89.26591682434082  Train_KL: 3.5152897238731384  Validation Loss : 92.73682022094727 Val_Reconstruction : 89.26409149169922 Val_KL : 3.47273051738739\n","Epoch: 3710/8000  Traning Loss: 92.74939250946045  Train_Reconstruction: 89.23104763031006  Train_KL: 3.518345832824707  Validation Loss : 92.55772018432617 Val_Reconstruction : 89.08120346069336 Val_KL : 3.47651743888855\n","Epoch: 3711/8000  Traning Loss: 92.84777450561523  Train_Reconstruction: 89.34180641174316  Train_KL: 3.505966901779175  Validation Loss : 92.70677185058594 Val_Reconstruction : 89.23891067504883 Val_KL : 3.4678585529327393\n","Epoch: 3712/8000  Traning Loss: 92.94797706604004  Train_Reconstruction: 89.43498039245605  Train_KL: 3.5129968225955963  Validation Loss : 92.7588119506836 Val_Reconstruction : 89.28124618530273 Val_KL : 3.4775670766830444\n","Epoch: 3713/8000  Traning Loss: 93.02951908111572  Train_Reconstruction: 89.50687885284424  Train_KL: 3.522641271352768  Validation Loss : 93.08192443847656 Val_Reconstruction : 89.60313034057617 Val_KL : 3.478793144226074\n","Epoch: 3714/8000  Traning Loss: 93.39929485321045  Train_Reconstruction: 89.87640190124512  Train_KL: 3.5228940546512604  Validation Loss : 93.3519515991211 Val_Reconstruction : 89.87542724609375 Val_KL : 3.4765255451202393\n","Epoch: 3715/8000  Traning Loss: 93.41031551361084  Train_Reconstruction: 89.89150428771973  Train_KL: 3.518811970949173  Validation Loss : 93.34908294677734 Val_Reconstruction : 89.86513137817383 Val_KL : 3.4839484691619873\n","Epoch: 3716/8000  Traning Loss: 93.19651508331299  Train_Reconstruction: 89.6789960861206  Train_KL: 3.517519950866699  Validation Loss : 92.91545104980469 Val_Reconstruction : 89.43672561645508 Val_KL : 3.478725552558899\n","Epoch: 3717/8000  Traning Loss: 93.16424655914307  Train_Reconstruction: 89.64994239807129  Train_KL: 3.514303296804428  Validation Loss : 92.43310165405273 Val_Reconstruction : 88.95991134643555 Val_KL : 3.4731898307800293\n","Epoch: 3718/8000  Traning Loss: 92.96436786651611  Train_Reconstruction: 89.45116996765137  Train_KL: 3.513198137283325  Validation Loss : 92.84595108032227 Val_Reconstruction : 89.3630142211914 Val_KL : 3.482937812805176\n","Epoch: 3719/8000  Traning Loss: 92.93310356140137  Train_Reconstruction: 89.4099235534668  Train_KL: 3.523178994655609  Validation Loss : 92.76065063476562 Val_Reconstruction : 89.27828979492188 Val_KL : 3.4823604822158813\n","Epoch: 3720/8000  Traning Loss: 92.63036727905273  Train_Reconstruction: 89.11123943328857  Train_KL: 3.5191266238689423  Validation Loss : 92.53367233276367 Val_Reconstruction : 89.05688095092773 Val_KL : 3.4767918586730957\n","Epoch: 3721/8000  Traning Loss: 92.5456314086914  Train_Reconstruction: 89.03318881988525  Train_KL: 3.512442499399185  Validation Loss : 92.40803527832031 Val_Reconstruction : 88.93679428100586 Val_KL : 3.471242308616638\n","Epoch: 3722/8000  Traning Loss: 92.55787944793701  Train_Reconstruction: 89.04609107971191  Train_KL: 3.5117875933647156  Validation Loss : 92.09378051757812 Val_Reconstruction : 88.61433029174805 Val_KL : 3.479448914527893\n","Epoch: 3723/8000  Traning Loss: 92.63603401184082  Train_Reconstruction: 89.11040496826172  Train_KL: 3.5256302058696747  Validation Loss : 92.42574310302734 Val_Reconstruction : 88.92900085449219 Val_KL : 3.496743321418762\n","Epoch: 3724/8000  Traning Loss: 92.74649143218994  Train_Reconstruction: 89.21950244903564  Train_KL: 3.5269888639450073  Validation Loss : 92.4311752319336 Val_Reconstruction : 88.95093154907227 Val_KL : 3.480242609977722\n","Epoch: 3725/8000  Traning Loss: 92.63904190063477  Train_Reconstruction: 89.12641906738281  Train_KL: 3.5126230716705322  Validation Loss : 92.15196228027344 Val_Reconstruction : 88.6790771484375 Val_KL : 3.4728846549987793\n","Epoch: 3726/8000  Traning Loss: 92.89859580993652  Train_Reconstruction: 89.38102340698242  Train_KL: 3.5175723135471344  Validation Loss : 92.5326919555664 Val_Reconstruction : 89.0539321899414 Val_KL : 3.47876238822937\n","Epoch: 3727/8000  Traning Loss: 92.94656658172607  Train_Reconstruction: 89.42934799194336  Train_KL: 3.5172179639339447  Validation Loss : 92.73079681396484 Val_Reconstruction : 89.25695419311523 Val_KL : 3.473842144012451\n","Epoch: 3728/8000  Traning Loss: 93.13833236694336  Train_Reconstruction: 89.62786865234375  Train_KL: 3.5104645490646362  Validation Loss : 93.02089309692383 Val_Reconstruction : 89.54726028442383 Val_KL : 3.473630666732788\n","Epoch: 3729/8000  Traning Loss: 92.90244579315186  Train_Reconstruction: 89.38765144348145  Train_KL: 3.5147942304611206  Validation Loss : 92.63084030151367 Val_Reconstruction : 89.15382385253906 Val_KL : 3.4770138263702393\n","Epoch: 3730/8000  Traning Loss: 92.9417610168457  Train_Reconstruction: 89.43153858184814  Train_KL: 3.5102234184741974  Validation Loss : 92.53087997436523 Val_Reconstruction : 89.06241989135742 Val_KL : 3.4684611558914185\n","Epoch: 3731/8000  Traning Loss: 93.18383407592773  Train_Reconstruction: 89.67534065246582  Train_KL: 3.508494645357132  Validation Loss : 93.16756439208984 Val_Reconstruction : 89.69429779052734 Val_KL : 3.473265290260315\n","Epoch: 3732/8000  Traning Loss: 93.22004127502441  Train_Reconstruction: 89.70392894744873  Train_KL: 3.516110748052597  Validation Loss : 92.71712112426758 Val_Reconstruction : 89.23883819580078 Val_KL : 3.478280186653137\n","Epoch: 3733/8000  Traning Loss: 92.63727283477783  Train_Reconstruction: 89.11196517944336  Train_KL: 3.525308132171631  Validation Loss : 92.03327941894531 Val_Reconstruction : 88.53692626953125 Val_KL : 3.496355175971985\n","Epoch: 3734/8000  Traning Loss: 92.40106964111328  Train_Reconstruction: 88.87848854064941  Train_KL: 3.522581249475479  Validation Loss : 91.88175582885742 Val_Reconstruction : 88.40500259399414 Val_KL : 3.476753830909729\n","Epoch: 3735/8000  Traning Loss: 92.38250541687012  Train_Reconstruction: 88.86237716674805  Train_KL: 3.520127683877945  Validation Loss : 92.19913482666016 Val_Reconstruction : 88.7104377746582 Val_KL : 3.4886996746063232\n","Epoch: 3736/8000  Traning Loss: 92.64743614196777  Train_Reconstruction: 89.11789417266846  Train_KL: 3.529542475938797  Validation Loss : 92.61062622070312 Val_Reconstruction : 89.11975860595703 Val_KL : 3.4908660650253296\n","Epoch: 3737/8000  Traning Loss: 93.09284019470215  Train_Reconstruction: 89.56733798980713  Train_KL: 3.5255034267902374  Validation Loss : 93.08105087280273 Val_Reconstruction : 89.59618377685547 Val_KL : 3.484869360923767\n","Epoch: 3738/8000  Traning Loss: 93.87895107269287  Train_Reconstruction: 90.36232948303223  Train_KL: 3.5166225135326385  Validation Loss : 93.14495849609375 Val_Reconstruction : 89.67300033569336 Val_KL : 3.4719566106796265\n","Epoch: 3739/8000  Traning Loss: 93.71635246276855  Train_Reconstruction: 90.20673942565918  Train_KL: 3.509613960981369  Validation Loss : 93.46855163574219 Val_Reconstruction : 89.99800872802734 Val_KL : 3.4705417156219482\n","Epoch: 3740/8000  Traning Loss: 93.77783393859863  Train_Reconstruction: 90.26452541351318  Train_KL: 3.5133083164691925  Validation Loss : 93.701904296875 Val_Reconstruction : 90.233642578125 Val_KL : 3.468262553215027\n","Epoch: 3741/8000  Traning Loss: 93.9557876586914  Train_Reconstruction: 90.45241928100586  Train_KL: 3.5033678114414215  Validation Loss : 93.39059448242188 Val_Reconstruction : 89.9281005859375 Val_KL : 3.4624942541122437\n","Epoch: 3742/8000  Traning Loss: 93.24890232086182  Train_Reconstruction: 89.73865604400635  Train_KL: 3.5102453231811523  Validation Loss : 93.0656852722168 Val_Reconstruction : 89.59200668334961 Val_KL : 3.4736777544021606\n","Epoch: 3743/8000  Traning Loss: 93.26162338256836  Train_Reconstruction: 89.74925804138184  Train_KL: 3.512365758419037  Validation Loss : 92.6506576538086 Val_Reconstruction : 89.18172073364258 Val_KL : 3.4689393043518066\n","Epoch: 3744/8000  Traning Loss: 92.82460403442383  Train_Reconstruction: 89.3091688156128  Train_KL: 3.5154340267181396  Validation Loss : 92.27505874633789 Val_Reconstruction : 88.80016326904297 Val_KL : 3.474892020225525\n","Epoch: 3745/8000  Traning Loss: 92.7449836730957  Train_Reconstruction: 89.2262020111084  Train_KL: 3.5187815725803375  Validation Loss : 92.58574676513672 Val_Reconstruction : 89.10460662841797 Val_KL : 3.4811402559280396\n","Epoch: 3746/8000  Traning Loss: 92.87845611572266  Train_Reconstruction: 89.35363483428955  Train_KL: 3.5248205065727234  Validation Loss : 92.88898849487305 Val_Reconstruction : 89.40398406982422 Val_KL : 3.485003709793091\n","Epoch: 3747/8000  Traning Loss: 93.18426895141602  Train_Reconstruction: 89.66457557678223  Train_KL: 3.5196934044361115  Validation Loss : 92.73109817504883 Val_Reconstruction : 89.25894165039062 Val_KL : 3.472154974937439\n","Epoch: 3748/8000  Traning Loss: 92.62158966064453  Train_Reconstruction: 89.10887718200684  Train_KL: 3.5127119421958923  Validation Loss : 92.35163497924805 Val_Reconstruction : 88.87305068969727 Val_KL : 3.4785845279693604\n","Epoch: 3749/8000  Traning Loss: 92.71381187438965  Train_Reconstruction: 89.18694400787354  Train_KL: 3.526867240667343  Validation Loss : 92.52473831176758 Val_Reconstruction : 89.04562377929688 Val_KL : 3.479112386703491\n","Epoch: 3750/8000  Traning Loss: 92.80065250396729  Train_Reconstruction: 89.29346370697021  Train_KL: 3.5071881711483  Validation Loss : 92.38284683227539 Val_Reconstruction : 88.92231750488281 Val_KL : 3.4605263471603394\n","Epoch: 3751/8000  Traning Loss: 92.48867702484131  Train_Reconstruction: 88.98148250579834  Train_KL: 3.507194310426712  Validation Loss : 92.08777618408203 Val_Reconstruction : 88.6180191040039 Val_KL : 3.4697571992874146\n","Epoch: 3752/8000  Traning Loss: 92.52398014068604  Train_Reconstruction: 89.00318336486816  Train_KL: 3.5207958221435547  Validation Loss : 92.70260238647461 Val_Reconstruction : 89.22094345092773 Val_KL : 3.4816603660583496\n","Epoch: 3753/8000  Traning Loss: 93.05650997161865  Train_Reconstruction: 89.54177474975586  Train_KL: 3.5147350430488586  Validation Loss : 92.97297286987305 Val_Reconstruction : 89.50571060180664 Val_KL : 3.467260479927063\n","Epoch: 3754/8000  Traning Loss: 93.38153457641602  Train_Reconstruction: 89.87306118011475  Train_KL: 3.5084761679172516  Validation Loss : 93.06792831420898 Val_Reconstruction : 89.59817886352539 Val_KL : 3.4697481393814087\n","Epoch: 3755/8000  Traning Loss: 93.45161628723145  Train_Reconstruction: 89.9330701828003  Train_KL: 3.5185471177101135  Validation Loss : 93.6455192565918 Val_Reconstruction : 90.1616439819336 Val_KL : 3.483873724937439\n","Epoch: 3756/8000  Traning Loss: 93.87039279937744  Train_Reconstruction: 90.3286247253418  Train_KL: 3.5417667627334595  Validation Loss : 94.30086898803711 Val_Reconstruction : 90.79675674438477 Val_KL : 3.5041136741638184\n","Epoch: 3757/8000  Traning Loss: 93.67146587371826  Train_Reconstruction: 90.14790534973145  Train_KL: 3.5235595703125  Validation Loss : 93.27418518066406 Val_Reconstruction : 89.80511474609375 Val_KL : 3.469071865081787\n","Epoch: 3758/8000  Traning Loss: 93.58620262145996  Train_Reconstruction: 90.08444213867188  Train_KL: 3.501759886741638  Validation Loss : 93.43083572387695 Val_Reconstruction : 89.96476745605469 Val_KL : 3.4660708904266357\n","Epoch: 3759/8000  Traning Loss: 93.28394985198975  Train_Reconstruction: 89.7707633972168  Train_KL: 3.5131865441799164  Validation Loss : 92.65281295776367 Val_Reconstruction : 89.17393112182617 Val_KL : 3.4788790941238403\n","Epoch: 3760/8000  Traning Loss: 93.06557083129883  Train_Reconstruction: 89.54072570800781  Train_KL: 3.5248446464538574  Validation Loss : 92.48691940307617 Val_Reconstruction : 89.00060653686523 Val_KL : 3.4863131046295166\n","Epoch: 3761/8000  Traning Loss: 92.97699546813965  Train_Reconstruction: 89.46059894561768  Train_KL: 3.516396939754486  Validation Loss : 92.70463562011719 Val_Reconstruction : 89.2308349609375 Val_KL : 3.4738008975982666\n","Epoch: 3762/8000  Traning Loss: 92.99712371826172  Train_Reconstruction: 89.4810037612915  Train_KL: 3.5161197781562805  Validation Loss : 92.59870910644531 Val_Reconstruction : 89.12358093261719 Val_KL : 3.475127935409546\n","Epoch: 3763/8000  Traning Loss: 92.72546768188477  Train_Reconstruction: 89.20093631744385  Train_KL: 3.5245315730571747  Validation Loss : 92.22612762451172 Val_Reconstruction : 88.73419189453125 Val_KL : 3.4919368028640747\n","Epoch: 3764/8000  Traning Loss: 92.77966690063477  Train_Reconstruction: 89.24743461608887  Train_KL: 3.5322321951389313  Validation Loss : 92.63938522338867 Val_Reconstruction : 89.15710830688477 Val_KL : 3.482276439666748\n","Epoch: 3765/8000  Traning Loss: 93.01622295379639  Train_Reconstruction: 89.49964714050293  Train_KL: 3.5165763795375824  Validation Loss : 92.67958450317383 Val_Reconstruction : 89.20206069946289 Val_KL : 3.4775246381759644\n","Epoch: 3766/8000  Traning Loss: 93.1877498626709  Train_Reconstruction: 89.67133712768555  Train_KL: 3.516413778066635  Validation Loss : 92.62627029418945 Val_Reconstruction : 89.14657592773438 Val_KL : 3.479696750640869\n","Epoch: 3767/8000  Traning Loss: 92.64039611816406  Train_Reconstruction: 89.12311744689941  Train_KL: 3.517278403043747  Validation Loss : 92.39360427856445 Val_Reconstruction : 88.91423416137695 Val_KL : 3.4793701171875\n","Epoch: 3768/8000  Traning Loss: 93.14764022827148  Train_Reconstruction: 89.62427425384521  Train_KL: 3.5233676731586456  Validation Loss : 92.59300994873047 Val_Reconstruction : 89.11412811279297 Val_KL : 3.4788812398910522\n","Epoch: 3769/8000  Traning Loss: 92.86572074890137  Train_Reconstruction: 89.35558414459229  Train_KL: 3.510135591030121  Validation Loss : 92.47709274291992 Val_Reconstruction : 89.0041732788086 Val_KL : 3.4729206562042236\n","Epoch: 3770/8000  Traning Loss: 92.71486759185791  Train_Reconstruction: 89.20999050140381  Train_KL: 3.5048776268959045  Validation Loss : 92.51162338256836 Val_Reconstruction : 89.03944778442383 Val_KL : 3.472176432609558\n","Epoch: 3771/8000  Traning Loss: 92.71645450592041  Train_Reconstruction: 89.20655727386475  Train_KL: 3.5098966658115387  Validation Loss : 92.66655731201172 Val_Reconstruction : 89.18748474121094 Val_KL : 3.479072332382202\n","Epoch: 3772/8000  Traning Loss: 92.93896484375  Train_Reconstruction: 89.4228458404541  Train_KL: 3.5161180198192596  Validation Loss : 92.68563842773438 Val_Reconstruction : 89.19818878173828 Val_KL : 3.4874494075775146\n","Epoch: 3773/8000  Traning Loss: 93.15833950042725  Train_Reconstruction: 89.63602542877197  Train_KL: 3.522315114736557  Validation Loss : 92.87946319580078 Val_Reconstruction : 89.40135955810547 Val_KL : 3.4781057834625244\n","Epoch: 3774/8000  Traning Loss: 93.18969631195068  Train_Reconstruction: 89.6791639328003  Train_KL: 3.510533094406128  Validation Loss : 93.02503967285156 Val_Reconstruction : 89.54858016967773 Val_KL : 3.4764591455459595\n","Epoch: 3775/8000  Traning Loss: 93.18602752685547  Train_Reconstruction: 89.6705207824707  Train_KL: 3.5155065953731537  Validation Loss : 92.6854019165039 Val_Reconstruction : 89.20892715454102 Val_KL : 3.476476311683655\n","Epoch: 3776/8000  Traning Loss: 92.80926036834717  Train_Reconstruction: 89.28578281402588  Train_KL: 3.5234771966934204  Validation Loss : 92.64809799194336 Val_Reconstruction : 89.15732955932617 Val_KL : 3.490769624710083\n","Epoch: 3777/8000  Traning Loss: 92.66434288024902  Train_Reconstruction: 89.13287353515625  Train_KL: 3.5314695835113525  Validation Loss : 92.34729385375977 Val_Reconstruction : 88.86160278320312 Val_KL : 3.485693335533142\n","Epoch: 3778/8000  Traning Loss: 92.74288845062256  Train_Reconstruction: 89.22988700866699  Train_KL: 3.513000577688217  Validation Loss : 92.6644287109375 Val_Reconstruction : 89.19250869750977 Val_KL : 3.4719187021255493\n","Epoch: 3779/8000  Traning Loss: 93.05112552642822  Train_Reconstruction: 89.53282356262207  Train_KL: 3.5183015167713165  Validation Loss : 92.53752517700195 Val_Reconstruction : 89.0504150390625 Val_KL : 3.487109661102295\n","Epoch: 3780/8000  Traning Loss: 92.5868148803711  Train_Reconstruction: 89.06678009033203  Train_KL: 3.5200345516204834  Validation Loss : 92.07700729370117 Val_Reconstruction : 88.60493087768555 Val_KL : 3.4720784425735474\n","Epoch: 3781/8000  Traning Loss: 92.55539894104004  Train_Reconstruction: 89.04572010040283  Train_KL: 3.5096793472766876  Validation Loss : 92.27301025390625 Val_Reconstruction : 88.80293273925781 Val_KL : 3.470076084136963\n","Epoch: 3782/8000  Traning Loss: 92.80344295501709  Train_Reconstruction: 89.29260730743408  Train_KL: 3.510837107896805  Validation Loss : 92.5446662902832 Val_Reconstruction : 89.06793212890625 Val_KL : 3.4767348766326904\n","Epoch: 3783/8000  Traning Loss: 92.97740936279297  Train_Reconstruction: 89.46123123168945  Train_KL: 3.516178160905838  Validation Loss : 92.85699462890625 Val_Reconstruction : 89.3742790222168 Val_KL : 3.4827167987823486\n","Epoch: 3784/8000  Traning Loss: 92.5878677368164  Train_Reconstruction: 89.07237339019775  Train_KL: 3.515494227409363  Validation Loss : 92.27544403076172 Val_Reconstruction : 88.79357528686523 Val_KL : 3.481868863105774\n","Epoch: 3785/8000  Traning Loss: 92.68048763275146  Train_Reconstruction: 89.16607475280762  Train_KL: 3.514412373304367  Validation Loss : 92.49367141723633 Val_Reconstruction : 89.02357482910156 Val_KL : 3.470094680786133\n","Epoch: 3786/8000  Traning Loss: 92.71158409118652  Train_Reconstruction: 89.20215606689453  Train_KL: 3.5094279646873474  Validation Loss : 92.27381134033203 Val_Reconstruction : 88.80352783203125 Val_KL : 3.470284581184387\n","Epoch: 3787/8000  Traning Loss: 93.17356300354004  Train_Reconstruction: 89.64709091186523  Train_KL: 3.526472717523575  Validation Loss : 93.1018295288086 Val_Reconstruction : 89.60533905029297 Val_KL : 3.496489644050598\n","Epoch: 3788/8000  Traning Loss: 93.71716022491455  Train_Reconstruction: 90.19601726531982  Train_KL: 3.5211425125598907  Validation Loss : 93.75634002685547 Val_Reconstruction : 90.28600692749023 Val_KL : 3.470333695411682\n","Epoch: 3789/8000  Traning Loss: 93.10587501525879  Train_Reconstruction: 89.58331871032715  Train_KL: 3.522556185722351  Validation Loss : 92.8234634399414 Val_Reconstruction : 89.34037017822266 Val_KL : 3.483092427253723\n","Epoch: 3790/8000  Traning Loss: 92.91785049438477  Train_Reconstruction: 89.39730739593506  Train_KL: 3.5205418169498444  Validation Loss : 92.5366325378418 Val_Reconstruction : 89.0598258972168 Val_KL : 3.4768067598342896\n","Epoch: 3791/8000  Traning Loss: 93.09278964996338  Train_Reconstruction: 89.5749979019165  Train_KL: 3.5177915692329407  Validation Loss : 92.9749755859375 Val_Reconstruction : 89.49613189697266 Val_KL : 3.478845000267029\n","Epoch: 3792/8000  Traning Loss: 93.26879978179932  Train_Reconstruction: 89.74765205383301  Train_KL: 3.5211468636989594  Validation Loss : 92.84892654418945 Val_Reconstruction : 89.3568229675293 Val_KL : 3.492105484008789\n","Epoch: 3793/8000  Traning Loss: 92.93871307373047  Train_Reconstruction: 89.40989685058594  Train_KL: 3.528816044330597  Validation Loss : 92.35503005981445 Val_Reconstruction : 88.8706169128418 Val_KL : 3.4844119548797607\n","Epoch: 3794/8000  Traning Loss: 92.75338935852051  Train_Reconstruction: 89.25166702270508  Train_KL: 3.5017217099666595  Validation Loss : 92.33583068847656 Val_Reconstruction : 88.87355041503906 Val_KL : 3.4622801542282104\n","Epoch: 3795/8000  Traning Loss: 92.48113632202148  Train_Reconstruction: 88.97137451171875  Train_KL: 3.5097629725933075  Validation Loss : 92.19927978515625 Val_Reconstruction : 88.7171516418457 Val_KL : 3.482130527496338\n","Epoch: 3796/8000  Traning Loss: 92.56272029876709  Train_Reconstruction: 89.04326820373535  Train_KL: 3.519451290369034  Validation Loss : 92.44211959838867 Val_Reconstruction : 88.96714782714844 Val_KL : 3.474968910217285\n","Epoch: 3797/8000  Traning Loss: 92.7631893157959  Train_Reconstruction: 89.2458086013794  Train_KL: 3.5173812806606293  Validation Loss : 92.4195671081543 Val_Reconstruction : 88.93814468383789 Val_KL : 3.481423258781433\n","Epoch: 3798/8000  Traning Loss: 93.30472183227539  Train_Reconstruction: 89.7816915512085  Train_KL: 3.523030638694763  Validation Loss : 92.78213882446289 Val_Reconstruction : 89.29666900634766 Val_KL : 3.485467553138733\n","Epoch: 3799/8000  Traning Loss: 92.96764087677002  Train_Reconstruction: 89.44835758209229  Train_KL: 3.519283562898636  Validation Loss : 92.6751823425293 Val_Reconstruction : 89.20186233520508 Val_KL : 3.4733186960220337\n","Epoch: 3800/8000  Traning Loss: 92.94137287139893  Train_Reconstruction: 89.42588806152344  Train_KL: 3.5154833793640137  Validation Loss : 92.40658950805664 Val_Reconstruction : 88.93404388427734 Val_KL : 3.472546696662903\n","Epoch: 3801/8000  Traning Loss: 92.75709342956543  Train_Reconstruction: 89.24295806884766  Train_KL: 3.5141343474388123  Validation Loss : 92.51421356201172 Val_Reconstruction : 89.04069519042969 Val_KL : 3.4735158681869507\n","Epoch: 3802/8000  Traning Loss: 92.9720811843872  Train_Reconstruction: 89.458251953125  Train_KL: 3.5138287246227264  Validation Loss : 92.41025924682617 Val_Reconstruction : 88.93379592895508 Val_KL : 3.47646164894104\n","Epoch: 3803/8000  Traning Loss: 92.78451633453369  Train_Reconstruction: 89.26636505126953  Train_KL: 3.5181502401828766  Validation Loss : 92.24568939208984 Val_Reconstruction : 88.76037979125977 Val_KL : 3.485309600830078\n","Epoch: 3804/8000  Traning Loss: 92.93613052368164  Train_Reconstruction: 89.41180896759033  Train_KL: 3.5243207812309265  Validation Loss : 92.77358627319336 Val_Reconstruction : 89.27518081665039 Val_KL : 3.4984050989151\n","Epoch: 3805/8000  Traning Loss: 92.82419776916504  Train_Reconstruction: 89.30118465423584  Train_KL: 3.523013472557068  Validation Loss : 92.55213165283203 Val_Reconstruction : 89.0673828125 Val_KL : 3.4847487211227417\n","Epoch: 3806/8000  Traning Loss: 92.9799747467041  Train_Reconstruction: 89.46234798431396  Train_KL: 3.517626404762268  Validation Loss : 92.86132431030273 Val_Reconstruction : 89.37841033935547 Val_KL : 3.4829131364822388\n","Epoch: 3807/8000  Traning Loss: 93.05138206481934  Train_Reconstruction: 89.53241729736328  Train_KL: 3.518964260816574  Validation Loss : 92.66267776489258 Val_Reconstruction : 89.18567657470703 Val_KL : 3.4769997596740723\n","Epoch: 3808/8000  Traning Loss: 92.64892196655273  Train_Reconstruction: 89.1347188949585  Train_KL: 3.5142039358615875  Validation Loss : 91.90815734863281 Val_Reconstruction : 88.429443359375 Val_KL : 3.47871470451355\n","Epoch: 3809/8000  Traning Loss: 92.28087711334229  Train_Reconstruction: 88.76304054260254  Train_KL: 3.517835646867752  Validation Loss : 91.87384414672852 Val_Reconstruction : 88.39118194580078 Val_KL : 3.482660174369812\n","Epoch: 3810/8000  Traning Loss: 92.61796855926514  Train_Reconstruction: 89.10037422180176  Train_KL: 3.517593890428543  Validation Loss : 92.59233474731445 Val_Reconstruction : 89.11756896972656 Val_KL : 3.4747668504714966\n","Epoch: 3811/8000  Traning Loss: 92.7712049484253  Train_Reconstruction: 89.26111221313477  Train_KL: 3.51009264588356  Validation Loss : 92.21415328979492 Val_Reconstruction : 88.74678802490234 Val_KL : 3.467364549636841\n","Epoch: 3812/8000  Traning Loss: 92.4561595916748  Train_Reconstruction: 88.94674396514893  Train_KL: 3.509415715932846  Validation Loss : 91.93620300292969 Val_Reconstruction : 88.46711730957031 Val_KL : 3.4690842628479004\n","Epoch: 3813/8000  Traning Loss: 92.66390705108643  Train_Reconstruction: 89.1520643234253  Train_KL: 3.5118423998355865  Validation Loss : 93.01318740844727 Val_Reconstruction : 89.53049087524414 Val_KL : 3.4826972484588623\n","Epoch: 3814/8000  Traning Loss: 93.27057552337646  Train_Reconstruction: 89.7550745010376  Train_KL: 3.515499860048294  Validation Loss : 93.35259246826172 Val_Reconstruction : 89.8753433227539 Val_KL : 3.477246642112732\n","Epoch: 3815/8000  Traning Loss: 94.07122325897217  Train_Reconstruction: 90.54945850372314  Train_KL: 3.521764874458313  Validation Loss : 93.91737365722656 Val_Reconstruction : 90.43198013305664 Val_KL : 3.4853962659835815\n","Epoch: 3816/8000  Traning Loss: 93.88442802429199  Train_Reconstruction: 90.35507678985596  Train_KL: 3.5293507874011993  Validation Loss : 93.56453704833984 Val_Reconstruction : 90.07544708251953 Val_KL : 3.4890923500061035\n","Epoch: 3817/8000  Traning Loss: 92.98560428619385  Train_Reconstruction: 89.46542835235596  Train_KL: 3.5201756060123444  Validation Loss : 92.43341827392578 Val_Reconstruction : 88.95448303222656 Val_KL : 3.4789347648620605\n","Epoch: 3818/8000  Traning Loss: 92.65784454345703  Train_Reconstruction: 89.14252471923828  Train_KL: 3.5153200030326843  Validation Loss : 92.52462768554688 Val_Reconstruction : 89.04857635498047 Val_KL : 3.476050615310669\n","Epoch: 3819/8000  Traning Loss: 92.70062828063965  Train_Reconstruction: 89.181884765625  Train_KL: 3.5187432765960693  Validation Loss : 92.23735046386719 Val_Reconstruction : 88.75715637207031 Val_KL : 3.4801942110061646\n","Epoch: 3820/8000  Traning Loss: 92.38239002227783  Train_Reconstruction: 88.86221218109131  Train_KL: 3.520178198814392  Validation Loss : 92.25252151489258 Val_Reconstruction : 88.7822036743164 Val_KL : 3.470318555831909\n","Epoch: 3821/8000  Traning Loss: 92.34076023101807  Train_Reconstruction: 88.82786750793457  Train_KL: 3.5128928124904633  Validation Loss : 92.12973022460938 Val_Reconstruction : 88.64543533325195 Val_KL : 3.4842976331710815\n","Epoch: 3822/8000  Traning Loss: 92.29406642913818  Train_Reconstruction: 88.7755355834961  Train_KL: 3.518530935049057  Validation Loss : 92.00566864013672 Val_Reconstruction : 88.51822662353516 Val_KL : 3.4874428510665894\n","Epoch: 3823/8000  Traning Loss: 92.55238819122314  Train_Reconstruction: 89.0246114730835  Train_KL: 3.5277758836746216  Validation Loss : 92.5299301147461 Val_Reconstruction : 89.0442123413086 Val_KL : 3.485720157623291\n","Epoch: 3824/8000  Traning Loss: 92.44717597961426  Train_Reconstruction: 88.92409229278564  Train_KL: 3.5230823755264282  Validation Loss : 92.1681900024414 Val_Reconstruction : 88.69384765625 Val_KL : 3.474340558052063\n","Epoch: 3825/8000  Traning Loss: 92.60357284545898  Train_Reconstruction: 89.08765125274658  Train_KL: 3.5159226655960083  Validation Loss : 92.83125305175781 Val_Reconstruction : 89.34804916381836 Val_KL : 3.483202815055847\n","Epoch: 3826/8000  Traning Loss: 93.16042518615723  Train_Reconstruction: 89.63609409332275  Train_KL: 3.5243302285671234  Validation Loss : 93.2593002319336 Val_Reconstruction : 89.77663803100586 Val_KL : 3.4826642274856567\n","Epoch: 3827/8000  Traning Loss: 93.10529232025146  Train_Reconstruction: 89.57875347137451  Train_KL: 3.5265398025512695  Validation Loss : 92.85239791870117 Val_Reconstruction : 89.36326217651367 Val_KL : 3.4891345500946045\n","Epoch: 3828/8000  Traning Loss: 92.7383804321289  Train_Reconstruction: 89.21966361999512  Train_KL: 3.5187163054943085  Validation Loss : 92.47451782226562 Val_Reconstruction : 88.99523544311523 Val_KL : 3.479284405708313\n","Epoch: 3829/8000  Traning Loss: 92.99635601043701  Train_Reconstruction: 89.47726440429688  Train_KL: 3.5190910696983337  Validation Loss : 92.6025161743164 Val_Reconstruction : 89.12312316894531 Val_KL : 3.4793920516967773\n","Epoch: 3830/8000  Traning Loss: 92.69524765014648  Train_Reconstruction: 89.17489051818848  Train_KL: 3.5203566551208496  Validation Loss : 92.49204635620117 Val_Reconstruction : 89.01192092895508 Val_KL : 3.480127453804016\n","Epoch: 3831/8000  Traning Loss: 92.5753812789917  Train_Reconstruction: 89.04858875274658  Train_KL: 3.5267940163612366  Validation Loss : 92.31163024902344 Val_Reconstruction : 88.82097244262695 Val_KL : 3.4906556606292725\n","Epoch: 3832/8000  Traning Loss: 92.5201416015625  Train_Reconstruction: 88.99118518829346  Train_KL: 3.528957188129425  Validation Loss : 92.39057540893555 Val_Reconstruction : 88.90755081176758 Val_KL : 3.483022093772888\n","Epoch: 3833/8000  Traning Loss: 92.67687892913818  Train_Reconstruction: 89.14977169036865  Train_KL: 3.5271069407463074  Validation Loss : 92.4981689453125 Val_Reconstruction : 89.01579666137695 Val_KL : 3.4823720455169678\n","Epoch: 3834/8000  Traning Loss: 92.48729228973389  Train_Reconstruction: 88.96262454986572  Train_KL: 3.524668663740158  Validation Loss : 92.1875 Val_Reconstruction : 88.70063018798828 Val_KL : 3.486871361732483\n","Epoch: 3835/8000  Traning Loss: 92.59750175476074  Train_Reconstruction: 89.06341361999512  Train_KL: 3.534088611602783  Validation Loss : 92.71623229980469 Val_Reconstruction : 89.2149887084961 Val_KL : 3.5012401342391968\n","Epoch: 3836/8000  Traning Loss: 92.7576732635498  Train_Reconstruction: 89.2280158996582  Train_KL: 3.5296570658683777  Validation Loss : 92.79866790771484 Val_Reconstruction : 89.31902313232422 Val_KL : 3.479644775390625\n","Epoch: 3837/8000  Traning Loss: 93.16129302978516  Train_Reconstruction: 89.6523265838623  Train_KL: 3.5089674293994904  Validation Loss : 92.91323471069336 Val_Reconstruction : 89.44185256958008 Val_KL : 3.4713844060897827\n","Epoch: 3838/8000  Traning Loss: 93.45632076263428  Train_Reconstruction: 89.94846534729004  Train_KL: 3.5078569054603577  Validation Loss : 93.17998123168945 Val_Reconstruction : 89.69944381713867 Val_KL : 3.4805378913879395\n","Epoch: 3839/8000  Traning Loss: 93.34024238586426  Train_Reconstruction: 89.81649971008301  Train_KL: 3.5237430930137634  Validation Loss : 92.91225814819336 Val_Reconstruction : 89.42294692993164 Val_KL : 3.489311099052429\n","Epoch: 3840/8000  Traning Loss: 93.17549896240234  Train_Reconstruction: 89.6558837890625  Train_KL: 3.5196144580841064  Validation Loss : 92.7269172668457 Val_Reconstruction : 89.24368667602539 Val_KL : 3.4832314252853394\n","Epoch: 3841/8000  Traning Loss: 92.9971399307251  Train_Reconstruction: 89.47870254516602  Train_KL: 3.5184367299079895  Validation Loss : 92.52378845214844 Val_Reconstruction : 89.05137252807617 Val_KL : 3.472415328025818\n","Epoch: 3842/8000  Traning Loss: 92.62834930419922  Train_Reconstruction: 89.11313056945801  Train_KL: 3.515219211578369  Validation Loss : 92.45564651489258 Val_Reconstruction : 88.97229385375977 Val_KL : 3.483351945877075\n","Epoch: 3843/8000  Traning Loss: 92.56274890899658  Train_Reconstruction: 89.03716373443604  Train_KL: 3.5255848169326782  Validation Loss : 92.15715026855469 Val_Reconstruction : 88.67628860473633 Val_KL : 3.480861783027649\n","Epoch: 3844/8000  Traning Loss: 92.5959119796753  Train_Reconstruction: 89.0769739151001  Train_KL: 3.5189377665519714  Validation Loss : 92.4951057434082 Val_Reconstruction : 89.01593017578125 Val_KL : 3.4791746139526367\n","Epoch: 3845/8000  Traning Loss: 92.73628425598145  Train_Reconstruction: 89.2234468460083  Train_KL: 3.5128370821475983  Validation Loss : 92.38311386108398 Val_Reconstruction : 88.9096908569336 Val_KL : 3.4734221696853638\n","Epoch: 3846/8000  Traning Loss: 92.87085437774658  Train_Reconstruction: 89.36074256896973  Train_KL: 3.510112166404724  Validation Loss : 92.84797286987305 Val_Reconstruction : 89.38347244262695 Val_KL : 3.464500665664673\n","Epoch: 3847/8000  Traning Loss: 92.95425224304199  Train_Reconstruction: 89.44654178619385  Train_KL: 3.5077100098133087  Validation Loss : 92.4516372680664 Val_Reconstruction : 88.96926498413086 Val_KL : 3.482373595237732\n","Epoch: 3848/8000  Traning Loss: 92.6519136428833  Train_Reconstruction: 89.12480163574219  Train_KL: 3.527112156152725  Validation Loss : 92.19022750854492 Val_Reconstruction : 88.70188522338867 Val_KL : 3.488341450691223\n","Epoch: 3849/8000  Traning Loss: 92.52263355255127  Train_Reconstruction: 89.0003776550293  Train_KL: 3.5222558081150055  Validation Loss : 92.2421760559082 Val_Reconstruction : 88.7611312866211 Val_KL : 3.4810456037521362\n","Epoch: 3850/8000  Traning Loss: 92.69643974304199  Train_Reconstruction: 89.1842212677002  Train_KL: 3.5122189819812775  Validation Loss : 92.54075622558594 Val_Reconstruction : 89.06869888305664 Val_KL : 3.4720590114593506\n","Epoch: 3851/8000  Traning Loss: 92.75753402709961  Train_Reconstruction: 89.2471113204956  Train_KL: 3.510422706604004  Validation Loss : 92.46583938598633 Val_Reconstruction : 88.99553680419922 Val_KL : 3.4703030586242676\n","Epoch: 3852/8000  Traning Loss: 92.72854995727539  Train_Reconstruction: 89.21762275695801  Train_KL: 3.510926753282547  Validation Loss : 92.21934509277344 Val_Reconstruction : 88.75111770629883 Val_KL : 3.4682306051254272\n","Epoch: 3853/8000  Traning Loss: 92.50912475585938  Train_Reconstruction: 88.99438190460205  Train_KL: 3.5147436261177063  Validation Loss : 92.34082794189453 Val_Reconstruction : 88.86076354980469 Val_KL : 3.4800615310668945\n","Epoch: 3854/8000  Traning Loss: 92.31064796447754  Train_Reconstruction: 88.79584693908691  Train_KL: 3.5148007571697235  Validation Loss : 92.07533264160156 Val_Reconstruction : 88.60582733154297 Val_KL : 3.469505548477173\n","Epoch: 3855/8000  Traning Loss: 92.69314956665039  Train_Reconstruction: 89.18178367614746  Train_KL: 3.5113652050495148  Validation Loss : 92.45323944091797 Val_Reconstruction : 88.97893142700195 Val_KL : 3.4743103981018066\n","Epoch: 3856/8000  Traning Loss: 93.36075973510742  Train_Reconstruction: 89.84619617462158  Train_KL: 3.514563262462616  Validation Loss : 93.0463638305664 Val_Reconstruction : 89.56444931030273 Val_KL : 3.481913685798645\n","Epoch: 3857/8000  Traning Loss: 93.19672393798828  Train_Reconstruction: 89.67602825164795  Train_KL: 3.520697057247162  Validation Loss : 92.61614608764648 Val_Reconstruction : 89.13098526000977 Val_KL : 3.4851588010787964\n","Epoch: 3858/8000  Traning Loss: 92.78667449951172  Train_Reconstruction: 89.26197910308838  Train_KL: 3.52469602227211  Validation Loss : 92.62372589111328 Val_Reconstruction : 89.14322280883789 Val_KL : 3.480501651763916\n","Epoch: 3859/8000  Traning Loss: 92.79853057861328  Train_Reconstruction: 89.28118515014648  Train_KL: 3.517345517873764  Validation Loss : 92.47967147827148 Val_Reconstruction : 89.00306701660156 Val_KL : 3.476603150367737\n","Epoch: 3860/8000  Traning Loss: 92.58267784118652  Train_Reconstruction: 89.06067180633545  Train_KL: 3.5220062732696533  Validation Loss : 92.62508010864258 Val_Reconstruction : 89.13803100585938 Val_KL : 3.4870505332946777\n","Epoch: 3861/8000  Traning Loss: 92.69069004058838  Train_Reconstruction: 89.1727066040039  Train_KL: 3.5179840326309204  Validation Loss : 92.28509902954102 Val_Reconstruction : 88.814697265625 Val_KL : 3.4704034328460693\n","Epoch: 3862/8000  Traning Loss: 92.71604251861572  Train_Reconstruction: 89.2088508605957  Train_KL: 3.5071913301944733  Validation Loss : 92.30383682250977 Val_Reconstruction : 88.83327102661133 Val_KL : 3.470564365386963\n","Epoch: 3863/8000  Traning Loss: 92.39175033569336  Train_Reconstruction: 88.87743091583252  Train_KL: 3.5143191814422607  Validation Loss : 91.98385620117188 Val_Reconstruction : 88.49575424194336 Val_KL : 3.4881011247634888\n","Epoch: 3864/8000  Traning Loss: 92.41208362579346  Train_Reconstruction: 88.88377285003662  Train_KL: 3.528312534093857  Validation Loss : 92.04524993896484 Val_Reconstruction : 88.55747604370117 Val_KL : 3.4877716302871704\n","Epoch: 3865/8000  Traning Loss: 92.1859884262085  Train_Reconstruction: 88.67175579071045  Train_KL: 3.5142312049865723  Validation Loss : 91.94124603271484 Val_Reconstruction : 88.4743881225586 Val_KL : 3.466858386993408\n","Epoch: 3866/8000  Traning Loss: 92.45069408416748  Train_Reconstruction: 88.95236682891846  Train_KL: 3.4983282685279846  Validation Loss : 92.58351516723633 Val_Reconstruction : 89.11724472045898 Val_KL : 3.4662725925445557\n","Epoch: 3867/8000  Traning Loss: 92.54258060455322  Train_Reconstruction: 89.02321243286133  Train_KL: 3.519370049238205  Validation Loss : 91.88555908203125 Val_Reconstruction : 88.39864730834961 Val_KL : 3.4869152307510376\n","Epoch: 3868/8000  Traning Loss: 92.47388553619385  Train_Reconstruction: 88.9487533569336  Train_KL: 3.5251314640045166  Validation Loss : 92.17708969116211 Val_Reconstruction : 88.69791030883789 Val_KL : 3.4791808128356934\n","Epoch: 3869/8000  Traning Loss: 92.36830711364746  Train_Reconstruction: 88.8509693145752  Train_KL: 3.517339438199997  Validation Loss : 92.11806106567383 Val_Reconstruction : 88.63401794433594 Val_KL : 3.484040856361389\n","Epoch: 3870/8000  Traning Loss: 92.5285997390747  Train_Reconstruction: 89.00591373443604  Train_KL: 3.52268585562706  Validation Loss : 92.3432388305664 Val_Reconstruction : 88.85096740722656 Val_KL : 3.4922714233398438\n","Epoch: 3871/8000  Traning Loss: 92.63294410705566  Train_Reconstruction: 89.11830234527588  Train_KL: 3.51463982462883  Validation Loss : 92.24775695800781 Val_Reconstruction : 88.77147674560547 Val_KL : 3.4762784242630005\n","Epoch: 3872/8000  Traning Loss: 92.30757141113281  Train_Reconstruction: 88.78876686096191  Train_KL: 3.518805116415024  Validation Loss : 91.99627304077148 Val_Reconstruction : 88.50882339477539 Val_KL : 3.487448811531067\n","Epoch: 3873/8000  Traning Loss: 92.41300773620605  Train_Reconstruction: 88.89284324645996  Train_KL: 3.5201634764671326  Validation Loss : 92.2650146484375 Val_Reconstruction : 88.78031539916992 Val_KL : 3.4846984148025513\n","Epoch: 3874/8000  Traning Loss: 92.5557918548584  Train_Reconstruction: 89.03172779083252  Train_KL: 3.524064213037491  Validation Loss : 92.39209747314453 Val_Reconstruction : 88.90375900268555 Val_KL : 3.488339424133301\n","Epoch: 3875/8000  Traning Loss: 92.4659538269043  Train_Reconstruction: 88.94590759277344  Train_KL: 3.520046979188919  Validation Loss : 92.15155029296875 Val_Reconstruction : 88.6690788269043 Val_KL : 3.48247230052948\n","Epoch: 3876/8000  Traning Loss: 92.71999454498291  Train_Reconstruction: 89.20077228546143  Train_KL: 3.519223213195801  Validation Loss : 92.49475479125977 Val_Reconstruction : 89.0110969543457 Val_KL : 3.483659029006958\n","Epoch: 3877/8000  Traning Loss: 92.79986000061035  Train_Reconstruction: 89.27969264984131  Train_KL: 3.520167499780655  Validation Loss : 92.48638153076172 Val_Reconstruction : 89.0179557800293 Val_KL : 3.4684287309646606\n","Epoch: 3878/8000  Traning Loss: 92.72612571716309  Train_Reconstruction: 89.21802806854248  Train_KL: 3.508097320795059  Validation Loss : 92.3157958984375 Val_Reconstruction : 88.83793258666992 Val_KL : 3.4778624773025513\n","Epoch: 3879/8000  Traning Loss: 92.46924209594727  Train_Reconstruction: 88.9340181350708  Train_KL: 3.5352231860160828  Validation Loss : 92.21442794799805 Val_Reconstruction : 88.72105026245117 Val_KL : 3.4933749437332153\n","Epoch: 3880/8000  Traning Loss: 92.44099712371826  Train_Reconstruction: 88.92243766784668  Train_KL: 3.5185588896274567  Validation Loss : 92.18463516235352 Val_Reconstruction : 88.71707916259766 Val_KL : 3.467555046081543\n","Epoch: 3881/8000  Traning Loss: 92.63027858734131  Train_Reconstruction: 89.11655139923096  Train_KL: 3.5137276649475098  Validation Loss : 92.75786590576172 Val_Reconstruction : 89.27737426757812 Val_KL : 3.4804909229278564\n","Epoch: 3882/8000  Traning Loss: 93.31032180786133  Train_Reconstruction: 89.78618049621582  Train_KL: 3.5241412222385406  Validation Loss : 93.77100372314453 Val_Reconstruction : 90.28604125976562 Val_KL : 3.484959602355957\n","Epoch: 3883/8000  Traning Loss: 93.72206115722656  Train_Reconstruction: 90.19564628601074  Train_KL: 3.5264143645763397  Validation Loss : 92.99024200439453 Val_Reconstruction : 89.50376510620117 Val_KL : 3.486479163169861\n","Epoch: 3884/8000  Traning Loss: 93.58721351623535  Train_Reconstruction: 90.06494331359863  Train_KL: 3.5222703516483307  Validation Loss : 93.2067985534668 Val_Reconstruction : 89.72922897338867 Val_KL : 3.4775699377059937\n","Epoch: 3885/8000  Traning Loss: 93.92824935913086  Train_Reconstruction: 90.41559219360352  Train_KL: 3.512658029794693  Validation Loss : 93.88151168823242 Val_Reconstruction : 90.40837860107422 Val_KL : 3.473130941390991\n","Epoch: 3886/8000  Traning Loss: 93.901686668396  Train_Reconstruction: 90.38537216186523  Train_KL: 3.5163140296936035  Validation Loss : 93.27135467529297 Val_Reconstruction : 89.78728103637695 Val_KL : 3.4840742349624634\n","Epoch: 3887/8000  Traning Loss: 93.34294891357422  Train_Reconstruction: 89.81854820251465  Train_KL: 3.5244013965129852  Validation Loss : 92.79199600219727 Val_Reconstruction : 89.29530334472656 Val_KL : 3.4966925382614136\n","Epoch: 3888/8000  Traning Loss: 92.89842796325684  Train_Reconstruction: 89.36278820037842  Train_KL: 3.535640776157379  Validation Loss : 92.20996475219727 Val_Reconstruction : 88.72359466552734 Val_KL : 3.486369013786316\n","Epoch: 3889/8000  Traning Loss: 92.55972385406494  Train_Reconstruction: 89.04025077819824  Train_KL: 3.519473373889923  Validation Loss : 92.28702545166016 Val_Reconstruction : 88.82328033447266 Val_KL : 3.463745951652527\n","Epoch: 3890/8000  Traning Loss: 92.83545398712158  Train_Reconstruction: 89.32546329498291  Train_KL: 3.5099896490573883  Validation Loss : 92.80259704589844 Val_Reconstruction : 89.33084869384766 Val_KL : 3.4717479944229126\n","Epoch: 3891/8000  Traning Loss: 92.84018039703369  Train_Reconstruction: 89.31599712371826  Train_KL: 3.5241834223270416  Validation Loss : 92.12586975097656 Val_Reconstruction : 88.63386535644531 Val_KL : 3.492003083229065\n","Epoch: 3892/8000  Traning Loss: 92.34651374816895  Train_Reconstruction: 88.81795978546143  Train_KL: 3.5285544097423553  Validation Loss : 91.8926010131836 Val_Reconstruction : 88.41582489013672 Val_KL : 3.4767757654190063\n","Epoch: 3893/8000  Traning Loss: 92.22886276245117  Train_Reconstruction: 88.72116565704346  Train_KL: 3.507697254419327  Validation Loss : 92.40406799316406 Val_Reconstruction : 88.93546295166016 Val_KL : 3.4686022996902466\n","Epoch: 3894/8000  Traning Loss: 93.19506645202637  Train_Reconstruction: 89.67462539672852  Train_KL: 3.5204407572746277  Validation Loss : 92.64965057373047 Val_Reconstruction : 89.15917205810547 Val_KL : 3.49047589302063\n","Epoch: 3895/8000  Traning Loss: 93.01016330718994  Train_Reconstruction: 89.49033832550049  Train_KL: 3.519824266433716  Validation Loss : 92.02038955688477 Val_Reconstruction : 88.54716873168945 Val_KL : 3.473220705986023\n","Epoch: 3896/8000  Traning Loss: 92.6740493774414  Train_Reconstruction: 89.16553783416748  Train_KL: 3.508512645959854  Validation Loss : 92.22231674194336 Val_Reconstruction : 88.75994110107422 Val_KL : 3.462377429008484\n","Epoch: 3897/8000  Traning Loss: 92.69320106506348  Train_Reconstruction: 89.1813440322876  Train_KL: 3.5118567943573  Validation Loss : 92.3476448059082 Val_Reconstruction : 88.86772918701172 Val_KL : 3.4799141883850098\n","Epoch: 3898/8000  Traning Loss: 92.86300563812256  Train_Reconstruction: 89.33471775054932  Train_KL: 3.5282877683639526  Validation Loss : 92.67092514038086 Val_Reconstruction : 89.18954849243164 Val_KL : 3.4813770055770874\n","Epoch: 3899/8000  Traning Loss: 92.76543140411377  Train_Reconstruction: 89.25068092346191  Train_KL: 3.5147510766983032  Validation Loss : 92.53021240234375 Val_Reconstruction : 89.05531692504883 Val_KL : 3.474893093109131\n","Epoch: 3900/8000  Traning Loss: 92.39822292327881  Train_Reconstruction: 88.88062286376953  Train_KL: 3.5175990164279938  Validation Loss : 91.92609405517578 Val_Reconstruction : 88.45116424560547 Val_KL : 3.474931001663208\n","Epoch: 3901/8000  Traning Loss: 92.27858448028564  Train_Reconstruction: 88.75815105438232  Train_KL: 3.5204340517520905  Validation Loss : 91.98395919799805 Val_Reconstruction : 88.50628662109375 Val_KL : 3.4776722192764282\n","Epoch: 3902/8000  Traning Loss: 92.61958122253418  Train_Reconstruction: 89.10161304473877  Train_KL: 3.517969459295273  Validation Loss : 92.2547721862793 Val_Reconstruction : 88.77676391601562 Val_KL : 3.478009581565857\n","Epoch: 3903/8000  Traning Loss: 92.38854026794434  Train_Reconstruction: 88.8691234588623  Train_KL: 3.519417852163315  Validation Loss : 92.06021499633789 Val_Reconstruction : 88.58382415771484 Val_KL : 3.4763906002044678\n","Epoch: 3904/8000  Traning Loss: 92.20651435852051  Train_Reconstruction: 88.69451141357422  Train_KL: 3.5120045244693756  Validation Loss : 91.92498016357422 Val_Reconstruction : 88.44974899291992 Val_KL : 3.4752310514450073\n","Epoch: 3905/8000  Traning Loss: 92.51948165893555  Train_Reconstruction: 89.00373077392578  Train_KL: 3.5157520473003387  Validation Loss : 92.42750930786133 Val_Reconstruction : 88.95301055908203 Val_KL : 3.474499821662903\n","Epoch: 3906/8000  Traning Loss: 92.81562995910645  Train_Reconstruction: 89.30295276641846  Train_KL: 3.5126765072345734  Validation Loss : 92.79176712036133 Val_Reconstruction : 89.31404495239258 Val_KL : 3.4777203798294067\n","Epoch: 3907/8000  Traning Loss: 93.08256244659424  Train_Reconstruction: 89.56258010864258  Train_KL: 3.5199823081493378  Validation Loss : 92.57206344604492 Val_Reconstruction : 89.0892105102539 Val_KL : 3.4828505516052246\n","Epoch: 3908/8000  Traning Loss: 92.79500961303711  Train_Reconstruction: 89.2787504196167  Train_KL: 3.5162597000598907  Validation Loss : 92.55328750610352 Val_Reconstruction : 89.07516098022461 Val_KL : 3.4781256914138794\n","Epoch: 3909/8000  Traning Loss: 92.58521747589111  Train_Reconstruction: 89.07294368743896  Train_KL: 3.5122735500335693  Validation Loss : 92.64086151123047 Val_Reconstruction : 89.15369415283203 Val_KL : 3.4871702194213867\n","Epoch: 3910/8000  Traning Loss: 92.79604911804199  Train_Reconstruction: 89.26393604278564  Train_KL: 3.5321123600006104  Validation Loss : 92.48785018920898 Val_Reconstruction : 88.99261093139648 Val_KL : 3.495240330696106\n","Epoch: 3911/8000  Traning Loss: 92.60473346710205  Train_Reconstruction: 89.07644748687744  Train_KL: 3.528286784887314  Validation Loss : 92.17441177368164 Val_Reconstruction : 88.69817733764648 Val_KL : 3.476232647895813\n","Epoch: 3912/8000  Traning Loss: 92.5073652267456  Train_Reconstruction: 88.98927688598633  Train_KL: 3.518088787794113  Validation Loss : 92.10062026977539 Val_Reconstruction : 88.62297058105469 Val_KL : 3.4776498079299927\n","Epoch: 3913/8000  Traning Loss: 92.34329319000244  Train_Reconstruction: 88.82883071899414  Train_KL: 3.514463424682617  Validation Loss : 92.07759094238281 Val_Reconstruction : 88.60519409179688 Val_KL : 3.472397565841675\n","Epoch: 3914/8000  Traning Loss: 92.38976383209229  Train_Reconstruction: 88.87738704681396  Train_KL: 3.5123774111270905  Validation Loss : 92.25847244262695 Val_Reconstruction : 88.77893447875977 Val_KL : 3.4795361757278442\n","Epoch: 3915/8000  Traning Loss: 92.38673973083496  Train_Reconstruction: 88.86251544952393  Train_KL: 3.5242244005203247  Validation Loss : 92.25436782836914 Val_Reconstruction : 88.76607131958008 Val_KL : 3.488296151161194\n","Epoch: 3916/8000  Traning Loss: 92.2750186920166  Train_Reconstruction: 88.75475215911865  Train_KL: 3.520265221595764  Validation Loss : 92.14517974853516 Val_Reconstruction : 88.66403579711914 Val_KL : 3.4811408519744873\n","Epoch: 3917/8000  Traning Loss: 92.31134033203125  Train_Reconstruction: 88.80165100097656  Train_KL: 3.5096891820430756  Validation Loss : 91.9996223449707 Val_Reconstruction : 88.5290298461914 Val_KL : 3.4705904722213745\n","Epoch: 3918/8000  Traning Loss: 92.41354560852051  Train_Reconstruction: 88.89711856842041  Train_KL: 3.516428142786026  Validation Loss : 92.19164657592773 Val_Reconstruction : 88.71025085449219 Val_KL : 3.4813953638076782\n","Epoch: 3919/8000  Traning Loss: 92.68277168273926  Train_Reconstruction: 89.1556282043457  Train_KL: 3.5271427929401398  Validation Loss : 92.73318099975586 Val_Reconstruction : 89.24624252319336 Val_KL : 3.4869402647018433\n","Epoch: 3920/8000  Traning Loss: 93.0700798034668  Train_Reconstruction: 89.54804611206055  Train_KL: 3.522032678127289  Validation Loss : 92.87698745727539 Val_Reconstruction : 89.3894157409668 Val_KL : 3.4875718355178833\n","Epoch: 3921/8000  Traning Loss: 93.1345386505127  Train_Reconstruction: 89.6178035736084  Train_KL: 3.516734153032303  Validation Loss : 92.8762321472168 Val_Reconstruction : 89.40256881713867 Val_KL : 3.4736651182174683\n","Epoch: 3922/8000  Traning Loss: 93.31524276733398  Train_Reconstruction: 89.801682472229  Train_KL: 3.513560652732849  Validation Loss : 93.34760284423828 Val_Reconstruction : 89.87284469604492 Val_KL : 3.4747583866119385\n","Epoch: 3923/8000  Traning Loss: 93.87035846710205  Train_Reconstruction: 90.34951496124268  Train_KL: 3.5208432972431183  Validation Loss : 94.09360122680664 Val_Reconstruction : 90.6032829284668 Val_KL : 3.4903173446655273\n","Epoch: 3924/8000  Traning Loss: 93.21736145019531  Train_Reconstruction: 89.69284152984619  Train_KL: 3.524521082639694  Validation Loss : 92.57467269897461 Val_Reconstruction : 89.10042953491211 Val_KL : 3.4742422103881836\n","Epoch: 3925/8000  Traning Loss: 92.5250129699707  Train_Reconstruction: 89.01216125488281  Train_KL: 3.5128518640995026  Validation Loss : 92.12950134277344 Val_Reconstruction : 88.64885711669922 Val_KL : 3.480644464492798\n","Epoch: 3926/8000  Traning Loss: 92.19102001190186  Train_Reconstruction: 88.66987419128418  Train_KL: 3.5211458802223206  Validation Loss : 91.89545440673828 Val_Reconstruction : 88.41143798828125 Val_KL : 3.4840152263641357\n","Epoch: 3927/8000  Traning Loss: 92.08594417572021  Train_Reconstruction: 88.57042694091797  Train_KL: 3.515517830848694  Validation Loss : 91.82126235961914 Val_Reconstruction : 88.34928512573242 Val_KL : 3.471979856491089\n","Epoch: 3928/8000  Traning Loss: 92.28524398803711  Train_Reconstruction: 88.75985908508301  Train_KL: 3.525385946035385  Validation Loss : 92.46977233886719 Val_Reconstruction : 88.97513961791992 Val_KL : 3.4946343898773193\n","Epoch: 3929/8000  Traning Loss: 92.5066785812378  Train_Reconstruction: 88.97200870513916  Train_KL: 3.5346680283546448  Validation Loss : 92.28637313842773 Val_Reconstruction : 88.79486846923828 Val_KL : 3.4915026426315308\n","Epoch: 3930/8000  Traning Loss: 93.22670459747314  Train_Reconstruction: 89.70691299438477  Train_KL: 3.5197910368442535  Validation Loss : 93.63874435424805 Val_Reconstruction : 90.16437530517578 Val_KL : 3.4743666648864746\n","Epoch: 3931/8000  Traning Loss: 93.05223751068115  Train_Reconstruction: 89.54164123535156  Train_KL: 3.510596513748169  Validation Loss : 92.90929794311523 Val_Reconstruction : 89.4215316772461 Val_KL : 3.48776912689209\n","Epoch: 3932/8000  Traning Loss: 92.96115970611572  Train_Reconstruction: 89.43425178527832  Train_KL: 3.52690851688385  Validation Loss : 92.5076904296875 Val_Reconstruction : 89.02207946777344 Val_KL : 3.4856098890304565\n","Epoch: 3933/8000  Traning Loss: 92.78876972198486  Train_Reconstruction: 89.2832441329956  Train_KL: 3.5055248737335205  Validation Loss : 92.59363174438477 Val_Reconstruction : 89.13018798828125 Val_KL : 3.4634435176849365\n","Epoch: 3934/8000  Traning Loss: 92.69115734100342  Train_Reconstruction: 89.17736339569092  Train_KL: 3.51379531621933  Validation Loss : 92.29845428466797 Val_Reconstruction : 88.81243133544922 Val_KL : 3.4860219955444336\n","Epoch: 3935/8000  Traning Loss: 92.86606216430664  Train_Reconstruction: 89.33683109283447  Train_KL: 3.5292300283908844  Validation Loss : 92.46425247192383 Val_Reconstruction : 88.97164916992188 Val_KL : 3.492605447769165\n","Epoch: 3936/8000  Traning Loss: 92.58709049224854  Train_Reconstruction: 89.0647325515747  Train_KL: 3.5223576426506042  Validation Loss : 92.17753982543945 Val_Reconstruction : 88.70426559448242 Val_KL : 3.473277449607849\n","Epoch: 3937/8000  Traning Loss: 92.35251045227051  Train_Reconstruction: 88.83716869354248  Train_KL: 3.515341192483902  Validation Loss : 92.02114486694336 Val_Reconstruction : 88.53442764282227 Val_KL : 3.4867172241210938\n","Epoch: 3938/8000  Traning Loss: 92.64214706420898  Train_Reconstruction: 89.12623691558838  Train_KL: 3.5159098505973816  Validation Loss : 92.60482406616211 Val_Reconstruction : 89.12762451171875 Val_KL : 3.4772002696990967\n","Epoch: 3939/8000  Traning Loss: 92.87622547149658  Train_Reconstruction: 89.358078956604  Train_KL: 3.5181483328342438  Validation Loss : 92.66775894165039 Val_Reconstruction : 89.18333053588867 Val_KL : 3.484429955482483\n","Epoch: 3940/8000  Traning Loss: 92.65335941314697  Train_Reconstruction: 89.14066886901855  Train_KL: 3.5126889050006866  Validation Loss : 92.45340347290039 Val_Reconstruction : 88.98337936401367 Val_KL : 3.470025062561035\n","Epoch: 3941/8000  Traning Loss: 92.52963924407959  Train_Reconstruction: 89.00975894927979  Train_KL: 3.519879162311554  Validation Loss : 92.37746047973633 Val_Reconstruction : 88.89569854736328 Val_KL : 3.4817596673965454\n","Epoch: 3942/8000  Traning Loss: 92.83469486236572  Train_Reconstruction: 89.32068347930908  Train_KL: 3.514010727405548  Validation Loss : 92.64306259155273 Val_Reconstruction : 89.17767333984375 Val_KL : 3.4653875827789307\n","Epoch: 3943/8000  Traning Loss: 92.79612922668457  Train_Reconstruction: 89.28849411010742  Train_KL: 3.5076355040073395  Validation Loss : 92.36080932617188 Val_Reconstruction : 88.88452529907227 Val_KL : 3.476283073425293\n","Epoch: 3944/8000  Traning Loss: 92.58680629730225  Train_Reconstruction: 89.07276248931885  Train_KL: 3.514043241739273  Validation Loss : 92.5155029296875 Val_Reconstruction : 89.04212188720703 Val_KL : 3.4733827114105225\n","Epoch: 3945/8000  Traning Loss: 92.45949745178223  Train_Reconstruction: 88.94726657867432  Train_KL: 3.5122320353984833  Validation Loss : 92.12166976928711 Val_Reconstruction : 88.64786911010742 Val_KL : 3.473803162574768\n","Epoch: 3946/8000  Traning Loss: 92.31181049346924  Train_Reconstruction: 88.79816722869873  Train_KL: 3.5136430859565735  Validation Loss : 92.00376892089844 Val_Reconstruction : 88.51912307739258 Val_KL : 3.484647274017334\n","Epoch: 3947/8000  Traning Loss: 92.20313358306885  Train_Reconstruction: 88.67003631591797  Train_KL: 3.533096969127655  Validation Loss : 91.95942306518555 Val_Reconstruction : 88.45934295654297 Val_KL : 3.5000802278518677\n","Epoch: 3948/8000  Traning Loss: 92.55670833587646  Train_Reconstruction: 89.03307628631592  Train_KL: 3.523632735013962  Validation Loss : 92.53748321533203 Val_Reconstruction : 89.0677604675293 Val_KL : 3.469719409942627\n","Epoch: 3949/8000  Traning Loss: 93.13361549377441  Train_Reconstruction: 89.6267204284668  Train_KL: 3.506894290447235  Validation Loss : 93.35411071777344 Val_Reconstruction : 89.893310546875 Val_KL : 3.4608007669448853\n","Epoch: 3950/8000  Traning Loss: 93.66422080993652  Train_Reconstruction: 90.14756679534912  Train_KL: 3.5166529715061188  Validation Loss : 93.254150390625 Val_Reconstruction : 89.76977157592773 Val_KL : 3.484378695487976\n","Epoch: 3951/8000  Traning Loss: 93.23521041870117  Train_Reconstruction: 89.70896434783936  Train_KL: 3.5262467861175537  Validation Loss : 93.33202362060547 Val_Reconstruction : 89.85749816894531 Val_KL : 3.474525570869446\n","Epoch: 3952/8000  Traning Loss: 93.01917362213135  Train_Reconstruction: 89.50292301177979  Train_KL: 3.516250103712082  Validation Loss : 92.40011596679688 Val_Reconstruction : 88.91564178466797 Val_KL : 3.4844751358032227\n","Epoch: 3953/8000  Traning Loss: 92.79063415527344  Train_Reconstruction: 89.27014446258545  Train_KL: 3.520488828420639  Validation Loss : 92.20603561401367 Val_Reconstruction : 88.72503280639648 Val_KL : 3.4810025691986084\n","Epoch: 3954/8000  Traning Loss: 92.31096649169922  Train_Reconstruction: 88.79167366027832  Train_KL: 3.5192925333976746  Validation Loss : 92.09239196777344 Val_Reconstruction : 88.61063385009766 Val_KL : 3.4817566871643066\n","Epoch: 3955/8000  Traning Loss: 92.27684307098389  Train_Reconstruction: 88.76272106170654  Train_KL: 3.514121502637863  Validation Loss : 91.95898056030273 Val_Reconstruction : 88.4864273071289 Val_KL : 3.472552537918091\n","Epoch: 3956/8000  Traning Loss: 92.37961387634277  Train_Reconstruction: 88.86601829528809  Train_KL: 3.5135959684848785  Validation Loss : 92.12112426757812 Val_Reconstruction : 88.64776992797852 Val_KL : 3.4733567237854004\n","Epoch: 3957/8000  Traning Loss: 92.25253772735596  Train_Reconstruction: 88.72929859161377  Train_KL: 3.5232392251491547  Validation Loss : 91.80240249633789 Val_Reconstruction : 88.31293106079102 Val_KL : 3.48947012424469\n","Epoch: 3958/8000  Traning Loss: 92.36398887634277  Train_Reconstruction: 88.83251476287842  Train_KL: 3.5314758121967316  Validation Loss : 92.49494171142578 Val_Reconstruction : 89.0009651184082 Val_KL : 3.493974447250366\n","Epoch: 3959/8000  Traning Loss: 92.35620403289795  Train_Reconstruction: 88.83748722076416  Train_KL: 3.518717259168625  Validation Loss : 92.02651596069336 Val_Reconstruction : 88.55223846435547 Val_KL : 3.4742767810821533\n","Epoch: 3960/8000  Traning Loss: 92.37607097625732  Train_Reconstruction: 88.85716819763184  Train_KL: 3.518901526927948  Validation Loss : 91.82358932495117 Val_Reconstruction : 88.33675384521484 Val_KL : 3.486836075782776\n","Epoch: 3961/8000  Traning Loss: 92.21743869781494  Train_Reconstruction: 88.6896104812622  Train_KL: 3.527828127145767  Validation Loss : 91.94091415405273 Val_Reconstruction : 88.46084976196289 Val_KL : 3.480064034461975\n","Epoch: 3962/8000  Traning Loss: 92.14186477661133  Train_Reconstruction: 88.62746620178223  Train_KL: 3.514398694038391  Validation Loss : 91.92445373535156 Val_Reconstruction : 88.44507217407227 Val_KL : 3.4793792963027954\n","Epoch: 3963/8000  Traning Loss: 92.16223812103271  Train_Reconstruction: 88.63975715637207  Train_KL: 3.522481471300125  Validation Loss : 92.04095840454102 Val_Reconstruction : 88.55749893188477 Val_KL : 3.483459234237671\n","Epoch: 3964/8000  Traning Loss: 92.22007751464844  Train_Reconstruction: 88.69762706756592  Train_KL: 3.5224512219429016  Validation Loss : 92.15070343017578 Val_Reconstruction : 88.66972351074219 Val_KL : 3.480982184410095\n","Epoch: 3965/8000  Traning Loss: 92.45876121520996  Train_Reconstruction: 88.94111061096191  Train_KL: 3.5176504850387573  Validation Loss : 92.25454711914062 Val_Reconstruction : 88.78746032714844 Val_KL : 3.4670854806900024\n","Epoch: 3966/8000  Traning Loss: 92.7291030883789  Train_Reconstruction: 89.22143077850342  Train_KL: 3.507670819759369  Validation Loss : 92.24282455444336 Val_Reconstruction : 88.77775573730469 Val_KL : 3.465067148208618\n","Epoch: 3967/8000  Traning Loss: 92.41177082061768  Train_Reconstruction: 88.90315818786621  Train_KL: 3.508612334728241  Validation Loss : 92.28972625732422 Val_Reconstruction : 88.82892990112305 Val_KL : 3.4607959985733032\n","Epoch: 3968/8000  Traning Loss: 92.24360370635986  Train_Reconstruction: 88.73129940032959  Train_KL: 3.5123040080070496  Validation Loss : 92.22395324707031 Val_Reconstruction : 88.75231170654297 Val_KL : 3.471639394760132\n","Epoch: 3969/8000  Traning Loss: 92.21674823760986  Train_Reconstruction: 88.69886684417725  Train_KL: 3.5178818106651306  Validation Loss : 91.94517517089844 Val_Reconstruction : 88.4658317565918 Val_KL : 3.479340672492981\n","Epoch: 3970/8000  Traning Loss: 92.0790958404541  Train_Reconstruction: 88.55863857269287  Train_KL: 3.5204574167728424  Validation Loss : 92.02867889404297 Val_Reconstruction : 88.54438400268555 Val_KL : 3.484295129776001\n","Epoch: 3971/8000  Traning Loss: 92.2035779953003  Train_Reconstruction: 88.67600154876709  Train_KL: 3.5275752246379852  Validation Loss : 91.93747329711914 Val_Reconstruction : 88.45064926147461 Val_KL : 3.4868215322494507\n","Epoch: 3972/8000  Traning Loss: 92.29982089996338  Train_Reconstruction: 88.78047943115234  Train_KL: 3.519340395927429  Validation Loss : 92.15422058105469 Val_Reconstruction : 88.68961715698242 Val_KL : 3.4646016359329224\n","Epoch: 3973/8000  Traning Loss: 92.39892864227295  Train_Reconstruction: 88.89302730560303  Train_KL: 3.5059016048908234  Validation Loss : 92.51499938964844 Val_Reconstruction : 89.04733657836914 Val_KL : 3.4676624536514282\n","Epoch: 3974/8000  Traning Loss: 92.73401641845703  Train_Reconstruction: 89.21741199493408  Train_KL: 3.5166049003601074  Validation Loss : 92.328369140625 Val_Reconstruction : 88.8498649597168 Val_KL : 3.4785056114196777\n","Epoch: 3975/8000  Traning Loss: 92.87141513824463  Train_Reconstruction: 89.35598564147949  Train_KL: 3.5154281854629517  Validation Loss : 92.41688537597656 Val_Reconstruction : 88.94818496704102 Val_KL : 3.4686992168426514\n","Epoch: 3976/8000  Traning Loss: 92.88215732574463  Train_Reconstruction: 89.36634063720703  Train_KL: 3.515816777944565  Validation Loss : 92.54951095581055 Val_Reconstruction : 89.06665420532227 Val_KL : 3.482856869697571\n","Epoch: 3977/8000  Traning Loss: 92.91201210021973  Train_Reconstruction: 89.39187145233154  Train_KL: 3.5201407074928284  Validation Loss : 92.50594711303711 Val_Reconstruction : 89.02760314941406 Val_KL : 3.4783469438552856\n","Epoch: 3978/8000  Traning Loss: 92.70442581176758  Train_Reconstruction: 89.18795013427734  Train_KL: 3.516475886106491  Validation Loss : 92.32285690307617 Val_Reconstruction : 88.85351181030273 Val_KL : 3.4693435430526733\n","Epoch: 3979/8000  Traning Loss: 93.01025104522705  Train_Reconstruction: 89.49284553527832  Train_KL: 3.5174054503440857  Validation Loss : 93.02893829345703 Val_Reconstruction : 89.55515670776367 Val_KL : 3.473781704902649\n","Epoch: 3980/8000  Traning Loss: 93.7599925994873  Train_Reconstruction: 90.24274063110352  Train_KL: 3.517251282930374  Validation Loss : 93.79675674438477 Val_Reconstruction : 90.31261825561523 Val_KL : 3.48413622379303\n","Epoch: 3981/8000  Traning Loss: 94.53870868682861  Train_Reconstruction: 91.0187873840332  Train_KL: 3.519921511411667  Validation Loss : 94.26808547973633 Val_Reconstruction : 90.79183197021484 Val_KL : 3.4762516021728516\n","Epoch: 3982/8000  Traning Loss: 93.83353519439697  Train_Reconstruction: 90.31870460510254  Train_KL: 3.514830708503723  Validation Loss : 93.32237243652344 Val_Reconstruction : 89.84358596801758 Val_KL : 3.47878634929657\n","Epoch: 3983/8000  Traning Loss: 93.40488052368164  Train_Reconstruction: 89.89115333557129  Train_KL: 3.5137282609939575  Validation Loss : 92.8370590209961 Val_Reconstruction : 89.36616516113281 Val_KL : 3.470894694328308\n","Epoch: 3984/8000  Traning Loss: 92.72576141357422  Train_Reconstruction: 89.21640396118164  Train_KL: 3.5093584656715393  Validation Loss : 92.35654830932617 Val_Reconstruction : 88.88700866699219 Val_KL : 3.469539165496826\n","Epoch: 3985/8000  Traning Loss: 92.52489280700684  Train_Reconstruction: 89.01244735717773  Train_KL: 3.5124454498291016  Validation Loss : 92.39731979370117 Val_Reconstruction : 88.91631317138672 Val_KL : 3.481007218360901\n","Epoch: 3986/8000  Traning Loss: 92.55948066711426  Train_Reconstruction: 89.04559898376465  Train_KL: 3.5138823091983795  Validation Loss : 92.3385009765625 Val_Reconstruction : 88.86567306518555 Val_KL : 3.4728307723999023\n","Epoch: 3987/8000  Traning Loss: 92.52438259124756  Train_Reconstruction: 89.01393413543701  Train_KL: 3.5104483366012573  Validation Loss : 92.29524230957031 Val_Reconstruction : 88.8244514465332 Val_KL : 3.4707902669906616\n","Epoch: 3988/8000  Traning Loss: 92.40960025787354  Train_Reconstruction: 88.90049362182617  Train_KL: 3.509106755256653  Validation Loss : 91.9062614440918 Val_Reconstruction : 88.43619918823242 Val_KL : 3.4700613021850586\n","Epoch: 3989/8000  Traning Loss: 92.30934524536133  Train_Reconstruction: 88.80142498016357  Train_KL: 3.507921427488327  Validation Loss : 92.06663131713867 Val_Reconstruction : 88.58731842041016 Val_KL : 3.4793132543563843\n","Epoch: 3990/8000  Traning Loss: 92.48659324645996  Train_Reconstruction: 88.95804691314697  Train_KL: 3.5285458266735077  Validation Loss : 92.41524124145508 Val_Reconstruction : 88.92700958251953 Val_KL : 3.488229513168335\n","Epoch: 3991/8000  Traning Loss: 92.55580806732178  Train_Reconstruction: 89.03552532196045  Train_KL: 3.520282417535782  Validation Loss : 92.29414749145508 Val_Reconstruction : 88.82876205444336 Val_KL : 3.4653868675231934\n","Epoch: 3992/8000  Traning Loss: 92.526198387146  Train_Reconstruction: 89.01444149017334  Train_KL: 3.511756807565689  Validation Loss : 91.88248825073242 Val_Reconstruction : 88.40053939819336 Val_KL : 3.481949806213379\n","Epoch: 3993/8000  Traning Loss: 92.22602558135986  Train_Reconstruction: 88.69894695281982  Train_KL: 3.527079224586487  Validation Loss : 92.2091293334961 Val_Reconstruction : 88.71563339233398 Val_KL : 3.493494987487793\n","Epoch: 3994/8000  Traning Loss: 92.19875907897949  Train_Reconstruction: 88.67906951904297  Train_KL: 3.519690304994583  Validation Loss : 91.94512176513672 Val_Reconstruction : 88.47185516357422 Val_KL : 3.4732662439346313\n","Epoch: 3995/8000  Traning Loss: 92.35115718841553  Train_Reconstruction: 88.83434104919434  Train_KL: 3.5168173909187317  Validation Loss : 92.17815017700195 Val_Reconstruction : 88.6965217590332 Val_KL : 3.4816296100616455\n","Epoch: 3996/8000  Traning Loss: 92.45177173614502  Train_Reconstruction: 88.93585014343262  Train_KL: 3.515921652317047  Validation Loss : 92.18217086791992 Val_Reconstruction : 88.71354675292969 Val_KL : 3.4686241149902344\n","Epoch: 3997/8000  Traning Loss: 92.60227394104004  Train_Reconstruction: 89.0893383026123  Train_KL: 3.5129360258579254  Validation Loss : 92.5739860534668 Val_Reconstruction : 89.09828567504883 Val_KL : 3.4756994247436523\n","Epoch: 3998/8000  Traning Loss: 92.72456169128418  Train_Reconstruction: 89.19704818725586  Train_KL: 3.527513772249222  Validation Loss : 92.1777229309082 Val_Reconstruction : 88.68889617919922 Val_KL : 3.4888248443603516\n","Epoch: 3999/8000  Traning Loss: 92.7298231124878  Train_Reconstruction: 89.2091588973999  Train_KL: 3.520664781332016  Validation Loss : 92.58021545410156 Val_Reconstruction : 89.10566329956055 Val_KL : 3.4745535850524902\n","Epoch: 4000/8000  Traning Loss: 92.62504386901855  Train_Reconstruction: 89.11879539489746  Train_KL: 3.5062490105628967  Validation Loss : 92.90172576904297 Val_Reconstruction : 89.43450927734375 Val_KL : 3.4672176837921143\n","Epoch: 4001/8000  Traning Loss: 92.7702522277832  Train_Reconstruction: 89.24824333190918  Train_KL: 3.522008389234543  Validation Loss : 92.47710418701172 Val_Reconstruction : 88.98101425170898 Val_KL : 3.4960930347442627\n","Epoch: 4002/8000  Traning Loss: 92.46123600006104  Train_Reconstruction: 88.9292345046997  Train_KL: 3.5320010781288147  Validation Loss : 92.25982666015625 Val_Reconstruction : 88.77511596679688 Val_KL : 3.4847136735916138\n","Epoch: 4003/8000  Traning Loss: 92.37344455718994  Train_Reconstruction: 88.8579511642456  Train_KL: 3.5154931247234344  Validation Loss : 91.84965133666992 Val_Reconstruction : 88.36812210083008 Val_KL : 3.4815304279327393\n","Epoch: 4004/8000  Traning Loss: 92.30375385284424  Train_Reconstruction: 88.777419090271  Train_KL: 3.526335060596466  Validation Loss : 92.34344482421875 Val_Reconstruction : 88.85837173461914 Val_KL : 3.4850735664367676\n","Epoch: 4005/8000  Traning Loss: 92.52174091339111  Train_Reconstruction: 89.0037431716919  Train_KL: 3.5179976522922516  Validation Loss : 92.50179672241211 Val_Reconstruction : 89.03121185302734 Val_KL : 3.4705846309661865\n","Epoch: 4006/8000  Traning Loss: 93.11387538909912  Train_Reconstruction: 89.59608268737793  Train_KL: 3.5177932381629944  Validation Loss : 92.18039321899414 Val_Reconstruction : 88.70198822021484 Val_KL : 3.478406071662903\n","Epoch: 4007/8000  Traning Loss: 92.22346210479736  Train_Reconstruction: 88.70446586608887  Train_KL: 3.518996298313141  Validation Loss : 91.92065048217773 Val_Reconstruction : 88.44833755493164 Val_KL : 3.4723126888275146\n","Epoch: 4008/8000  Traning Loss: 92.42661762237549  Train_Reconstruction: 88.90724182128906  Train_KL: 3.519376277923584  Validation Loss : 92.11513900756836 Val_Reconstruction : 88.62644577026367 Val_KL : 3.488695502281189\n","Epoch: 4009/8000  Traning Loss: 92.57900714874268  Train_Reconstruction: 89.04781436920166  Train_KL: 3.531191200017929  Validation Loss : 92.1916618347168 Val_Reconstruction : 88.70866012573242 Val_KL : 3.4830000400543213\n","Epoch: 4010/8000  Traning Loss: 92.72293758392334  Train_Reconstruction: 89.20537090301514  Train_KL: 3.5175647735595703  Validation Loss : 92.38149642944336 Val_Reconstruction : 88.90680694580078 Val_KL : 3.4746912717819214\n","Epoch: 4011/8000  Traning Loss: 92.30152130126953  Train_Reconstruction: 88.79059314727783  Train_KL: 3.5109287202358246  Validation Loss : 91.97274398803711 Val_Reconstruction : 88.49488067626953 Val_KL : 3.4778647422790527\n","Epoch: 4012/8000  Traning Loss: 92.33006381988525  Train_Reconstruction: 88.81541538238525  Train_KL: 3.5146477222442627  Validation Loss : 92.20207214355469 Val_Reconstruction : 88.71767807006836 Val_KL : 3.4843902587890625\n","Epoch: 4013/8000  Traning Loss: 92.1191177368164  Train_Reconstruction: 88.6014928817749  Train_KL: 3.5176244378089905  Validation Loss : 91.7201919555664 Val_Reconstruction : 88.24480438232422 Val_KL : 3.4753884077072144\n","Epoch: 4014/8000  Traning Loss: 92.12837505340576  Train_Reconstruction: 88.6127405166626  Train_KL: 3.515633851289749  Validation Loss : 91.79753112792969 Val_Reconstruction : 88.31508255004883 Val_KL : 3.4824471473693848\n","Epoch: 4015/8000  Traning Loss: 92.18929767608643  Train_Reconstruction: 88.66431140899658  Train_KL: 3.524987369775772  Validation Loss : 91.93795013427734 Val_Reconstruction : 88.45489883422852 Val_KL : 3.483052372932434\n","Epoch: 4016/8000  Traning Loss: 91.99267101287842  Train_Reconstruction: 88.4684247970581  Train_KL: 3.5242463648319244  Validation Loss : 91.96269989013672 Val_Reconstruction : 88.47051620483398 Val_KL : 3.4921826124191284\n","Epoch: 4017/8000  Traning Loss: 92.13975524902344  Train_Reconstruction: 88.61051177978516  Train_KL: 3.529243379831314  Validation Loss : 92.06135940551758 Val_Reconstruction : 88.56916046142578 Val_KL : 3.4921971559524536\n","Epoch: 4018/8000  Traning Loss: 92.37038803100586  Train_Reconstruction: 88.84805393218994  Train_KL: 3.5223337411880493  Validation Loss : 92.20154571533203 Val_Reconstruction : 88.72765731811523 Val_KL : 3.473887801170349\n","Epoch: 4019/8000  Traning Loss: 92.75951385498047  Train_Reconstruction: 89.25260639190674  Train_KL: 3.506907194852829  Validation Loss : 92.5241470336914 Val_Reconstruction : 89.05289077758789 Val_KL : 3.4712588787078857\n","Epoch: 4020/8000  Traning Loss: 92.42309761047363  Train_Reconstruction: 88.90296268463135  Train_KL: 3.5201360285282135  Validation Loss : 92.0946159362793 Val_Reconstruction : 88.61201858520508 Val_KL : 3.4825985431671143\n","Epoch: 4021/8000  Traning Loss: 92.01864528656006  Train_Reconstruction: 88.50060367584229  Train_KL: 3.5180422961711884  Validation Loss : 91.60529327392578 Val_Reconstruction : 88.13343048095703 Val_KL : 3.471863865852356\n","Epoch: 4022/8000  Traning Loss: 92.06872367858887  Train_Reconstruction: 88.54518604278564  Train_KL: 3.5235370099544525  Validation Loss : 91.79365539550781 Val_Reconstruction : 88.31416320800781 Val_KL : 3.4794929027557373\n","Epoch: 4023/8000  Traning Loss: 92.40615844726562  Train_Reconstruction: 88.88659763336182  Train_KL: 3.5195618867874146  Validation Loss : 92.19386291503906 Val_Reconstruction : 88.71648788452148 Val_KL : 3.477376103401184\n","Epoch: 4024/8000  Traning Loss: 92.40672969818115  Train_Reconstruction: 88.89726734161377  Train_KL: 3.5094619691371918  Validation Loss : 91.85195541381836 Val_Reconstruction : 88.38066101074219 Val_KL : 3.4712958335876465\n","Epoch: 4025/8000  Traning Loss: 92.53148365020752  Train_Reconstruction: 89.01870536804199  Train_KL: 3.5127796828746796  Validation Loss : 92.39325714111328 Val_Reconstruction : 88.91459655761719 Val_KL : 3.478658676147461\n","Epoch: 4026/8000  Traning Loss: 92.61010932922363  Train_Reconstruction: 89.08848571777344  Train_KL: 3.5216238498687744  Validation Loss : 92.21790313720703 Val_Reconstruction : 88.72857284545898 Val_KL : 3.48932945728302\n","Epoch: 4027/8000  Traning Loss: 92.54328441619873  Train_Reconstruction: 89.01556873321533  Train_KL: 3.527715802192688  Validation Loss : 92.02859115600586 Val_Reconstruction : 88.54302978515625 Val_KL : 3.4855597019195557\n","Epoch: 4028/8000  Traning Loss: 92.18986511230469  Train_Reconstruction: 88.66752338409424  Train_KL: 3.522342711687088  Validation Loss : 91.7739143371582 Val_Reconstruction : 88.29599380493164 Val_KL : 3.477922558784485\n","Epoch: 4029/8000  Traning Loss: 92.27539920806885  Train_Reconstruction: 88.75990962982178  Train_KL: 3.515488862991333  Validation Loss : 91.97575759887695 Val_Reconstruction : 88.50629806518555 Val_KL : 3.4694602489471436\n","Epoch: 4030/8000  Traning Loss: 92.59963130950928  Train_Reconstruction: 89.09222030639648  Train_KL: 3.5074101090431213  Validation Loss : 92.23541641235352 Val_Reconstruction : 88.76448059082031 Val_KL : 3.4709346294403076\n","Epoch: 4031/8000  Traning Loss: 92.80526351928711  Train_Reconstruction: 89.28185176849365  Train_KL: 3.5234128534793854  Validation Loss : 92.80343246459961 Val_Reconstruction : 89.31436157226562 Val_KL : 3.489074230194092\n","Epoch: 4032/8000  Traning Loss: 92.66361999511719  Train_Reconstruction: 89.13276195526123  Train_KL: 3.530858337879181  Validation Loss : 92.30182647705078 Val_Reconstruction : 88.82205581665039 Val_KL : 3.4797714948654175\n","Epoch: 4033/8000  Traning Loss: 92.71128845214844  Train_Reconstruction: 89.19577980041504  Train_KL: 3.5155082643032074  Validation Loss : 92.29299926757812 Val_Reconstruction : 88.81867599487305 Val_KL : 3.474321961402893\n","Epoch: 4034/8000  Traning Loss: 92.57268333435059  Train_Reconstruction: 89.05416488647461  Train_KL: 3.5185176730155945  Validation Loss : 92.0102653503418 Val_Reconstruction : 88.52875900268555 Val_KL : 3.4815083742141724\n","Epoch: 4035/8000  Traning Loss: 92.7877426147461  Train_Reconstruction: 89.27139854431152  Train_KL: 3.5163437724113464  Validation Loss : 92.68975067138672 Val_Reconstruction : 89.21693801879883 Val_KL : 3.4728143215179443\n","Epoch: 4036/8000  Traning Loss: 92.38167953491211  Train_Reconstruction: 88.86704444885254  Train_KL: 3.5146345496177673  Validation Loss : 91.6146011352539 Val_Reconstruction : 88.14101791381836 Val_KL : 3.4735844135284424\n","Epoch: 4037/8000  Traning Loss: 92.01869583129883  Train_Reconstruction: 88.50918292999268  Train_KL: 3.5095109045505524  Validation Loss : 91.84561157226562 Val_Reconstruction : 88.37214279174805 Val_KL : 3.4734688997268677\n","Epoch: 4038/8000  Traning Loss: 92.5045223236084  Train_Reconstruction: 88.98373985290527  Train_KL: 3.520783543586731  Validation Loss : 92.74963760375977 Val_Reconstruction : 89.26908111572266 Val_KL : 3.480557680130005\n","Epoch: 4039/8000  Traning Loss: 92.93033599853516  Train_Reconstruction: 89.40271663665771  Train_KL: 3.5276185870170593  Validation Loss : 92.77009201049805 Val_Reconstruction : 89.28009796142578 Val_KL : 3.489993095397949\n","Epoch: 4040/8000  Traning Loss: 93.43807697296143  Train_Reconstruction: 89.91635799407959  Train_KL: 3.5217176973819733  Validation Loss : 93.58130264282227 Val_Reconstruction : 90.10631942749023 Val_KL : 3.4749830961227417\n","Epoch: 4041/8000  Traning Loss: 93.13220596313477  Train_Reconstruction: 89.61680507659912  Train_KL: 3.515401691198349  Validation Loss : 93.06173706054688 Val_Reconstruction : 89.58571243286133 Val_KL : 3.476027250289917\n","Epoch: 4042/8000  Traning Loss: 92.53716278076172  Train_Reconstruction: 89.02478504180908  Train_KL: 3.5123771727085114  Validation Loss : 92.36902618408203 Val_Reconstruction : 88.89722061157227 Val_KL : 3.471803665161133\n","Epoch: 4043/8000  Traning Loss: 92.38915348052979  Train_Reconstruction: 88.87388134002686  Train_KL: 3.5152717232704163  Validation Loss : 92.02912902832031 Val_Reconstruction : 88.54950332641602 Val_KL : 3.479628562927246\n","Epoch: 4044/8000  Traning Loss: 92.5152702331543  Train_Reconstruction: 88.9938497543335  Train_KL: 3.521421194076538  Validation Loss : 92.314453125 Val_Reconstruction : 88.83734893798828 Val_KL : 3.4771060943603516\n","Epoch: 4045/8000  Traning Loss: 92.84620380401611  Train_Reconstruction: 89.33390712738037  Train_KL: 3.512297213077545  Validation Loss : 93.11712265014648 Val_Reconstruction : 89.64806747436523 Val_KL : 3.4690542221069336\n","Epoch: 4046/8000  Traning Loss: 93.24405479431152  Train_Reconstruction: 89.73232078552246  Train_KL: 3.5117352306842804  Validation Loss : 93.25032043457031 Val_Reconstruction : 89.77503967285156 Val_KL : 3.475280523300171\n","Epoch: 4047/8000  Traning Loss: 93.5081033706665  Train_Reconstruction: 89.98373126983643  Train_KL: 3.524372786283493  Validation Loss : 93.10123443603516 Val_Reconstruction : 89.61349487304688 Val_KL : 3.4877398014068604\n","Epoch: 4048/8000  Traning Loss: 92.94341278076172  Train_Reconstruction: 89.42204189300537  Train_KL: 3.5213711261749268  Validation Loss : 92.7757682800293 Val_Reconstruction : 89.29927444458008 Val_KL : 3.476494312286377\n","Epoch: 4049/8000  Traning Loss: 92.64398384094238  Train_Reconstruction: 89.12918090820312  Train_KL: 3.5148019194602966  Validation Loss : 92.42252731323242 Val_Reconstruction : 88.94561767578125 Val_KL : 3.4769095182418823\n","Epoch: 4050/8000  Traning Loss: 92.41793823242188  Train_Reconstruction: 88.90555191040039  Train_KL: 3.5123859643936157  Validation Loss : 92.20212936401367 Val_Reconstruction : 88.73261642456055 Val_KL : 3.4695119857788086\n","Epoch: 4051/8000  Traning Loss: 92.37615585327148  Train_Reconstruction: 88.85415458679199  Train_KL: 3.5220006108283997  Validation Loss : 92.3698844909668 Val_Reconstruction : 88.88461685180664 Val_KL : 3.485266327857971\n","Epoch: 4052/8000  Traning Loss: 92.43799209594727  Train_Reconstruction: 88.9169225692749  Train_KL: 3.5210688412189484  Validation Loss : 91.95141983032227 Val_Reconstruction : 88.48127746582031 Val_KL : 3.4701424837112427\n","Epoch: 4053/8000  Traning Loss: 92.49731922149658  Train_Reconstruction: 88.98786449432373  Train_KL: 3.509453773498535  Validation Loss : 92.22185134887695 Val_Reconstruction : 88.75333786010742 Val_KL : 3.4685128927230835\n","Epoch: 4054/8000  Traning Loss: 92.68521785736084  Train_Reconstruction: 89.16983699798584  Train_KL: 3.5153805315494537  Validation Loss : 92.6893539428711 Val_Reconstruction : 89.2028694152832 Val_KL : 3.486483097076416\n","Epoch: 4055/8000  Traning Loss: 92.86540412902832  Train_Reconstruction: 89.34591770172119  Train_KL: 3.5194852352142334  Validation Loss : 92.38097763061523 Val_Reconstruction : 88.90049362182617 Val_KL : 3.4804844856262207\n","Epoch: 4056/8000  Traning Loss: 92.38875389099121  Train_Reconstruction: 88.86771965026855  Train_KL: 3.521034300327301  Validation Loss : 91.9810562133789 Val_Reconstruction : 88.50072860717773 Val_KL : 3.480326533317566\n","Epoch: 4057/8000  Traning Loss: 92.42888259887695  Train_Reconstruction: 88.90324974060059  Train_KL: 3.5256336331367493  Validation Loss : 92.20317459106445 Val_Reconstruction : 88.71067428588867 Val_KL : 3.4924991130828857\n","Epoch: 4058/8000  Traning Loss: 92.46434307098389  Train_Reconstruction: 88.94143486022949  Train_KL: 3.522908002138138  Validation Loss : 92.22140884399414 Val_Reconstruction : 88.73988342285156 Val_KL : 3.481528162956238\n","Epoch: 4059/8000  Traning Loss: 92.46066951751709  Train_Reconstruction: 88.9446439743042  Train_KL: 3.5160249769687653  Validation Loss : 92.29035949707031 Val_Reconstruction : 88.80994033813477 Val_KL : 3.48042094707489\n","Epoch: 4060/8000  Traning Loss: 92.34416580200195  Train_Reconstruction: 88.81941223144531  Train_KL: 3.5247529447078705  Validation Loss : 91.70200729370117 Val_Reconstruction : 88.21489715576172 Val_KL : 3.487108588218689\n","Epoch: 4061/8000  Traning Loss: 92.16161346435547  Train_Reconstruction: 88.64455890655518  Train_KL: 3.517053544521332  Validation Loss : 91.69821166992188 Val_Reconstruction : 88.2262077331543 Val_KL : 3.4720041751861572\n","Epoch: 4062/8000  Traning Loss: 92.06612014770508  Train_Reconstruction: 88.55564403533936  Train_KL: 3.5104754269123077  Validation Loss : 91.85303115844727 Val_Reconstruction : 88.3774185180664 Val_KL : 3.4756112098693848\n","Epoch: 4063/8000  Traning Loss: 92.28439044952393  Train_Reconstruction: 88.7660436630249  Train_KL: 3.5183467864990234  Validation Loss : 92.56293869018555 Val_Reconstruction : 89.0745735168457 Val_KL : 3.4883657693862915\n","Epoch: 4064/8000  Traning Loss: 92.24640560150146  Train_Reconstruction: 88.72508430480957  Train_KL: 3.521322399377823  Validation Loss : 92.10128402709961 Val_Reconstruction : 88.62213134765625 Val_KL : 3.4791537523269653\n","Epoch: 4065/8000  Traning Loss: 92.68123722076416  Train_Reconstruction: 89.16876983642578  Train_KL: 3.512468606233597  Validation Loss : 92.47665405273438 Val_Reconstruction : 88.99671936035156 Val_KL : 3.479936122894287\n","Epoch: 4066/8000  Traning Loss: 92.31668949127197  Train_Reconstruction: 88.7908878326416  Train_KL: 3.525801718235016  Validation Loss : 91.80873489379883 Val_Reconstruction : 88.3152847290039 Val_KL : 3.49345064163208\n","Epoch: 4067/8000  Traning Loss: 92.18731689453125  Train_Reconstruction: 88.67068767547607  Train_KL: 3.5166288018226624  Validation Loss : 91.94341278076172 Val_Reconstruction : 88.47049331665039 Val_KL : 3.47292160987854\n","Epoch: 4068/8000  Traning Loss: 92.40750503540039  Train_Reconstruction: 88.88650321960449  Train_KL: 3.521001845598221  Validation Loss : 92.41529083251953 Val_Reconstruction : 88.93717956542969 Val_KL : 3.478110194206238\n","Epoch: 4069/8000  Traning Loss: 92.58061122894287  Train_Reconstruction: 89.06209087371826  Train_KL: 3.518521159887314  Validation Loss : 92.31252670288086 Val_Reconstruction : 88.83058547973633 Val_KL : 3.481942653656006\n","Epoch: 4070/8000  Traning Loss: 92.08932113647461  Train_Reconstruction: 88.55982971191406  Train_KL: 3.5294910073280334  Validation Loss : 91.5474853515625 Val_Reconstruction : 88.05095672607422 Val_KL : 3.4965298175811768\n","Epoch: 4071/8000  Traning Loss: 92.12408828735352  Train_Reconstruction: 88.59554767608643  Train_KL: 3.5285402834415436  Validation Loss : 92.51402282714844 Val_Reconstruction : 89.03623962402344 Val_KL : 3.4777809381484985\n","Epoch: 4072/8000  Traning Loss: 92.5899076461792  Train_Reconstruction: 89.08300876617432  Train_KL: 3.506898820400238  Validation Loss : 92.81806564331055 Val_Reconstruction : 89.36201095581055 Val_KL : 3.4560524225234985\n","Epoch: 4073/8000  Traning Loss: 92.52567863464355  Train_Reconstruction: 89.02200698852539  Train_KL: 3.503671884536743  Validation Loss : 92.25911331176758 Val_Reconstruction : 88.78976058959961 Val_KL : 3.469352960586548\n","Epoch: 4074/8000  Traning Loss: 92.30111503601074  Train_Reconstruction: 88.77840518951416  Train_KL: 3.522711008787155  Validation Loss : 91.95320892333984 Val_Reconstruction : 88.47206115722656 Val_KL : 3.4811480045318604\n","Epoch: 4075/8000  Traning Loss: 92.05642318725586  Train_Reconstruction: 88.53842735290527  Train_KL: 3.5179950892925262  Validation Loss : 91.8243408203125 Val_Reconstruction : 88.34705352783203 Val_KL : 3.4772855043411255\n","Epoch: 4076/8000  Traning Loss: 92.2946367263794  Train_Reconstruction: 88.77136898040771  Train_KL: 3.5232681334018707  Validation Loss : 92.33161926269531 Val_Reconstruction : 88.84278106689453 Val_KL : 3.48883855342865\n","Epoch: 4077/8000  Traning Loss: 92.83413314819336  Train_Reconstruction: 89.31195163726807  Train_KL: 3.522181451320648  Validation Loss : 92.39844131469727 Val_Reconstruction : 88.92247009277344 Val_KL : 3.475972294807434\n","Epoch: 4078/8000  Traning Loss: 92.29878902435303  Train_Reconstruction: 88.77696895599365  Train_KL: 3.5218178927898407  Validation Loss : 91.94990921020508 Val_Reconstruction : 88.47260665893555 Val_KL : 3.4773056507110596\n","Epoch: 4079/8000  Traning Loss: 92.2692461013794  Train_Reconstruction: 88.74913597106934  Train_KL: 3.520110011100769  Validation Loss : 92.2994155883789 Val_Reconstruction : 88.8303451538086 Val_KL : 3.4690715074539185\n","Epoch: 4080/8000  Traning Loss: 92.64480590820312  Train_Reconstruction: 89.13164806365967  Train_KL: 3.513158440589905  Validation Loss : 92.45656204223633 Val_Reconstruction : 88.98028564453125 Val_KL : 3.476278305053711\n","Epoch: 4081/8000  Traning Loss: 92.6784143447876  Train_Reconstruction: 89.15448188781738  Train_KL: 3.523933470249176  Validation Loss : 92.0294189453125 Val_Reconstruction : 88.54389572143555 Val_KL : 3.4855228662490845\n","Epoch: 4082/8000  Traning Loss: 92.31872081756592  Train_Reconstruction: 88.80163764953613  Train_KL: 3.517082780599594  Validation Loss : 92.07701110839844 Val_Reconstruction : 88.60622787475586 Val_KL : 3.4707834720611572\n","Epoch: 4083/8000  Traning Loss: 92.22209548950195  Train_Reconstruction: 88.70969486236572  Train_KL: 3.5124009549617767  Validation Loss : 92.20657348632812 Val_Reconstruction : 88.72578430175781 Val_KL : 3.480791449546814\n","Epoch: 4084/8000  Traning Loss: 92.24673748016357  Train_Reconstruction: 88.71864318847656  Train_KL: 3.5280933678150177  Validation Loss : 91.88933563232422 Val_Reconstruction : 88.39908599853516 Val_KL : 3.4902503490448\n","Epoch: 4085/8000  Traning Loss: 92.56836223602295  Train_Reconstruction: 89.04321670532227  Train_KL: 3.5251452028751373  Validation Loss : 92.40579223632812 Val_Reconstruction : 88.91711044311523 Val_KL : 3.48868191242218\n","Epoch: 4086/8000  Traning Loss: 93.2286605834961  Train_Reconstruction: 89.7165584564209  Train_KL: 3.512102246284485  Validation Loss : 93.18894958496094 Val_Reconstruction : 89.72161865234375 Val_KL : 3.467329263687134\n","Epoch: 4087/8000  Traning Loss: 93.16857433319092  Train_Reconstruction: 89.65838527679443  Train_KL: 3.5101892352104187  Validation Loss : 92.5921630859375 Val_Reconstruction : 89.10812377929688 Val_KL : 3.4840370416641235\n","Epoch: 4088/8000  Traning Loss: 92.39995956420898  Train_Reconstruction: 88.87284660339355  Train_KL: 3.527111202478409  Validation Loss : 92.0363655090332 Val_Reconstruction : 88.54100036621094 Val_KL : 3.495365262031555\n","Epoch: 4089/8000  Traning Loss: 92.15735149383545  Train_Reconstruction: 88.6334753036499  Train_KL: 3.523875743150711  Validation Loss : 91.95344161987305 Val_Reconstruction : 88.47175979614258 Val_KL : 3.4816797971725464\n","Epoch: 4090/8000  Traning Loss: 92.18181610107422  Train_Reconstruction: 88.65719318389893  Train_KL: 3.524623781442642  Validation Loss : 91.89749908447266 Val_Reconstruction : 88.40695571899414 Val_KL : 3.4905428886413574\n","Epoch: 4091/8000  Traning Loss: 92.08895015716553  Train_Reconstruction: 88.566725730896  Train_KL: 3.522225797176361  Validation Loss : 91.79143142700195 Val_Reconstruction : 88.31340026855469 Val_KL : 3.47802996635437\n","Epoch: 4092/8000  Traning Loss: 92.3172721862793  Train_Reconstruction: 88.80195426940918  Train_KL: 3.5153170824050903  Validation Loss : 92.09230422973633 Val_Reconstruction : 88.60755157470703 Val_KL : 3.4847521781921387\n","Epoch: 4093/8000  Traning Loss: 92.31783390045166  Train_Reconstruction: 88.79323387145996  Train_KL: 3.524601489305496  Validation Loss : 92.03515625 Val_Reconstruction : 88.54788589477539 Val_KL : 3.4872684478759766\n","Epoch: 4094/8000  Traning Loss: 92.35851860046387  Train_Reconstruction: 88.85161209106445  Train_KL: 3.5069062411785126  Validation Loss : 92.3275260925293 Val_Reconstruction : 88.85873794555664 Val_KL : 3.4687851667404175\n","Epoch: 4095/8000  Traning Loss: 92.59167289733887  Train_Reconstruction: 89.08183193206787  Train_KL: 3.509841114282608  Validation Loss : 92.52151489257812 Val_Reconstruction : 89.049072265625 Val_KL : 3.472442626953125\n","Epoch: 4096/8000  Traning Loss: 92.91300868988037  Train_Reconstruction: 89.40222930908203  Train_KL: 3.5107787549495697  Validation Loss : 92.50418472290039 Val_Reconstruction : 89.03220748901367 Val_KL : 3.4719746112823486\n","Epoch: 4097/8000  Traning Loss: 92.72837543487549  Train_Reconstruction: 89.20924949645996  Train_KL: 3.5191251635551453  Validation Loss : 92.33075332641602 Val_Reconstruction : 88.84316635131836 Val_KL : 3.4875845909118652\n","Epoch: 4098/8000  Traning Loss: 92.44590187072754  Train_Reconstruction: 88.92679786682129  Train_KL: 3.5191027224063873  Validation Loss : 91.95674133300781 Val_Reconstruction : 88.48876190185547 Val_KL : 3.4679800271987915\n","Epoch: 4099/8000  Traning Loss: 92.17532730102539  Train_Reconstruction: 88.66619396209717  Train_KL: 3.5091337263584137  Validation Loss : 91.93936538696289 Val_Reconstruction : 88.46390914916992 Val_KL : 3.475456714630127\n","Epoch: 4100/8000  Traning Loss: 92.14969730377197  Train_Reconstruction: 88.63333988189697  Train_KL: 3.516357958316803  Validation Loss : 92.03223419189453 Val_Reconstruction : 88.55582809448242 Val_KL : 3.476406693458557\n","Epoch: 4101/8000  Traning Loss: 92.50695323944092  Train_Reconstruction: 88.98752117156982  Train_KL: 3.519432097673416  Validation Loss : 92.37129974365234 Val_Reconstruction : 88.88010025024414 Val_KL : 3.491201162338257\n","Epoch: 4102/8000  Traning Loss: 92.7184362411499  Train_Reconstruction: 89.1909990310669  Train_KL: 3.5274374783039093  Validation Loss : 92.24913024902344 Val_Reconstruction : 88.75347900390625 Val_KL : 3.4956512451171875\n","Epoch: 4103/8000  Traning Loss: 92.74057579040527  Train_Reconstruction: 89.21803760528564  Train_KL: 3.522537797689438  Validation Loss : 92.99057388305664 Val_Reconstruction : 89.51643753051758 Val_KL : 3.4741334915161133\n","Epoch: 4104/8000  Traning Loss: 92.54042911529541  Train_Reconstruction: 89.02873516082764  Train_KL: 3.511692315340042  Validation Loss : 92.17065811157227 Val_Reconstruction : 88.69840621948242 Val_KL : 3.4722509384155273\n","Epoch: 4105/8000  Traning Loss: 92.86578178405762  Train_Reconstruction: 89.35221290588379  Train_KL: 3.513568162918091  Validation Loss : 93.09806060791016 Val_Reconstruction : 89.6153450012207 Val_KL : 3.4827159643173218\n","Epoch: 4106/8000  Traning Loss: 93.67177486419678  Train_Reconstruction: 90.14924907684326  Train_KL: 3.5225255489349365  Validation Loss : 93.88327026367188 Val_Reconstruction : 90.39740753173828 Val_KL : 3.4858633279800415\n","Epoch: 4107/8000  Traning Loss: 92.9369649887085  Train_Reconstruction: 89.41396713256836  Train_KL: 3.522996723651886  Validation Loss : 92.28833770751953 Val_Reconstruction : 88.79982376098633 Val_KL : 3.4885122776031494\n","Epoch: 4108/8000  Traning Loss: 92.08126831054688  Train_Reconstruction: 88.56393146514893  Train_KL: 3.5173372328281403  Validation Loss : 91.66587448120117 Val_Reconstruction : 88.18650436401367 Val_KL : 3.4793686866760254\n","Epoch: 4109/8000  Traning Loss: 92.0823564529419  Train_Reconstruction: 88.56940841674805  Train_KL: 3.5129485726356506  Validation Loss : 91.83299255371094 Val_Reconstruction : 88.35221099853516 Val_KL : 3.4807807207107544\n","Epoch: 4110/8000  Traning Loss: 92.3769588470459  Train_Reconstruction: 88.8602237701416  Train_KL: 3.5167344212532043  Validation Loss : 92.21794128417969 Val_Reconstruction : 88.73585891723633 Val_KL : 3.4820839166641235\n","Epoch: 4111/8000  Traning Loss: 92.32498359680176  Train_Reconstruction: 88.80609035491943  Train_KL: 3.518892467021942  Validation Loss : 92.3011703491211 Val_Reconstruction : 88.82136917114258 Val_KL : 3.4798011779785156\n","Epoch: 4112/8000  Traning Loss: 92.65679454803467  Train_Reconstruction: 89.13558292388916  Train_KL: 3.52121165394783  Validation Loss : 92.55202102661133 Val_Reconstruction : 89.06028747558594 Val_KL : 3.4917306900024414\n","Epoch: 4113/8000  Traning Loss: 92.69756031036377  Train_Reconstruction: 89.16872787475586  Train_KL: 3.528831899166107  Validation Loss : 92.26821899414062 Val_Reconstruction : 88.7781982421875 Val_KL : 3.490019917488098\n","Epoch: 4114/8000  Traning Loss: 92.45296096801758  Train_Reconstruction: 88.93681240081787  Train_KL: 3.5161488950252533  Validation Loss : 92.05654907226562 Val_Reconstruction : 88.58070755004883 Val_KL : 3.475840449333191\n","Epoch: 4115/8000  Traning Loss: 92.3297872543335  Train_Reconstruction: 88.82424640655518  Train_KL: 3.505540370941162  Validation Loss : 92.24199295043945 Val_Reconstruction : 88.77678680419922 Val_KL : 3.465206503868103\n","Epoch: 4116/8000  Traning Loss: 92.77167987823486  Train_Reconstruction: 89.26409816741943  Train_KL: 3.507581889629364  Validation Loss : 93.14433670043945 Val_Reconstruction : 89.67255020141602 Val_KL : 3.4717893600463867\n","Epoch: 4117/8000  Traning Loss: 93.13545322418213  Train_Reconstruction: 89.6192398071289  Train_KL: 3.516213297843933  Validation Loss : 92.76744842529297 Val_Reconstruction : 89.2874641418457 Val_KL : 3.479983329772949\n","Epoch: 4118/8000  Traning Loss: 92.32194137573242  Train_Reconstruction: 88.81173706054688  Train_KL: 3.5102037489414215  Validation Loss : 91.59671020507812 Val_Reconstruction : 88.12696075439453 Val_KL : 3.469747304916382\n","Epoch: 4119/8000  Traning Loss: 91.88942909240723  Train_Reconstruction: 88.36875438690186  Train_KL: 3.520675301551819  Validation Loss : 91.87031936645508 Val_Reconstruction : 88.39009094238281 Val_KL : 3.4802294969558716\n","Epoch: 4120/8000  Traning Loss: 92.26736736297607  Train_Reconstruction: 88.7413330078125  Train_KL: 3.526034653186798  Validation Loss : 92.0790786743164 Val_Reconstruction : 88.5926513671875 Val_KL : 3.486430048942566\n","Epoch: 4121/8000  Traning Loss: 92.56908226013184  Train_Reconstruction: 89.04889106750488  Train_KL: 3.5201915204524994  Validation Loss : 92.30791854858398 Val_Reconstruction : 88.83808898925781 Val_KL : 3.4698292016983032\n","Epoch: 4122/8000  Traning Loss: 92.66525745391846  Train_Reconstruction: 89.15096855163574  Train_KL: 3.5142891108989716  Validation Loss : 92.18591690063477 Val_Reconstruction : 88.70794677734375 Val_KL : 3.4779690504074097\n","Epoch: 4123/8000  Traning Loss: 92.493971824646  Train_Reconstruction: 88.9831771850586  Train_KL: 3.5107929706573486  Validation Loss : 92.40641021728516 Val_Reconstruction : 88.93962478637695 Val_KL : 3.4667856693267822\n","Epoch: 4124/8000  Traning Loss: 92.68591403961182  Train_Reconstruction: 89.18202209472656  Train_KL: 3.503892183303833  Validation Loss : 92.43693161010742 Val_Reconstruction : 88.96808624267578 Val_KL : 3.468845009803772\n","Epoch: 4125/8000  Traning Loss: 92.96687602996826  Train_Reconstruction: 89.45258045196533  Train_KL: 3.5142961144447327  Validation Loss : 92.86063003540039 Val_Reconstruction : 89.38464736938477 Val_KL : 3.4759832620620728\n","Epoch: 4126/8000  Traning Loss: 93.1589002609253  Train_Reconstruction: 89.63959407806396  Train_KL: 3.5193066895008087  Validation Loss : 93.42301559448242 Val_Reconstruction : 89.94411849975586 Val_KL : 3.478897452354431\n","Epoch: 4127/8000  Traning Loss: 93.062087059021  Train_Reconstruction: 89.54829025268555  Train_KL: 3.513797491788864  Validation Loss : 92.35693740844727 Val_Reconstruction : 88.88298416137695 Val_KL : 3.4739508628845215\n","Epoch: 4128/8000  Traning Loss: 92.32717037200928  Train_Reconstruction: 88.80941963195801  Train_KL: 3.5177519023418427  Validation Loss : 92.08907318115234 Val_Reconstruction : 88.60877227783203 Val_KL : 3.480300784111023\n","Epoch: 4129/8000  Traning Loss: 92.1004114151001  Train_Reconstruction: 88.57400894165039  Train_KL: 3.526403099298477  Validation Loss : 92.0122299194336 Val_Reconstruction : 88.5260238647461 Val_KL : 3.4862070083618164\n","Epoch: 4130/8000  Traning Loss: 91.99038219451904  Train_Reconstruction: 88.46400451660156  Train_KL: 3.5263777375221252  Validation Loss : 91.81987762451172 Val_Reconstruction : 88.34106826782227 Val_KL : 3.4788076877593994\n","Epoch: 4131/8000  Traning Loss: 92.12645435333252  Train_Reconstruction: 88.61526489257812  Train_KL: 3.511188507080078  Validation Loss : 91.80606842041016 Val_Reconstruction : 88.33974838256836 Val_KL : 3.466317653656006\n","Epoch: 4132/8000  Traning Loss: 91.9401626586914  Train_Reconstruction: 88.42427635192871  Train_KL: 3.5158861577510834  Validation Loss : 91.69488143920898 Val_Reconstruction : 88.20940780639648 Val_KL : 3.485471725463867\n","Epoch: 4133/8000  Traning Loss: 92.2630386352539  Train_Reconstruction: 88.73755931854248  Train_KL: 3.525479733943939  Validation Loss : 92.11880874633789 Val_Reconstruction : 88.63740158081055 Val_KL : 3.4814064502716064\n","Epoch: 4134/8000  Traning Loss: 92.45602512359619  Train_Reconstruction: 88.93203353881836  Train_KL: 3.52399218082428  Validation Loss : 92.51241302490234 Val_Reconstruction : 89.04129409790039 Val_KL : 3.4711196422576904\n","Epoch: 4135/8000  Traning Loss: 92.06450176239014  Train_Reconstruction: 88.54533767700195  Train_KL: 3.519164562225342  Validation Loss : 91.76085662841797 Val_Reconstruction : 88.28366470336914 Val_KL : 3.4771931171417236\n","Epoch: 4136/8000  Traning Loss: 92.26868915557861  Train_Reconstruction: 88.74521064758301  Train_KL: 3.5234798192977905  Validation Loss : 92.25084686279297 Val_Reconstruction : 88.76963806152344 Val_KL : 3.481208562850952\n","Epoch: 4137/8000  Traning Loss: 92.31342124938965  Train_Reconstruction: 88.79672431945801  Train_KL: 3.516696512699127  Validation Loss : 92.16539764404297 Val_Reconstruction : 88.68113708496094 Val_KL : 3.484259247779846\n","Epoch: 4138/8000  Traning Loss: 92.20981216430664  Train_Reconstruction: 88.68838310241699  Train_KL: 3.5214294493198395  Validation Loss : 92.20140838623047 Val_Reconstruction : 88.72075653076172 Val_KL : 3.4806522130966187\n","Epoch: 4139/8000  Traning Loss: 91.99503231048584  Train_Reconstruction: 88.46870708465576  Train_KL: 3.5263250172138214  Validation Loss : 92.00067138671875 Val_Reconstruction : 88.50657272338867 Val_KL : 3.4940985441207886\n","Epoch: 4140/8000  Traning Loss: 92.26630878448486  Train_Reconstruction: 88.74155616760254  Train_KL: 3.5247530341148376  Validation Loss : 92.45414733886719 Val_Reconstruction : 88.97296142578125 Val_KL : 3.4811854362487793\n","Epoch: 4141/8000  Traning Loss: 92.51931190490723  Train_Reconstruction: 89.00813388824463  Train_KL: 3.5111784636974335  Validation Loss : 92.93093490600586 Val_Reconstruction : 89.46048736572266 Val_KL : 3.4704450368881226\n","Epoch: 4142/8000  Traning Loss: 93.0397596359253  Train_Reconstruction: 89.5287094116211  Train_KL: 3.511050283908844  Validation Loss : 92.96287536621094 Val_Reconstruction : 89.49475860595703 Val_KL : 3.4681167602539062\n","Epoch: 4143/8000  Traning Loss: 92.90255355834961  Train_Reconstruction: 89.39359283447266  Train_KL: 3.508960723876953  Validation Loss : 92.76691818237305 Val_Reconstruction : 89.29451370239258 Val_KL : 3.472403407096863\n","Epoch: 4144/8000  Traning Loss: 92.98369026184082  Train_Reconstruction: 89.47059726715088  Train_KL: 3.5130920112133026  Validation Loss : 92.51405715942383 Val_Reconstruction : 89.03695678710938 Val_KL : 3.4771032333374023\n","Epoch: 4145/8000  Traning Loss: 92.7878007888794  Train_Reconstruction: 89.27035331726074  Train_KL: 3.5174472332000732  Validation Loss : 92.73442077636719 Val_Reconstruction : 89.2537841796875 Val_KL : 3.480638265609741\n","Epoch: 4146/8000  Traning Loss: 92.49048042297363  Train_Reconstruction: 88.97056293487549  Train_KL: 3.519915610551834  Validation Loss : 92.21401596069336 Val_Reconstruction : 88.73614120483398 Val_KL : 3.477871894836426\n","Epoch: 4147/8000  Traning Loss: 92.68779277801514  Train_Reconstruction: 89.17311763763428  Train_KL: 3.5146749019622803  Validation Loss : 92.12187576293945 Val_Reconstruction : 88.64064025878906 Val_KL : 3.4812350273132324\n","Epoch: 4148/8000  Traning Loss: 92.42087459564209  Train_Reconstruction: 88.90411376953125  Train_KL: 3.516760468482971  Validation Loss : 92.21644592285156 Val_Reconstruction : 88.73721313476562 Val_KL : 3.4792298078536987\n","Epoch: 4149/8000  Traning Loss: 92.32529830932617  Train_Reconstruction: 88.81436443328857  Train_KL: 3.5109336972236633  Validation Loss : 92.15547180175781 Val_Reconstruction : 88.68014144897461 Val_KL : 3.4753299951553345\n","Epoch: 4150/8000  Traning Loss: 92.32973194122314  Train_Reconstruction: 88.81361103057861  Train_KL: 3.516120970249176  Validation Loss : 92.0012321472168 Val_Reconstruction : 88.52443313598633 Val_KL : 3.476800560951233\n","Epoch: 4151/8000  Traning Loss: 92.03589153289795  Train_Reconstruction: 88.51772594451904  Train_KL: 3.5181664526462555  Validation Loss : 91.80177307128906 Val_Reconstruction : 88.32417297363281 Val_KL : 3.4776004552841187\n","Epoch: 4152/8000  Traning Loss: 91.84961605072021  Train_Reconstruction: 88.3355073928833  Train_KL: 3.514109253883362  Validation Loss : 91.54056549072266 Val_Reconstruction : 88.07384490966797 Val_KL : 3.4667216539382935\n","Epoch: 4153/8000  Traning Loss: 91.75364112854004  Train_Reconstruction: 88.24386405944824  Train_KL: 3.509776383638382  Validation Loss : 91.69073104858398 Val_Reconstruction : 88.22237777709961 Val_KL : 3.468353509902954\n","Epoch: 4154/8000  Traning Loss: 91.81376647949219  Train_Reconstruction: 88.29466342926025  Train_KL: 3.5191031992435455  Validation Loss : 91.55436706542969 Val_Reconstruction : 88.07258605957031 Val_KL : 3.4817826747894287\n","Epoch: 4155/8000  Traning Loss: 92.02365398406982  Train_Reconstruction: 88.51043319702148  Train_KL: 3.513219565153122  Validation Loss : 91.7054443359375 Val_Reconstruction : 88.2315788269043 Val_KL : 3.473869204521179\n","Epoch: 4156/8000  Traning Loss: 92.05060291290283  Train_Reconstruction: 88.53245449066162  Train_KL: 3.5181485414505005  Validation Loss : 92.1135368347168 Val_Reconstruction : 88.62554550170898 Val_KL : 3.4879902601242065\n","Epoch: 4157/8000  Traning Loss: 92.03786849975586  Train_Reconstruction: 88.51555156707764  Train_KL: 3.522317498922348  Validation Loss : 91.9101448059082 Val_Reconstruction : 88.4373893737793 Val_KL : 3.4727574586868286\n","Epoch: 4158/8000  Traning Loss: 92.03975486755371  Train_Reconstruction: 88.52722835540771  Train_KL: 3.5125253200531006  Validation Loss : 91.8749008178711 Val_Reconstruction : 88.39429473876953 Val_KL : 3.480605125427246\n","Epoch: 4159/8000  Traning Loss: 92.08905029296875  Train_Reconstruction: 88.56433200836182  Train_KL: 3.5247181057929993  Validation Loss : 92.15584182739258 Val_Reconstruction : 88.66973876953125 Val_KL : 3.4861021041870117\n","Epoch: 4160/8000  Traning Loss: 92.21400737762451  Train_Reconstruction: 88.69700145721436  Train_KL: 3.51700496673584  Validation Loss : 91.85003280639648 Val_Reconstruction : 88.37466430664062 Val_KL : 3.475367546081543\n","Epoch: 4161/8000  Traning Loss: 92.21570205688477  Train_Reconstruction: 88.69625473022461  Train_KL: 3.51944699883461  Validation Loss : 91.99877548217773 Val_Reconstruction : 88.5118293762207 Val_KL : 3.48694384098053\n","Epoch: 4162/8000  Traning Loss: 92.1440782546997  Train_Reconstruction: 88.61841678619385  Train_KL: 3.525661140680313  Validation Loss : 91.741943359375 Val_Reconstruction : 88.25542831420898 Val_KL : 3.486514449119568\n","Epoch: 4163/8000  Traning Loss: 92.16986274719238  Train_Reconstruction: 88.6549711227417  Train_KL: 3.5148920714855194  Validation Loss : 92.04859924316406 Val_Reconstruction : 88.57410430908203 Val_KL : 3.4744940996170044\n","Epoch: 4164/8000  Traning Loss: 92.18806743621826  Train_Reconstruction: 88.66441917419434  Train_KL: 3.5236491560935974  Validation Loss : 92.20219039916992 Val_Reconstruction : 88.70663833618164 Val_KL : 3.495552182197571\n","Epoch: 4165/8000  Traning Loss: 92.65800666809082  Train_Reconstruction: 89.12258243560791  Train_KL: 3.535425513982773  Validation Loss : 92.5975570678711 Val_Reconstruction : 89.11225509643555 Val_KL : 3.4853029251098633\n","Epoch: 4166/8000  Traning Loss: 92.6155309677124  Train_Reconstruction: 89.10060024261475  Train_KL: 3.514931559562683  Validation Loss : 92.5809326171875 Val_Reconstruction : 89.10706329345703 Val_KL : 3.473869800567627\n","Epoch: 4167/8000  Traning Loss: 92.7599458694458  Train_Reconstruction: 89.24621200561523  Train_KL: 3.5137330293655396  Validation Loss : 92.5652084350586 Val_Reconstruction : 89.08670425415039 Val_KL : 3.478505849838257\n","Epoch: 4168/8000  Traning Loss: 92.58735466003418  Train_Reconstruction: 89.06662940979004  Train_KL: 3.520725190639496  Validation Loss : 92.21757888793945 Val_Reconstruction : 88.73529815673828 Val_KL : 3.4822804927825928\n","Epoch: 4169/8000  Traning Loss: 92.26401233673096  Train_Reconstruction: 88.74123001098633  Train_KL: 3.52278271317482  Validation Loss : 91.8885612487793 Val_Reconstruction : 88.39791870117188 Val_KL : 3.490643620491028\n","Epoch: 4170/8000  Traning Loss: 92.01727962493896  Train_Reconstruction: 88.49736881256104  Train_KL: 3.5199103355407715  Validation Loss : 91.89656066894531 Val_Reconstruction : 88.4207763671875 Val_KL : 3.4757853746414185\n","Epoch: 4171/8000  Traning Loss: 92.12830543518066  Train_Reconstruction: 88.61405944824219  Train_KL: 3.5142470002174377  Validation Loss : 91.8036003112793 Val_Reconstruction : 88.32896041870117 Val_KL : 3.4746415615081787\n","Epoch: 4172/8000  Traning Loss: 92.50906276702881  Train_Reconstruction: 88.99577045440674  Train_KL: 3.5132913291454315  Validation Loss : 92.1004524230957 Val_Reconstruction : 88.62306594848633 Val_KL : 3.4773879051208496\n","Epoch: 4173/8000  Traning Loss: 92.53925228118896  Train_Reconstruction: 89.02609539031982  Train_KL: 3.5131557881832123  Validation Loss : 92.0985107421875 Val_Reconstruction : 88.62281036376953 Val_KL : 3.4756999015808105\n","Epoch: 4174/8000  Traning Loss: 92.21404933929443  Train_Reconstruction: 88.69748210906982  Train_KL: 3.5165667831897736  Validation Loss : 91.68536376953125 Val_Reconstruction : 88.20752334594727 Val_KL : 3.4778432846069336\n","Epoch: 4175/8000  Traning Loss: 91.87145233154297  Train_Reconstruction: 88.35116577148438  Train_KL: 3.520284742116928  Validation Loss : 91.89590454101562 Val_Reconstruction : 88.41292953491211 Val_KL : 3.4829742908477783\n","Epoch: 4176/8000  Traning Loss: 92.70918941497803  Train_Reconstruction: 89.19486331939697  Train_KL: 3.5143266916275024  Validation Loss : 92.79497909545898 Val_Reconstruction : 89.32194137573242 Val_KL : 3.4730384349823\n","Epoch: 4177/8000  Traning Loss: 93.34353923797607  Train_Reconstruction: 89.83092594146729  Train_KL: 3.512614130973816  Validation Loss : 93.84562683105469 Val_Reconstruction : 90.36747741699219 Val_KL : 3.4781482219696045\n","Epoch: 4178/8000  Traning Loss: 93.42758369445801  Train_Reconstruction: 89.90660572052002  Train_KL: 3.520977884531021  Validation Loss : 92.8050308227539 Val_Reconstruction : 89.32279205322266 Val_KL : 3.4822393655776978\n","Epoch: 4179/8000  Traning Loss: 92.548903465271  Train_Reconstruction: 89.02826690673828  Train_KL: 3.520637333393097  Validation Loss : 92.17269515991211 Val_Reconstruction : 88.69472885131836 Val_KL : 3.4779648780822754\n","Epoch: 4180/8000  Traning Loss: 92.2166109085083  Train_Reconstruction: 88.70062446594238  Train_KL: 3.5159855782985687  Validation Loss : 92.19684219360352 Val_Reconstruction : 88.71980667114258 Val_KL : 3.4770320653915405\n","Epoch: 4181/8000  Traning Loss: 92.04153156280518  Train_Reconstruction: 88.52267837524414  Train_KL: 3.5188539922237396  Validation Loss : 91.82490921020508 Val_Reconstruction : 88.3498649597168 Val_KL : 3.475044369697571\n","Epoch: 4182/8000  Traning Loss: 92.00737953186035  Train_Reconstruction: 88.48798274993896  Train_KL: 3.5193986892700195  Validation Loss : 91.79373168945312 Val_Reconstruction : 88.31986236572266 Val_KL : 3.4738693237304688\n","Epoch: 4183/8000  Traning Loss: 92.3254623413086  Train_Reconstruction: 88.81412887573242  Train_KL: 3.511332720518112  Validation Loss : 91.90822982788086 Val_Reconstruction : 88.4283676147461 Val_KL : 3.479864478111267\n","Epoch: 4184/8000  Traning Loss: 92.24831485748291  Train_Reconstruction: 88.73154067993164  Train_KL: 3.516772747039795  Validation Loss : 92.24647903442383 Val_Reconstruction : 88.77218627929688 Val_KL : 3.474294662475586\n","Epoch: 4185/8000  Traning Loss: 92.4101915359497  Train_Reconstruction: 88.9048957824707  Train_KL: 3.505296468734741  Validation Loss : 92.13897705078125 Val_Reconstruction : 88.6702766418457 Val_KL : 3.4687029123306274\n","Epoch: 4186/8000  Traning Loss: 92.22352886199951  Train_Reconstruction: 88.70978164672852  Train_KL: 3.513746678829193  Validation Loss : 92.19673156738281 Val_Reconstruction : 88.71667861938477 Val_KL : 3.4800539016723633\n","Epoch: 4187/8000  Traning Loss: 92.44845676422119  Train_Reconstruction: 88.91300106048584  Train_KL: 3.5354556143283844  Validation Loss : 92.32107925415039 Val_Reconstruction : 88.82193756103516 Val_KL : 3.499143958091736\n","Epoch: 4188/8000  Traning Loss: 92.37045955657959  Train_Reconstruction: 88.83888530731201  Train_KL: 3.5315748155117035  Validation Loss : 91.8646011352539 Val_Reconstruction : 88.38333129882812 Val_KL : 3.4812710285186768\n","Epoch: 4189/8000  Traning Loss: 92.04641819000244  Train_Reconstruction: 88.53130531311035  Train_KL: 3.5151138603687286  Validation Loss : 91.52896499633789 Val_Reconstruction : 88.05402374267578 Val_KL : 3.4749414920806885\n","Epoch: 4190/8000  Traning Loss: 92.12515830993652  Train_Reconstruction: 88.60500717163086  Train_KL: 3.5201508700847626  Validation Loss : 92.17763900756836 Val_Reconstruction : 88.68513488769531 Val_KL : 3.492504596710205\n","Epoch: 4191/8000  Traning Loss: 92.28028774261475  Train_Reconstruction: 88.75093460083008  Train_KL: 3.5293537080287933  Validation Loss : 92.02959060668945 Val_Reconstruction : 88.54037857055664 Val_KL : 3.489213705062866\n","Epoch: 4192/8000  Traning Loss: 92.66122150421143  Train_Reconstruction: 89.14161777496338  Train_KL: 3.5196053087711334  Validation Loss : 93.04748916625977 Val_Reconstruction : 89.57264709472656 Val_KL : 3.4748443365097046\n","Epoch: 4193/8000  Traning Loss: 92.66205883026123  Train_Reconstruction: 89.1430721282959  Train_KL: 3.5189874470233917  Validation Loss : 92.07747268676758 Val_Reconstruction : 88.60062026977539 Val_KL : 3.4768550395965576\n","Epoch: 4194/8000  Traning Loss: 92.2615909576416  Train_Reconstruction: 88.7503890991211  Train_KL: 3.511200964450836  Validation Loss : 92.1359748840332 Val_Reconstruction : 88.66049194335938 Val_KL : 3.475480079650879\n","Epoch: 4195/8000  Traning Loss: 92.74860858917236  Train_Reconstruction: 89.23022174835205  Train_KL: 3.5183852314949036  Validation Loss : 93.11756134033203 Val_Reconstruction : 89.63457489013672 Val_KL : 3.4829864501953125\n","Epoch: 4196/8000  Traning Loss: 92.58182525634766  Train_Reconstruction: 89.06797122955322  Train_KL: 3.513853996992111  Validation Loss : 92.09137725830078 Val_Reconstruction : 88.61395645141602 Val_KL : 3.4774175882339478\n","Epoch: 4197/8000  Traning Loss: 92.12631416320801  Train_Reconstruction: 88.61254024505615  Train_KL: 3.5137745141983032  Validation Loss : 92.14381790161133 Val_Reconstruction : 88.66025161743164 Val_KL : 3.483566999435425\n","Epoch: 4198/8000  Traning Loss: 92.11412143707275  Train_Reconstruction: 88.58254337310791  Train_KL: 3.531576782464981  Validation Loss : 91.86643981933594 Val_Reconstruction : 88.3700065612793 Val_KL : 3.4964332580566406\n","Epoch: 4199/8000  Traning Loss: 92.22503185272217  Train_Reconstruction: 88.70451831817627  Train_KL: 3.5205143094062805  Validation Loss : 92.42027282714844 Val_Reconstruction : 88.9454460144043 Val_KL : 3.474825620651245\n","Epoch: 4200/8000  Traning Loss: 92.5790491104126  Train_Reconstruction: 89.07169723510742  Train_KL: 3.5073517560958862  Validation Loss : 92.34154510498047 Val_Reconstruction : 88.86960220336914 Val_KL : 3.471943974494934\n","Epoch: 4201/8000  Traning Loss: 92.86960411071777  Train_Reconstruction: 89.3552417755127  Train_KL: 3.514360189437866  Validation Loss : 92.8980484008789 Val_Reconstruction : 89.4159164428711 Val_KL : 3.482130527496338\n","Epoch: 4202/8000  Traning Loss: 92.77235889434814  Train_Reconstruction: 89.24654388427734  Train_KL: 3.5258160829544067  Validation Loss : 92.23675918579102 Val_Reconstruction : 88.74367904663086 Val_KL : 3.493081569671631\n","Epoch: 4203/8000  Traning Loss: 91.85888862609863  Train_Reconstruction: 88.33603954315186  Train_KL: 3.5228488445281982  Validation Loss : 91.4658317565918 Val_Reconstruction : 87.98089599609375 Val_KL : 3.4849348068237305\n","Epoch: 4204/8000  Traning Loss: 91.77203273773193  Train_Reconstruction: 88.25225448608398  Train_KL: 3.5197775959968567  Validation Loss : 91.50116348266602 Val_Reconstruction : 88.02513885498047 Val_KL : 3.4760241508483887\n","Epoch: 4205/8000  Traning Loss: 92.2399206161499  Train_Reconstruction: 88.72370529174805  Train_KL: 3.5162146389484406  Validation Loss : 91.9677963256836 Val_Reconstruction : 88.48947143554688 Val_KL : 3.4783226251602173\n","Epoch: 4206/8000  Traning Loss: 92.05112838745117  Train_Reconstruction: 88.52876567840576  Train_KL: 3.5223620235919952  Validation Loss : 91.82647323608398 Val_Reconstruction : 88.34734344482422 Val_KL : 3.4791276454925537\n","Epoch: 4207/8000  Traning Loss: 92.58951568603516  Train_Reconstruction: 89.06859588623047  Train_KL: 3.5209203958511353  Validation Loss : 92.63126754760742 Val_Reconstruction : 89.1376953125 Val_KL : 3.493569493293762\n","Epoch: 4208/8000  Traning Loss: 92.62797546386719  Train_Reconstruction: 89.10160636901855  Train_KL: 3.526369720697403  Validation Loss : 92.36040115356445 Val_Reconstruction : 88.87973022460938 Val_KL : 3.4806708097457886\n","Epoch: 4209/8000  Traning Loss: 92.13343906402588  Train_Reconstruction: 88.61717224121094  Train_KL: 3.516266494989395  Validation Loss : 91.7151870727539 Val_Reconstruction : 88.23934173583984 Val_KL : 3.4758434295654297\n","Epoch: 4210/8000  Traning Loss: 91.9679946899414  Train_Reconstruction: 88.45241928100586  Train_KL: 3.5155763924121857  Validation Loss : 91.83474349975586 Val_Reconstruction : 88.3625373840332 Val_KL : 3.4722042083740234\n","Epoch: 4211/8000  Traning Loss: 92.21191596984863  Train_Reconstruction: 88.6996488571167  Train_KL: 3.5122680962085724  Validation Loss : 92.13740158081055 Val_Reconstruction : 88.66935348510742 Val_KL : 3.468047022819519\n","Epoch: 4212/8000  Traning Loss: 92.72391414642334  Train_Reconstruction: 89.20740127563477  Train_KL: 3.5165125727653503  Validation Loss : 92.4845085144043 Val_Reconstruction : 89.0017204284668 Val_KL : 3.482789635658264\n","Epoch: 4213/8000  Traning Loss: 92.22262668609619  Train_Reconstruction: 88.70726203918457  Train_KL: 3.5153649151325226  Validation Loss : 91.89463806152344 Val_Reconstruction : 88.42264556884766 Val_KL : 3.4719905853271484\n","Epoch: 4214/8000  Traning Loss: 92.07044410705566  Train_Reconstruction: 88.56413650512695  Train_KL: 3.506307750940323  Validation Loss : 91.83781814575195 Val_Reconstruction : 88.36302185058594 Val_KL : 3.474796414375305\n","Epoch: 4215/8000  Traning Loss: 91.8031063079834  Train_Reconstruction: 88.28746128082275  Train_KL: 3.5156451165676117  Validation Loss : 91.46669006347656 Val_Reconstruction : 87.98178100585938 Val_KL : 3.484908103942871\n","Epoch: 4216/8000  Traning Loss: 91.84090995788574  Train_Reconstruction: 88.32021999359131  Train_KL: 3.5206888914108276  Validation Loss : 91.70835876464844 Val_Reconstruction : 88.22625350952148 Val_KL : 3.482105255126953\n","Epoch: 4217/8000  Traning Loss: 92.1384687423706  Train_Reconstruction: 88.63160419464111  Train_KL: 3.5068635642528534  Validation Loss : 91.85280227661133 Val_Reconstruction : 88.38603973388672 Val_KL : 3.4667630195617676\n","Epoch: 4218/8000  Traning Loss: 92.52846813201904  Train_Reconstruction: 89.02041625976562  Train_KL: 3.5080526173114777  Validation Loss : 92.32237243652344 Val_Reconstruction : 88.83761978149414 Val_KL : 3.484752893447876\n","Epoch: 4219/8000  Traning Loss: 92.8191270828247  Train_Reconstruction: 89.28837776184082  Train_KL: 3.5307483077049255  Validation Loss : 92.74441146850586 Val_Reconstruction : 89.25143432617188 Val_KL : 3.4929778575897217\n","Epoch: 4220/8000  Traning Loss: 92.49768733978271  Train_Reconstruction: 88.97705936431885  Train_KL: 3.520627409219742  Validation Loss : 91.99018859863281 Val_Reconstruction : 88.5186882019043 Val_KL : 3.4714988470077515\n","Epoch: 4221/8000  Traning Loss: 92.33320045471191  Train_Reconstruction: 88.82163906097412  Train_KL: 3.5115612149238586  Validation Loss : 92.10848999023438 Val_Reconstruction : 88.63611221313477 Val_KL : 3.4723784923553467\n","Epoch: 4222/8000  Traning Loss: 92.2069845199585  Train_Reconstruction: 88.6854076385498  Train_KL: 3.521577686071396  Validation Loss : 91.82460021972656 Val_Reconstruction : 88.34403991699219 Val_KL : 3.4805580377578735\n","Epoch: 4223/8000  Traning Loss: 91.94964504241943  Train_Reconstruction: 88.42549514770508  Train_KL: 3.524150788784027  Validation Loss : 91.71531677246094 Val_Reconstruction : 88.22846221923828 Val_KL : 3.486855387687683\n","Epoch: 4224/8000  Traning Loss: 91.93025588989258  Train_Reconstruction: 88.41096115112305  Train_KL: 3.5192948281764984  Validation Loss : 91.76020431518555 Val_Reconstruction : 88.2807731628418 Val_KL : 3.4794304370880127\n","Epoch: 4225/8000  Traning Loss: 92.14817142486572  Train_Reconstruction: 88.63511085510254  Train_KL: 3.513060837984085  Validation Loss : 92.15978622436523 Val_Reconstruction : 88.68500518798828 Val_KL : 3.474784255027771\n","Epoch: 4226/8000  Traning Loss: 92.78216552734375  Train_Reconstruction: 89.26630973815918  Train_KL: 3.5158559381961823  Validation Loss : 92.61635208129883 Val_Reconstruction : 89.13201522827148 Val_KL : 3.484338402748108\n","Epoch: 4227/8000  Traning Loss: 92.44715309143066  Train_Reconstruction: 88.90806865692139  Train_KL: 3.539085954427719  Validation Loss : 91.87649536132812 Val_Reconstruction : 88.36768341064453 Val_KL : 3.5088131427764893\n","Epoch: 4228/8000  Traning Loss: 92.47239017486572  Train_Reconstruction: 88.93657112121582  Train_KL: 3.535819560289383  Validation Loss : 92.30897903442383 Val_Reconstruction : 88.81560134887695 Val_KL : 3.4933784008026123\n","Epoch: 4229/8000  Traning Loss: 92.5856409072876  Train_Reconstruction: 89.06314182281494  Train_KL: 3.522499978542328  Validation Loss : 92.75899505615234 Val_Reconstruction : 89.27812576293945 Val_KL : 3.4808684587478638\n","Epoch: 4230/8000  Traning Loss: 92.89623355865479  Train_Reconstruction: 89.377760887146  Train_KL: 3.5184727609157562  Validation Loss : 93.07582092285156 Val_Reconstruction : 89.59075164794922 Val_KL : 3.485066294670105\n","Epoch: 4231/8000  Traning Loss: 92.76438331604004  Train_Reconstruction: 89.23429775238037  Train_KL: 3.53008496761322  Validation Loss : 92.13617324829102 Val_Reconstruction : 88.6444206237793 Val_KL : 3.4917519092559814\n","Epoch: 4232/8000  Traning Loss: 92.35204601287842  Train_Reconstruction: 88.83094120025635  Train_KL: 3.5211052298545837  Validation Loss : 92.13460922241211 Val_Reconstruction : 88.65832138061523 Val_KL : 3.476288318634033\n","Epoch: 4233/8000  Traning Loss: 92.16706466674805  Train_Reconstruction: 88.64825916290283  Train_KL: 3.5188048183918  Validation Loss : 91.81869888305664 Val_Reconstruction : 88.33154678344727 Val_KL : 3.487151026725769\n","Epoch: 4234/8000  Traning Loss: 92.14094066619873  Train_Reconstruction: 88.61152839660645  Train_KL: 3.529411166906357  Validation Loss : 92.06143188476562 Val_Reconstruction : 88.5708122253418 Val_KL : 3.490618586540222\n","Epoch: 4235/8000  Traning Loss: 92.37188053131104  Train_Reconstruction: 88.84702682495117  Train_KL: 3.524854391813278  Validation Loss : 92.49709701538086 Val_Reconstruction : 89.01303482055664 Val_KL : 3.4840643405914307\n","Epoch: 4236/8000  Traning Loss: 92.52677249908447  Train_Reconstruction: 89.00492763519287  Train_KL: 3.5218453407287598  Validation Loss : 92.20705795288086 Val_Reconstruction : 88.72998046875 Val_KL : 3.477078437805176\n","Epoch: 4237/8000  Traning Loss: 92.27483940124512  Train_Reconstruction: 88.75451183319092  Train_KL: 3.520328164100647  Validation Loss : 91.98095321655273 Val_Reconstruction : 88.49908065795898 Val_KL : 3.481874942779541\n","Epoch: 4238/8000  Traning Loss: 92.45087432861328  Train_Reconstruction: 88.9292459487915  Train_KL: 3.521627277135849  Validation Loss : 91.97149658203125 Val_Reconstruction : 88.48962020874023 Val_KL : 3.481877326965332\n","Epoch: 4239/8000  Traning Loss: 92.46888542175293  Train_Reconstruction: 88.94818210601807  Train_KL: 3.5207021832466125  Validation Loss : 92.1631851196289 Val_Reconstruction : 88.67913818359375 Val_KL : 3.484047055244446\n","Epoch: 4240/8000  Traning Loss: 92.33047676086426  Train_Reconstruction: 88.80630111694336  Train_KL: 3.5241751074790955  Validation Loss : 92.0318489074707 Val_Reconstruction : 88.54605484008789 Val_KL : 3.485793352127075\n","Epoch: 4241/8000  Traning Loss: 92.0570821762085  Train_Reconstruction: 88.53491973876953  Train_KL: 3.5221628844738007  Validation Loss : 91.72213745117188 Val_Reconstruction : 88.23507308959961 Val_KL : 3.4870660305023193\n","Epoch: 4242/8000  Traning Loss: 91.88848400115967  Train_Reconstruction: 88.36591625213623  Train_KL: 3.5225676000118256  Validation Loss : 91.82658767700195 Val_Reconstruction : 88.34615707397461 Val_KL : 3.4804317951202393\n","Epoch: 4243/8000  Traning Loss: 91.88983154296875  Train_Reconstruction: 88.37291622161865  Train_KL: 3.5169146358966827  Validation Loss : 91.84986114501953 Val_Reconstruction : 88.38718795776367 Val_KL : 3.4626729488372803\n","Epoch: 4244/8000  Traning Loss: 92.28508186340332  Train_Reconstruction: 88.77798748016357  Train_KL: 3.507095068693161  Validation Loss : 92.50993728637695 Val_Reconstruction : 89.0322380065918 Val_KL : 3.477699637413025\n","Epoch: 4245/8000  Traning Loss: 92.58936500549316  Train_Reconstruction: 89.05316352844238  Train_KL: 3.5362029671669006  Validation Loss : 92.35322189331055 Val_Reconstruction : 88.85427856445312 Val_KL : 3.498944640159607\n","Epoch: 4246/8000  Traning Loss: 92.27152156829834  Train_Reconstruction: 88.74771976470947  Train_KL: 3.523802101612091  Validation Loss : 91.91537475585938 Val_Reconstruction : 88.43204879760742 Val_KL : 3.48332941532135\n","Epoch: 4247/8000  Traning Loss: 91.89767837524414  Train_Reconstruction: 88.3769702911377  Train_KL: 3.5207073986530304  Validation Loss : 91.72291564941406 Val_Reconstruction : 88.23209762573242 Val_KL : 3.490818738937378\n","Epoch: 4248/8000  Traning Loss: 92.09187316894531  Train_Reconstruction: 88.56190872192383  Train_KL: 3.5299639105796814  Validation Loss : 91.81418991088867 Val_Reconstruction : 88.3260383605957 Val_KL : 3.488152265548706\n","Epoch: 4249/8000  Traning Loss: 92.00943851470947  Train_Reconstruction: 88.48490142822266  Train_KL: 3.524537056684494  Validation Loss : 91.77828216552734 Val_Reconstruction : 88.29783630371094 Val_KL : 3.4804487228393555\n","Epoch: 4250/8000  Traning Loss: 92.29082584381104  Train_Reconstruction: 88.77835178375244  Train_KL: 3.512473464012146  Validation Loss : 92.20457077026367 Val_Reconstruction : 88.7291374206543 Val_KL : 3.475432515144348\n","Epoch: 4251/8000  Traning Loss: 92.33234119415283  Train_Reconstruction: 88.82038593292236  Train_KL: 3.5119547843933105  Validation Loss : 92.12210464477539 Val_Reconstruction : 88.6492805480957 Val_KL : 3.4728235006332397\n","Epoch: 4252/8000  Traning Loss: 92.34346580505371  Train_Reconstruction: 88.83699607849121  Train_KL: 3.5064693689346313  Validation Loss : 92.17428588867188 Val_Reconstruction : 88.70566940307617 Val_KL : 3.4686163663864136\n","Epoch: 4253/8000  Traning Loss: 92.70125198364258  Train_Reconstruction: 89.18377780914307  Train_KL: 3.5174747109413147  Validation Loss : 92.51097869873047 Val_Reconstruction : 89.02054977416992 Val_KL : 3.4904303550720215\n","Epoch: 4254/8000  Traning Loss: 92.53630924224854  Train_Reconstruction: 89.0048599243164  Train_KL: 3.5314489006996155  Validation Loss : 92.00096130371094 Val_Reconstruction : 88.50537109375 Val_KL : 3.4955910444259644\n","Epoch: 4255/8000  Traning Loss: 92.5910873413086  Train_Reconstruction: 89.06344890594482  Train_KL: 3.527639240026474  Validation Loss : 92.04145812988281 Val_Reconstruction : 88.55384826660156 Val_KL : 3.487610101699829\n","Epoch: 4256/8000  Traning Loss: 92.21720218658447  Train_Reconstruction: 88.68860816955566  Train_KL: 3.528594195842743  Validation Loss : 91.74872207641602 Val_Reconstruction : 88.2662124633789 Val_KL : 3.4825072288513184\n","Epoch: 4257/8000  Traning Loss: 92.35261535644531  Train_Reconstruction: 88.83435344696045  Train_KL: 3.5182629227638245  Validation Loss : 92.18836212158203 Val_Reconstruction : 88.71081161499023 Val_KL : 3.4775503873825073\n","Epoch: 4258/8000  Traning Loss: 92.61705017089844  Train_Reconstruction: 89.09463024139404  Train_KL: 3.522420108318329  Validation Loss : 92.16829299926758 Val_Reconstruction : 88.6815414428711 Val_KL : 3.4867526292800903\n","Epoch: 4259/8000  Traning Loss: 92.52499580383301  Train_Reconstruction: 89.0071210861206  Train_KL: 3.5178741216659546  Validation Loss : 92.26507186889648 Val_Reconstruction : 88.7927360534668 Val_KL : 3.4723364114761353\n","Epoch: 4260/8000  Traning Loss: 92.70770263671875  Train_Reconstruction: 89.20513916015625  Train_KL: 3.5025631487369537  Validation Loss : 92.43365097045898 Val_Reconstruction : 88.96831130981445 Val_KL : 3.4653395414352417\n","Epoch: 4261/8000  Traning Loss: 92.41706371307373  Train_Reconstruction: 88.90049934387207  Train_KL: 3.5165648758411407  Validation Loss : 92.14137268066406 Val_Reconstruction : 88.65288543701172 Val_KL : 3.488488793373108\n","Epoch: 4262/8000  Traning Loss: 92.24723434448242  Train_Reconstruction: 88.72403240203857  Train_KL: 3.523201048374176  Validation Loss : 92.08227920532227 Val_Reconstruction : 88.59790420532227 Val_KL : 3.484373688697815\n","Epoch: 4263/8000  Traning Loss: 92.36456298828125  Train_Reconstruction: 88.8511734008789  Train_KL: 3.513390213251114  Validation Loss : 92.31318664550781 Val_Reconstruction : 88.8377685546875 Val_KL : 3.475417375564575\n","Epoch: 4264/8000  Traning Loss: 92.22873878479004  Train_Reconstruction: 88.70939064025879  Train_KL: 3.51934877038002  Validation Loss : 92.14557266235352 Val_Reconstruction : 88.6585693359375 Val_KL : 3.48700487613678\n","Epoch: 4265/8000  Traning Loss: 92.23456382751465  Train_Reconstruction: 88.71249103546143  Train_KL: 3.522073358297348  Validation Loss : 92.5557861328125 Val_Reconstruction : 89.08019638061523 Val_KL : 3.4755897521972656\n","Epoch: 4266/8000  Traning Loss: 92.15294647216797  Train_Reconstruction: 88.63501358032227  Train_KL: 3.517934948205948  Validation Loss : 91.65166473388672 Val_Reconstruction : 88.16696166992188 Val_KL : 3.4847028255462646\n","Epoch: 4267/8000  Traning Loss: 91.78227138519287  Train_Reconstruction: 88.25686740875244  Train_KL: 3.5254029035568237  Validation Loss : 91.6968765258789 Val_Reconstruction : 88.2070198059082 Val_KL : 3.4898546934127808\n","Epoch: 4268/8000  Traning Loss: 91.92040920257568  Train_Reconstruction: 88.39696025848389  Train_KL: 3.5234502255916595  Validation Loss : 92.17052459716797 Val_Reconstruction : 88.68435668945312 Val_KL : 3.486170172691345\n","Epoch: 4269/8000  Traning Loss: 92.22471618652344  Train_Reconstruction: 88.70566749572754  Train_KL: 3.5190491676330566  Validation Loss : 91.87239074707031 Val_Reconstruction : 88.39335632324219 Val_KL : 3.479031562805176\n","Epoch: 4270/8000  Traning Loss: 92.0698938369751  Train_Reconstruction: 88.55288219451904  Train_KL: 3.517011731863022  Validation Loss : 92.05452728271484 Val_Reconstruction : 88.58353805541992 Val_KL : 3.470989465713501\n","Epoch: 4271/8000  Traning Loss: 92.10874462127686  Train_Reconstruction: 88.59407234191895  Train_KL: 3.514673113822937  Validation Loss : 91.76715087890625 Val_Reconstruction : 88.296142578125 Val_KL : 3.471007227897644\n","Epoch: 4272/8000  Traning Loss: 92.04823684692383  Train_Reconstruction: 88.53863143920898  Train_KL: 3.509604662656784  Validation Loss : 91.97074127197266 Val_Reconstruction : 88.4952621459961 Val_KL : 3.4754788875579834\n","Epoch: 4273/8000  Traning Loss: 92.32140827178955  Train_Reconstruction: 88.79672527313232  Train_KL: 3.5246829092502594  Validation Loss : 92.1747817993164 Val_Reconstruction : 88.67857360839844 Val_KL : 3.4962069988250732\n","Epoch: 4274/8000  Traning Loss: 92.4403715133667  Train_Reconstruction: 88.91281318664551  Train_KL: 3.5275577008724213  Validation Loss : 92.01469421386719 Val_Reconstruction : 88.52536010742188 Val_KL : 3.4893338680267334\n","Epoch: 4275/8000  Traning Loss: 92.21024799346924  Train_Reconstruction: 88.68853569030762  Train_KL: 3.521712511777878  Validation Loss : 92.35104370117188 Val_Reconstruction : 88.86351013183594 Val_KL : 3.4875328540802\n","Epoch: 4276/8000  Traning Loss: 92.34731101989746  Train_Reconstruction: 88.82653045654297  Train_KL: 3.5207802951335907  Validation Loss : 92.38030242919922 Val_Reconstruction : 88.89204025268555 Val_KL : 3.488263726234436\n","Epoch: 4277/8000  Traning Loss: 92.20681285858154  Train_Reconstruction: 88.6787919998169  Train_KL: 3.5280202329158783  Validation Loss : 91.90908432006836 Val_Reconstruction : 88.41098022460938 Val_KL : 3.4981026649475098\n","Epoch: 4278/8000  Traning Loss: 92.13979053497314  Train_Reconstruction: 88.61564350128174  Train_KL: 3.5241473615169525  Validation Loss : 91.68889617919922 Val_Reconstruction : 88.20249938964844 Val_KL : 3.486395478248596\n","Epoch: 4279/8000  Traning Loss: 92.04539966583252  Train_Reconstruction: 88.52761363983154  Train_KL: 3.517785370349884  Validation Loss : 92.07675552368164 Val_Reconstruction : 88.60137939453125 Val_KL : 3.4753782749176025\n","Epoch: 4280/8000  Traning Loss: 92.05214500427246  Train_Reconstruction: 88.54735851287842  Train_KL: 3.5047867596149445  Validation Loss : 91.80216979980469 Val_Reconstruction : 88.32508087158203 Val_KL : 3.477088212966919\n","Epoch: 4281/8000  Traning Loss: 92.25035190582275  Train_Reconstruction: 88.73181629180908  Train_KL: 3.5185351967811584  Validation Loss : 92.41320419311523 Val_Reconstruction : 88.92681121826172 Val_KL : 3.4863929748535156\n","Epoch: 4282/8000  Traning Loss: 92.14806175231934  Train_Reconstruction: 88.62952995300293  Train_KL: 3.5185302197933197  Validation Loss : 92.21440124511719 Val_Reconstruction : 88.73179244995117 Val_KL : 3.482611656188965\n","Epoch: 4283/8000  Traning Loss: 92.1722412109375  Train_Reconstruction: 88.64684772491455  Train_KL: 3.5253933668136597  Validation Loss : 92.00373458862305 Val_Reconstruction : 88.51163101196289 Val_KL : 3.4921035766601562\n","Epoch: 4284/8000  Traning Loss: 92.03332424163818  Train_Reconstruction: 88.51083183288574  Train_KL: 3.522493153810501  Validation Loss : 92.0758285522461 Val_Reconstruction : 88.5980339050293 Val_KL : 3.4777965545654297\n","Epoch: 4285/8000  Traning Loss: 92.39547443389893  Train_Reconstruction: 88.88517570495605  Train_KL: 3.510298579931259  Validation Loss : 92.1958999633789 Val_Reconstruction : 88.72456359863281 Val_KL : 3.471338152885437\n","Epoch: 4286/8000  Traning Loss: 92.3126630783081  Train_Reconstruction: 88.7993745803833  Train_KL: 3.513288736343384  Validation Loss : 91.7597770690918 Val_Reconstruction : 88.2748031616211 Val_KL : 3.4849733114242554\n","Epoch: 4287/8000  Traning Loss: 91.88987255096436  Train_Reconstruction: 88.36057949066162  Train_KL: 3.529293328523636  Validation Loss : 91.64657592773438 Val_Reconstruction : 88.1572380065918 Val_KL : 3.4893382787704468\n","Epoch: 4288/8000  Traning Loss: 92.14223670959473  Train_Reconstruction: 88.6281213760376  Train_KL: 3.514116406440735  Validation Loss : 91.98267364501953 Val_Reconstruction : 88.50225448608398 Val_KL : 3.4804179668426514\n","Epoch: 4289/8000  Traning Loss: 91.8867654800415  Train_Reconstruction: 88.37031841278076  Train_KL: 3.5164466500282288  Validation Loss : 91.90306854248047 Val_Reconstruction : 88.42272186279297 Val_KL : 3.480349898338318\n","Epoch: 4290/8000  Traning Loss: 92.08453559875488  Train_Reconstruction: 88.55874156951904  Train_KL: 3.5257935523986816  Validation Loss : 92.35652923583984 Val_Reconstruction : 88.86272048950195 Val_KL : 3.4938098192214966\n","Epoch: 4291/8000  Traning Loss: 92.14017486572266  Train_Reconstruction: 88.6106185913086  Train_KL: 3.529556840658188  Validation Loss : 91.86357498168945 Val_Reconstruction : 88.37629699707031 Val_KL : 3.48727810382843\n","Epoch: 4292/8000  Traning Loss: 92.10384941101074  Train_Reconstruction: 88.58295631408691  Train_KL: 3.520893156528473  Validation Loss : 91.87604904174805 Val_Reconstruction : 88.39627456665039 Val_KL : 3.4797719717025757\n","Epoch: 4293/8000  Traning Loss: 92.44737434387207  Train_Reconstruction: 88.9328031539917  Train_KL: 3.514571100473404  Validation Loss : 92.45011138916016 Val_Reconstruction : 88.96571731567383 Val_KL : 3.4843932390213013\n","Epoch: 4294/8000  Traning Loss: 92.79520606994629  Train_Reconstruction: 89.27539920806885  Train_KL: 3.519804149866104  Validation Loss : 92.4107666015625 Val_Reconstruction : 88.92350006103516 Val_KL : 3.487266421318054\n","Epoch: 4295/8000  Traning Loss: 92.91604232788086  Train_Reconstruction: 89.39283275604248  Train_KL: 3.5232086777687073  Validation Loss : 92.9643783569336 Val_Reconstruction : 89.4849624633789 Val_KL : 3.479416251182556\n","Epoch: 4296/8000  Traning Loss: 92.78987789154053  Train_Reconstruction: 89.27185249328613  Train_KL: 3.5180263817310333  Validation Loss : 92.65171813964844 Val_Reconstruction : 89.16498184204102 Val_KL : 3.4867334365844727\n","Epoch: 4297/8000  Traning Loss: 92.61050033569336  Train_Reconstruction: 89.08875560760498  Train_KL: 3.5217449069023132  Validation Loss : 91.88572311401367 Val_Reconstruction : 88.40682220458984 Val_KL : 3.4789005517959595\n","Epoch: 4298/8000  Traning Loss: 92.02194881439209  Train_Reconstruction: 88.50370788574219  Train_KL: 3.5182410776615143  Validation Loss : 91.86314392089844 Val_Reconstruction : 88.38660049438477 Val_KL : 3.476543664932251\n","Epoch: 4299/8000  Traning Loss: 91.90023231506348  Train_Reconstruction: 88.37625789642334  Train_KL: 3.5239738821983337  Validation Loss : 91.65739059448242 Val_Reconstruction : 88.17169952392578 Val_KL : 3.48569118976593\n","Epoch: 4300/8000  Traning Loss: 91.96133995056152  Train_Reconstruction: 88.43642139434814  Train_KL: 3.524919331073761  Validation Loss : 91.80607604980469 Val_Reconstruction : 88.32995986938477 Val_KL : 3.4761180877685547\n","Epoch: 4301/8000  Traning Loss: 92.10401248931885  Train_Reconstruction: 88.59410858154297  Train_KL: 3.509905159473419  Validation Loss : 91.77333450317383 Val_Reconstruction : 88.30718231201172 Val_KL : 3.466152548789978\n","Epoch: 4302/8000  Traning Loss: 92.03742599487305  Train_Reconstruction: 88.5306749343872  Train_KL: 3.5067511200904846  Validation Loss : 91.98319244384766 Val_Reconstruction : 88.5138053894043 Val_KL : 3.4693868160247803\n","Epoch: 4303/8000  Traning Loss: 92.11534118652344  Train_Reconstruction: 88.59794044494629  Train_KL: 3.5174005031585693  Validation Loss : 91.88678359985352 Val_Reconstruction : 88.4061508178711 Val_KL : 3.48063325881958\n","Epoch: 4304/8000  Traning Loss: 91.9454345703125  Train_Reconstruction: 88.42296695709229  Train_KL: 3.522467941045761  Validation Loss : 91.5843620300293 Val_Reconstruction : 88.09817886352539 Val_KL : 3.4861830472946167\n","Epoch: 4305/8000  Traning Loss: 91.95252227783203  Train_Reconstruction: 88.42793083190918  Train_KL: 3.524592012166977  Validation Loss : 92.02828216552734 Val_Reconstruction : 88.53865432739258 Val_KL : 3.4896271228790283\n","Epoch: 4306/8000  Traning Loss: 92.26422882080078  Train_Reconstruction: 88.74048328399658  Train_KL: 3.523746073246002  Validation Loss : 92.06575775146484 Val_Reconstruction : 88.58146667480469 Val_KL : 3.4842878580093384\n","Epoch: 4307/8000  Traning Loss: 92.32974338531494  Train_Reconstruction: 88.80819892883301  Train_KL: 3.5215451419353485  Validation Loss : 92.25876998901367 Val_Reconstruction : 88.77463150024414 Val_KL : 3.484138607978821\n","Epoch: 4308/8000  Traning Loss: 92.9026985168457  Train_Reconstruction: 89.37251567840576  Train_KL: 3.530182659626007  Validation Loss : 92.85964965820312 Val_Reconstruction : 89.36567687988281 Val_KL : 3.4939723014831543\n","Epoch: 4309/8000  Traning Loss: 92.85147666931152  Train_Reconstruction: 89.32598495483398  Train_KL: 3.5254916548728943  Validation Loss : 92.29644012451172 Val_Reconstruction : 88.81281280517578 Val_KL : 3.483628273010254\n","Epoch: 4310/8000  Traning Loss: 92.80185127258301  Train_Reconstruction: 89.2835168838501  Train_KL: 3.5183351933956146  Validation Loss : 92.44189453125 Val_Reconstruction : 88.95907211303711 Val_KL : 3.482821822166443\n","Epoch: 4311/8000  Traning Loss: 92.18283653259277  Train_Reconstruction: 88.66030502319336  Train_KL: 3.5225319266319275  Validation Loss : 92.70341110229492 Val_Reconstruction : 89.22402954101562 Val_KL : 3.479379177093506\n","Epoch: 4312/8000  Traning Loss: 92.32482624053955  Train_Reconstruction: 88.80900001525879  Train_KL: 3.5158259570598602  Validation Loss : 92.08721923828125 Val_Reconstruction : 88.60476303100586 Val_KL : 3.4824554920196533\n","Epoch: 4313/8000  Traning Loss: 92.18516731262207  Train_Reconstruction: 88.66261100769043  Train_KL: 3.522555023431778  Validation Loss : 92.15305709838867 Val_Reconstruction : 88.66614151000977 Val_KL : 3.4869145154953003\n","Epoch: 4314/8000  Traning Loss: 92.04650783538818  Train_Reconstruction: 88.5224666595459  Train_KL: 3.5240417420864105  Validation Loss : 91.7446060180664 Val_Reconstruction : 88.25625991821289 Val_KL : 3.488344192504883\n","Epoch: 4315/8000  Traning Loss: 92.21968078613281  Train_Reconstruction: 88.69141578674316  Train_KL: 3.5282632410526276  Validation Loss : 92.07170867919922 Val_Reconstruction : 88.58527755737305 Val_KL : 3.4864301681518555\n","Epoch: 4316/8000  Traning Loss: 92.17996311187744  Train_Reconstruction: 88.6623182296753  Train_KL: 3.51764515042305  Validation Loss : 91.84009170532227 Val_Reconstruction : 88.3631477355957 Val_KL : 3.476943016052246\n","Epoch: 4317/8000  Traning Loss: 91.8780689239502  Train_Reconstruction: 88.3621129989624  Train_KL: 3.5159557163715363  Validation Loss : 91.59689712524414 Val_Reconstruction : 88.12184524536133 Val_KL : 3.4750540256500244\n","Epoch: 4318/8000  Traning Loss: 91.66418933868408  Train_Reconstruction: 88.13716793060303  Train_KL: 3.527021825313568  Validation Loss : 91.36082077026367 Val_Reconstruction : 87.86125183105469 Val_KL : 3.4995683431625366\n","Epoch: 4319/8000  Traning Loss: 91.76125621795654  Train_Reconstruction: 88.23112392425537  Train_KL: 3.530133545398712  Validation Loss : 91.47454833984375 Val_Reconstruction : 87.99003601074219 Val_KL : 3.4845123291015625\n","Epoch: 4320/8000  Traning Loss: 92.26699924468994  Train_Reconstruction: 88.75299835205078  Train_KL: 3.5140009224414825  Validation Loss : 92.53290557861328 Val_Reconstruction : 89.05833435058594 Val_KL : 3.47457218170166\n","Epoch: 4321/8000  Traning Loss: 92.82028484344482  Train_Reconstruction: 89.29940509796143  Train_KL: 3.520879954099655  Validation Loss : 92.19705200195312 Val_Reconstruction : 88.70970153808594 Val_KL : 3.4873515367507935\n","Epoch: 4322/8000  Traning Loss: 92.57673835754395  Train_Reconstruction: 89.04801750183105  Train_KL: 3.528721421957016  Validation Loss : 92.29197692871094 Val_Reconstruction : 88.80858612060547 Val_KL : 3.483389973640442\n","Epoch: 4323/8000  Traning Loss: 93.28781223297119  Train_Reconstruction: 89.77051734924316  Train_KL: 3.5172950625419617  Validation Loss : 93.7475700378418 Val_Reconstruction : 90.2710075378418 Val_KL : 3.4765626192092896\n","Epoch: 4324/8000  Traning Loss: 93.1846570968628  Train_Reconstruction: 89.66616535186768  Train_KL: 3.518492043018341  Validation Loss : 92.08891677856445 Val_Reconstruction : 88.60895919799805 Val_KL : 3.479957103729248\n","Epoch: 4325/8000  Traning Loss: 91.96908283233643  Train_Reconstruction: 88.44555568695068  Train_KL: 3.5235272347927094  Validation Loss : 91.75786590576172 Val_Reconstruction : 88.26983642578125 Val_KL : 3.4880282878875732\n","Epoch: 4326/8000  Traning Loss: 92.01445770263672  Train_Reconstruction: 88.48805713653564  Train_KL: 3.5264016687870026  Validation Loss : 91.82795333862305 Val_Reconstruction : 88.33743286132812 Val_KL : 3.4905203580856323\n","Epoch: 4327/8000  Traning Loss: 92.16621780395508  Train_Reconstruction: 88.65042972564697  Train_KL: 3.5157876908779144  Validation Loss : 91.9848861694336 Val_Reconstruction : 88.50957489013672 Val_KL : 3.4753106832504272\n","Epoch: 4328/8000  Traning Loss: 92.29283332824707  Train_Reconstruction: 88.77774238586426  Train_KL: 3.5150912702083588  Validation Loss : 92.05354690551758 Val_Reconstruction : 88.5733528137207 Val_KL : 3.480196475982666\n","Epoch: 4329/8000  Traning Loss: 92.16567325592041  Train_Reconstruction: 88.63480949401855  Train_KL: 3.53086256980896  Validation Loss : 91.86605072021484 Val_Reconstruction : 88.37332916259766 Val_KL : 3.49272358417511\n","Epoch: 4330/8000  Traning Loss: 91.9579553604126  Train_Reconstruction: 88.4234266281128  Train_KL: 3.534529536962509  Validation Loss : 91.80705261230469 Val_Reconstruction : 88.31034088134766 Val_KL : 3.496708631515503\n","Epoch: 4331/8000  Traning Loss: 92.06513595581055  Train_Reconstruction: 88.55114555358887  Train_KL: 3.51399165391922  Validation Loss : 91.95854949951172 Val_Reconstruction : 88.4891471862793 Val_KL : 3.469401240348816\n","Epoch: 4332/8000  Traning Loss: 92.06843948364258  Train_Reconstruction: 88.5546007156372  Train_KL: 3.5138385593891144  Validation Loss : 91.81287002563477 Val_Reconstruction : 88.3377456665039 Val_KL : 3.4751232862472534\n","Epoch: 4333/8000  Traning Loss: 92.06680107116699  Train_Reconstruction: 88.54677963256836  Train_KL: 3.520021140575409  Validation Loss : 92.11624145507812 Val_Reconstruction : 88.63672637939453 Val_KL : 3.479512929916382\n","Epoch: 4334/8000  Traning Loss: 92.22386932373047  Train_Reconstruction: 88.7067346572876  Train_KL: 3.517134666442871  Validation Loss : 92.10469055175781 Val_Reconstruction : 88.6305160522461 Val_KL : 3.4741735458374023\n","Epoch: 4335/8000  Traning Loss: 92.24630928039551  Train_Reconstruction: 88.73801517486572  Train_KL: 3.50829416513443  Validation Loss : 92.2626724243164 Val_Reconstruction : 88.79148483276367 Val_KL : 3.4711869955062866\n","Epoch: 4336/8000  Traning Loss: 92.44863510131836  Train_Reconstruction: 88.93868255615234  Train_KL: 3.5099521577358246  Validation Loss : 92.09872436523438 Val_Reconstruction : 88.61637496948242 Val_KL : 3.4823524951934814\n","Epoch: 4337/8000  Traning Loss: 92.30920124053955  Train_Reconstruction: 88.78710556030273  Train_KL: 3.5220945477485657  Validation Loss : 91.87223052978516 Val_Reconstruction : 88.39198684692383 Val_KL : 3.480244278907776\n","Epoch: 4338/8000  Traning Loss: 92.42618656158447  Train_Reconstruction: 88.91517448425293  Train_KL: 3.5110122561454773  Validation Loss : 92.50727462768555 Val_Reconstruction : 89.0451545715332 Val_KL : 3.4621177911758423\n","Epoch: 4339/8000  Traning Loss: 92.22911643981934  Train_Reconstruction: 88.7166576385498  Train_KL: 3.5124579071998596  Validation Loss : 91.5669937133789 Val_Reconstruction : 88.09079360961914 Val_KL : 3.4761974811553955\n","Epoch: 4340/8000  Traning Loss: 91.93426704406738  Train_Reconstruction: 88.4030590057373  Train_KL: 3.5312069952487946  Validation Loss : 91.67523956298828 Val_Reconstruction : 88.17768478393555 Val_KL : 3.49755597114563\n","Epoch: 4341/8000  Traning Loss: 91.74494934082031  Train_Reconstruction: 88.21135234832764  Train_KL: 3.533597081899643  Validation Loss : 91.5851936340332 Val_Reconstruction : 88.09733581542969 Val_KL : 3.487859606742859\n","Epoch: 4342/8000  Traning Loss: 91.77891445159912  Train_Reconstruction: 88.2596492767334  Train_KL: 3.519265830516815  Validation Loss : 91.99910736083984 Val_Reconstruction : 88.51757049560547 Val_KL : 3.481537342071533\n","Epoch: 4343/8000  Traning Loss: 92.11488628387451  Train_Reconstruction: 88.59096240997314  Train_KL: 3.5239237546920776  Validation Loss : 91.88031768798828 Val_Reconstruction : 88.39252090454102 Val_KL : 3.487794518470764\n","Epoch: 4344/8000  Traning Loss: 92.02221584320068  Train_Reconstruction: 88.49717044830322  Train_KL: 3.525047242641449  Validation Loss : 91.91909790039062 Val_Reconstruction : 88.4314956665039 Val_KL : 3.4876009225845337\n","Epoch: 4345/8000  Traning Loss: 91.96334838867188  Train_Reconstruction: 88.44028282165527  Train_KL: 3.5230648815631866  Validation Loss : 92.03598403930664 Val_Reconstruction : 88.55939483642578 Val_KL : 3.4765907526016235\n","Epoch: 4346/8000  Traning Loss: 92.3443489074707  Train_Reconstruction: 88.8275957107544  Train_KL: 3.516752749681473  Validation Loss : 92.24722290039062 Val_Reconstruction : 88.7677001953125 Val_KL : 3.479523777961731\n","Epoch: 4347/8000  Traning Loss: 92.38688850402832  Train_Reconstruction: 88.86561107635498  Train_KL: 3.521277129650116  Validation Loss : 91.9229736328125 Val_Reconstruction : 88.43442916870117 Val_KL : 3.4885436296463013\n","Epoch: 4348/8000  Traning Loss: 92.09532737731934  Train_Reconstruction: 88.56588554382324  Train_KL: 3.5294419825077057  Validation Loss : 92.0720443725586 Val_Reconstruction : 88.58871459960938 Val_KL : 3.4833292961120605\n","Epoch: 4349/8000  Traning Loss: 92.50974082946777  Train_Reconstruction: 88.98812007904053  Train_KL: 3.521620213985443  Validation Loss : 92.45690536499023 Val_Reconstruction : 88.97441101074219 Val_KL : 3.4824912548065186\n","Epoch: 4350/8000  Traning Loss: 92.8890209197998  Train_Reconstruction: 89.36562633514404  Train_KL: 3.5233940184116364  Validation Loss : 92.71805953979492 Val_Reconstruction : 89.23530578613281 Val_KL : 3.482752203941345\n","Epoch: 4351/8000  Traning Loss: 92.19922161102295  Train_Reconstruction: 88.67049407958984  Train_KL: 3.5287285447120667  Validation Loss : 91.89342880249023 Val_Reconstruction : 88.40791702270508 Val_KL : 3.48551344871521\n","Epoch: 4352/8000  Traning Loss: 92.15786933898926  Train_Reconstruction: 88.64327430725098  Train_KL: 3.514595627784729  Validation Loss : 92.26323318481445 Val_Reconstruction : 88.79913330078125 Val_KL : 3.4641002416610718\n","Epoch: 4353/8000  Traning Loss: 92.40972328186035  Train_Reconstruction: 88.90474224090576  Train_KL: 3.5049810707569122  Validation Loss : 92.2414779663086 Val_Reconstruction : 88.77361297607422 Val_KL : 3.4678629636764526\n","Epoch: 4354/8000  Traning Loss: 92.27726078033447  Train_Reconstruction: 88.76202583312988  Train_KL: 3.515234738588333  Validation Loss : 92.08734130859375 Val_Reconstruction : 88.60940933227539 Val_KL : 3.4779322147369385\n","Epoch: 4355/8000  Traning Loss: 92.21957778930664  Train_Reconstruction: 88.69867324829102  Train_KL: 3.5209047198295593  Validation Loss : 92.3313980102539 Val_Reconstruction : 88.85368728637695 Val_KL : 3.4777122735977173\n","Epoch: 4356/8000  Traning Loss: 92.37238502502441  Train_Reconstruction: 88.85988712310791  Train_KL: 3.5124982595443726  Validation Loss : 91.79436492919922 Val_Reconstruction : 88.32272338867188 Val_KL : 3.4716405868530273\n","Epoch: 4357/8000  Traning Loss: 91.92813205718994  Train_Reconstruction: 88.40550327301025  Train_KL: 3.5226297974586487  Validation Loss : 91.60160827636719 Val_Reconstruction : 88.11398315429688 Val_KL : 3.4876248836517334\n","Epoch: 4358/8000  Traning Loss: 91.84620094299316  Train_Reconstruction: 88.31810474395752  Train_KL: 3.528096318244934  Validation Loss : 91.52826690673828 Val_Reconstruction : 88.04440689086914 Val_KL : 3.483862042427063\n","Epoch: 4359/8000  Traning Loss: 91.79547786712646  Train_Reconstruction: 88.27502727508545  Train_KL: 3.520450860261917  Validation Loss : 91.65987014770508 Val_Reconstruction : 88.18553161621094 Val_KL : 3.4743404388427734\n","Epoch: 4360/8000  Traning Loss: 91.89500141143799  Train_Reconstruction: 88.38501167297363  Train_KL: 3.509989470243454  Validation Loss : 91.86601257324219 Val_Reconstruction : 88.39233779907227 Val_KL : 3.4736733436584473\n","Epoch: 4361/8000  Traning Loss: 91.8479642868042  Train_Reconstruction: 88.3297758102417  Train_KL: 3.518188714981079  Validation Loss : 91.5765495300293 Val_Reconstruction : 88.09106826782227 Val_KL : 3.4854819774627686\n","Epoch: 4362/8000  Traning Loss: 92.23380470275879  Train_Reconstruction: 88.71056652069092  Train_KL: 3.5232388079166412  Validation Loss : 92.1790771484375 Val_Reconstruction : 88.6910514831543 Val_KL : 3.488025426864624\n","Epoch: 4363/8000  Traning Loss: 92.33099555969238  Train_Reconstruction: 88.8133716583252  Train_KL: 3.5176239013671875  Validation Loss : 92.23047256469727 Val_Reconstruction : 88.7452507019043 Val_KL : 3.485224962234497\n","Epoch: 4364/8000  Traning Loss: 92.09137725830078  Train_Reconstruction: 88.55896854400635  Train_KL: 3.532408505678177  Validation Loss : 91.8287353515625 Val_Reconstruction : 88.33157348632812 Val_KL : 3.4971628189086914\n","Epoch: 4365/8000  Traning Loss: 92.21952724456787  Train_Reconstruction: 88.69683265686035  Train_KL: 3.522694855928421  Validation Loss : 92.42222213745117 Val_Reconstruction : 88.95092010498047 Val_KL : 3.4713019132614136\n","Epoch: 4366/8000  Traning Loss: 92.39706802368164  Train_Reconstruction: 88.88135528564453  Train_KL: 3.5157130658626556  Validation Loss : 92.00185012817383 Val_Reconstruction : 88.52565002441406 Val_KL : 3.4761977195739746\n","Epoch: 4367/8000  Traning Loss: 92.28489875793457  Train_Reconstruction: 88.76645946502686  Train_KL: 3.5184386670589447  Validation Loss : 92.23357391357422 Val_Reconstruction : 88.75262832641602 Val_KL : 3.4809428453445435\n","Epoch: 4368/8000  Traning Loss: 92.07248306274414  Train_Reconstruction: 88.55517101287842  Train_KL: 3.5173115730285645  Validation Loss : 91.89044952392578 Val_Reconstruction : 88.41793060302734 Val_KL : 3.4725195169448853\n","Epoch: 4369/8000  Traning Loss: 92.2805118560791  Train_Reconstruction: 88.77597236633301  Train_KL: 3.504538834095001  Validation Loss : 92.06722640991211 Val_Reconstruction : 88.59658813476562 Val_KL : 3.470638632774353\n","Epoch: 4370/8000  Traning Loss: 91.96636199951172  Train_Reconstruction: 88.45275115966797  Train_KL: 3.5136103332042694  Validation Loss : 91.72599792480469 Val_Reconstruction : 88.25331497192383 Val_KL : 3.4726834297180176\n","Epoch: 4371/8000  Traning Loss: 91.809006690979  Train_Reconstruction: 88.29607200622559  Train_KL: 3.5129347443580627  Validation Loss : 91.6013069152832 Val_Reconstruction : 88.13153457641602 Val_KL : 3.4697728157043457\n","Epoch: 4372/8000  Traning Loss: 91.77644729614258  Train_Reconstruction: 88.26500606536865  Train_KL: 3.5114405751228333  Validation Loss : 91.48095703125 Val_Reconstruction : 88.00542068481445 Val_KL : 3.4755382537841797\n","Epoch: 4373/8000  Traning Loss: 91.65693092346191  Train_Reconstruction: 88.13346767425537  Train_KL: 3.5234622061252594  Validation Loss : 91.84074783325195 Val_Reconstruction : 88.35184478759766 Val_KL : 3.4889053106307983\n","Epoch: 4374/8000  Traning Loss: 92.05239963531494  Train_Reconstruction: 88.52104663848877  Train_KL: 3.531352072954178  Validation Loss : 91.93111419677734 Val_Reconstruction : 88.4363021850586 Val_KL : 3.4948126077651978\n","Epoch: 4375/8000  Traning Loss: 91.96427631378174  Train_Reconstruction: 88.4373025894165  Train_KL: 3.5269725620746613  Validation Loss : 91.83570861816406 Val_Reconstruction : 88.35081100463867 Val_KL : 3.4848976135253906\n","Epoch: 4376/8000  Traning Loss: 91.92416763305664  Train_Reconstruction: 88.4027156829834  Train_KL: 3.5214522182941437  Validation Loss : 92.10257720947266 Val_Reconstruction : 88.61722946166992 Val_KL : 3.4853453636169434\n","Epoch: 4377/8000  Traning Loss: 92.18069648742676  Train_Reconstruction: 88.65379047393799  Train_KL: 3.5269057154655457  Validation Loss : 92.34544372558594 Val_Reconstruction : 88.85865783691406 Val_KL : 3.4867862462997437\n","Epoch: 4378/8000  Traning Loss: 92.07351779937744  Train_Reconstruction: 88.55211067199707  Train_KL: 3.5214069485664368  Validation Loss : 91.53383255004883 Val_Reconstruction : 88.05073928833008 Val_KL : 3.483093738555908\n","Epoch: 4379/8000  Traning Loss: 91.82883930206299  Train_Reconstruction: 88.31617069244385  Train_KL: 3.5126690566539764  Validation Loss : 91.9057388305664 Val_Reconstruction : 88.43393325805664 Val_KL : 3.4718058109283447\n","Epoch: 4380/8000  Traning Loss: 92.1067762374878  Train_Reconstruction: 88.59622097015381  Train_KL: 3.5105561017990112  Validation Loss : 92.07956314086914 Val_Reconstruction : 88.60074234008789 Val_KL : 3.478819966316223\n","Epoch: 4381/8000  Traning Loss: 92.3900032043457  Train_Reconstruction: 88.87003326416016  Train_KL: 3.519970566034317  Validation Loss : 92.15884017944336 Val_Reconstruction : 88.66899108886719 Val_KL : 3.4898475408554077\n","Epoch: 4382/8000  Traning Loss: 93.38396549224854  Train_Reconstruction: 89.8628797531128  Train_KL: 3.521084427833557  Validation Loss : 93.32418823242188 Val_Reconstruction : 89.85890579223633 Val_KL : 3.465282917022705\n","Epoch: 4383/8000  Traning Loss: 92.70686721801758  Train_Reconstruction: 89.20058536529541  Train_KL: 3.506280690431595  Validation Loss : 92.09082794189453 Val_Reconstruction : 88.62144088745117 Val_KL : 3.4693877696990967\n","Epoch: 4384/8000  Traning Loss: 92.23857593536377  Train_Reconstruction: 88.71640491485596  Train_KL: 3.5221702456474304  Validation Loss : 91.88089370727539 Val_Reconstruction : 88.3891716003418 Val_KL : 3.491721272468567\n","Epoch: 4385/8000  Traning Loss: 92.54362392425537  Train_Reconstruction: 89.01622486114502  Train_KL: 3.5273998975753784  Validation Loss : 92.44108963012695 Val_Reconstruction : 88.95891952514648 Val_KL : 3.4821696281433105\n","Epoch: 4386/8000  Traning Loss: 92.53924560546875  Train_Reconstruction: 89.02726936340332  Train_KL: 3.5119771659374237  Validation Loss : 92.1340560913086 Val_Reconstruction : 88.66178131103516 Val_KL : 3.472274422645569\n","Epoch: 4387/8000  Traning Loss: 91.9513692855835  Train_Reconstruction: 88.43716430664062  Train_KL: 3.5142052471637726  Validation Loss : 91.78673553466797 Val_Reconstruction : 88.30167388916016 Val_KL : 3.485059976577759\n","Epoch: 4388/8000  Traning Loss: 92.54253196716309  Train_Reconstruction: 89.01917171478271  Train_KL: 3.523359715938568  Validation Loss : 92.44618225097656 Val_Reconstruction : 88.95132446289062 Val_KL : 3.4948569536209106\n","Epoch: 4389/8000  Traning Loss: 92.43594551086426  Train_Reconstruction: 88.91335964202881  Train_KL: 3.522585779428482  Validation Loss : 92.10090637207031 Val_Reconstruction : 88.62004089355469 Val_KL : 3.48086416721344\n","Epoch: 4390/8000  Traning Loss: 92.64094161987305  Train_Reconstruction: 89.12322521209717  Train_KL: 3.5177153944969177  Validation Loss : 92.47427749633789 Val_Reconstruction : 88.99856948852539 Val_KL : 3.475704550743103\n","Epoch: 4391/8000  Traning Loss: 92.18569374084473  Train_Reconstruction: 88.67335891723633  Train_KL: 3.5123355090618134  Validation Loss : 91.8274154663086 Val_Reconstruction : 88.35938262939453 Val_KL : 3.4680323600769043\n","Epoch: 4392/8000  Traning Loss: 92.05384540557861  Train_Reconstruction: 88.54264068603516  Train_KL: 3.511205315589905  Validation Loss : 91.62968444824219 Val_Reconstruction : 88.15558242797852 Val_KL : 3.4741016626358032\n","Epoch: 4393/8000  Traning Loss: 91.93259525299072  Train_Reconstruction: 88.41360282897949  Train_KL: 3.5189937353134155  Validation Loss : 91.5529899597168 Val_Reconstruction : 88.07125091552734 Val_KL : 3.4817408323287964\n","Epoch: 4394/8000  Traning Loss: 92.08499240875244  Train_Reconstruction: 88.5672664642334  Train_KL: 3.5177260637283325  Validation Loss : 91.81328582763672 Val_Reconstruction : 88.33244705200195 Val_KL : 3.4808382987976074\n","Epoch: 4395/8000  Traning Loss: 92.2134256362915  Train_Reconstruction: 88.69647598266602  Train_KL: 3.5169501304626465  Validation Loss : 91.96683502197266 Val_Reconstruction : 88.484375 Val_KL : 3.4824613332748413\n","Epoch: 4396/8000  Traning Loss: 92.34189796447754  Train_Reconstruction: 88.82111358642578  Train_KL: 3.5207853615283966  Validation Loss : 92.48329544067383 Val_Reconstruction : 89.00395584106445 Val_KL : 3.4793401956558228\n","Epoch: 4397/8000  Traning Loss: 92.19449520111084  Train_Reconstruction: 88.67987537384033  Train_KL: 3.5146197378635406  Validation Loss : 91.78593063354492 Val_Reconstruction : 88.31381607055664 Val_KL : 3.4721126556396484\n","Epoch: 4398/8000  Traning Loss: 91.82540798187256  Train_Reconstruction: 88.3116512298584  Train_KL: 3.5137573778629303  Validation Loss : 91.37560653686523 Val_Reconstruction : 87.89845657348633 Val_KL : 3.477147340774536\n","Epoch: 4399/8000  Traning Loss: 91.7746057510376  Train_Reconstruction: 88.24296569824219  Train_KL: 3.531640887260437  Validation Loss : 91.63948822021484 Val_Reconstruction : 88.13615417480469 Val_KL : 3.5033336877822876\n","Epoch: 4400/8000  Traning Loss: 92.46915435791016  Train_Reconstruction: 88.93067455291748  Train_KL: 3.538481056690216  Validation Loss : 92.59346389770508 Val_Reconstruction : 89.09811401367188 Val_KL : 3.495348334312439\n","Epoch: 4401/8000  Traning Loss: 92.64327144622803  Train_Reconstruction: 89.12381267547607  Train_KL: 3.5194576382637024  Validation Loss : 92.2564926147461 Val_Reconstruction : 88.77943801879883 Val_KL : 3.477052927017212\n","Epoch: 4402/8000  Traning Loss: 92.19253921508789  Train_Reconstruction: 88.6700849533081  Train_KL: 3.522454649209976  Validation Loss : 91.8900032043457 Val_Reconstruction : 88.40496063232422 Val_KL : 3.4850423336029053\n","Epoch: 4403/8000  Traning Loss: 92.30741500854492  Train_Reconstruction: 88.77310371398926  Train_KL: 3.5343113839626312  Validation Loss : 91.87754821777344 Val_Reconstruction : 88.38285827636719 Val_KL : 3.494692325592041\n","Epoch: 4404/8000  Traning Loss: 92.28046798706055  Train_Reconstruction: 88.752272605896  Train_KL: 3.528196394443512  Validation Loss : 91.59096145629883 Val_Reconstruction : 88.10935974121094 Val_KL : 3.4816031455993652\n","Epoch: 4405/8000  Traning Loss: 91.75792694091797  Train_Reconstruction: 88.23610019683838  Train_KL: 3.521826833486557  Validation Loss : 91.5444450378418 Val_Reconstruction : 88.06419372558594 Val_KL : 3.4802515506744385\n","Epoch: 4406/8000  Traning Loss: 91.67292594909668  Train_Reconstruction: 88.15408897399902  Train_KL: 3.5188369154930115  Validation Loss : 91.28893661499023 Val_Reconstruction : 87.80364990234375 Val_KL : 3.4852852821350098\n","Epoch: 4407/8000  Traning Loss: 91.62476444244385  Train_Reconstruction: 88.09485340118408  Train_KL: 3.529911071062088  Validation Loss : 91.58789825439453 Val_Reconstruction : 88.08709335327148 Val_KL : 3.5008041858673096\n","Epoch: 4408/8000  Traning Loss: 91.70230388641357  Train_Reconstruction: 88.17201137542725  Train_KL: 3.5302933156490326  Validation Loss : 91.65677642822266 Val_Reconstruction : 88.17764663696289 Val_KL : 3.479131579399109\n","Epoch: 4409/8000  Traning Loss: 92.05648708343506  Train_Reconstruction: 88.53722953796387  Train_KL: 3.519257038831711  Validation Loss : 91.63847351074219 Val_Reconstruction : 88.16260528564453 Val_KL : 3.4758676290512085\n","Epoch: 4410/8000  Traning Loss: 92.7335376739502  Train_Reconstruction: 89.21247673034668  Train_KL: 3.5210603177547455  Validation Loss : 92.80315780639648 Val_Reconstruction : 89.31932067871094 Val_KL : 3.4838374853134155\n","Epoch: 4411/8000  Traning Loss: 92.63824653625488  Train_Reconstruction: 89.11547756195068  Train_KL: 3.522768974304199  Validation Loss : 92.03422164916992 Val_Reconstruction : 88.56073379516602 Val_KL : 3.4734867811203003\n","Epoch: 4412/8000  Traning Loss: 92.90454959869385  Train_Reconstruction: 89.39268970489502  Train_KL: 3.5118599832057953  Validation Loss : 92.6291732788086 Val_Reconstruction : 89.15476989746094 Val_KL : 3.4744054079055786\n","Epoch: 4413/8000  Traning Loss: 92.87626647949219  Train_Reconstruction: 89.35464382171631  Train_KL: 3.5216229259967804  Validation Loss : 92.30211639404297 Val_Reconstruction : 88.82207870483398 Val_KL : 3.4800376892089844\n","Epoch: 4414/8000  Traning Loss: 92.03899955749512  Train_Reconstruction: 88.51606464385986  Train_KL: 3.522934764623642  Validation Loss : 91.60066604614258 Val_Reconstruction : 88.1301383972168 Val_KL : 3.470528483390808\n","Epoch: 4415/8000  Traning Loss: 91.83285140991211  Train_Reconstruction: 88.32112789154053  Train_KL: 3.5117221772670746  Validation Loss : 91.63154220581055 Val_Reconstruction : 88.158935546875 Val_KL : 3.4726065397262573\n","Epoch: 4416/8000  Traning Loss: 91.9900484085083  Train_Reconstruction: 88.4698257446289  Train_KL: 3.5202222168445587  Validation Loss : 91.77447509765625 Val_Reconstruction : 88.29303359985352 Val_KL : 3.481444478034973\n","Epoch: 4417/8000  Traning Loss: 92.372239112854  Train_Reconstruction: 88.85030174255371  Train_KL: 3.521937370300293  Validation Loss : 91.84708023071289 Val_Reconstruction : 88.36923599243164 Val_KL : 3.4778425693511963\n","Epoch: 4418/8000  Traning Loss: 92.41729736328125  Train_Reconstruction: 88.90420341491699  Train_KL: 3.5130938589572906  Validation Loss : 92.29568862915039 Val_Reconstruction : 88.82836532592773 Val_KL : 3.467321515083313\n","Epoch: 4419/8000  Traning Loss: 92.28651332855225  Train_Reconstruction: 88.76655387878418  Train_KL: 3.5199579894542694  Validation Loss : 92.1412582397461 Val_Reconstruction : 88.65447235107422 Val_KL : 3.486783504486084\n","Epoch: 4420/8000  Traning Loss: 92.2480697631836  Train_Reconstruction: 88.71493530273438  Train_KL: 3.533134400844574  Validation Loss : 91.8976821899414 Val_Reconstruction : 88.40558242797852 Val_KL : 3.4921016693115234\n","Epoch: 4421/8000  Traning Loss: 91.86928749084473  Train_Reconstruction: 88.35181427001953  Train_KL: 3.517472594976425  Validation Loss : 91.31451034545898 Val_Reconstruction : 87.8437385559082 Val_KL : 3.4707733392715454\n","Epoch: 4422/8000  Traning Loss: 91.66504669189453  Train_Reconstruction: 88.14603328704834  Train_KL: 3.5190123319625854  Validation Loss : 91.44152450561523 Val_Reconstruction : 87.96061325073242 Val_KL : 3.4809120893478394\n","Epoch: 4423/8000  Traning Loss: 91.5582685470581  Train_Reconstruction: 88.03322505950928  Train_KL: 3.5250433683395386  Validation Loss : 91.44927597045898 Val_Reconstruction : 87.96514129638672 Val_KL : 3.484134793281555\n","Epoch: 4424/8000  Traning Loss: 91.5211763381958  Train_Reconstruction: 88.0016679763794  Train_KL: 3.5195076167583466  Validation Loss : 91.52692031860352 Val_Reconstruction : 88.04310607910156 Val_KL : 3.48381245136261\n","Epoch: 4425/8000  Traning Loss: 91.56438827514648  Train_Reconstruction: 88.04463768005371  Train_KL: 3.5197502076625824  Validation Loss : 91.64921951293945 Val_Reconstruction : 88.16170883178711 Val_KL : 3.4875080585479736\n","Epoch: 4426/8000  Traning Loss: 91.72220516204834  Train_Reconstruction: 88.18835735321045  Train_KL: 3.533847063779831  Validation Loss : 91.57548522949219 Val_Reconstruction : 88.08057022094727 Val_KL : 3.4949153661727905\n","Epoch: 4427/8000  Traning Loss: 91.75746154785156  Train_Reconstruction: 88.22714328765869  Train_KL: 3.5303179025650024  Validation Loss : 91.71220397949219 Val_Reconstruction : 88.21950149536133 Val_KL : 3.492703676223755\n","Epoch: 4428/8000  Traning Loss: 91.68934535980225  Train_Reconstruction: 88.16467666625977  Train_KL: 3.5246696174144745  Validation Loss : 91.52061080932617 Val_Reconstruction : 88.04084014892578 Val_KL : 3.4797704219818115\n","Epoch: 4429/8000  Traning Loss: 92.07750225067139  Train_Reconstruction: 88.55742645263672  Train_KL: 3.520075261592865  Validation Loss : 92.47052001953125 Val_Reconstruction : 88.98776245117188 Val_KL : 3.4827582836151123\n","Epoch: 4430/8000  Traning Loss: 92.8634614944458  Train_Reconstruction: 89.33875846862793  Train_KL: 3.5247028172016144  Validation Loss : 93.18832015991211 Val_Reconstruction : 89.70300674438477 Val_KL : 3.4853099584579468\n","Epoch: 4431/8000  Traning Loss: 92.43876647949219  Train_Reconstruction: 88.91520690917969  Train_KL: 3.523559331893921  Validation Loss : 91.7967758178711 Val_Reconstruction : 88.31156158447266 Val_KL : 3.4852123260498047\n","Epoch: 4432/8000  Traning Loss: 92.17373752593994  Train_Reconstruction: 88.65480136871338  Train_KL: 3.5189370214939117  Validation Loss : 92.07808303833008 Val_Reconstruction : 88.59844207763672 Val_KL : 3.479640007019043\n","Epoch: 4433/8000  Traning Loss: 92.32598114013672  Train_Reconstruction: 88.80775451660156  Train_KL: 3.5182255804538727  Validation Loss : 92.02070236206055 Val_Reconstruction : 88.53642272949219 Val_KL : 3.484278917312622\n","Epoch: 4434/8000  Traning Loss: 92.14560127258301  Train_Reconstruction: 88.61659812927246  Train_KL: 3.5290035605430603  Validation Loss : 92.0618667602539 Val_Reconstruction : 88.57572937011719 Val_KL : 3.486140727996826\n","Epoch: 4435/8000  Traning Loss: 92.3711051940918  Train_Reconstruction: 88.84516525268555  Train_KL: 3.525940030813217  Validation Loss : 91.85822296142578 Val_Reconstruction : 88.37792205810547 Val_KL : 3.4803022146224976\n","Epoch: 4436/8000  Traning Loss: 91.94232845306396  Train_Reconstruction: 88.41933345794678  Train_KL: 3.522995561361313  Validation Loss : 91.78913879394531 Val_Reconstruction : 88.30258560180664 Val_KL : 3.4865537881851196\n","Epoch: 4437/8000  Traning Loss: 91.82071876525879  Train_Reconstruction: 88.29232978820801  Train_KL: 3.5283890664577484  Validation Loss : 91.69498443603516 Val_Reconstruction : 88.2008285522461 Val_KL : 3.4941548109054565\n","Epoch: 4438/8000  Traning Loss: 91.77724933624268  Train_Reconstruction: 88.25651550292969  Train_KL: 3.5207343995571136  Validation Loss : 91.63711547851562 Val_Reconstruction : 88.16461944580078 Val_KL : 3.4724972248077393\n","Epoch: 4439/8000  Traning Loss: 91.79660511016846  Train_Reconstruction: 88.29163551330566  Train_KL: 3.5049704909324646  Validation Loss : 91.73502731323242 Val_Reconstruction : 88.27240371704102 Val_KL : 3.4626212120056152\n","Epoch: 4440/8000  Traning Loss: 91.61413097381592  Train_Reconstruction: 88.10462856292725  Train_KL: 3.5095010101795197  Validation Loss : 91.54680252075195 Val_Reconstruction : 88.06717681884766 Val_KL : 3.479625940322876\n","Epoch: 4441/8000  Traning Loss: 91.58969020843506  Train_Reconstruction: 88.075852394104  Train_KL: 3.5138385891914368  Validation Loss : 91.57402038574219 Val_Reconstruction : 88.09885025024414 Val_KL : 3.4751691818237305\n","Epoch: 4442/8000  Traning Loss: 91.7507677078247  Train_Reconstruction: 88.23462581634521  Train_KL: 3.516142249107361  Validation Loss : 91.73217010498047 Val_Reconstruction : 88.2506103515625 Val_KL : 3.4815593957901\n","Epoch: 4443/8000  Traning Loss: 92.0509443283081  Train_Reconstruction: 88.53254985809326  Train_KL: 3.518392890691757  Validation Loss : 91.44414138793945 Val_Reconstruction : 87.9615707397461 Val_KL : 3.48257315158844\n","Epoch: 4444/8000  Traning Loss: 91.82475852966309  Train_Reconstruction: 88.30613803863525  Train_KL: 3.5186208188533783  Validation Loss : 91.48165130615234 Val_Reconstruction : 87.99164581298828 Val_KL : 3.4900065660476685\n","Epoch: 4445/8000  Traning Loss: 91.96597957611084  Train_Reconstruction: 88.43788051605225  Train_KL: 3.5280982851982117  Validation Loss : 91.83794021606445 Val_Reconstruction : 88.34104537963867 Val_KL : 3.496895432472229\n","Epoch: 4446/8000  Traning Loss: 92.09718036651611  Train_Reconstruction: 88.57488536834717  Train_KL: 3.522295206785202  Validation Loss : 91.78340911865234 Val_Reconstruction : 88.29815673828125 Val_KL : 3.4852521419525146\n","Epoch: 4447/8000  Traning Loss: 92.03884410858154  Train_Reconstruction: 88.52304553985596  Train_KL: 3.5157983899116516  Validation Loss : 91.73873138427734 Val_Reconstruction : 88.27140045166016 Val_KL : 3.467331886291504\n","Epoch: 4448/8000  Traning Loss: 91.97090530395508  Train_Reconstruction: 88.45867538452148  Train_KL: 3.5122303664684296  Validation Loss : 92.22614669799805 Val_Reconstruction : 88.75418090820312 Val_KL : 3.4719661474227905\n","Epoch: 4449/8000  Traning Loss: 92.19885158538818  Train_Reconstruction: 88.67812252044678  Train_KL: 3.5207284092903137  Validation Loss : 91.91898727416992 Val_Reconstruction : 88.42977142333984 Val_KL : 3.4892163276672363\n","Epoch: 4450/8000  Traning Loss: 91.85578727722168  Train_Reconstruction: 88.33241367340088  Train_KL: 3.523373454809189  Validation Loss : 91.46651840209961 Val_Reconstruction : 87.97877502441406 Val_KL : 3.4877413511276245\n","Epoch: 4451/8000  Traning Loss: 91.80664348602295  Train_Reconstruction: 88.28063583374023  Train_KL: 3.5260073244571686  Validation Loss : 91.75092697143555 Val_Reconstruction : 88.26607513427734 Val_KL : 3.484854817390442\n","Epoch: 4452/8000  Traning Loss: 91.97267246246338  Train_Reconstruction: 88.45092582702637  Train_KL: 3.5217456817626953  Validation Loss : 91.66619110107422 Val_Reconstruction : 88.18365478515625 Val_KL : 3.482535481452942\n","Epoch: 4453/8000  Traning Loss: 91.96432876586914  Train_Reconstruction: 88.44292068481445  Train_KL: 3.521407514810562  Validation Loss : 91.96417617797852 Val_Reconstruction : 88.47897338867188 Val_KL : 3.485202431678772\n","Epoch: 4454/8000  Traning Loss: 92.24507331848145  Train_Reconstruction: 88.71725177764893  Train_KL: 3.5278219282627106  Validation Loss : 92.20858001708984 Val_Reconstruction : 88.71980667114258 Val_KL : 3.488774538040161\n","Epoch: 4455/8000  Traning Loss: 92.40920734405518  Train_Reconstruction: 88.88575839996338  Train_KL: 3.5234501361846924  Validation Loss : 92.90290451049805 Val_Reconstruction : 89.42074203491211 Val_KL : 3.482162356376648\n","Epoch: 4456/8000  Traning Loss: 92.47755432128906  Train_Reconstruction: 88.9600658416748  Train_KL: 3.517487734556198  Validation Loss : 92.98090362548828 Val_Reconstruction : 89.50277328491211 Val_KL : 3.478131651878357\n","Epoch: 4457/8000  Traning Loss: 92.52263355255127  Train_Reconstruction: 89.00513076782227  Train_KL: 3.5175029933452606  Validation Loss : 92.32835006713867 Val_Reconstruction : 88.85010147094727 Val_KL : 3.478248715400696\n","Epoch: 4458/8000  Traning Loss: 92.01645755767822  Train_Reconstruction: 88.50294399261475  Train_KL: 3.513513505458832  Validation Loss : 91.95544052124023 Val_Reconstruction : 88.48620223999023 Val_KL : 3.469239354133606\n","Epoch: 4459/8000  Traning Loss: 91.99249839782715  Train_Reconstruction: 88.47626781463623  Train_KL: 3.5162323713302612  Validation Loss : 91.86483001708984 Val_Reconstruction : 88.38235092163086 Val_KL : 3.4824817180633545\n","Epoch: 4460/8000  Traning Loss: 91.88095760345459  Train_Reconstruction: 88.35809898376465  Train_KL: 3.5228598415851593  Validation Loss : 91.5475082397461 Val_Reconstruction : 88.06533432006836 Val_KL : 3.482174038887024\n","Epoch: 4461/8000  Traning Loss: 91.70242309570312  Train_Reconstruction: 88.1737174987793  Train_KL: 3.528705358505249  Validation Loss : 91.59171295166016 Val_Reconstruction : 88.10395050048828 Val_KL : 3.4877625703811646\n","Epoch: 4462/8000  Traning Loss: 91.5264310836792  Train_Reconstruction: 88.00277519226074  Train_KL: 3.523655891418457  Validation Loss : 91.36824035644531 Val_Reconstruction : 87.88554763793945 Val_KL : 3.4826894998550415\n","Epoch: 4463/8000  Traning Loss: 91.63158512115479  Train_Reconstruction: 88.10787296295166  Train_KL: 3.5237119495868683  Validation Loss : 91.50088500976562 Val_Reconstruction : 88.02372741699219 Val_KL : 3.4771560430526733\n","Epoch: 4464/8000  Traning Loss: 91.55466556549072  Train_Reconstruction: 88.03384971618652  Train_KL: 3.52081698179245  Validation Loss : 91.5120964050293 Val_Reconstruction : 88.02669906616211 Val_KL : 3.485396981239319\n","Epoch: 4465/8000  Traning Loss: 91.68357372283936  Train_Reconstruction: 88.15742301940918  Train_KL: 3.5261495411396027  Validation Loss : 91.54270553588867 Val_Reconstruction : 88.0521125793457 Val_KL : 3.490594267845154\n","Epoch: 4466/8000  Traning Loss: 91.62585067749023  Train_Reconstruction: 88.10569095611572  Train_KL: 3.5201604664325714  Validation Loss : 91.41282272338867 Val_Reconstruction : 87.9394760131836 Val_KL : 3.4733468294143677\n","Epoch: 4467/8000  Traning Loss: 91.6213493347168  Train_Reconstruction: 88.11025714874268  Train_KL: 3.5110913813114166  Validation Loss : 91.76325988769531 Val_Reconstruction : 88.28113555908203 Val_KL : 3.4821231365203857\n","Epoch: 4468/8000  Traning Loss: 92.07662868499756  Train_Reconstruction: 88.54989719390869  Train_KL: 3.5267317593097687  Validation Loss : 92.48825454711914 Val_Reconstruction : 88.99503707885742 Val_KL : 3.4932193756103516\n","Epoch: 4469/8000  Traning Loss: 92.05323791503906  Train_Reconstruction: 88.52586555480957  Train_KL: 3.527372807264328  Validation Loss : 91.89608001708984 Val_Reconstruction : 88.41077041625977 Val_KL : 3.4853121042251587\n","Epoch: 4470/8000  Traning Loss: 91.79646587371826  Train_Reconstruction: 88.28133487701416  Train_KL: 3.5151319205760956  Validation Loss : 91.51881790161133 Val_Reconstruction : 88.04826736450195 Val_KL : 3.470549702644348\n","Epoch: 4471/8000  Traning Loss: 91.93270683288574  Train_Reconstruction: 88.42446708679199  Train_KL: 3.508239060640335  Validation Loss : 91.81048965454102 Val_Reconstruction : 88.3359489440918 Val_KL : 3.474541187286377\n","Epoch: 4472/8000  Traning Loss: 92.26745700836182  Train_Reconstruction: 88.74943828582764  Train_KL: 3.518018215894699  Validation Loss : 92.15422058105469 Val_Reconstruction : 88.67707061767578 Val_KL : 3.4771519899368286\n","Epoch: 4473/8000  Traning Loss: 92.45255851745605  Train_Reconstruction: 88.9296760559082  Train_KL: 3.522882401943207  Validation Loss : 92.2673225402832 Val_Reconstruction : 88.78041458129883 Val_KL : 3.4869062900543213\n","Epoch: 4474/8000  Traning Loss: 91.84216022491455  Train_Reconstruction: 88.31520748138428  Train_KL: 3.5269518196582794  Validation Loss : 91.6562614440918 Val_Reconstruction : 88.16242599487305 Val_KL : 3.4938371181488037\n","Epoch: 4475/8000  Traning Loss: 91.80003452301025  Train_Reconstruction: 88.26593780517578  Train_KL: 3.5340967178344727  Validation Loss : 91.97487258911133 Val_Reconstruction : 88.4814567565918 Val_KL : 3.49341881275177\n","Epoch: 4476/8000  Traning Loss: 92.31812763214111  Train_Reconstruction: 88.79182624816895  Train_KL: 3.52630153298378  Validation Loss : 92.05247116088867 Val_Reconstruction : 88.57046890258789 Val_KL : 3.482003092765808\n","Epoch: 4477/8000  Traning Loss: 92.07510662078857  Train_Reconstruction: 88.55699443817139  Train_KL: 3.5181104242801666  Validation Loss : 91.68194580078125 Val_Reconstruction : 88.20241165161133 Val_KL : 3.4795361757278442\n","Epoch: 4478/8000  Traning Loss: 92.06731414794922  Train_Reconstruction: 88.54381561279297  Train_KL: 3.5234987437725067  Validation Loss : 91.86686325073242 Val_Reconstruction : 88.37702941894531 Val_KL : 3.489834427833557\n","Epoch: 4479/8000  Traning Loss: 91.8304615020752  Train_Reconstruction: 88.30739879608154  Train_KL: 3.52306267619133  Validation Loss : 91.6651611328125 Val_Reconstruction : 88.18224334716797 Val_KL : 3.482916235923767\n","Epoch: 4480/8000  Traning Loss: 91.60094165802002  Train_Reconstruction: 88.07360363006592  Train_KL: 3.5273378789424896  Validation Loss : 91.46183395385742 Val_Reconstruction : 87.97627639770508 Val_KL : 3.4855594635009766\n","Epoch: 4481/8000  Traning Loss: 91.69162940979004  Train_Reconstruction: 88.16675472259521  Train_KL: 3.5248743295669556  Validation Loss : 91.4895133972168 Val_Reconstruction : 88.00497055053711 Val_KL : 3.4845428466796875\n","Epoch: 4482/8000  Traning Loss: 91.95426082611084  Train_Reconstruction: 88.43641471862793  Train_KL: 3.517847090959549  Validation Loss : 91.94404602050781 Val_Reconstruction : 88.47496795654297 Val_KL : 3.469076991081238\n","Epoch: 4483/8000  Traning Loss: 92.1897611618042  Train_Reconstruction: 88.67704677581787  Train_KL: 3.5127145648002625  Validation Loss : 92.74446105957031 Val_Reconstruction : 89.26993179321289 Val_KL : 3.4745302200317383\n","Epoch: 4484/8000  Traning Loss: 92.26794147491455  Train_Reconstruction: 88.74526977539062  Train_KL: 3.5226714611053467  Validation Loss : 91.64432907104492 Val_Reconstruction : 88.16064834594727 Val_KL : 3.4836819171905518\n","Epoch: 4485/8000  Traning Loss: 91.69161891937256  Train_Reconstruction: 88.17420673370361  Train_KL: 3.517412483692169  Validation Loss : 91.42977905273438 Val_Reconstruction : 87.95808029174805 Val_KL : 3.4716975688934326\n","Epoch: 4486/8000  Traning Loss: 91.64197540283203  Train_Reconstruction: 88.12753391265869  Train_KL: 3.5144415497779846  Validation Loss : 91.51945877075195 Val_Reconstruction : 88.04377746582031 Val_KL : 3.4756805896759033\n","Epoch: 4487/8000  Traning Loss: 91.65590476989746  Train_Reconstruction: 88.13215732574463  Train_KL: 3.5237480998039246  Validation Loss : 91.81243896484375 Val_Reconstruction : 88.32275390625 Val_KL : 3.489684224128723\n","Epoch: 4488/8000  Traning Loss: 92.06735420227051  Train_Reconstruction: 88.53760147094727  Train_KL: 3.5297522842884064  Validation Loss : 91.8840103149414 Val_Reconstruction : 88.40029907226562 Val_KL : 3.483710527420044\n","Epoch: 4489/8000  Traning Loss: 92.37705898284912  Train_Reconstruction: 88.86121368408203  Train_KL: 3.515844911336899  Validation Loss : 92.2183723449707 Val_Reconstruction : 88.74163055419922 Val_KL : 3.4767398834228516\n","Epoch: 4490/8000  Traning Loss: 92.58781433105469  Train_Reconstruction: 89.06747436523438  Train_KL: 3.5203396379947662  Validation Loss : 92.68746566772461 Val_Reconstruction : 89.20743179321289 Val_KL : 3.480036735534668\n","Epoch: 4491/8000  Traning Loss: 92.14460563659668  Train_Reconstruction: 88.62870693206787  Train_KL: 3.5159001648426056  Validation Loss : 91.80752944946289 Val_Reconstruction : 88.3371353149414 Val_KL : 3.470397710800171\n","Epoch: 4492/8000  Traning Loss: 91.84099674224854  Train_Reconstruction: 88.32721424102783  Train_KL: 3.5137815177440643  Validation Loss : 91.64892578125 Val_Reconstruction : 88.17412567138672 Val_KL : 3.4747989177703857\n","Epoch: 4493/8000  Traning Loss: 91.8010721206665  Train_Reconstruction: 88.28373908996582  Train_KL: 3.5173327326774597  Validation Loss : 91.7016372680664 Val_Reconstruction : 88.22324752807617 Val_KL : 3.478389859199524\n","Epoch: 4494/8000  Traning Loss: 92.113356590271  Train_Reconstruction: 88.58842945098877  Train_KL: 3.524926096200943  Validation Loss : 92.13974380493164 Val_Reconstruction : 88.64891052246094 Val_KL : 3.490832567214966\n","Epoch: 4495/8000  Traning Loss: 91.9867582321167  Train_Reconstruction: 88.4615707397461  Train_KL: 3.5251881778240204  Validation Loss : 91.71611785888672 Val_Reconstruction : 88.24478530883789 Val_KL : 3.471331477165222\n","Epoch: 4496/8000  Traning Loss: 91.9567928314209  Train_Reconstruction: 88.44994926452637  Train_KL: 3.506843239068985  Validation Loss : 91.82345581054688 Val_Reconstruction : 88.3536262512207 Val_KL : 3.4698328971862793\n","Epoch: 4497/8000  Traning Loss: 92.14611148834229  Train_Reconstruction: 88.63525390625  Train_KL: 3.510856509208679  Validation Loss : 91.58508682250977 Val_Reconstruction : 88.10771942138672 Val_KL : 3.4773677587509155\n","Epoch: 4498/8000  Traning Loss: 91.93025779724121  Train_Reconstruction: 88.41119480133057  Train_KL: 3.519064575433731  Validation Loss : 91.70397567749023 Val_Reconstruction : 88.22809982299805 Val_KL : 3.4758740663528442\n","Epoch: 4499/8000  Traning Loss: 92.18284225463867  Train_Reconstruction: 88.6673755645752  Train_KL: 3.5154662430286407  Validation Loss : 92.42258834838867 Val_Reconstruction : 88.9513931274414 Val_KL : 3.4711943864822388\n","Epoch: 4500/8000  Traning Loss: 93.3058671951294  Train_Reconstruction: 89.78744983673096  Train_KL: 3.5184179842472076  Validation Loss : 93.27909851074219 Val_Reconstruction : 89.8025131225586 Val_KL : 3.476582884788513\n","Epoch: 4501/8000  Traning Loss: 93.96073913574219  Train_Reconstruction: 90.43863296508789  Train_KL: 3.5221068263053894  Validation Loss : 93.8244514465332 Val_Reconstruction : 90.34789276123047 Val_KL : 3.4765557050704956\n","Epoch: 4502/8000  Traning Loss: 92.79839324951172  Train_Reconstruction: 89.2776985168457  Train_KL: 3.520695596933365  Validation Loss : 91.70960235595703 Val_Reconstruction : 88.23238754272461 Val_KL : 3.4772132635116577\n","Epoch: 4503/8000  Traning Loss: 91.68082809448242  Train_Reconstruction: 88.16575336456299  Train_KL: 3.5150735080242157  Validation Loss : 91.30739212036133 Val_Reconstruction : 87.82737731933594 Val_KL : 3.4800117015838623\n","Epoch: 4504/8000  Traning Loss: 91.55155181884766  Train_Reconstruction: 88.0286865234375  Train_KL: 3.5228648483753204  Validation Loss : 91.42461776733398 Val_Reconstruction : 87.93709182739258 Val_KL : 3.487523317337036\n","Epoch: 4505/8000  Traning Loss: 91.70924282073975  Train_Reconstruction: 88.18969440460205  Train_KL: 3.5195471048355103  Validation Loss : 91.51581192016602 Val_Reconstruction : 88.0425796508789 Val_KL : 3.473234176635742\n","Epoch: 4506/8000  Traning Loss: 91.90638542175293  Train_Reconstruction: 88.38886070251465  Train_KL: 3.5175244212150574  Validation Loss : 91.9211311340332 Val_Reconstruction : 88.43765258789062 Val_KL : 3.4834799766540527\n","Epoch: 4507/8000  Traning Loss: 92.03789806365967  Train_Reconstruction: 88.51188564300537  Train_KL: 3.5260131657123566  Validation Loss : 92.07693099975586 Val_Reconstruction : 88.58657455444336 Val_KL : 3.490357995033264\n","Epoch: 4508/8000  Traning Loss: 91.89964580535889  Train_Reconstruction: 88.38114833831787  Train_KL: 3.518498182296753  Validation Loss : 92.07068252563477 Val_Reconstruction : 88.59674453735352 Val_KL : 3.473939061164856\n","Epoch: 4509/8000  Traning Loss: 91.84943675994873  Train_Reconstruction: 88.33637619018555  Train_KL: 3.513059288263321  Validation Loss : 91.69478988647461 Val_Reconstruction : 88.21673583984375 Val_KL : 3.4780540466308594\n","Epoch: 4510/8000  Traning Loss: 91.80313301086426  Train_Reconstruction: 88.27935409545898  Train_KL: 3.5237790942192078  Validation Loss : 91.93875122070312 Val_Reconstruction : 88.45545959472656 Val_KL : 3.483293652534485\n","Epoch: 4511/8000  Traning Loss: 92.00070858001709  Train_Reconstruction: 88.49059200286865  Train_KL: 3.510116845369339  Validation Loss : 91.5171890258789 Val_Reconstruction : 88.04486465454102 Val_KL : 3.472323179244995\n","Epoch: 4512/8000  Traning Loss: 92.01760292053223  Train_Reconstruction: 88.49684238433838  Train_KL: 3.5207602083683014  Validation Loss : 91.91741180419922 Val_Reconstruction : 88.41938781738281 Val_KL : 3.4980236291885376\n","Epoch: 4513/8000  Traning Loss: 91.93031311035156  Train_Reconstruction: 88.39666271209717  Train_KL: 3.5336499214172363  Validation Loss : 91.61737823486328 Val_Reconstruction : 88.12630462646484 Val_KL : 3.4910709857940674\n","Epoch: 4514/8000  Traning Loss: 91.71235847473145  Train_Reconstruction: 88.19327163696289  Train_KL: 3.5190865993499756  Validation Loss : 91.4156379699707 Val_Reconstruction : 87.94524383544922 Val_KL : 3.4703906774520874\n","Epoch: 4515/8000  Traning Loss: 91.86674308776855  Train_Reconstruction: 88.34387588500977  Train_KL: 3.5228683054447174  Validation Loss : 91.68512725830078 Val_Reconstruction : 88.19713973999023 Val_KL : 3.4879872798919678\n","Epoch: 4516/8000  Traning Loss: 91.94586563110352  Train_Reconstruction: 88.41647434234619  Train_KL: 3.529392808675766  Validation Loss : 91.94445419311523 Val_Reconstruction : 88.46136093139648 Val_KL : 3.4830915927886963\n","Epoch: 4517/8000  Traning Loss: 91.72268295288086  Train_Reconstruction: 88.20678043365479  Train_KL: 3.5159030854701996  Validation Loss : 91.69633865356445 Val_Reconstruction : 88.2215461730957 Val_KL : 3.474792718887329\n","Epoch: 4518/8000  Traning Loss: 92.01051425933838  Train_Reconstruction: 88.50402736663818  Train_KL: 3.506487488746643  Validation Loss : 92.16600036621094 Val_Reconstruction : 88.69728469848633 Val_KL : 3.468715786933899\n","Epoch: 4519/8000  Traning Loss: 92.47085189819336  Train_Reconstruction: 88.95655632019043  Train_KL: 3.514295667409897  Validation Loss : 92.44597244262695 Val_Reconstruction : 88.96894454956055 Val_KL : 3.4770275354385376\n","Epoch: 4520/8000  Traning Loss: 92.61751556396484  Train_Reconstruction: 89.10215377807617  Train_KL: 3.515361934900284  Validation Loss : 92.26795196533203 Val_Reconstruction : 88.7951774597168 Val_KL : 3.4727723598480225\n","Epoch: 4521/8000  Traning Loss: 91.95101833343506  Train_Reconstruction: 88.43534278869629  Train_KL: 3.515674501657486  Validation Loss : 91.65916061401367 Val_Reconstruction : 88.17642593383789 Val_KL : 3.4827362298965454\n","Epoch: 4522/8000  Traning Loss: 91.88365840911865  Train_Reconstruction: 88.35655879974365  Train_KL: 3.5270990431308746  Validation Loss : 91.9692268371582 Val_Reconstruction : 88.48224639892578 Val_KL : 3.486977696418762\n","Epoch: 4523/8000  Traning Loss: 91.9351396560669  Train_Reconstruction: 88.40832805633545  Train_KL: 3.526811569929123  Validation Loss : 91.76766967773438 Val_Reconstruction : 88.28726959228516 Val_KL : 3.4804006814956665\n","Epoch: 4524/8000  Traning Loss: 92.13380527496338  Train_Reconstruction: 88.61334896087646  Train_KL: 3.5204564332962036  Validation Loss : 91.96481704711914 Val_Reconstruction : 88.4903450012207 Val_KL : 3.474472403526306\n","Epoch: 4525/8000  Traning Loss: 91.84822463989258  Train_Reconstruction: 88.32933044433594  Train_KL: 3.51889431476593  Validation Loss : 91.7750473022461 Val_Reconstruction : 88.28981399536133 Val_KL : 3.4852317571640015\n","Epoch: 4526/8000  Traning Loss: 91.63678932189941  Train_Reconstruction: 88.10657024383545  Train_KL: 3.5302166044712067  Validation Loss : 91.5412483215332 Val_Reconstruction : 88.04443359375 Val_KL : 3.4968161582946777\n","Epoch: 4527/8000  Traning Loss: 91.42033576965332  Train_Reconstruction: 87.8920431137085  Train_KL: 3.528293251991272  Validation Loss : 91.28924179077148 Val_Reconstruction : 87.79719924926758 Val_KL : 3.4920451641082764\n","Epoch: 4528/8000  Traning Loss: 91.56877040863037  Train_Reconstruction: 88.04454708099365  Train_KL: 3.5242225527763367  Validation Loss : 91.62537002563477 Val_Reconstruction : 88.14230346679688 Val_KL : 3.4830660820007324\n","Epoch: 4529/8000  Traning Loss: 91.62839889526367  Train_Reconstruction: 88.1085433959961  Train_KL: 3.519854336977005  Validation Loss : 91.34876251220703 Val_Reconstruction : 87.8647575378418 Val_KL : 3.4840049743652344\n","Epoch: 4530/8000  Traning Loss: 91.54338455200195  Train_Reconstruction: 88.02653503417969  Train_KL: 3.516849011182785  Validation Loss : 91.41484451293945 Val_Reconstruction : 87.9415397644043 Val_KL : 3.473305344581604\n","Epoch: 4531/8000  Traning Loss: 91.84674835205078  Train_Reconstruction: 88.33308792114258  Train_KL: 3.51366063952446  Validation Loss : 91.68196868896484 Val_Reconstruction : 88.20544052124023 Val_KL : 3.4765299558639526\n","Epoch: 4532/8000  Traning Loss: 92.1158504486084  Train_Reconstruction: 88.59521865844727  Train_KL: 3.5206333100795746  Validation Loss : 91.97787857055664 Val_Reconstruction : 88.4930534362793 Val_KL : 3.484826683998108\n","Epoch: 4533/8000  Traning Loss: 92.19111251831055  Train_Reconstruction: 88.6654920578003  Train_KL: 3.5256216526031494  Validation Loss : 92.19629669189453 Val_Reconstruction : 88.71409225463867 Val_KL : 3.482204556465149\n","Epoch: 4534/8000  Traning Loss: 92.01303768157959  Train_Reconstruction: 88.49742126464844  Train_KL: 3.515616923570633  Validation Loss : 91.56815338134766 Val_Reconstruction : 88.09326171875 Val_KL : 3.4748932123184204\n","Epoch: 4535/8000  Traning Loss: 91.8495044708252  Train_Reconstruction: 88.33830833435059  Train_KL: 3.5111958384513855  Validation Loss : 91.69527435302734 Val_Reconstruction : 88.22561264038086 Val_KL : 3.4696621894836426\n","Epoch: 4536/8000  Traning Loss: 91.78007507324219  Train_Reconstruction: 88.26272296905518  Train_KL: 3.5173519253730774  Validation Loss : 91.43408203125 Val_Reconstruction : 87.9491195678711 Val_KL : 3.484959602355957\n","Epoch: 4537/8000  Traning Loss: 91.55097389221191  Train_Reconstruction: 88.0229024887085  Train_KL: 3.528070956468582  Validation Loss : 91.53626251220703 Val_Reconstruction : 88.04567337036133 Val_KL : 3.4905911684036255\n","Epoch: 4538/8000  Traning Loss: 91.64093112945557  Train_Reconstruction: 88.11591625213623  Train_KL: 3.5250138640403748  Validation Loss : 91.25019454956055 Val_Reconstruction : 87.76612854003906 Val_KL : 3.4840673208236694\n","Epoch: 4539/8000  Traning Loss: 91.40262794494629  Train_Reconstruction: 87.88207817077637  Train_KL: 3.520548701286316  Validation Loss : 91.28751754760742 Val_Reconstruction : 87.80686569213867 Val_KL : 3.480650782585144\n","Epoch: 4540/8000  Traning Loss: 91.59681129455566  Train_Reconstruction: 88.07330226898193  Train_KL: 3.523509234189987  Validation Loss : 91.74617004394531 Val_Reconstruction : 88.26242446899414 Val_KL : 3.4837489128112793\n","Epoch: 4541/8000  Traning Loss: 91.79316997528076  Train_Reconstruction: 88.27287483215332  Train_KL: 3.5202941596508026  Validation Loss : 91.76832962036133 Val_Reconstruction : 88.29171371459961 Val_KL : 3.4766141176223755\n","Epoch: 4542/8000  Traning Loss: 92.02470684051514  Train_Reconstruction: 88.5066967010498  Train_KL: 3.5180107057094574  Validation Loss : 91.73876953125 Val_Reconstruction : 88.2531967163086 Val_KL : 3.485573649406433\n","Epoch: 4543/8000  Traning Loss: 91.8975477218628  Train_Reconstruction: 88.37216472625732  Train_KL: 3.52538138628006  Validation Loss : 91.69169998168945 Val_Reconstruction : 88.20034790039062 Val_KL : 3.4913523197174072\n","Epoch: 4544/8000  Traning Loss: 91.90369987487793  Train_Reconstruction: 88.37867069244385  Train_KL: 3.525029808282852  Validation Loss : 91.5925064086914 Val_Reconstruction : 88.10717391967773 Val_KL : 3.4853333234786987\n","Epoch: 4545/8000  Traning Loss: 91.74277687072754  Train_Reconstruction: 88.22127342224121  Train_KL: 3.521503299474716  Validation Loss : 91.43922424316406 Val_Reconstruction : 87.97298049926758 Val_KL : 3.4662444591522217\n","Epoch: 4546/8000  Traning Loss: 91.67501068115234  Train_Reconstruction: 88.17116832733154  Train_KL: 3.5038414001464844  Validation Loss : 91.49308776855469 Val_Reconstruction : 88.0269775390625 Val_KL : 3.4661078453063965\n","Epoch: 4547/8000  Traning Loss: 92.0482120513916  Train_Reconstruction: 88.53561115264893  Train_KL: 3.5126017034053802  Validation Loss : 92.09920120239258 Val_Reconstruction : 88.6234245300293 Val_KL : 3.4757765531539917\n","Epoch: 4548/8000  Traning Loss: 92.06606101989746  Train_Reconstruction: 88.54857349395752  Train_KL: 3.517486721277237  Validation Loss : 91.7997932434082 Val_Reconstruction : 88.32178115844727 Val_KL : 3.4780123233795166\n","Epoch: 4549/8000  Traning Loss: 91.99990653991699  Train_Reconstruction: 88.4706335067749  Train_KL: 3.5292740166187286  Validation Loss : 91.46673202514648 Val_Reconstruction : 87.98078536987305 Val_KL : 3.485946774482727\n","Epoch: 4550/8000  Traning Loss: 91.63308811187744  Train_Reconstruction: 88.10925197601318  Train_KL: 3.523837924003601  Validation Loss : 91.55413818359375 Val_Reconstruction : 88.07105255126953 Val_KL : 3.483085870742798\n","Epoch: 4551/8000  Traning Loss: 91.45628261566162  Train_Reconstruction: 87.92533016204834  Train_KL: 3.530952602624893  Validation Loss : 91.23416519165039 Val_Reconstruction : 87.73567962646484 Val_KL : 3.498488187789917\n","Epoch: 4552/8000  Traning Loss: 92.01079940795898  Train_Reconstruction: 88.47854137420654  Train_KL: 3.5322569012641907  Validation Loss : 92.05011749267578 Val_Reconstruction : 88.56610870361328 Val_KL : 3.484010338783264\n","Epoch: 4553/8000  Traning Loss: 92.04395008087158  Train_Reconstruction: 88.52724361419678  Train_KL: 3.5167065858840942  Validation Loss : 91.65063858032227 Val_Reconstruction : 88.17561721801758 Val_KL : 3.4750189781188965\n","Epoch: 4554/8000  Traning Loss: 91.95816802978516  Train_Reconstruction: 88.44526767730713  Train_KL: 3.5129002928733826  Validation Loss : 92.05973815917969 Val_Reconstruction : 88.5830307006836 Val_KL : 3.476707696914673\n","Epoch: 4555/8000  Traning Loss: 91.76077842712402  Train_Reconstruction: 88.23922443389893  Train_KL: 3.5215534567832947  Validation Loss : 91.51468276977539 Val_Reconstruction : 88.0228500366211 Val_KL : 3.4918304681777954\n","Epoch: 4556/8000  Traning Loss: 91.80597114562988  Train_Reconstruction: 88.28258991241455  Train_KL: 3.5233807265758514  Validation Loss : 91.76300048828125 Val_Reconstruction : 88.27997970581055 Val_KL : 3.4830187559127808\n","Epoch: 4557/8000  Traning Loss: 91.57452583312988  Train_Reconstruction: 88.05086898803711  Train_KL: 3.5236574709415436  Validation Loss : 91.18764877319336 Val_Reconstruction : 87.69468688964844 Val_KL : 3.4929614067077637\n","Epoch: 4558/8000  Traning Loss: 91.39359951019287  Train_Reconstruction: 87.86488723754883  Train_KL: 3.5287120938301086  Validation Loss : 91.37986755371094 Val_Reconstruction : 87.88956451416016 Val_KL : 3.490302801132202\n","Epoch: 4559/8000  Traning Loss: 91.43707084655762  Train_Reconstruction: 87.9144115447998  Train_KL: 3.5226582288742065  Validation Loss : 91.2524528503418 Val_Reconstruction : 87.76900863647461 Val_KL : 3.483442783355713\n","Epoch: 4560/8000  Traning Loss: 91.37962532043457  Train_Reconstruction: 87.86079692840576  Train_KL: 3.5188296139240265  Validation Loss : 91.06369400024414 Val_Reconstruction : 87.59360885620117 Val_KL : 3.4700846672058105\n","Epoch: 4561/8000  Traning Loss: 91.4009199142456  Train_Reconstruction: 87.88529682159424  Train_KL: 3.5156224071979523  Validation Loss : 91.65234375 Val_Reconstruction : 88.17971420288086 Val_KL : 3.4726297855377197\n","Epoch: 4562/8000  Traning Loss: 91.95201206207275  Train_Reconstruction: 88.43592643737793  Train_KL: 3.5160852670669556  Validation Loss : 92.09383010864258 Val_Reconstruction : 88.6130485534668 Val_KL : 3.480782628059387\n","Epoch: 4563/8000  Traning Loss: 92.36979961395264  Train_Reconstruction: 88.84466648101807  Train_KL: 3.525133728981018  Validation Loss : 91.94525146484375 Val_Reconstruction : 88.45221328735352 Val_KL : 3.4930368661880493\n","Epoch: 4564/8000  Traning Loss: 91.84962940216064  Train_Reconstruction: 88.31970596313477  Train_KL: 3.5299247205257416  Validation Loss : 91.40745544433594 Val_Reconstruction : 87.92745971679688 Val_KL : 3.4799946546554565\n","Epoch: 4565/8000  Traning Loss: 91.63134479522705  Train_Reconstruction: 88.11749267578125  Train_KL: 3.5138521790504456  Validation Loss : 91.51440811157227 Val_Reconstruction : 88.04004669189453 Val_KL : 3.474363327026367\n","Epoch: 4566/8000  Traning Loss: 91.6262092590332  Train_Reconstruction: 88.10794067382812  Train_KL: 3.5182699263095856  Validation Loss : 91.38235855102539 Val_Reconstruction : 87.89361953735352 Val_KL : 3.4887397289276123\n","Epoch: 4567/8000  Traning Loss: 91.67340850830078  Train_Reconstruction: 88.14861679077148  Train_KL: 3.5247932076454163  Validation Loss : 91.64897155761719 Val_Reconstruction : 88.15924835205078 Val_KL : 3.489722490310669\n","Epoch: 4568/8000  Traning Loss: 91.74029159545898  Train_Reconstruction: 88.21898078918457  Train_KL: 3.5213094651699066  Validation Loss : 91.71853256225586 Val_Reconstruction : 88.23540878295898 Val_KL : 3.483121871948242\n","Epoch: 4569/8000  Traning Loss: 91.51614189147949  Train_Reconstruction: 88.00116443634033  Train_KL: 3.5149780809879303  Validation Loss : 91.1290283203125 Val_Reconstruction : 87.6548957824707 Val_KL : 3.474129796028137\n","Epoch: 4570/8000  Traning Loss: 91.50476169586182  Train_Reconstruction: 87.98384284973145  Train_KL: 3.520919591188431  Validation Loss : 91.37035369873047 Val_Reconstruction : 87.88483810424805 Val_KL : 3.485515594482422\n","Epoch: 4571/8000  Traning Loss: 91.70463180541992  Train_Reconstruction: 88.18205261230469  Train_KL: 3.522579789161682  Validation Loss : 91.61704635620117 Val_Reconstruction : 88.1364517211914 Val_KL : 3.4805946350097656\n","Epoch: 4572/8000  Traning Loss: 91.80652523040771  Train_Reconstruction: 88.29168319702148  Train_KL: 3.5148407220840454  Validation Loss : 91.7047348022461 Val_Reconstruction : 88.2288932800293 Val_KL : 3.475843667984009\n","Epoch: 4573/8000  Traning Loss: 91.7288007736206  Train_Reconstruction: 88.21331214904785  Train_KL: 3.515488862991333  Validation Loss : 91.74885559082031 Val_Reconstruction : 88.2659683227539 Val_KL : 3.4828871488571167\n","Epoch: 4574/8000  Traning Loss: 91.87661266326904  Train_Reconstruction: 88.34907913208008  Train_KL: 3.527535557746887  Validation Loss : 91.74881744384766 Val_Reconstruction : 88.25759887695312 Val_KL : 3.4912153482437134\n","Epoch: 4575/8000  Traning Loss: 92.03498840332031  Train_Reconstruction: 88.51622581481934  Train_KL: 3.5187624990940094  Validation Loss : 92.01382064819336 Val_Reconstruction : 88.54003524780273 Val_KL : 3.473784923553467\n","Epoch: 4576/8000  Traning Loss: 91.80316543579102  Train_Reconstruction: 88.29897499084473  Train_KL: 3.5041886270046234  Validation Loss : 91.79762649536133 Val_Reconstruction : 88.31990051269531 Val_KL : 3.4777257442474365\n","Epoch: 4577/8000  Traning Loss: 91.97750282287598  Train_Reconstruction: 88.46387577056885  Train_KL: 3.5136274099349976  Validation Loss : 91.44204711914062 Val_Reconstruction : 87.9567756652832 Val_KL : 3.485270142555237\n","Epoch: 4578/8000  Traning Loss: 91.90342807769775  Train_Reconstruction: 88.37952709197998  Train_KL: 3.5239013135433197  Validation Loss : 91.87348556518555 Val_Reconstruction : 88.38762664794922 Val_KL : 3.485858678817749\n","Epoch: 4579/8000  Traning Loss: 91.82265758514404  Train_Reconstruction: 88.29684066772461  Train_KL: 3.525817185640335  Validation Loss : 91.58008575439453 Val_Reconstruction : 88.0905876159668 Val_KL : 3.4894999265670776\n","Epoch: 4580/8000  Traning Loss: 91.72965240478516  Train_Reconstruction: 88.21462726593018  Train_KL: 3.515025347471237  Validation Loss : 91.65010070800781 Val_Reconstruction : 88.17434310913086 Val_KL : 3.4757574796676636\n","Epoch: 4581/8000  Traning Loss: 92.272216796875  Train_Reconstruction: 88.75472068786621  Train_KL: 3.517496883869171  Validation Loss : 92.5557861328125 Val_Reconstruction : 89.07045364379883 Val_KL : 3.485333800315857\n","Epoch: 4582/8000  Traning Loss: 92.1673231124878  Train_Reconstruction: 88.64213848114014  Train_KL: 3.5251834094524384  Validation Loss : 91.69931030273438 Val_Reconstruction : 88.2232666015625 Val_KL : 3.4760414361953735\n","Epoch: 4583/8000  Traning Loss: 91.62349891662598  Train_Reconstruction: 88.1136827468872  Train_KL: 3.509815216064453  Validation Loss : 91.48563766479492 Val_Reconstruction : 88.02253341674805 Val_KL : 3.4631036520004272\n","Epoch: 4584/8000  Traning Loss: 91.69023132324219  Train_Reconstruction: 88.16941738128662  Train_KL: 3.5208151042461395  Validation Loss : 91.5119857788086 Val_Reconstruction : 88.01826095581055 Val_KL : 3.493723750114441\n","Epoch: 4585/8000  Traning Loss: 91.65889167785645  Train_Reconstruction: 88.12755680084229  Train_KL: 3.531334728002548  Validation Loss : 91.52945327758789 Val_Reconstruction : 88.03913116455078 Val_KL : 3.4903247356414795\n","Epoch: 4586/8000  Traning Loss: 91.72527599334717  Train_Reconstruction: 88.21167945861816  Train_KL: 3.5135965645313263  Validation Loss : 91.69546508789062 Val_Reconstruction : 88.21853256225586 Val_KL : 3.476929783821106\n","Epoch: 4587/8000  Traning Loss: 91.71444892883301  Train_Reconstruction: 88.19986343383789  Train_KL: 3.514585494995117  Validation Loss : 91.46876907348633 Val_Reconstruction : 87.99349975585938 Val_KL : 3.4752726554870605\n","Epoch: 4588/8000  Traning Loss: 91.9743595123291  Train_Reconstruction: 88.46511745452881  Train_KL: 3.509241223335266  Validation Loss : 91.85736846923828 Val_Reconstruction : 88.38843154907227 Val_KL : 3.468936562538147\n","Epoch: 4589/8000  Traning Loss: 91.96363735198975  Train_Reconstruction: 88.45017910003662  Train_KL: 3.5134592056274414  Validation Loss : 91.98371124267578 Val_Reconstruction : 88.51115036010742 Val_KL : 3.472562789916992\n","Epoch: 4590/8000  Traning Loss: 91.91847133636475  Train_Reconstruction: 88.39310073852539  Train_KL: 3.5253709256649017  Validation Loss : 91.8913803100586 Val_Reconstruction : 88.3978271484375 Val_KL : 3.493554949760437\n","Epoch: 4591/8000  Traning Loss: 91.59600162506104  Train_Reconstruction: 88.07555103302002  Train_KL: 3.520449936389923  Validation Loss : 91.61310195922852 Val_Reconstruction : 88.13718032836914 Val_KL : 3.4759223461151123\n","Epoch: 4592/8000  Traning Loss: 91.5548906326294  Train_Reconstruction: 88.04528617858887  Train_KL: 3.5096039474010468  Validation Loss : 91.6976089477539 Val_Reconstruction : 88.22051620483398 Val_KL : 3.477094888687134\n","Epoch: 4593/8000  Traning Loss: 91.5727310180664  Train_Reconstruction: 88.0474739074707  Train_KL: 3.525257408618927  Validation Loss : 91.6257095336914 Val_Reconstruction : 88.12739562988281 Val_KL : 3.4983131885528564\n","Epoch: 4594/8000  Traning Loss: 91.72207164764404  Train_Reconstruction: 88.19483184814453  Train_KL: 3.5272403061389923  Validation Loss : 91.75580215454102 Val_Reconstruction : 88.27717590332031 Val_KL : 3.4786242246627808\n","Epoch: 4595/8000  Traning Loss: 91.58727264404297  Train_Reconstruction: 88.07892417907715  Train_KL: 3.5083487927913666  Validation Loss : 91.72433853149414 Val_Reconstruction : 88.25202560424805 Val_KL : 3.4723132848739624\n","Epoch: 4596/8000  Traning Loss: 91.82801628112793  Train_Reconstruction: 88.30624866485596  Train_KL: 3.5217682123184204  Validation Loss : 92.00236892700195 Val_Reconstruction : 88.50204086303711 Val_KL : 3.500326991081238\n","Epoch: 4597/8000  Traning Loss: 92.06639575958252  Train_Reconstruction: 88.53014755249023  Train_KL: 3.5362486839294434  Validation Loss : 91.62705612182617 Val_Reconstruction : 88.13242721557617 Val_KL : 3.4946287870407104\n","Epoch: 4598/8000  Traning Loss: 91.98535251617432  Train_Reconstruction: 88.46502304077148  Train_KL: 3.5203291475772858  Validation Loss : 91.30438232421875 Val_Reconstruction : 87.82279586791992 Val_KL : 3.4815871715545654\n","Epoch: 4599/8000  Traning Loss: 91.93862533569336  Train_Reconstruction: 88.4284610748291  Train_KL: 3.510163813829422  Validation Loss : 91.93631744384766 Val_Reconstruction : 88.45363235473633 Val_KL : 3.4826825857162476\n","Epoch: 4600/8000  Traning Loss: 91.91153335571289  Train_Reconstruction: 88.38533210754395  Train_KL: 3.5262008905410767  Validation Loss : 91.51264953613281 Val_Reconstruction : 88.02149200439453 Val_KL : 3.4911571741104126\n","Epoch: 4601/8000  Traning Loss: 91.5420503616333  Train_Reconstruction: 88.01866722106934  Train_KL: 3.523382604122162  Validation Loss : 91.53438949584961 Val_Reconstruction : 88.05574035644531 Val_KL : 3.478649139404297\n","Epoch: 4602/8000  Traning Loss: 91.66890716552734  Train_Reconstruction: 88.15530014038086  Train_KL: 3.5136055052280426  Validation Loss : 91.7621841430664 Val_Reconstruction : 88.28654479980469 Val_KL : 3.4756418466567993\n","Epoch: 4603/8000  Traning Loss: 92.05271911621094  Train_Reconstruction: 88.52787208557129  Train_KL: 3.524847149848938  Validation Loss : 91.63199234008789 Val_Reconstruction : 88.1421127319336 Val_KL : 3.4898792505264282\n","Epoch: 4604/8000  Traning Loss: 91.7924919128418  Train_Reconstruction: 88.27268886566162  Train_KL: 3.5198031961917877  Validation Loss : 91.9897575378418 Val_Reconstruction : 88.50333023071289 Val_KL : 3.4864264726638794\n","Epoch: 4605/8000  Traning Loss: 91.55788326263428  Train_Reconstruction: 88.03864002227783  Train_KL: 3.5192423462867737  Validation Loss : 91.30458450317383 Val_Reconstruction : 87.82292556762695 Val_KL : 3.4816569089889526\n","Epoch: 4606/8000  Traning Loss: 91.36697483062744  Train_Reconstruction: 87.85109519958496  Train_KL: 3.5158793926239014  Validation Loss : 91.42616271972656 Val_Reconstruction : 87.94061279296875 Val_KL : 3.4855493307113647\n","Epoch: 4607/8000  Traning Loss: 91.40151691436768  Train_Reconstruction: 87.87382221221924  Train_KL: 3.527694195508957  Validation Loss : 91.5313835144043 Val_Reconstruction : 88.03634643554688 Val_KL : 3.49503755569458\n","Epoch: 4608/8000  Traning Loss: 91.66687774658203  Train_Reconstruction: 88.13852596282959  Train_KL: 3.528350979089737  Validation Loss : 91.552978515625 Val_Reconstruction : 88.0656967163086 Val_KL : 3.4872848987579346\n","Epoch: 4609/8000  Traning Loss: 92.2395076751709  Train_Reconstruction: 88.72768497467041  Train_KL: 3.5118214786052704  Validation Loss : 92.2186393737793 Val_Reconstruction : 88.74874877929688 Val_KL : 3.4698883295059204\n","Epoch: 4610/8000  Traning Loss: 92.0728530883789  Train_Reconstruction: 88.5546464920044  Train_KL: 3.518207550048828  Validation Loss : 91.74665832519531 Val_Reconstruction : 88.25378036499023 Val_KL : 3.49287748336792\n","Epoch: 4611/8000  Traning Loss: 91.72004318237305  Train_Reconstruction: 88.18971347808838  Train_KL: 3.530330002307892  Validation Loss : 91.59065246582031 Val_Reconstruction : 88.09461975097656 Val_KL : 3.4960323572158813\n","Epoch: 4612/8000  Traning Loss: 91.77098178863525  Train_Reconstruction: 88.24127960205078  Train_KL: 3.529702067375183  Validation Loss : 91.50821685791016 Val_Reconstruction : 88.02050399780273 Val_KL : 3.4877156019210815\n","Epoch: 4613/8000  Traning Loss: 92.02621269226074  Train_Reconstruction: 88.49804306030273  Train_KL: 3.528169333934784  Validation Loss : 92.42497634887695 Val_Reconstruction : 88.9352912902832 Val_KL : 3.4896844625473022\n","Epoch: 4614/8000  Traning Loss: 92.19309520721436  Train_Reconstruction: 88.66925430297852  Train_KL: 3.5238415002822876  Validation Loss : 91.82210540771484 Val_Reconstruction : 88.33529281616211 Val_KL : 3.486811399459839\n","Epoch: 4615/8000  Traning Loss: 91.81298637390137  Train_Reconstruction: 88.29687023162842  Train_KL: 3.5161158740520477  Validation Loss : 91.4740219116211 Val_Reconstruction : 87.99580383300781 Val_KL : 3.478218674659729\n","Epoch: 4616/8000  Traning Loss: 91.80830955505371  Train_Reconstruction: 88.2884292602539  Train_KL: 3.5198806524276733  Validation Loss : 91.79084014892578 Val_Reconstruction : 88.29734802246094 Val_KL : 3.4934916496276855\n","Epoch: 4617/8000  Traning Loss: 92.05763626098633  Train_Reconstruction: 88.52670001983643  Train_KL: 3.530936121940613  Validation Loss : 91.59054946899414 Val_Reconstruction : 88.10590744018555 Val_KL : 3.484643578529358\n","Epoch: 4618/8000  Traning Loss: 91.54551601409912  Train_Reconstruction: 88.0298318862915  Train_KL: 3.515683740377426  Validation Loss : 91.47640991210938 Val_Reconstruction : 87.998046875 Val_KL : 3.4783626794815063\n","Epoch: 4619/8000  Traning Loss: 91.49733352661133  Train_Reconstruction: 87.9793586730957  Train_KL: 3.5179730653762817  Validation Loss : 91.41282272338867 Val_Reconstruction : 87.9316177368164 Val_KL : 3.4812041521072388\n","Epoch: 4620/8000  Traning Loss: 91.2792854309082  Train_Reconstruction: 87.75148868560791  Train_KL: 3.527795672416687  Validation Loss : 91.0622673034668 Val_Reconstruction : 87.56573104858398 Val_KL : 3.496534824371338\n","Epoch: 4621/8000  Traning Loss: 91.51079559326172  Train_Reconstruction: 87.99193477630615  Train_KL: 3.518860787153244  Validation Loss : 91.89544296264648 Val_Reconstruction : 88.42704010009766 Val_KL : 3.4684029817581177\n","Epoch: 4622/8000  Traning Loss: 91.79970169067383  Train_Reconstruction: 88.28469944000244  Train_KL: 3.515003114938736  Validation Loss : 92.31671142578125 Val_Reconstruction : 88.83195114135742 Val_KL : 3.4847620725631714\n","Epoch: 4623/8000  Traning Loss: 92.2883768081665  Train_Reconstruction: 88.75470638275146  Train_KL: 3.533670336008072  Validation Loss : 92.3346939086914 Val_Reconstruction : 88.83517074584961 Val_KL : 3.49952232837677\n","Epoch: 4624/8000  Traning Loss: 92.22736740112305  Train_Reconstruction: 88.69976234436035  Train_KL: 3.5276049077510834  Validation Loss : 92.03425216674805 Val_Reconstruction : 88.5536003112793 Val_KL : 3.4806511402130127\n","Epoch: 4625/8000  Traning Loss: 91.72618961334229  Train_Reconstruction: 88.2081708908081  Train_KL: 3.5180201530456543  Validation Loss : 91.52838897705078 Val_Reconstruction : 88.05043411254883 Val_KL : 3.477953553199768\n","Epoch: 4626/8000  Traning Loss: 91.42064380645752  Train_Reconstruction: 87.89941120147705  Train_KL: 3.521232485771179  Validation Loss : 91.32687377929688 Val_Reconstruction : 87.83438873291016 Val_KL : 3.4924864768981934\n","Epoch: 4627/8000  Traning Loss: 91.49228954315186  Train_Reconstruction: 87.96774005889893  Train_KL: 3.5245510637760162  Validation Loss : 91.56598663330078 Val_Reconstruction : 88.07717514038086 Val_KL : 3.4888105392456055\n","Epoch: 4628/8000  Traning Loss: 91.52096557617188  Train_Reconstruction: 87.99357795715332  Train_KL: 3.5273872911930084  Validation Loss : 91.47113037109375 Val_Reconstruction : 87.97085952758789 Val_KL : 3.500273108482361\n","Epoch: 4629/8000  Traning Loss: 91.89931774139404  Train_Reconstruction: 88.36406326293945  Train_KL: 3.5352542996406555  Validation Loss : 91.76752853393555 Val_Reconstruction : 88.26497650146484 Val_KL : 3.5025514364242554\n","Epoch: 4630/8000  Traning Loss: 92.52658557891846  Train_Reconstruction: 88.99204635620117  Train_KL: 3.5345411002635956  Validation Loss : 92.43267440795898 Val_Reconstruction : 88.95022201538086 Val_KL : 3.4824531078338623\n","Epoch: 4631/8000  Traning Loss: 92.32806587219238  Train_Reconstruction: 88.8191556930542  Train_KL: 3.5089087784290314  Validation Loss : 92.00801086425781 Val_Reconstruction : 88.5451889038086 Val_KL : 3.4628236293792725\n","Epoch: 4632/8000  Traning Loss: 91.98666667938232  Train_Reconstruction: 88.48101711273193  Train_KL: 3.5056493282318115  Validation Loss : 92.24088668823242 Val_Reconstruction : 88.75951766967773 Val_KL : 3.481371760368347\n","Epoch: 4633/8000  Traning Loss: 91.95947742462158  Train_Reconstruction: 88.43536853790283  Train_KL: 3.524108976125717  Validation Loss : 91.93812942504883 Val_Reconstruction : 88.4505386352539 Val_KL : 3.487591505050659\n","Epoch: 4634/8000  Traning Loss: 91.85288906097412  Train_Reconstruction: 88.33757305145264  Train_KL: 3.515316814184189  Validation Loss : 91.54586410522461 Val_Reconstruction : 88.08068466186523 Val_KL : 3.465178608894348\n","Epoch: 4635/8000  Traning Loss: 91.56592655181885  Train_Reconstruction: 88.06121063232422  Train_KL: 3.504714608192444  Validation Loss : 91.51001358032227 Val_Reconstruction : 88.03725814819336 Val_KL : 3.4727543592453003\n","Epoch: 4636/8000  Traning Loss: 91.67862129211426  Train_Reconstruction: 88.15291404724121  Train_KL: 3.5257057547569275  Validation Loss : 91.80389404296875 Val_Reconstruction : 88.30829238891602 Val_KL : 3.4956016540527344\n","Epoch: 4637/8000  Traning Loss: 91.98210144042969  Train_Reconstruction: 88.44605922698975  Train_KL: 3.5360425114631653  Validation Loss : 91.87447738647461 Val_Reconstruction : 88.38390350341797 Val_KL : 3.4905720949172974\n","Epoch: 4638/8000  Traning Loss: 91.97896480560303  Train_Reconstruction: 88.4677562713623  Train_KL: 3.5112076699733734  Validation Loss : 91.7376480102539 Val_Reconstruction : 88.27372741699219 Val_KL : 3.463921904563904\n","Epoch: 4639/8000  Traning Loss: 92.17095565795898  Train_Reconstruction: 88.66435623168945  Train_KL: 3.5066002309322357  Validation Loss : 91.88162612915039 Val_Reconstruction : 88.40599822998047 Val_KL : 3.4756282567977905\n","Epoch: 4640/8000  Traning Loss: 92.21652221679688  Train_Reconstruction: 88.70283317565918  Train_KL: 3.5136889815330505  Validation Loss : 91.96990585327148 Val_Reconstruction : 88.50123977661133 Val_KL : 3.468666911125183\n","Epoch: 4641/8000  Traning Loss: 91.97711753845215  Train_Reconstruction: 88.46816730499268  Train_KL: 3.508950114250183  Validation Loss : 91.53683090209961 Val_Reconstruction : 88.06729507446289 Val_KL : 3.4695388078689575\n","Epoch: 4642/8000  Traning Loss: 91.66803073883057  Train_Reconstruction: 88.15628051757812  Train_KL: 3.5117505490779877  Validation Loss : 91.43001556396484 Val_Reconstruction : 87.9525375366211 Val_KL : 3.477479577064514\n","Epoch: 4643/8000  Traning Loss: 91.49380874633789  Train_Reconstruction: 87.97816848754883  Train_KL: 3.5156407356262207  Validation Loss : 91.31874084472656 Val_Reconstruction : 87.84125137329102 Val_KL : 3.4774893522262573\n","Epoch: 4644/8000  Traning Loss: 91.44644737243652  Train_Reconstruction: 87.92297554016113  Train_KL: 3.523471176624298  Validation Loss : 91.14864349365234 Val_Reconstruction : 87.65719985961914 Val_KL : 3.491443395614624\n","Epoch: 4645/8000  Traning Loss: 91.52134323120117  Train_Reconstruction: 87.99245548248291  Train_KL: 3.528887629508972  Validation Loss : 91.69935607910156 Val_Reconstruction : 88.20806121826172 Val_KL : 3.4912922382354736\n","Epoch: 4646/8000  Traning Loss: 92.12956142425537  Train_Reconstruction: 88.60436725616455  Train_KL: 3.525193303823471  Validation Loss : 91.99324798583984 Val_Reconstruction : 88.5046157836914 Val_KL : 3.488630533218384\n","Epoch: 4647/8000  Traning Loss: 91.79144668579102  Train_Reconstruction: 88.27001190185547  Train_KL: 3.5214361250400543  Validation Loss : 91.45121383666992 Val_Reconstruction : 87.96267700195312 Val_KL : 3.4885363578796387\n","Epoch: 4648/8000  Traning Loss: 91.64991188049316  Train_Reconstruction: 88.1245698928833  Train_KL: 3.525341808795929  Validation Loss : 91.70671844482422 Val_Reconstruction : 88.22087097167969 Val_KL : 3.4858484268188477\n","Epoch: 4649/8000  Traning Loss: 91.82244777679443  Train_Reconstruction: 88.2898302078247  Train_KL: 3.5326181948184967  Validation Loss : 91.7751235961914 Val_Reconstruction : 88.2772216796875 Val_KL : 3.4978986978530884\n","Epoch: 4650/8000  Traning Loss: 91.79058074951172  Train_Reconstruction: 88.26442909240723  Train_KL: 3.526151120662689  Validation Loss : 91.71173095703125 Val_Reconstruction : 88.23313903808594 Val_KL : 3.4785925149917603\n","Epoch: 4651/8000  Traning Loss: 91.77102470397949  Train_Reconstruction: 88.25874614715576  Train_KL: 3.5122781693935394  Validation Loss : 91.40124130249023 Val_Reconstruction : 87.92606735229492 Val_KL : 3.475175142288208\n","Epoch: 4652/8000  Traning Loss: 91.74460029602051  Train_Reconstruction: 88.23246479034424  Train_KL: 3.5121353566646576  Validation Loss : 92.01021957397461 Val_Reconstruction : 88.53802871704102 Val_KL : 3.4721916913986206\n","Epoch: 4653/8000  Traning Loss: 91.81913948059082  Train_Reconstruction: 88.29732704162598  Train_KL: 3.5218124389648438  Validation Loss : 91.69597625732422 Val_Reconstruction : 88.20182037353516 Val_KL : 3.494154930114746\n","Epoch: 4654/8000  Traning Loss: 92.17381000518799  Train_Reconstruction: 88.64762878417969  Train_KL: 3.526180624961853  Validation Loss : 92.25808715820312 Val_Reconstruction : 88.77726745605469 Val_KL : 3.480816960334778\n","Epoch: 4655/8000  Traning Loss: 91.8546838760376  Train_Reconstruction: 88.34069347381592  Train_KL: 3.513989895582199  Validation Loss : 91.31319046020508 Val_Reconstruction : 87.83512878417969 Val_KL : 3.478063464164734\n","Epoch: 4656/8000  Traning Loss: 91.52697849273682  Train_Reconstruction: 88.01351642608643  Train_KL: 3.513461858034134  Validation Loss : 91.64239501953125 Val_Reconstruction : 88.16106033325195 Val_KL : 3.481335401535034\n","Epoch: 4657/8000  Traning Loss: 91.85584354400635  Train_Reconstruction: 88.32939052581787  Train_KL: 3.526452660560608  Validation Loss : 92.01481246948242 Val_Reconstruction : 88.52622604370117 Val_KL : 3.4885863065719604\n","Epoch: 4658/8000  Traning Loss: 91.7807788848877  Train_Reconstruction: 88.25700569152832  Train_KL: 3.523772895336151  Validation Loss : 91.5015983581543 Val_Reconstruction : 88.0298957824707 Val_KL : 3.471701145172119\n","Epoch: 4659/8000  Traning Loss: 91.59276962280273  Train_Reconstruction: 88.07953357696533  Train_KL: 3.513236254453659  Validation Loss : 91.51768112182617 Val_Reconstruction : 88.03697204589844 Val_KL : 3.4807082414627075\n","Epoch: 4660/8000  Traning Loss: 91.48354816436768  Train_Reconstruction: 87.95628643035889  Train_KL: 3.5272606313228607  Validation Loss : 91.24017333984375 Val_Reconstruction : 87.75479888916016 Val_KL : 3.4853737354278564\n","Epoch: 4661/8000  Traning Loss: 91.53627586364746  Train_Reconstruction: 88.02167892456055  Train_KL: 3.514596313238144  Validation Loss : 91.41633224487305 Val_Reconstruction : 87.93436813354492 Val_KL : 3.4819648265838623\n","Epoch: 4662/8000  Traning Loss: 91.63047218322754  Train_Reconstruction: 88.10525226593018  Train_KL: 3.525219827890396  Validation Loss : 91.28797149658203 Val_Reconstruction : 87.79961776733398 Val_KL : 3.4883527755737305\n","Epoch: 4663/8000  Traning Loss: 91.53411197662354  Train_Reconstruction: 88.00643920898438  Train_KL: 3.5276734828948975  Validation Loss : 91.6345100402832 Val_Reconstruction : 88.14381408691406 Val_KL : 3.4906939268112183\n","Epoch: 4664/8000  Traning Loss: 91.87247371673584  Train_Reconstruction: 88.34811687469482  Train_KL: 3.5243566036224365  Validation Loss : 91.9556884765625 Val_Reconstruction : 88.46674346923828 Val_KL : 3.4889429807662964\n","Epoch: 4665/8000  Traning Loss: 91.76309776306152  Train_Reconstruction: 88.23485660552979  Train_KL: 3.528241753578186  Validation Loss : 91.49461364746094 Val_Reconstruction : 87.99721908569336 Val_KL : 3.4973937273025513\n","Epoch: 4666/8000  Traning Loss: 91.45361232757568  Train_Reconstruction: 87.92430686950684  Train_KL: 3.5293055176734924  Validation Loss : 91.15079498291016 Val_Reconstruction : 87.66675186157227 Val_KL : 3.4840409755706787\n","Epoch: 4667/8000  Traning Loss: 91.50496482849121  Train_Reconstruction: 87.98293399810791  Train_KL: 3.5220307111740112  Validation Loss : 91.32845687866211 Val_Reconstruction : 87.84657669067383 Val_KL : 3.4818814992904663\n","Epoch: 4668/8000  Traning Loss: 91.71331691741943  Train_Reconstruction: 88.18590259552002  Train_KL: 3.5274142026901245  Validation Loss : 91.87823867797852 Val_Reconstruction : 88.39019775390625 Val_KL : 3.4880393743515015\n","Epoch: 4669/8000  Traning Loss: 92.12029075622559  Train_Reconstruction: 88.59046840667725  Train_KL: 3.52982234954834  Validation Loss : 92.03361511230469 Val_Reconstruction : 88.54713439941406 Val_KL : 3.486479163169861\n","Epoch: 4670/8000  Traning Loss: 91.92350387573242  Train_Reconstruction: 88.41247272491455  Train_KL: 3.5110307931900024  Validation Loss : 91.67293167114258 Val_Reconstruction : 88.20169830322266 Val_KL : 3.4712339639663696\n","Epoch: 4671/8000  Traning Loss: 91.65102577209473  Train_Reconstruction: 88.13337326049805  Train_KL: 3.5176510214805603  Validation Loss : 91.56682968139648 Val_Reconstruction : 88.08182525634766 Val_KL : 3.4850047826766968\n","Epoch: 4672/8000  Traning Loss: 91.82163429260254  Train_Reconstruction: 88.3002872467041  Train_KL: 3.521347165107727  Validation Loss : 91.63325881958008 Val_Reconstruction : 88.15303421020508 Val_KL : 3.4802247285842896\n","Epoch: 4673/8000  Traning Loss: 91.92586612701416  Train_Reconstruction: 88.4061336517334  Train_KL: 3.5197325348854065  Validation Loss : 91.82652282714844 Val_Reconstruction : 88.33884811401367 Val_KL : 3.487674832344055\n","Epoch: 4674/8000  Traning Loss: 92.03511619567871  Train_Reconstruction: 88.51212978363037  Train_KL: 3.5229873061180115  Validation Loss : 91.75543212890625 Val_Reconstruction : 88.26577377319336 Val_KL : 3.4896578788757324\n","Epoch: 4675/8000  Traning Loss: 91.87222671508789  Train_Reconstruction: 88.3520622253418  Train_KL: 3.5201648473739624  Validation Loss : 91.69818115234375 Val_Reconstruction : 88.20919799804688 Val_KL : 3.4889835119247437\n","Epoch: 4676/8000  Traning Loss: 91.8978042602539  Train_Reconstruction: 88.3766450881958  Train_KL: 3.5211604237556458  Validation Loss : 91.56853103637695 Val_Reconstruction : 88.09217071533203 Val_KL : 3.4763587713241577\n","Epoch: 4677/8000  Traning Loss: 92.01411056518555  Train_Reconstruction: 88.49661445617676  Train_KL: 3.517495185136795  Validation Loss : 92.1558952331543 Val_Reconstruction : 88.67621231079102 Val_KL : 3.479683756828308\n","Epoch: 4678/8000  Traning Loss: 91.81433868408203  Train_Reconstruction: 88.29245662689209  Train_KL: 3.5218819975852966  Validation Loss : 91.42321014404297 Val_Reconstruction : 87.94912338256836 Val_KL : 3.4740872383117676\n","Epoch: 4679/8000  Traning Loss: 91.5429277420044  Train_Reconstruction: 88.02930355072021  Train_KL: 3.5136246979236603  Validation Loss : 91.27813339233398 Val_Reconstruction : 87.80054092407227 Val_KL : 3.477593779563904\n","Epoch: 4680/8000  Traning Loss: 91.41193771362305  Train_Reconstruction: 87.89210796356201  Train_KL: 3.5198304653167725  Validation Loss : 91.3866081237793 Val_Reconstruction : 87.90043258666992 Val_KL : 3.486176371574402\n","Epoch: 4681/8000  Traning Loss: 91.41738605499268  Train_Reconstruction: 87.88634300231934  Train_KL: 3.53104230761528  Validation Loss : 91.35339736938477 Val_Reconstruction : 87.85442352294922 Val_KL : 3.498970866203308\n","Epoch: 4682/8000  Traning Loss: 91.76186847686768  Train_Reconstruction: 88.22775745391846  Train_KL: 3.5341104865074158  Validation Loss : 91.63397979736328 Val_Reconstruction : 88.14050674438477 Val_KL : 3.493472099304199\n","Epoch: 4683/8000  Traning Loss: 92.09849834442139  Train_Reconstruction: 88.5752067565918  Train_KL: 3.5232914984226227  Validation Loss : 91.82110977172852 Val_Reconstruction : 88.33677291870117 Val_KL : 3.484336733818054\n","Epoch: 4684/8000  Traning Loss: 91.76583576202393  Train_Reconstruction: 88.23824501037598  Train_KL: 3.5275887548923492  Validation Loss : 91.5756607055664 Val_Reconstruction : 88.08393859863281 Val_KL : 3.4917231798171997\n","Epoch: 4685/8000  Traning Loss: 91.68866920471191  Train_Reconstruction: 88.16449737548828  Train_KL: 3.524171531200409  Validation Loss : 91.87178039550781 Val_Reconstruction : 88.39089584350586 Val_KL : 3.480885863304138\n","Epoch: 4686/8000  Traning Loss: 92.30731678009033  Train_Reconstruction: 88.79585456848145  Train_KL: 3.5114623308181763  Validation Loss : 92.58595275878906 Val_Reconstruction : 89.11640930175781 Val_KL : 3.4695438146591187\n","Epoch: 4687/8000  Traning Loss: 92.29299259185791  Train_Reconstruction: 88.77026081085205  Train_KL: 3.522730529308319  Validation Loss : 91.7629280090332 Val_Reconstruction : 88.27497100830078 Val_KL : 3.4879579544067383\n","Epoch: 4688/8000  Traning Loss: 91.7699613571167  Train_Reconstruction: 88.23734283447266  Train_KL: 3.5326191186904907  Validation Loss : 91.54335403442383 Val_Reconstruction : 88.04651260375977 Val_KL : 3.4968427419662476\n","Epoch: 4689/8000  Traning Loss: 91.73774147033691  Train_Reconstruction: 88.2105941772461  Train_KL: 3.527148485183716  Validation Loss : 91.64117050170898 Val_Reconstruction : 88.15616607666016 Val_KL : 3.485003113746643\n","Epoch: 4690/8000  Traning Loss: 91.69319534301758  Train_Reconstruction: 88.16421794891357  Train_KL: 3.5289773643016815  Validation Loss : 91.54540252685547 Val_Reconstruction : 88.05009078979492 Val_KL : 3.4953094720840454\n","Epoch: 4691/8000  Traning Loss: 92.03607654571533  Train_Reconstruction: 88.50369548797607  Train_KL: 3.5323815047740936  Validation Loss : 91.8992919921875 Val_Reconstruction : 88.40743637084961 Val_KL : 3.4918545484542847\n","Epoch: 4692/8000  Traning Loss: 92.69212055206299  Train_Reconstruction: 89.1751480102539  Train_KL: 3.5169728100299835  Validation Loss : 92.0235824584961 Val_Reconstruction : 88.54247283935547 Val_KL : 3.4811068773269653\n","Epoch: 4693/8000  Traning Loss: 91.97013664245605  Train_Reconstruction: 88.44360256195068  Train_KL: 3.5265336632728577  Validation Loss : 91.2674331665039 Val_Reconstruction : 87.76948165893555 Val_KL : 3.49795138835907\n","Epoch: 4694/8000  Traning Loss: 91.51067161560059  Train_Reconstruction: 87.98029518127441  Train_KL: 3.5303770303726196  Validation Loss : 91.32590866088867 Val_Reconstruction : 87.84206008911133 Val_KL : 3.4838472604751587\n","Epoch: 4695/8000  Traning Loss: 91.51488590240479  Train_Reconstruction: 87.99780368804932  Train_KL: 3.5170827209949493  Validation Loss : 91.46590042114258 Val_Reconstruction : 87.97659301757812 Val_KL : 3.4893078804016113\n","Epoch: 4696/8000  Traning Loss: 91.43065357208252  Train_Reconstruction: 87.90142631530762  Train_KL: 3.5292272567749023  Validation Loss : 91.4040756225586 Val_Reconstruction : 87.90663146972656 Val_KL : 3.4974440336227417\n","Epoch: 4697/8000  Traning Loss: 91.59335136413574  Train_Reconstruction: 88.06857490539551  Train_KL: 3.5247757136821747  Validation Loss : 91.94074249267578 Val_Reconstruction : 88.45616912841797 Val_KL : 3.4845736026763916\n","Epoch: 4698/8000  Traning Loss: 91.62281036376953  Train_Reconstruction: 88.10128688812256  Train_KL: 3.5215229094028473  Validation Loss : 91.59174728393555 Val_Reconstruction : 88.10342788696289 Val_KL : 3.4883188009262085\n","Epoch: 4699/8000  Traning Loss: 91.37547588348389  Train_Reconstruction: 87.85227298736572  Train_KL: 3.5232031643390656  Validation Loss : 91.18701171875 Val_Reconstruction : 87.69822311401367 Val_KL : 3.488788604736328\n","Epoch: 4700/8000  Traning Loss: 91.37599754333496  Train_Reconstruction: 87.85272598266602  Train_KL: 3.523270934820175  Validation Loss : 91.18306732177734 Val_Reconstruction : 87.69903945922852 Val_KL : 3.484030604362488\n","Epoch: 4701/8000  Traning Loss: 91.18129920959473  Train_Reconstruction: 87.66853046417236  Train_KL: 3.512767404317856  Validation Loss : 90.8980941772461 Val_Reconstruction : 87.4242172241211 Val_KL : 3.4738770723342896\n","Epoch: 4702/8000  Traning Loss: 91.2383337020874  Train_Reconstruction: 87.71148014068604  Train_KL: 3.5268535912036896  Validation Loss : 91.46712112426758 Val_Reconstruction : 87.97120666503906 Val_KL : 3.495917558670044\n","Epoch: 4703/8000  Traning Loss: 91.84271717071533  Train_Reconstruction: 88.30570411682129  Train_KL: 3.5370134711265564  Validation Loss : 91.58305740356445 Val_Reconstruction : 88.07861709594727 Val_KL : 3.5044416189193726\n","Epoch: 4704/8000  Traning Loss: 91.73404788970947  Train_Reconstruction: 88.20777034759521  Train_KL: 3.5262775123119354  Validation Loss : 91.43291091918945 Val_Reconstruction : 87.947998046875 Val_KL : 3.4849133491516113\n","Epoch: 4705/8000  Traning Loss: 91.79986095428467  Train_Reconstruction: 88.28623962402344  Train_KL: 3.5136216282844543  Validation Loss : 91.59338760375977 Val_Reconstruction : 88.1208610534668 Val_KL : 3.472525477409363\n","Epoch: 4706/8000  Traning Loss: 91.7708625793457  Train_Reconstruction: 88.25335693359375  Train_KL: 3.517507314682007  Validation Loss : 92.22004699707031 Val_Reconstruction : 88.7314682006836 Val_KL : 3.488576650619507\n","Epoch: 4707/8000  Traning Loss: 91.7233772277832  Train_Reconstruction: 88.20202541351318  Train_KL: 3.5213524401187897  Validation Loss : 91.52200698852539 Val_Reconstruction : 88.04325485229492 Val_KL : 3.4787521362304688\n","Epoch: 4708/8000  Traning Loss: 91.6998291015625  Train_Reconstruction: 88.1935167312622  Train_KL: 3.5063125491142273  Validation Loss : 91.65700149536133 Val_Reconstruction : 88.18916702270508 Val_KL : 3.4678313732147217\n","Epoch: 4709/8000  Traning Loss: 91.54967594146729  Train_Reconstruction: 88.03883743286133  Train_KL: 3.51083904504776  Validation Loss : 91.6726188659668 Val_Reconstruction : 88.19318389892578 Val_KL : 3.4794353246688843\n","Epoch: 4710/8000  Traning Loss: 91.5481185913086  Train_Reconstruction: 88.01819705963135  Train_KL: 3.5299219489097595  Validation Loss : 91.48229217529297 Val_Reconstruction : 87.98367309570312 Val_KL : 3.498618483543396\n","Epoch: 4711/8000  Traning Loss: 91.53751373291016  Train_Reconstruction: 88.01455879211426  Train_KL: 3.522954821586609  Validation Loss : 91.70317840576172 Val_Reconstruction : 88.22518539428711 Val_KL : 3.4779917001724243\n","Epoch: 4712/8000  Traning Loss: 92.09929275512695  Train_Reconstruction: 88.58217144012451  Train_KL: 3.517119884490967  Validation Loss : 92.0365219116211 Val_Reconstruction : 88.55359649658203 Val_KL : 3.482923984527588\n","Epoch: 4713/8000  Traning Loss: 92.2902717590332  Train_Reconstruction: 88.76820850372314  Train_KL: 3.5220623910427094  Validation Loss : 92.30089950561523 Val_Reconstruction : 88.81246948242188 Val_KL : 3.4884285926818848\n","Epoch: 4714/8000  Traning Loss: 92.24006748199463  Train_Reconstruction: 88.72010612487793  Train_KL: 3.519960343837738  Validation Loss : 92.2324104309082 Val_Reconstruction : 88.7530288696289 Val_KL : 3.4793812036514282\n","Epoch: 4715/8000  Traning Loss: 92.0098991394043  Train_Reconstruction: 88.49678993225098  Train_KL: 3.513110429048538  Validation Loss : 91.58198928833008 Val_Reconstruction : 88.11333847045898 Val_KL : 3.468651056289673\n","Epoch: 4716/8000  Traning Loss: 91.72551727294922  Train_Reconstruction: 88.20896816253662  Train_KL: 3.5165493488311768  Validation Loss : 91.62651062011719 Val_Reconstruction : 88.1438217163086 Val_KL : 3.4826892614364624\n","Epoch: 4717/8000  Traning Loss: 91.5202579498291  Train_Reconstruction: 87.99265384674072  Train_KL: 3.527603566646576  Validation Loss : 91.37923812866211 Val_Reconstruction : 87.89505386352539 Val_KL : 3.484184145927429\n","Epoch: 4718/8000  Traning Loss: 91.48138332366943  Train_Reconstruction: 87.9622745513916  Train_KL: 3.519108831882477  Validation Loss : 91.26492691040039 Val_Reconstruction : 87.78877639770508 Val_KL : 3.4761507511138916\n","Epoch: 4719/8000  Traning Loss: 91.60859298706055  Train_Reconstruction: 88.0958137512207  Train_KL: 3.512779325246811  Validation Loss : 91.37253189086914 Val_Reconstruction : 87.89327621459961 Val_KL : 3.4792572259902954\n","Epoch: 4720/8000  Traning Loss: 91.57204151153564  Train_Reconstruction: 88.05358409881592  Train_KL: 3.5184578895568848  Validation Loss : 91.29353332519531 Val_Reconstruction : 87.81797409057617 Val_KL : 3.4755603075027466\n","Epoch: 4721/8000  Traning Loss: 91.34527492523193  Train_Reconstruction: 87.8315544128418  Train_KL: 3.51371967792511  Validation Loss : 91.3336067199707 Val_Reconstruction : 87.85863876342773 Val_KL : 3.4749679565429688\n","Epoch: 4722/8000  Traning Loss: 91.35498714447021  Train_Reconstruction: 87.83580493927002  Train_KL: 3.519182562828064  Validation Loss : 91.11033248901367 Val_Reconstruction : 87.62791442871094 Val_KL : 3.4824169874191284\n","Epoch: 4723/8000  Traning Loss: 91.2849349975586  Train_Reconstruction: 87.7578535079956  Train_KL: 3.527082324028015  Validation Loss : 91.28527069091797 Val_Reconstruction : 87.80071640014648 Val_KL : 3.484553337097168\n","Epoch: 4724/8000  Traning Loss: 91.48187351226807  Train_Reconstruction: 87.96269226074219  Train_KL: 3.5191808938980103  Validation Loss : 91.26552200317383 Val_Reconstruction : 87.79845428466797 Val_KL : 3.4670690298080444\n","Epoch: 4725/8000  Traning Loss: 91.56847858428955  Train_Reconstruction: 88.05994892120361  Train_KL: 3.508529841899872  Validation Loss : 91.47783660888672 Val_Reconstruction : 88.01562881469727 Val_KL : 3.4622074365615845\n","Epoch: 4726/8000  Traning Loss: 92.03611087799072  Train_Reconstruction: 88.52119636535645  Train_KL: 3.514914870262146  Validation Loss : 91.70550155639648 Val_Reconstruction : 88.22438430786133 Val_KL : 3.481119394302368\n","Epoch: 4727/8000  Traning Loss: 91.84028339385986  Train_Reconstruction: 88.31944751739502  Train_KL: 3.520834892988205  Validation Loss : 91.61285400390625 Val_Reconstruction : 88.13481140136719 Val_KL : 3.4780454635620117\n","Epoch: 4728/8000  Traning Loss: 91.80270290374756  Train_Reconstruction: 88.27694129943848  Train_KL: 3.5257627367973328  Validation Loss : 91.76082992553711 Val_Reconstruction : 88.27130508422852 Val_KL : 3.4895219802856445\n","Epoch: 4729/8000  Traning Loss: 91.96159744262695  Train_Reconstruction: 88.43563938140869  Train_KL: 3.5259576737880707  Validation Loss : 92.36823272705078 Val_Reconstruction : 88.88594055175781 Val_KL : 3.4822909832000732\n","Epoch: 4730/8000  Traning Loss: 92.0926399230957  Train_Reconstruction: 88.58536148071289  Train_KL: 3.507278174161911  Validation Loss : 91.76720809936523 Val_Reconstruction : 88.29839324951172 Val_KL : 3.4688133001327515\n","Epoch: 4731/8000  Traning Loss: 91.96505737304688  Train_Reconstruction: 88.45251274108887  Train_KL: 3.5125442147254944  Validation Loss : 92.09639358520508 Val_Reconstruction : 88.61491775512695 Val_KL : 3.4814752340316772\n","Epoch: 4732/8000  Traning Loss: 92.01960945129395  Train_Reconstruction: 88.49660396575928  Train_KL: 3.5230065286159515  Validation Loss : 92.07508850097656 Val_Reconstruction : 88.58744049072266 Val_KL : 3.4876478910446167\n","Epoch: 4733/8000  Traning Loss: 91.81137084960938  Train_Reconstruction: 88.287428855896  Train_KL: 3.523942321538925  Validation Loss : 91.68598556518555 Val_Reconstruction : 88.19363021850586 Val_KL : 3.492354393005371\n","Epoch: 4734/8000  Traning Loss: 91.84900951385498  Train_Reconstruction: 88.3267469406128  Train_KL: 3.522261142730713  Validation Loss : 91.36404800415039 Val_Reconstruction : 87.89129638671875 Val_KL : 3.4727511405944824\n","Epoch: 4735/8000  Traning Loss: 91.93169784545898  Train_Reconstruction: 88.41858863830566  Train_KL: 3.513109505176544  Validation Loss : 91.65092468261719 Val_Reconstruction : 88.16531372070312 Val_KL : 3.485612630844116\n","Epoch: 4736/8000  Traning Loss: 91.77199649810791  Train_Reconstruction: 88.2425594329834  Train_KL: 3.529436320066452  Validation Loss : 91.20967102050781 Val_Reconstruction : 87.71881484985352 Val_KL : 3.490855574607849\n","Epoch: 4737/8000  Traning Loss: 91.58810615539551  Train_Reconstruction: 88.0645112991333  Train_KL: 3.5235952138900757  Validation Loss : 91.38717651367188 Val_Reconstruction : 87.89859771728516 Val_KL : 3.4885783195495605\n","Epoch: 4738/8000  Traning Loss: 91.6201000213623  Train_Reconstruction: 88.0986099243164  Train_KL: 3.521488845348358  Validation Loss : 91.69327545166016 Val_Reconstruction : 88.20690155029297 Val_KL : 3.4863734245300293\n","Epoch: 4739/8000  Traning Loss: 91.67358207702637  Train_Reconstruction: 88.1529893875122  Train_KL: 3.520593076944351  Validation Loss : 91.20431137084961 Val_Reconstruction : 87.7166976928711 Val_KL : 3.4876121282577515\n","Epoch: 4740/8000  Traning Loss: 91.76631546020508  Train_Reconstruction: 88.24338054656982  Train_KL: 3.5229346454143524  Validation Loss : 91.74830627441406 Val_Reconstruction : 88.26747512817383 Val_KL : 3.4808298349380493\n","Epoch: 4741/8000  Traning Loss: 91.94577980041504  Train_Reconstruction: 88.42623519897461  Train_KL: 3.519543170928955  Validation Loss : 91.64441680908203 Val_Reconstruction : 88.16162490844727 Val_KL : 3.4827911853790283\n","Epoch: 4742/8000  Traning Loss: 91.84296607971191  Train_Reconstruction: 88.31901931762695  Train_KL: 3.5239460170269012  Validation Loss : 91.78489303588867 Val_Reconstruction : 88.29592514038086 Val_KL : 3.488969087600708\n","Epoch: 4743/8000  Traning Loss: 91.95126819610596  Train_Reconstruction: 88.43196773529053  Train_KL: 3.5193001329898834  Validation Loss : 91.78567123413086 Val_Reconstruction : 88.31063461303711 Val_KL : 3.4750401973724365\n","Epoch: 4744/8000  Traning Loss: 91.81266021728516  Train_Reconstruction: 88.29702663421631  Train_KL: 3.5156344771385193  Validation Loss : 91.72844696044922 Val_Reconstruction : 88.24235534667969 Val_KL : 3.4860942363739014\n","Epoch: 4745/8000  Traning Loss: 91.57579040527344  Train_Reconstruction: 88.04193496704102  Train_KL: 3.533853679895401  Validation Loss : 91.30071258544922 Val_Reconstruction : 87.80206680297852 Val_KL : 3.498644232749939\n","Epoch: 4746/8000  Traning Loss: 91.81546592712402  Train_Reconstruction: 88.29347705841064  Train_KL: 3.5219895243644714  Validation Loss : 91.41699981689453 Val_Reconstruction : 87.93315887451172 Val_KL : 3.4838387966156006\n","Epoch: 4747/8000  Traning Loss: 91.61844158172607  Train_Reconstruction: 88.09991264343262  Train_KL: 3.518527537584305  Validation Loss : 91.38204574584961 Val_Reconstruction : 87.90214920043945 Val_KL : 3.4798964262008667\n","Epoch: 4748/8000  Traning Loss: 91.57537078857422  Train_Reconstruction: 88.05125045776367  Train_KL: 3.5241203606128693  Validation Loss : 91.2783432006836 Val_Reconstruction : 87.79222106933594 Val_KL : 3.486119866371155\n","Epoch: 4749/8000  Traning Loss: 91.46661758422852  Train_Reconstruction: 87.93947410583496  Train_KL: 3.5271434485912323  Validation Loss : 91.31779861450195 Val_Reconstruction : 87.83523559570312 Val_KL : 3.4825644493103027\n","Epoch: 4750/8000  Traning Loss: 91.2970027923584  Train_Reconstruction: 87.769211769104  Train_KL: 3.52779084444046  Validation Loss : 91.18107604980469 Val_Reconstruction : 87.69205856323242 Val_KL : 3.489019513130188\n","Epoch: 4751/8000  Traning Loss: 91.33647060394287  Train_Reconstruction: 87.81391143798828  Train_KL: 3.522558093070984  Validation Loss : 91.22930526733398 Val_Reconstruction : 87.75430679321289 Val_KL : 3.4749969244003296\n","Epoch: 4752/8000  Traning Loss: 91.72238159179688  Train_Reconstruction: 88.21062660217285  Train_KL: 3.5117557644844055  Validation Loss : 91.85302352905273 Val_Reconstruction : 88.37714767456055 Val_KL : 3.4758758544921875\n","Epoch: 4753/8000  Traning Loss: 91.73254680633545  Train_Reconstruction: 88.20614433288574  Train_KL: 3.526401996612549  Validation Loss : 91.73426818847656 Val_Reconstruction : 88.23921966552734 Val_KL : 3.4950482845306396\n","Epoch: 4754/8000  Traning Loss: 91.45962142944336  Train_Reconstruction: 87.92927837371826  Train_KL: 3.5303439795970917  Validation Loss : 91.46056365966797 Val_Reconstruction : 87.9738998413086 Val_KL : 3.486666202545166\n","Epoch: 4755/8000  Traning Loss: 91.82157897949219  Train_Reconstruction: 88.30255508422852  Train_KL: 3.519024610519409  Validation Loss : 91.43023681640625 Val_Reconstruction : 87.95066452026367 Val_KL : 3.4795724153518677\n","Epoch: 4756/8000  Traning Loss: 91.80967903137207  Train_Reconstruction: 88.2901611328125  Train_KL: 3.5195174515247345  Validation Loss : 91.86241912841797 Val_Reconstruction : 88.38574981689453 Val_KL : 3.476668357849121\n","Epoch: 4757/8000  Traning Loss: 91.53038597106934  Train_Reconstruction: 88.01955890655518  Train_KL: 3.5108274817466736  Validation Loss : 91.00687026977539 Val_Reconstruction : 87.53144454956055 Val_KL : 3.475428581237793\n","Epoch: 4758/8000  Traning Loss: 91.51436996459961  Train_Reconstruction: 87.99106502532959  Train_KL: 3.523303836584091  Validation Loss : 91.6630630493164 Val_Reconstruction : 88.1698989868164 Val_KL : 3.4931663274765015\n","Epoch: 4759/8000  Traning Loss: 92.03716564178467  Train_Reconstruction: 88.50211715698242  Train_KL: 3.5350493490695953  Validation Loss : 92.43922805786133 Val_Reconstruction : 88.94419479370117 Val_KL : 3.4950356483459473\n","Epoch: 4760/8000  Traning Loss: 92.27833843231201  Train_Reconstruction: 88.7538652420044  Train_KL: 3.524472266435623  Validation Loss : 91.9974479675293 Val_Reconstruction : 88.50931549072266 Val_KL : 3.4881327152252197\n","Epoch: 4761/8000  Traning Loss: 92.00781345367432  Train_Reconstruction: 88.48235511779785  Train_KL: 3.5254582166671753  Validation Loss : 91.52042007446289 Val_Reconstruction : 88.02801132202148 Val_KL : 3.492408275604248\n","Epoch: 4762/8000  Traning Loss: 91.95776271820068  Train_Reconstruction: 88.42455577850342  Train_KL: 3.5332067608833313  Validation Loss : 91.8983268737793 Val_Reconstruction : 88.40948104858398 Val_KL : 3.4888471364974976\n","Epoch: 4763/8000  Traning Loss: 91.9366455078125  Train_Reconstruction: 88.41480159759521  Train_KL: 3.5218450129032135  Validation Loss : 91.45948791503906 Val_Reconstruction : 87.97706604003906 Val_KL : 3.482421040534973\n","Epoch: 4764/8000  Traning Loss: 91.85989665985107  Train_Reconstruction: 88.32880592346191  Train_KL: 3.5310907661914825  Validation Loss : 91.48978424072266 Val_Reconstruction : 88.00552749633789 Val_KL : 3.484257936477661\n","Epoch: 4765/8000  Traning Loss: 91.62426090240479  Train_Reconstruction: 88.1130313873291  Train_KL: 3.5112286806106567  Validation Loss : 91.3582992553711 Val_Reconstruction : 87.88980102539062 Val_KL : 3.4684948921203613\n","Epoch: 4766/8000  Traning Loss: 91.3541612625122  Train_Reconstruction: 87.83672618865967  Train_KL: 3.517435699701309  Validation Loss : 91.12533569335938 Val_Reconstruction : 87.64060592651367 Val_KL : 3.484732151031494\n","Epoch: 4767/8000  Traning Loss: 91.60326766967773  Train_Reconstruction: 88.0831241607666  Train_KL: 3.5201434791088104  Validation Loss : 91.68153381347656 Val_Reconstruction : 88.20490646362305 Val_KL : 3.4766255617141724\n","Epoch: 4768/8000  Traning Loss: 92.36034965515137  Train_Reconstruction: 88.84295177459717  Train_KL: 3.517397314310074  Validation Loss : 92.27577209472656 Val_Reconstruction : 88.7920036315918 Val_KL : 3.4837673902511597\n","Epoch: 4769/8000  Traning Loss: 92.08494472503662  Train_Reconstruction: 88.56491374969482  Train_KL: 3.5200305581092834  Validation Loss : 91.70703887939453 Val_Reconstruction : 88.22556686401367 Val_KL : 3.4814740419387817\n","Epoch: 4770/8000  Traning Loss: 91.7609634399414  Train_Reconstruction: 88.24329566955566  Train_KL: 3.5176678597927094  Validation Loss : 91.60600280761719 Val_Reconstruction : 88.11836624145508 Val_KL : 3.487638473510742\n","Epoch: 4771/8000  Traning Loss: 92.1080322265625  Train_Reconstruction: 88.58315563201904  Train_KL: 3.5248758792877197  Validation Loss : 92.21632385253906 Val_Reconstruction : 88.73392868041992 Val_KL : 3.482394576072693\n","Epoch: 4772/8000  Traning Loss: 92.22995376586914  Train_Reconstruction: 88.7057876586914  Train_KL: 3.5241661071777344  Validation Loss : 91.55643463134766 Val_Reconstruction : 88.07304000854492 Val_KL : 3.4833961725234985\n","Epoch: 4773/8000  Traning Loss: 91.6797227859497  Train_Reconstruction: 88.1562271118164  Train_KL: 3.523497074842453  Validation Loss : 92.37447357177734 Val_Reconstruction : 88.88954544067383 Val_KL : 3.4849250316619873\n","Epoch: 4774/8000  Traning Loss: 91.95170497894287  Train_Reconstruction: 88.4329423904419  Train_KL: 3.518762707710266  Validation Loss : 91.77632904052734 Val_Reconstruction : 88.30700302124023 Val_KL : 3.4693268537521362\n","Epoch: 4775/8000  Traning Loss: 91.84928512573242  Train_Reconstruction: 88.3363733291626  Train_KL: 3.512911796569824  Validation Loss : 91.83896255493164 Val_Reconstruction : 88.35965347290039 Val_KL : 3.479309916496277\n","Epoch: 4776/8000  Traning Loss: 91.7032470703125  Train_Reconstruction: 88.18658256530762  Train_KL: 3.516665667295456  Validation Loss : 91.7923698425293 Val_Reconstruction : 88.31766510009766 Val_KL : 3.474705219268799\n","Epoch: 4777/8000  Traning Loss: 92.20080375671387  Train_Reconstruction: 88.68234348297119  Train_KL: 3.5184594690799713  Validation Loss : 91.87578964233398 Val_Reconstruction : 88.40069961547852 Val_KL : 3.475090980529785\n","Epoch: 4778/8000  Traning Loss: 91.92997646331787  Train_Reconstruction: 88.4086742401123  Train_KL: 3.521302044391632  Validation Loss : 92.39498901367188 Val_Reconstruction : 88.9028434753418 Val_KL : 3.4921470880508423\n","Epoch: 4779/8000  Traning Loss: 91.96150016784668  Train_Reconstruction: 88.43146228790283  Train_KL: 3.5300373136997223  Validation Loss : 92.0306510925293 Val_Reconstruction : 88.5466537475586 Val_KL : 3.483997106552124\n","Epoch: 4780/8000  Traning Loss: 91.8857192993164  Train_Reconstruction: 88.36627674102783  Train_KL: 3.519442081451416  Validation Loss : 91.88358688354492 Val_Reconstruction : 88.40638732910156 Val_KL : 3.4772013425827026\n","Epoch: 4781/8000  Traning Loss: 91.97888374328613  Train_Reconstruction: 88.45878791809082  Train_KL: 3.5200946927070618  Validation Loss : 91.74042129516602 Val_Reconstruction : 88.25883483886719 Val_KL : 3.481584310531616\n","Epoch: 4782/8000  Traning Loss: 91.66061401367188  Train_Reconstruction: 88.1367998123169  Train_KL: 3.5238128900527954  Validation Loss : 91.50817489624023 Val_Reconstruction : 88.02839279174805 Val_KL : 3.479779601097107\n","Epoch: 4783/8000  Traning Loss: 91.41100025177002  Train_Reconstruction: 87.88991451263428  Train_KL: 3.52108696103096  Validation Loss : 91.30606079101562 Val_Reconstruction : 87.82359313964844 Val_KL : 3.4824658632278442\n","Epoch: 4784/8000  Traning Loss: 91.46637916564941  Train_Reconstruction: 87.94694423675537  Train_KL: 3.519433915615082  Validation Loss : 91.71871948242188 Val_Reconstruction : 88.23866271972656 Val_KL : 3.480056643486023\n","Epoch: 4785/8000  Traning Loss: 91.72099781036377  Train_Reconstruction: 88.19831562042236  Train_KL: 3.5226815938949585  Validation Loss : 91.94064712524414 Val_Reconstruction : 88.46610641479492 Val_KL : 3.47454035282135\n","Epoch: 4786/8000  Traning Loss: 91.59360599517822  Train_Reconstruction: 88.07308197021484  Train_KL: 3.5205238461494446  Validation Loss : 91.33763885498047 Val_Reconstruction : 87.85503387451172 Val_KL : 3.482604146003723\n","Epoch: 4787/8000  Traning Loss: 91.27773857116699  Train_Reconstruction: 87.74868869781494  Train_KL: 3.529050499200821  Validation Loss : 91.18901824951172 Val_Reconstruction : 87.69844436645508 Val_KL : 3.4905734062194824\n","Epoch: 4788/8000  Traning Loss: 91.2384033203125  Train_Reconstruction: 87.71518898010254  Train_KL: 3.5232141315937042  Validation Loss : 91.09551239013672 Val_Reconstruction : 87.62131881713867 Val_KL : 3.4741947650909424\n","Epoch: 4789/8000  Traning Loss: 91.38349056243896  Train_Reconstruction: 87.868821144104  Train_KL: 3.514669120311737  Validation Loss : 91.84666061401367 Val_Reconstruction : 88.37493896484375 Val_KL : 3.4717193841934204\n","Epoch: 4790/8000  Traning Loss: 92.10961437225342  Train_Reconstruction: 88.59514808654785  Train_KL: 3.5144658386707306  Validation Loss : 92.74394607543945 Val_Reconstruction : 89.26250839233398 Val_KL : 3.481438398361206\n","Epoch: 4791/8000  Traning Loss: 92.76779460906982  Train_Reconstruction: 89.24899578094482  Train_KL: 3.518798053264618  Validation Loss : 93.11859130859375 Val_Reconstruction : 89.6435661315918 Val_KL : 3.475026249885559\n","Epoch: 4792/8000  Traning Loss: 92.74663925170898  Train_Reconstruction: 89.2324447631836  Train_KL: 3.5141959190368652  Validation Loss : 92.84408950805664 Val_Reconstruction : 89.37530899047852 Val_KL : 3.4687819480895996\n","Epoch: 4793/8000  Traning Loss: 92.35544395446777  Train_Reconstruction: 88.83637809753418  Train_KL: 3.5190660655498505  Validation Loss : 92.16815948486328 Val_Reconstruction : 88.68212127685547 Val_KL : 3.4860345125198364\n","Epoch: 4794/8000  Traning Loss: 92.09199333190918  Train_Reconstruction: 88.57243728637695  Train_KL: 3.519557297229767  Validation Loss : 92.22842788696289 Val_Reconstruction : 88.75420761108398 Val_KL : 3.474221706390381\n","Epoch: 4795/8000  Traning Loss: 91.8523302078247  Train_Reconstruction: 88.33668899536133  Train_KL: 3.5156416594982147  Validation Loss : 91.45495986938477 Val_Reconstruction : 87.9722671508789 Val_KL : 3.482690691947937\n","Epoch: 4796/8000  Traning Loss: 91.52041721343994  Train_Reconstruction: 87.99300575256348  Train_KL: 3.5274114310741425  Validation Loss : 91.2844352722168 Val_Reconstruction : 87.79986953735352 Val_KL : 3.4845669269561768\n","Epoch: 4797/8000  Traning Loss: 91.58590793609619  Train_Reconstruction: 88.0534257888794  Train_KL: 3.532481461763382  Validation Loss : 91.2625846862793 Val_Reconstruction : 87.76156997680664 Val_KL : 3.501015543937683\n","Epoch: 4798/8000  Traning Loss: 91.2485580444336  Train_Reconstruction: 87.72366905212402  Train_KL: 3.5248895585536957  Validation Loss : 91.07218551635742 Val_Reconstruction : 87.59036636352539 Val_KL : 3.481821298599243\n","Epoch: 4799/8000  Traning Loss: 91.42290878295898  Train_Reconstruction: 87.90644454956055  Train_KL: 3.516464650630951  Validation Loss : 91.73085403442383 Val_Reconstruction : 88.24927139282227 Val_KL : 3.4815834760665894\n","Epoch: 4800/8000  Traning Loss: 91.8206787109375  Train_Reconstruction: 88.29795169830322  Train_KL: 3.5227267742156982  Validation Loss : 91.54499053955078 Val_Reconstruction : 88.07159042358398 Val_KL : 3.4734030961990356\n","Epoch: 4801/8000  Traning Loss: 91.55849170684814  Train_Reconstruction: 88.03989505767822  Train_KL: 3.518595725297928  Validation Loss : 91.26987075805664 Val_Reconstruction : 87.79769897460938 Val_KL : 3.4721721410751343\n","Epoch: 4802/8000  Traning Loss: 91.36056518554688  Train_Reconstruction: 87.84359550476074  Train_KL: 3.516969919204712  Validation Loss : 91.3807373046875 Val_Reconstruction : 87.89804077148438 Val_KL : 3.4826971292495728\n","Epoch: 4803/8000  Traning Loss: 91.52399349212646  Train_Reconstruction: 88.00148677825928  Train_KL: 3.522507280111313  Validation Loss : 91.67084884643555 Val_Reconstruction : 88.19226837158203 Val_KL : 3.478578209877014\n","Epoch: 4804/8000  Traning Loss: 91.3800573348999  Train_Reconstruction: 87.85537433624268  Train_KL: 3.524683117866516  Validation Loss : 91.09052276611328 Val_Reconstruction : 87.5992431640625 Val_KL : 3.491280436515808\n","Epoch: 4805/8000  Traning Loss: 91.3400650024414  Train_Reconstruction: 87.80886936187744  Train_KL: 3.5311953127384186  Validation Loss : 91.2956657409668 Val_Reconstruction : 87.8112564086914 Val_KL : 3.48440945148468\n","Epoch: 4806/8000  Traning Loss: 91.17091274261475  Train_Reconstruction: 87.65120792388916  Train_KL: 3.519703835248947  Validation Loss : 91.07709884643555 Val_Reconstruction : 87.59974670410156 Val_KL : 3.47735059261322\n","Epoch: 4807/8000  Traning Loss: 91.49960899353027  Train_Reconstruction: 87.9825029373169  Train_KL: 3.5171058177948  Validation Loss : 91.64189910888672 Val_Reconstruction : 88.15693283081055 Val_KL : 3.4849636554718018\n","Epoch: 4808/8000  Traning Loss: 91.81453609466553  Train_Reconstruction: 88.28744983673096  Train_KL: 3.527086019515991  Validation Loss : 91.54555130004883 Val_Reconstruction : 88.06097793579102 Val_KL : 3.484574317932129\n","Epoch: 4809/8000  Traning Loss: 91.7011022567749  Train_Reconstruction: 88.19007396697998  Train_KL: 3.5110287964344025  Validation Loss : 91.69721603393555 Val_Reconstruction : 88.22962951660156 Val_KL : 3.467586398124695\n","Epoch: 4810/8000  Traning Loss: 92.57796382904053  Train_Reconstruction: 89.05906677246094  Train_KL: 3.518895834684372  Validation Loss : 92.14802932739258 Val_Reconstruction : 88.65926361083984 Val_KL : 3.488762617111206\n","Epoch: 4811/8000  Traning Loss: 92.03994941711426  Train_Reconstruction: 88.50704288482666  Train_KL: 3.532907009124756  Validation Loss : 91.77183151245117 Val_Reconstruction : 88.28672790527344 Val_KL : 3.485101342201233\n","Epoch: 4812/8000  Traning Loss: 91.81425380706787  Train_Reconstruction: 88.29386043548584  Train_KL: 3.520394653081894  Validation Loss : 91.84254837036133 Val_Reconstruction : 88.36543273925781 Val_KL : 3.4771147966384888\n","Epoch: 4813/8000  Traning Loss: 92.06777381896973  Train_Reconstruction: 88.54835033416748  Train_KL: 3.519424021244049  Validation Loss : 92.2525634765625 Val_Reconstruction : 88.76954650878906 Val_KL : 3.4830161333084106\n","Epoch: 4814/8000  Traning Loss: 92.06468200683594  Train_Reconstruction: 88.54110050201416  Train_KL: 3.5235811471939087  Validation Loss : 91.49563217163086 Val_Reconstruction : 88.01638412475586 Val_KL : 3.4792490005493164\n","Epoch: 4815/8000  Traning Loss: 91.30679130554199  Train_Reconstruction: 87.79459571838379  Train_KL: 3.5121944546699524  Validation Loss : 91.0519790649414 Val_Reconstruction : 87.57954406738281 Val_KL : 3.472434639930725\n","Epoch: 4816/8000  Traning Loss: 91.22243213653564  Train_Reconstruction: 87.70672035217285  Train_KL: 3.515712320804596  Validation Loss : 91.19244384765625 Val_Reconstruction : 87.70820617675781 Val_KL : 3.484236478805542\n","Epoch: 4817/8000  Traning Loss: 91.2425594329834  Train_Reconstruction: 87.71240615844727  Train_KL: 3.530154138803482  Validation Loss : 90.9910774230957 Val_Reconstruction : 87.50188827514648 Val_KL : 3.489189624786377\n","Epoch: 4818/8000  Traning Loss: 91.62195491790771  Train_Reconstruction: 88.09725666046143  Train_KL: 3.524697959423065  Validation Loss : 91.61574935913086 Val_Reconstruction : 88.12796783447266 Val_KL : 3.4877800941467285\n","Epoch: 4819/8000  Traning Loss: 91.7099552154541  Train_Reconstruction: 88.18788719177246  Train_KL: 3.5220685601234436  Validation Loss : 91.49814987182617 Val_Reconstruction : 88.0144271850586 Val_KL : 3.4837249517440796\n","Epoch: 4820/8000  Traning Loss: 91.6014986038208  Train_Reconstruction: 88.0870304107666  Train_KL: 3.514468252658844  Validation Loss : 91.38352584838867 Val_Reconstruction : 87.90998458862305 Val_KL : 3.4735387563705444\n","Epoch: 4821/8000  Traning Loss: 91.49178791046143  Train_Reconstruction: 87.97652435302734  Train_KL: 3.5152634978294373  Validation Loss : 91.57753372192383 Val_Reconstruction : 88.10432815551758 Val_KL : 3.4732043743133545\n","Epoch: 4822/8000  Traning Loss: 91.8812084197998  Train_Reconstruction: 88.36812591552734  Train_KL: 3.5130816996097565  Validation Loss : 92.25843048095703 Val_Reconstruction : 88.78291702270508 Val_KL : 3.4755117893218994\n","Epoch: 4823/8000  Traning Loss: 91.93771743774414  Train_Reconstruction: 88.4121265411377  Train_KL: 3.525589793920517  Validation Loss : 91.41875076293945 Val_Reconstruction : 87.9300765991211 Val_KL : 3.488674283027649\n","Epoch: 4824/8000  Traning Loss: 91.95853233337402  Train_Reconstruction: 88.43992328643799  Train_KL: 3.5186091363430023  Validation Loss : 91.76438522338867 Val_Reconstruction : 88.28848648071289 Val_KL : 3.475900411605835\n","Epoch: 4825/8000  Traning Loss: 91.85605239868164  Train_Reconstruction: 88.34017276763916  Train_KL: 3.515880197286606  Validation Loss : 91.5462646484375 Val_Reconstruction : 88.05888366699219 Val_KL : 3.487381339073181\n","Epoch: 4826/8000  Traning Loss: 91.52191162109375  Train_Reconstruction: 88.00296115875244  Train_KL: 3.5189515948295593  Validation Loss : 91.14090728759766 Val_Reconstruction : 87.67173385620117 Val_KL : 3.4691736698150635\n","Epoch: 4827/8000  Traning Loss: 91.35859870910645  Train_Reconstruction: 87.84768676757812  Train_KL: 3.510912448167801  Validation Loss : 91.29916381835938 Val_Reconstruction : 87.82048034667969 Val_KL : 3.478683829307556\n","Epoch: 4828/8000  Traning Loss: 91.44157409667969  Train_Reconstruction: 87.91583633422852  Train_KL: 3.5257380306720734  Validation Loss : 91.24585342407227 Val_Reconstruction : 87.75444793701172 Val_KL : 3.4914051294326782\n","Epoch: 4829/8000  Traning Loss: 91.37756443023682  Train_Reconstruction: 87.84643363952637  Train_KL: 3.531129628419876  Validation Loss : 91.16143798828125 Val_Reconstruction : 87.67174911499023 Val_KL : 3.489691138267517\n","Epoch: 4830/8000  Traning Loss: 91.7684736251831  Train_Reconstruction: 88.2491865158081  Train_KL: 3.519287258386612  Validation Loss : 91.13748550415039 Val_Reconstruction : 87.6602668762207 Val_KL : 3.4772186279296875\n","Epoch: 4831/8000  Traning Loss: 91.58533000946045  Train_Reconstruction: 88.07169342041016  Train_KL: 3.5136367976665497  Validation Loss : 91.82628631591797 Val_Reconstruction : 88.35308456420898 Val_KL : 3.473199725151062\n","Epoch: 4832/8000  Traning Loss: 91.98207187652588  Train_Reconstruction: 88.45483303070068  Train_KL: 3.52723827958107  Validation Loss : 91.72990417480469 Val_Reconstruction : 88.23250198364258 Val_KL : 3.4974008798599243\n","Epoch: 4833/8000  Traning Loss: 91.6714563369751  Train_Reconstruction: 88.14585971832275  Train_KL: 3.525595724582672  Validation Loss : 91.81891250610352 Val_Reconstruction : 88.34789657592773 Val_KL : 3.471018433570862\n","Epoch: 4834/8000  Traning Loss: 91.65324020385742  Train_Reconstruction: 88.13403797149658  Train_KL: 3.5192025005817413  Validation Loss : 91.44353866577148 Val_Reconstruction : 87.97031784057617 Val_KL : 3.4732221364974976\n","Epoch: 4835/8000  Traning Loss: 91.55090236663818  Train_Reconstruction: 88.02103805541992  Train_KL: 3.529864579439163  Validation Loss : 91.7988166809082 Val_Reconstruction : 88.30727005004883 Val_KL : 3.491544485092163\n","Epoch: 4836/8000  Traning Loss: 91.3512134552002  Train_Reconstruction: 87.82858467102051  Train_KL: 3.522629052400589  Validation Loss : 91.10120391845703 Val_Reconstruction : 87.6231918334961 Val_KL : 3.478010892868042\n","Epoch: 4837/8000  Traning Loss: 91.24544048309326  Train_Reconstruction: 87.72452735900879  Train_KL: 3.5209142863750458  Validation Loss : 91.2151107788086 Val_Reconstruction : 87.72433853149414 Val_KL : 3.4907718896865845\n","Epoch: 4838/8000  Traning Loss: 91.37066841125488  Train_Reconstruction: 87.84031963348389  Train_KL: 3.530347466468811  Validation Loss : 91.22916793823242 Val_Reconstruction : 87.74097442626953 Val_KL : 3.4881926774978638\n","Epoch: 4839/8000  Traning Loss: 91.57079696655273  Train_Reconstruction: 88.05018424987793  Train_KL: 3.520613521337509  Validation Loss : 91.62842178344727 Val_Reconstruction : 88.16314315795898 Val_KL : 3.465277910232544\n","Epoch: 4840/8000  Traning Loss: 91.61559581756592  Train_Reconstruction: 88.10235404968262  Train_KL: 3.5132411420345306  Validation Loss : 91.07511901855469 Val_Reconstruction : 87.59711456298828 Val_KL : 3.478005051612854\n","Epoch: 4841/8000  Traning Loss: 91.67621994018555  Train_Reconstruction: 88.15258979797363  Train_KL: 3.523630291223526  Validation Loss : 91.5435676574707 Val_Reconstruction : 88.05854415893555 Val_KL : 3.485026478767395\n","Epoch: 4842/8000  Traning Loss: 91.4034776687622  Train_Reconstruction: 87.88035297393799  Train_KL: 3.5231246054172516  Validation Loss : 91.23060607910156 Val_Reconstruction : 87.74848937988281 Val_KL : 3.482117533683777\n","Epoch: 4843/8000  Traning Loss: 91.28535747528076  Train_Reconstruction: 87.76285934448242  Train_KL: 3.522498518228531  Validation Loss : 91.18923568725586 Val_Reconstruction : 87.7134017944336 Val_KL : 3.475832223892212\n","Epoch: 4844/8000  Traning Loss: 91.66557312011719  Train_Reconstruction: 88.14771175384521  Train_KL: 3.517861306667328  Validation Loss : 91.4522476196289 Val_Reconstruction : 87.97651290893555 Val_KL : 3.47573459148407\n","Epoch: 4845/8000  Traning Loss: 91.4911937713623  Train_Reconstruction: 87.97047996520996  Train_KL: 3.5207111537456512  Validation Loss : 91.56776428222656 Val_Reconstruction : 88.07558059692383 Val_KL : 3.492185950279236\n","Epoch: 4846/8000  Traning Loss: 91.36623096466064  Train_Reconstruction: 87.83728218078613  Train_KL: 3.5289498567581177  Validation Loss : 91.40068435668945 Val_Reconstruction : 87.91447448730469 Val_KL : 3.486209273338318\n","Epoch: 4847/8000  Traning Loss: 92.21512603759766  Train_Reconstruction: 88.6989278793335  Train_KL: 3.5161974132061005  Validation Loss : 92.48641204833984 Val_Reconstruction : 89.01166534423828 Val_KL : 3.474745750427246\n","Epoch: 4848/8000  Traning Loss: 91.98448848724365  Train_Reconstruction: 88.45993518829346  Train_KL: 3.5245529413223267  Validation Loss : 91.5363998413086 Val_Reconstruction : 88.04562759399414 Val_KL : 3.490773916244507\n","Epoch: 4849/8000  Traning Loss: 91.51532459259033  Train_Reconstruction: 87.98432445526123  Train_KL: 3.531000792980194  Validation Loss : 91.72531127929688 Val_Reconstruction : 88.22872924804688 Val_KL : 3.496581196784973\n","Epoch: 4850/8000  Traning Loss: 91.57296371459961  Train_Reconstruction: 88.04346752166748  Train_KL: 3.5294970273971558  Validation Loss : 91.50851821899414 Val_Reconstruction : 88.02626419067383 Val_KL : 3.4822555780410767\n","Epoch: 4851/8000  Traning Loss: 91.80929183959961  Train_Reconstruction: 88.29297256469727  Train_KL: 3.516318678855896  Validation Loss : 92.02587890625 Val_Reconstruction : 88.54459381103516 Val_KL : 3.4812875986099243\n","Epoch: 4852/8000  Traning Loss: 91.50178813934326  Train_Reconstruction: 87.97942924499512  Train_KL: 3.5223585665225983  Validation Loss : 91.29241180419922 Val_Reconstruction : 87.80142974853516 Val_KL : 3.4909814596176147\n","Epoch: 4853/8000  Traning Loss: 91.22092628479004  Train_Reconstruction: 87.70100212097168  Train_KL: 3.5199228525161743  Validation Loss : 91.32031631469727 Val_Reconstruction : 87.84379577636719 Val_KL : 3.4765207767486572\n","Epoch: 4854/8000  Traning Loss: 91.22557544708252  Train_Reconstruction: 87.71280765533447  Train_KL: 3.5127669274806976  Validation Loss : 91.59820175170898 Val_Reconstruction : 88.12775421142578 Val_KL : 3.470446467399597\n","Epoch: 4855/8000  Traning Loss: 91.60907554626465  Train_Reconstruction: 88.09095096588135  Train_KL: 3.518123060464859  Validation Loss : 91.86737442016602 Val_Reconstruction : 88.39204788208008 Val_KL : 3.4753297567367554\n","Epoch: 4856/8000  Traning Loss: 92.2319107055664  Train_Reconstruction: 88.71544647216797  Train_KL: 3.516463816165924  Validation Loss : 92.16209411621094 Val_Reconstruction : 88.68379592895508 Val_KL : 3.478297710418701\n","Epoch: 4857/8000  Traning Loss: 92.13460350036621  Train_Reconstruction: 88.62559223175049  Train_KL: 3.509010761976242  Validation Loss : 91.82732391357422 Val_Reconstruction : 88.3582534790039 Val_KL : 3.469070076942444\n","Epoch: 4858/8000  Traning Loss: 91.92950344085693  Train_Reconstruction: 88.41562080383301  Train_KL: 3.513881802558899  Validation Loss : 91.9349365234375 Val_Reconstruction : 88.44549942016602 Val_KL : 3.4894344806671143\n","Epoch: 4859/8000  Traning Loss: 91.7136287689209  Train_Reconstruction: 88.19516468048096  Train_KL: 3.5184642672538757  Validation Loss : 91.45064163208008 Val_Reconstruction : 87.97281265258789 Val_KL : 3.4778300523757935\n","Epoch: 4860/8000  Traning Loss: 91.82866668701172  Train_Reconstruction: 88.31819725036621  Train_KL: 3.5104709565639496  Validation Loss : 91.97811889648438 Val_Reconstruction : 88.51145553588867 Val_KL : 3.466661810874939\n","Epoch: 4861/8000  Traning Loss: 91.8517017364502  Train_Reconstruction: 88.33476543426514  Train_KL: 3.5169349908828735  Validation Loss : 91.58568954467773 Val_Reconstruction : 88.10821914672852 Val_KL : 3.4774709939956665\n","Epoch: 4862/8000  Traning Loss: 91.79334926605225  Train_Reconstruction: 88.27047348022461  Train_KL: 3.522876054048538  Validation Loss : 91.38553619384766 Val_Reconstruction : 87.90631866455078 Val_KL : 3.479218006134033\n","Epoch: 4863/8000  Traning Loss: 91.50670909881592  Train_Reconstruction: 87.99241161346436  Train_KL: 3.5142982602119446  Validation Loss : 91.16850280761719 Val_Reconstruction : 87.68669891357422 Val_KL : 3.4818066358566284\n","Epoch: 4864/8000  Traning Loss: 91.69633293151855  Train_Reconstruction: 88.17343616485596  Train_KL: 3.522897720336914  Validation Loss : 92.39371490478516 Val_Reconstruction : 88.91274642944336 Val_KL : 3.480966567993164\n","Epoch: 4865/8000  Traning Loss: 91.85906982421875  Train_Reconstruction: 88.33598613739014  Train_KL: 3.5230831801891327  Validation Loss : 91.2692642211914 Val_Reconstruction : 87.78532791137695 Val_KL : 3.4839389324188232\n","Epoch: 4866/8000  Traning Loss: 91.49272441864014  Train_Reconstruction: 87.96919345855713  Train_KL: 3.5235320925712585  Validation Loss : 91.43899536132812 Val_Reconstruction : 87.9600715637207 Val_KL : 3.4789257049560547\n","Epoch: 4867/8000  Traning Loss: 91.56040000915527  Train_Reconstruction: 88.03936290740967  Train_KL: 3.5210364758968353  Validation Loss : 91.54465866088867 Val_Reconstruction : 88.06536483764648 Val_KL : 3.4792916774749756\n","Epoch: 4868/8000  Traning Loss: 91.42400741577148  Train_Reconstruction: 87.90357494354248  Train_KL: 3.5204314291477203  Validation Loss : 91.25503158569336 Val_Reconstruction : 87.7708511352539 Val_KL : 3.4841784238815308\n","Epoch: 4869/8000  Traning Loss: 91.43824768066406  Train_Reconstruction: 87.91651630401611  Train_KL: 3.5217300951480865  Validation Loss : 91.10037612915039 Val_Reconstruction : 87.6186752319336 Val_KL : 3.4817014932632446\n","Epoch: 4870/8000  Traning Loss: 91.54651069641113  Train_Reconstruction: 88.02993679046631  Train_KL: 3.516574054956436  Validation Loss : 91.68589782714844 Val_Reconstruction : 88.2069091796875 Val_KL : 3.4789918661117554\n","Epoch: 4871/8000  Traning Loss: 91.72624397277832  Train_Reconstruction: 88.20558738708496  Train_KL: 3.5206552743911743  Validation Loss : 91.8239974975586 Val_Reconstruction : 88.34394836425781 Val_KL : 3.4800487756729126\n","Epoch: 4872/8000  Traning Loss: 91.56082153320312  Train_Reconstruction: 88.04901313781738  Train_KL: 3.511809378862381  Validation Loss : 91.42655563354492 Val_Reconstruction : 87.95677185058594 Val_KL : 3.4697853326797485\n","Epoch: 4873/8000  Traning Loss: 91.18775844573975  Train_Reconstruction: 87.66860961914062  Train_KL: 3.519148051738739  Validation Loss : 91.2806625366211 Val_Reconstruction : 87.7916145324707 Val_KL : 3.4890477657318115\n","Epoch: 4874/8000  Traning Loss: 91.38933086395264  Train_Reconstruction: 87.8608627319336  Train_KL: 3.52846822142601  Validation Loss : 91.26773452758789 Val_Reconstruction : 87.77654647827148 Val_KL : 3.4911866188049316\n","Epoch: 4875/8000  Traning Loss: 91.53642272949219  Train_Reconstruction: 88.0099983215332  Train_KL: 3.526423066854477  Validation Loss : 91.51857376098633 Val_Reconstruction : 88.02875518798828 Val_KL : 3.4898171424865723\n","Epoch: 4876/8000  Traning Loss: 91.56019687652588  Train_Reconstruction: 88.03926467895508  Train_KL: 3.520932286977768  Validation Loss : 91.4350357055664 Val_Reconstruction : 87.96419906616211 Val_KL : 3.4708383083343506\n","Epoch: 4877/8000  Traning Loss: 91.53452682495117  Train_Reconstruction: 88.01862812042236  Train_KL: 3.5158989131450653  Validation Loss : 91.45681381225586 Val_Reconstruction : 87.97564697265625 Val_KL : 3.4811664819717407\n","Epoch: 4878/8000  Traning Loss: 91.40985488891602  Train_Reconstruction: 87.88746356964111  Train_KL: 3.5223927795886993  Validation Loss : 91.47016906738281 Val_Reconstruction : 87.9942626953125 Val_KL : 3.4759039878845215\n","Epoch: 4879/8000  Traning Loss: 91.48463153839111  Train_Reconstruction: 87.97345352172852  Train_KL: 3.5111769139766693  Validation Loss : 91.38421249389648 Val_Reconstruction : 87.90680694580078 Val_KL : 3.47740375995636\n","Epoch: 4880/8000  Traning Loss: 91.5886116027832  Train_Reconstruction: 88.06604099273682  Train_KL: 3.522571623325348  Validation Loss : 91.3906364440918 Val_Reconstruction : 87.90269470214844 Val_KL : 3.4879446029663086\n","Epoch: 4881/8000  Traning Loss: 91.5222864151001  Train_Reconstruction: 87.99164390563965  Train_KL: 3.530643343925476  Validation Loss : 91.15763092041016 Val_Reconstruction : 87.67038345336914 Val_KL : 3.4872496128082275\n","Epoch: 4882/8000  Traning Loss: 91.47590255737305  Train_Reconstruction: 87.96292209625244  Train_KL: 3.512981802225113  Validation Loss : 91.38741302490234 Val_Reconstruction : 87.91990280151367 Val_KL : 3.4675084352493286\n","Epoch: 4883/8000  Traning Loss: 91.84607887268066  Train_Reconstruction: 88.33533096313477  Train_KL: 3.5107488930225372  Validation Loss : 91.89100646972656 Val_Reconstruction : 88.41428756713867 Val_KL : 3.476720094680786\n","Epoch: 4884/8000  Traning Loss: 91.82102966308594  Train_Reconstruction: 88.29475784301758  Train_KL: 3.526269644498825  Validation Loss : 92.10696411132812 Val_Reconstruction : 88.61829376220703 Val_KL : 3.4886704683303833\n","Epoch: 4885/8000  Traning Loss: 91.8989086151123  Train_Reconstruction: 88.3797836303711  Train_KL: 3.5191259682178497  Validation Loss : 91.39738845825195 Val_Reconstruction : 87.92191314697266 Val_KL : 3.4754719734191895\n","Epoch: 4886/8000  Traning Loss: 91.54819011688232  Train_Reconstruction: 88.02695846557617  Train_KL: 3.5212314426898956  Validation Loss : 91.27159881591797 Val_Reconstruction : 87.78783416748047 Val_KL : 3.4837639331817627\n","Epoch: 4887/8000  Traning Loss: 91.51287841796875  Train_Reconstruction: 87.9972915649414  Train_KL: 3.5155875086784363  Validation Loss : 91.67533493041992 Val_Reconstruction : 88.20251083374023 Val_KL : 3.472825050354004\n","Epoch: 4888/8000  Traning Loss: 91.41825771331787  Train_Reconstruction: 87.9061222076416  Train_KL: 3.512135922908783  Validation Loss : 90.8503646850586 Val_Reconstruction : 87.3742446899414 Val_KL : 3.4761195182800293\n","Epoch: 4889/8000  Traning Loss: 91.37086009979248  Train_Reconstruction: 87.85323238372803  Train_KL: 3.517627328634262  Validation Loss : 91.19580078125 Val_Reconstruction : 87.71514892578125 Val_KL : 3.4806514978408813\n","Epoch: 4890/8000  Traning Loss: 91.47361755371094  Train_Reconstruction: 87.95379447937012  Train_KL: 3.5198231041431427  Validation Loss : 91.37248992919922 Val_Reconstruction : 87.88727569580078 Val_KL : 3.485214591026306\n","Epoch: 4891/8000  Traning Loss: 91.75049018859863  Train_Reconstruction: 88.22613620758057  Train_KL: 3.524352937936783  Validation Loss : 91.63647842407227 Val_Reconstruction : 88.1565933227539 Val_KL : 3.4798827171325684\n","Epoch: 4892/8000  Traning Loss: 91.53630447387695  Train_Reconstruction: 88.02033519744873  Train_KL: 3.515969604253769  Validation Loss : 91.21734619140625 Val_Reconstruction : 87.74906539916992 Val_KL : 3.468281865119934\n","Epoch: 4893/8000  Traning Loss: 91.41806125640869  Train_Reconstruction: 87.90589618682861  Train_KL: 3.5121663212776184  Validation Loss : 91.13128662109375 Val_Reconstruction : 87.66096496582031 Val_KL : 3.4703195095062256\n","Epoch: 4894/8000  Traning Loss: 91.25326347351074  Train_Reconstruction: 87.72771835327148  Train_KL: 3.525545060634613  Validation Loss : 90.91476440429688 Val_Reconstruction : 87.42057800292969 Val_KL : 3.494186520576477\n","Epoch: 4895/8000  Traning Loss: 91.22338104248047  Train_Reconstruction: 87.69198036193848  Train_KL: 3.5314006209373474  Validation Loss : 91.17822265625 Val_Reconstruction : 87.6939811706543 Val_KL : 3.484238028526306\n","Epoch: 4896/8000  Traning Loss: 91.37793922424316  Train_Reconstruction: 87.86076354980469  Train_KL: 3.5171757638454437  Validation Loss : 91.070068359375 Val_Reconstruction : 87.59206008911133 Val_KL : 3.4780079126358032\n","Epoch: 4897/8000  Traning Loss: 91.28518486022949  Train_Reconstruction: 87.76840400695801  Train_KL: 3.516781598329544  Validation Loss : 91.2701301574707 Val_Reconstruction : 87.78857040405273 Val_KL : 3.481559991836548\n","Epoch: 4898/8000  Traning Loss: 91.4143419265747  Train_Reconstruction: 87.8962049484253  Train_KL: 3.5181378722190857  Validation Loss : 91.24505996704102 Val_Reconstruction : 87.76534652709961 Val_KL : 3.479714035987854\n","Epoch: 4899/8000  Traning Loss: 91.73431968688965  Train_Reconstruction: 88.21287059783936  Train_KL: 3.5214487612247467  Validation Loss : 91.73662567138672 Val_Reconstruction : 88.25617218017578 Val_KL : 3.480453133583069\n","Epoch: 4900/8000  Traning Loss: 91.76335430145264  Train_Reconstruction: 88.24173069000244  Train_KL: 3.521623879671097  Validation Loss : 91.5944595336914 Val_Reconstruction : 88.1179313659668 Val_KL : 3.4765294790267944\n","Epoch: 4901/8000  Traning Loss: 91.2757978439331  Train_Reconstruction: 87.75736141204834  Train_KL: 3.518436998128891  Validation Loss : 91.09693908691406 Val_Reconstruction : 87.62946319580078 Val_KL : 3.4674752950668335\n","Epoch: 4902/8000  Traning Loss: 91.05446147918701  Train_Reconstruction: 87.54554653167725  Train_KL: 3.508915662765503  Validation Loss : 90.95919036865234 Val_Reconstruction : 87.48733520507812 Val_KL : 3.471855044364929\n","Epoch: 4903/8000  Traning Loss: 91.12275981903076  Train_Reconstruction: 87.59914207458496  Train_KL: 3.5236178934574127  Validation Loss : 90.9832992553711 Val_Reconstruction : 87.49280548095703 Val_KL : 3.490492582321167\n","Epoch: 4904/8000  Traning Loss: 91.1865758895874  Train_Reconstruction: 87.65418148040771  Train_KL: 3.5323941707611084  Validation Loss : 91.32046508789062 Val_Reconstruction : 87.8344612121582 Val_KL : 3.486004590988159\n","Epoch: 4905/8000  Traning Loss: 91.42018222808838  Train_Reconstruction: 87.9010124206543  Train_KL: 3.5191697776317596  Validation Loss : 91.01367568969727 Val_Reconstruction : 87.52848434448242 Val_KL : 3.4851890802383423\n","Epoch: 4906/8000  Traning Loss: 91.22208499908447  Train_Reconstruction: 87.70296573638916  Train_KL: 3.51911860704422  Validation Loss : 90.9171028137207 Val_Reconstruction : 87.43561553955078 Val_KL : 3.481486201286316\n","Epoch: 4907/8000  Traning Loss: 91.37116813659668  Train_Reconstruction: 87.84483909606934  Train_KL: 3.526330143213272  Validation Loss : 91.2032699584961 Val_Reconstruction : 87.72268676757812 Val_KL : 3.4805809259414673\n","Epoch: 4908/8000  Traning Loss: 91.48801136016846  Train_Reconstruction: 87.97136878967285  Train_KL: 3.516642242670059  Validation Loss : 91.56013870239258 Val_Reconstruction : 88.10098266601562 Val_KL : 3.459155559539795\n","Epoch: 4909/8000  Traning Loss: 91.78175640106201  Train_Reconstruction: 88.27617931365967  Train_KL: 3.5055781602859497  Validation Loss : 91.75910949707031 Val_Reconstruction : 88.28376388549805 Val_KL : 3.4753458499908447\n","Epoch: 4910/8000  Traning Loss: 91.79314708709717  Train_Reconstruction: 88.27126502990723  Train_KL: 3.52188178896904  Validation Loss : 91.12140274047852 Val_Reconstruction : 87.63174438476562 Val_KL : 3.4896581172943115\n","Epoch: 4911/8000  Traning Loss: 91.19232559204102  Train_Reconstruction: 87.67261600494385  Train_KL: 3.519709050655365  Validation Loss : 91.04075241088867 Val_Reconstruction : 87.55998992919922 Val_KL : 3.4807621240615845\n","Epoch: 4912/8000  Traning Loss: 91.51832866668701  Train_Reconstruction: 87.98837184906006  Train_KL: 3.529956132173538  Validation Loss : 91.48819351196289 Val_Reconstruction : 87.99942016601562 Val_KL : 3.488773226737976\n","Epoch: 4913/8000  Traning Loss: 91.24292755126953  Train_Reconstruction: 87.72045516967773  Train_KL: 3.5224722623825073  Validation Loss : 90.93265533447266 Val_Reconstruction : 87.45110321044922 Val_KL : 3.4815534353256226\n","Epoch: 4914/8000  Traning Loss: 90.94740200042725  Train_Reconstruction: 87.43202209472656  Train_KL: 3.515381157398224  Validation Loss : 90.77861022949219 Val_Reconstruction : 87.29869079589844 Val_KL : 3.479917049407959\n","Epoch: 4915/8000  Traning Loss: 91.07641124725342  Train_Reconstruction: 87.55632781982422  Train_KL: 3.520083487033844  Validation Loss : 91.42560195922852 Val_Reconstruction : 87.94706726074219 Val_KL : 3.478534460067749\n","Epoch: 4916/8000  Traning Loss: 91.68875026702881  Train_Reconstruction: 88.16857814788818  Train_KL: 3.5201724469661713  Validation Loss : 91.97219848632812 Val_Reconstruction : 88.4920425415039 Val_KL : 3.4801565408706665\n","Epoch: 4917/8000  Traning Loss: 92.10128879547119  Train_Reconstruction: 88.58050537109375  Train_KL: 3.5207843482494354  Validation Loss : 92.07968521118164 Val_Reconstruction : 88.59806823730469 Val_KL : 3.481616497039795\n","Epoch: 4918/8000  Traning Loss: 91.60051918029785  Train_Reconstruction: 88.07796001434326  Train_KL: 3.522559553384781  Validation Loss : 91.2138671875 Val_Reconstruction : 87.72475051879883 Val_KL : 3.4891167879104614\n","Epoch: 4919/8000  Traning Loss: 91.23298072814941  Train_Reconstruction: 87.71403980255127  Train_KL: 3.518940716981888  Validation Loss : 91.16412353515625 Val_Reconstruction : 87.68538665771484 Val_KL : 3.4787375926971436\n","Epoch: 4920/8000  Traning Loss: 91.23024082183838  Train_Reconstruction: 87.71342945098877  Train_KL: 3.5168113112449646  Validation Loss : 91.01921081542969 Val_Reconstruction : 87.54043960571289 Val_KL : 3.4787691831588745\n","Epoch: 4921/8000  Traning Loss: 91.27403736114502  Train_Reconstruction: 87.75575733184814  Train_KL: 3.518280506134033  Validation Loss : 91.26862716674805 Val_Reconstruction : 87.78590393066406 Val_KL : 3.4827229976654053\n","Epoch: 4922/8000  Traning Loss: 91.29194164276123  Train_Reconstruction: 87.77111339569092  Train_KL: 3.5208268761634827  Validation Loss : 91.08271789550781 Val_Reconstruction : 87.59304428100586 Val_KL : 3.489673614501953\n","Epoch: 4923/8000  Traning Loss: 91.30225276947021  Train_Reconstruction: 87.77096748352051  Train_KL: 3.5312854051589966  Validation Loss : 91.32545852661133 Val_Reconstruction : 87.82595443725586 Val_KL : 3.4995052814483643\n","Epoch: 4924/8000  Traning Loss: 91.4407844543457  Train_Reconstruction: 87.90762424468994  Train_KL: 3.5331583619117737  Validation Loss : 91.27268981933594 Val_Reconstruction : 87.78506088256836 Val_KL : 3.4876301288604736\n","Epoch: 4925/8000  Traning Loss: 91.2692985534668  Train_Reconstruction: 87.7462797164917  Train_KL: 3.5230176746845245  Validation Loss : 91.07973861694336 Val_Reconstruction : 87.60419845581055 Val_KL : 3.475540280342102\n","Epoch: 4926/8000  Traning Loss: 91.51375389099121  Train_Reconstruction: 88.0052661895752  Train_KL: 3.508486896753311  Validation Loss : 91.8242416381836 Val_Reconstruction : 88.3563003540039 Val_KL : 3.4679397344589233\n","Epoch: 4927/8000  Traning Loss: 92.22967529296875  Train_Reconstruction: 88.71662616729736  Train_KL: 3.5130503475666046  Validation Loss : 92.3734016418457 Val_Reconstruction : 88.89219665527344 Val_KL : 3.4812049865722656\n","Epoch: 4928/8000  Traning Loss: 92.18947982788086  Train_Reconstruction: 88.66613292694092  Train_KL: 3.5233469009399414  Validation Loss : 92.0677261352539 Val_Reconstruction : 88.58125305175781 Val_KL : 3.486471176147461\n","Epoch: 4929/8000  Traning Loss: 91.97718811035156  Train_Reconstruction: 88.45544624328613  Train_KL: 3.5217411518096924  Validation Loss : 91.54690551757812 Val_Reconstruction : 88.06290435791016 Val_KL : 3.4839996099472046\n","Epoch: 4930/8000  Traning Loss: 91.19881820678711  Train_Reconstruction: 87.67942523956299  Train_KL: 3.519391417503357  Validation Loss : 90.87724685668945 Val_Reconstruction : 87.39486312866211 Val_KL : 3.4823845624923706\n","Epoch: 4931/8000  Traning Loss: 91.0298261642456  Train_Reconstruction: 87.5034704208374  Train_KL: 3.5263566374778748  Validation Loss : 91.10530471801758 Val_Reconstruction : 87.62060546875 Val_KL : 3.4847017526626587\n","Epoch: 4932/8000  Traning Loss: 91.39648151397705  Train_Reconstruction: 87.87521934509277  Train_KL: 3.521262437105179  Validation Loss : 91.25469970703125 Val_Reconstruction : 87.77822875976562 Val_KL : 3.4764708280563354\n","Epoch: 4933/8000  Traning Loss: 91.56202602386475  Train_Reconstruction: 88.03593254089355  Train_KL: 3.526093512773514  Validation Loss : 91.38470840454102 Val_Reconstruction : 87.89073181152344 Val_KL : 3.493975281715393\n","Epoch: 4934/8000  Traning Loss: 91.86048984527588  Train_Reconstruction: 88.3387508392334  Train_KL: 3.5217393040657043  Validation Loss : 92.09823608398438 Val_Reconstruction : 88.61385345458984 Val_KL : 3.484380006790161\n","Epoch: 4935/8000  Traning Loss: 91.3778600692749  Train_Reconstruction: 87.865065574646  Train_KL: 3.512795150279999  Validation Loss : 91.12665939331055 Val_Reconstruction : 87.65074920654297 Val_KL : 3.4759098291397095\n","Epoch: 4936/8000  Traning Loss: 91.54028129577637  Train_Reconstruction: 88.01599788665771  Train_KL: 3.5242820382118225  Validation Loss : 91.62548065185547 Val_Reconstruction : 88.13904571533203 Val_KL : 3.4864346981048584\n","Epoch: 4937/8000  Traning Loss: 91.64853096008301  Train_Reconstruction: 88.12851333618164  Train_KL: 3.5200180411338806  Validation Loss : 91.67622375488281 Val_Reconstruction : 88.20697784423828 Val_KL : 3.469245672225952\n","Epoch: 4938/8000  Traning Loss: 91.5118637084961  Train_Reconstruction: 88.00706195831299  Train_KL: 3.504801094532013  Validation Loss : 91.24058532714844 Val_Reconstruction : 87.77030181884766 Val_KL : 3.4702833890914917\n","Epoch: 4939/8000  Traning Loss: 91.30778789520264  Train_Reconstruction: 87.78077411651611  Train_KL: 3.527013838291168  Validation Loss : 91.29789352416992 Val_Reconstruction : 87.80799102783203 Val_KL : 3.489903688430786\n","Epoch: 4940/8000  Traning Loss: 91.15871238708496  Train_Reconstruction: 87.63268566131592  Train_KL: 3.526026964187622  Validation Loss : 91.12256622314453 Val_Reconstruction : 87.64551544189453 Val_KL : 3.4770495891571045\n","Epoch: 4941/8000  Traning Loss: 91.34292793273926  Train_Reconstruction: 87.82977485656738  Train_KL: 3.5131527483463287  Validation Loss : 91.35692596435547 Val_Reconstruction : 87.8833999633789 Val_KL : 3.4735255241394043\n","Epoch: 4942/8000  Traning Loss: 91.46788787841797  Train_Reconstruction: 87.94935512542725  Train_KL: 3.518532872200012  Validation Loss : 91.41262435913086 Val_Reconstruction : 87.92702865600586 Val_KL : 3.4855953454971313\n","Epoch: 4943/8000  Traning Loss: 91.83186531066895  Train_Reconstruction: 88.31192398071289  Train_KL: 3.519942581653595  Validation Loss : 91.78247833251953 Val_Reconstruction : 88.29297256469727 Val_KL : 3.4895042181015015\n","Epoch: 4944/8000  Traning Loss: 91.89192771911621  Train_Reconstruction: 88.36744785308838  Train_KL: 3.524480879306793  Validation Loss : 91.8482551574707 Val_Reconstruction : 88.36793899536133 Val_KL : 3.4803160429000854\n","Epoch: 4945/8000  Traning Loss: 91.81282138824463  Train_Reconstruction: 88.29428005218506  Train_KL: 3.5185413360595703  Validation Loss : 91.98055267333984 Val_Reconstruction : 88.50362014770508 Val_KL : 3.4769309759140015\n","Epoch: 4946/8000  Traning Loss: 91.5816822052002  Train_Reconstruction: 88.06820392608643  Train_KL: 3.513477772474289  Validation Loss : 91.21001052856445 Val_Reconstruction : 87.73484420776367 Val_KL : 3.47516930103302\n","Epoch: 4947/8000  Traning Loss: 91.26867198944092  Train_Reconstruction: 87.74835586547852  Train_KL: 3.5203165113925934  Validation Loss : 91.30475616455078 Val_Reconstruction : 87.81661605834961 Val_KL : 3.488140821456909\n","Epoch: 4948/8000  Traning Loss: 91.62446308135986  Train_Reconstruction: 88.09976291656494  Train_KL: 3.524700790643692  Validation Loss : 91.7132339477539 Val_Reconstruction : 88.23592376708984 Val_KL : 3.4773114919662476\n","Epoch: 4949/8000  Traning Loss: 91.5958309173584  Train_Reconstruction: 88.07870864868164  Train_KL: 3.517122060060501  Validation Loss : 91.68285369873047 Val_Reconstruction : 88.20725631713867 Val_KL : 3.475600004196167\n","Epoch: 4950/8000  Traning Loss: 91.41086673736572  Train_Reconstruction: 87.89325714111328  Train_KL: 3.5176106691360474  Validation Loss : 91.20438385009766 Val_Reconstruction : 87.71549606323242 Val_KL : 3.4888893365859985\n","Epoch: 4951/8000  Traning Loss: 91.4294376373291  Train_Reconstruction: 87.90087127685547  Train_KL: 3.5285660922527313  Validation Loss : 91.53453826904297 Val_Reconstruction : 88.04896545410156 Val_KL : 3.4855724573135376\n","Epoch: 4952/8000  Traning Loss: 91.34496974945068  Train_Reconstruction: 87.82701206207275  Train_KL: 3.5179570019245148  Validation Loss : 91.4449691772461 Val_Reconstruction : 87.96702194213867 Val_KL : 3.4779480695724487\n","Epoch: 4953/8000  Traning Loss: 91.36891460418701  Train_Reconstruction: 87.8482780456543  Train_KL: 3.5206357538700104  Validation Loss : 91.3604621887207 Val_Reconstruction : 87.8759994506836 Val_KL : 3.484464406967163\n","Epoch: 4954/8000  Traning Loss: 91.595139503479  Train_Reconstruction: 88.07391357421875  Train_KL: 3.5212266743183136  Validation Loss : 91.51874923706055 Val_Reconstruction : 88.03992462158203 Val_KL : 3.4788224697113037\n","Epoch: 4955/8000  Traning Loss: 91.40199184417725  Train_Reconstruction: 87.88347434997559  Train_KL: 3.5185177326202393  Validation Loss : 91.3597412109375 Val_Reconstruction : 87.871337890625 Val_KL : 3.4884034395217896\n","Epoch: 4956/8000  Traning Loss: 91.92595481872559  Train_Reconstruction: 88.39743900299072  Train_KL: 3.528517007827759  Validation Loss : 92.37932205200195 Val_Reconstruction : 88.88609313964844 Val_KL : 3.4932286739349365\n","Epoch: 4957/8000  Traning Loss: 92.04531669616699  Train_Reconstruction: 88.516921043396  Train_KL: 3.5283957421779633  Validation Loss : 91.74890899658203 Val_Reconstruction : 88.25885391235352 Val_KL : 3.4900535345077515\n","Epoch: 4958/8000  Traning Loss: 91.73284816741943  Train_Reconstruction: 88.2101526260376  Train_KL: 3.5226936638355255  Validation Loss : 91.3854866027832 Val_Reconstruction : 87.90401840209961 Val_KL : 3.4814674854278564\n","Epoch: 4959/8000  Traning Loss: 91.53629684448242  Train_Reconstruction: 88.01958751678467  Train_KL: 3.5167095363140106  Validation Loss : 91.3034439086914 Val_Reconstruction : 87.82720565795898 Val_KL : 3.4762401580810547\n","Epoch: 4960/8000  Traning Loss: 91.37215805053711  Train_Reconstruction: 87.85241985321045  Train_KL: 3.5197378993034363  Validation Loss : 91.11420440673828 Val_Reconstruction : 87.63263320922852 Val_KL : 3.481571078300476\n","Epoch: 4961/8000  Traning Loss: 91.15609169006348  Train_Reconstruction: 87.63814163208008  Train_KL: 3.517951101064682  Validation Loss : 91.13307571411133 Val_Reconstruction : 87.66200637817383 Val_KL : 3.471070408821106\n","Epoch: 4962/8000  Traning Loss: 91.47451210021973  Train_Reconstruction: 87.95643901824951  Train_KL: 3.518073797225952  Validation Loss : 91.83970642089844 Val_Reconstruction : 88.3628921508789 Val_KL : 3.4768149852752686\n","Epoch: 4963/8000  Traning Loss: 91.26312732696533  Train_Reconstruction: 87.73960781097412  Train_KL: 3.5235187113285065  Validation Loss : 91.12687301635742 Val_Reconstruction : 87.63671112060547 Val_KL : 3.4901615381240845\n","Epoch: 4964/8000  Traning Loss: 91.14286041259766  Train_Reconstruction: 87.62089824676514  Train_KL: 3.5219623744487762  Validation Loss : 91.1422233581543 Val_Reconstruction : 87.66167831420898 Val_KL : 3.4805448055267334\n","Epoch: 4965/8000  Traning Loss: 91.45054531097412  Train_Reconstruction: 87.93214893341064  Train_KL: 3.5183957517147064  Validation Loss : 91.42424392700195 Val_Reconstruction : 87.9486312866211 Val_KL : 3.475609540939331\n","Epoch: 4966/8000  Traning Loss: 91.53396606445312  Train_Reconstruction: 88.0193099975586  Train_KL: 3.514654666185379  Validation Loss : 91.34597778320312 Val_Reconstruction : 87.86832046508789 Val_KL : 3.4776569604873657\n","Epoch: 4967/8000  Traning Loss: 91.2719373703003  Train_Reconstruction: 87.75434875488281  Train_KL: 3.517589122056961  Validation Loss : 90.96396255493164 Val_Reconstruction : 87.48861694335938 Val_KL : 3.475345492362976\n","Epoch: 4968/8000  Traning Loss: 91.59906959533691  Train_Reconstruction: 88.08482933044434  Train_KL: 3.5142394304275513  Validation Loss : 91.70517349243164 Val_Reconstruction : 88.22633361816406 Val_KL : 3.4788392782211304\n","Epoch: 4969/8000  Traning Loss: 91.64144515991211  Train_Reconstruction: 88.1197452545166  Train_KL: 3.5216998159885406  Validation Loss : 91.1986083984375 Val_Reconstruction : 87.71078109741211 Val_KL : 3.4878292083740234\n","Epoch: 4970/8000  Traning Loss: 91.50654792785645  Train_Reconstruction: 87.98013114929199  Train_KL: 3.526415377855301  Validation Loss : 91.43987655639648 Val_Reconstruction : 87.94795989990234 Val_KL : 3.4919155836105347\n","Epoch: 4971/8000  Traning Loss: 91.37969303131104  Train_Reconstruction: 87.86510467529297  Train_KL: 3.5145885944366455  Validation Loss : 91.0854721069336 Val_Reconstruction : 87.60920715332031 Val_KL : 3.4762662649154663\n","Epoch: 4972/8000  Traning Loss: 91.40966892242432  Train_Reconstruction: 87.89120483398438  Train_KL: 3.5184634923934937  Validation Loss : 91.33087921142578 Val_Reconstruction : 87.85195541381836 Val_KL : 3.4789220094680786\n","Epoch: 4973/8000  Traning Loss: 91.46397876739502  Train_Reconstruction: 87.9460563659668  Train_KL: 3.5179220736026764  Validation Loss : 91.15056228637695 Val_Reconstruction : 87.65869140625 Val_KL : 3.4918688535690308\n","Epoch: 4974/8000  Traning Loss: 91.41008472442627  Train_Reconstruction: 87.88576793670654  Train_KL: 3.5243173241615295  Validation Loss : 91.51662063598633 Val_Reconstruction : 88.03643035888672 Val_KL : 3.480191469192505\n","Epoch: 4975/8000  Traning Loss: 91.66258144378662  Train_Reconstruction: 88.14733695983887  Train_KL: 3.5152447521686554  Validation Loss : 91.8820571899414 Val_Reconstruction : 88.40161895751953 Val_KL : 3.480439305305481\n","Epoch: 4976/8000  Traning Loss: 91.44583988189697  Train_Reconstruction: 87.92649555206299  Train_KL: 3.5193431675434113  Validation Loss : 91.4204216003418 Val_Reconstruction : 87.93539810180664 Val_KL : 3.485020399093628\n","Epoch: 4977/8000  Traning Loss: 91.52933311462402  Train_Reconstruction: 88.00996017456055  Train_KL: 3.519373059272766  Validation Loss : 91.57770156860352 Val_Reconstruction : 88.09529495239258 Val_KL : 3.482406497001648\n","Epoch: 4978/8000  Traning Loss: 91.79249382019043  Train_Reconstruction: 88.270751953125  Train_KL: 3.5217423737049103  Validation Loss : 91.3095703125 Val_Reconstruction : 87.83307266235352 Val_KL : 3.4764996767044067\n","Epoch: 4979/8000  Traning Loss: 91.42963314056396  Train_Reconstruction: 87.9023323059082  Train_KL: 3.527300387620926  Validation Loss : 90.96025085449219 Val_Reconstruction : 87.46899795532227 Val_KL : 3.491253137588501\n","Epoch: 4980/8000  Traning Loss: 91.07679176330566  Train_Reconstruction: 87.54449939727783  Train_KL: 3.532293289899826  Validation Loss : 91.16985702514648 Val_Reconstruction : 87.67990112304688 Val_KL : 3.4899550676345825\n","Epoch: 4981/8000  Traning Loss: 91.2544937133789  Train_Reconstruction: 87.73596858978271  Train_KL: 3.5185256600379944  Validation Loss : 91.4279556274414 Val_Reconstruction : 87.95904922485352 Val_KL : 3.4689059257507324\n","Epoch: 4982/8000  Traning Loss: 91.37742805480957  Train_Reconstruction: 87.86718559265137  Train_KL: 3.5102432668209076  Validation Loss : 91.27986526489258 Val_Reconstruction : 87.80012512207031 Val_KL : 3.47974169254303\n","Epoch: 4983/8000  Traning Loss: 91.43466854095459  Train_Reconstruction: 87.91498565673828  Train_KL: 3.519683539867401  Validation Loss : 91.35798263549805 Val_Reconstruction : 87.87857818603516 Val_KL : 3.4794050455093384\n","Epoch: 4984/8000  Traning Loss: 91.33798217773438  Train_Reconstruction: 87.81600189208984  Train_KL: 3.521980881690979  Validation Loss : 91.36986541748047 Val_Reconstruction : 87.88626098632812 Val_KL : 3.4836056232452393\n","Epoch: 4985/8000  Traning Loss: 91.53897762298584  Train_Reconstruction: 88.01860332489014  Train_KL: 3.5203734636306763  Validation Loss : 91.11306762695312 Val_Reconstruction : 87.63100051879883 Val_KL : 3.4820648431777954\n","Epoch: 4986/8000  Traning Loss: 91.40281772613525  Train_Reconstruction: 87.88381290435791  Train_KL: 3.519005209207535  Validation Loss : 91.0994873046875 Val_Reconstruction : 87.6201171875 Val_KL : 3.479368805885315\n","Epoch: 4987/8000  Traning Loss: 91.36789226531982  Train_Reconstruction: 87.84691047668457  Train_KL: 3.5209818482398987  Validation Loss : 91.1553955078125 Val_Reconstruction : 87.66326904296875 Val_KL : 3.492126226425171\n","Epoch: 4988/8000  Traning Loss: 91.53029346466064  Train_Reconstruction: 88.00053215026855  Train_KL: 3.5297616124153137  Validation Loss : 91.27222442626953 Val_Reconstruction : 87.79002380371094 Val_KL : 3.482200860977173\n","Epoch: 4989/8000  Traning Loss: 91.60257148742676  Train_Reconstruction: 88.08476161956787  Train_KL: 3.5178096294403076  Validation Loss : 91.2795524597168 Val_Reconstruction : 87.8034896850586 Val_KL : 3.4760653972625732\n","Epoch: 4990/8000  Traning Loss: 91.32785129547119  Train_Reconstruction: 87.81053638458252  Train_KL: 3.51731476187706  Validation Loss : 91.08976364135742 Val_Reconstruction : 87.5992431640625 Val_KL : 3.4905225038528442\n","Epoch: 4991/8000  Traning Loss: 91.05724048614502  Train_Reconstruction: 87.53020763397217  Train_KL: 3.527033120393753  Validation Loss : 90.87113952636719 Val_Reconstruction : 87.3885726928711 Val_KL : 3.482568621635437\n","Epoch: 4992/8000  Traning Loss: 91.09939765930176  Train_Reconstruction: 87.58037662506104  Train_KL: 3.5190227031707764  Validation Loss : 90.94994354248047 Val_Reconstruction : 87.47274017333984 Val_KL : 3.4772019386291504\n","Epoch: 4993/8000  Traning Loss: 91.33298015594482  Train_Reconstruction: 87.80913352966309  Train_KL: 3.523846834897995  Validation Loss : 91.2900390625 Val_Reconstruction : 87.81233215332031 Val_KL : 3.4777050018310547\n","Epoch: 4994/8000  Traning Loss: 91.20343399047852  Train_Reconstruction: 87.67557334899902  Train_KL: 3.52786123752594  Validation Loss : 91.24045944213867 Val_Reconstruction : 87.7459945678711 Val_KL : 3.4944651126861572\n","Epoch: 4995/8000  Traning Loss: 91.50603771209717  Train_Reconstruction: 87.97317218780518  Train_KL: 3.5328658521175385  Validation Loss : 91.51528930664062 Val_Reconstruction : 88.02484893798828 Val_KL : 3.4904435873031616\n","Epoch: 4996/8000  Traning Loss: 91.66510105133057  Train_Reconstruction: 88.14410209655762  Train_KL: 3.520999640226364  Validation Loss : 91.63306045532227 Val_Reconstruction : 88.15963745117188 Val_KL : 3.473421573638916\n","Epoch: 4997/8000  Traning Loss: 91.69166374206543  Train_Reconstruction: 88.18039226531982  Train_KL: 3.511271208524704  Validation Loss : 91.66598129272461 Val_Reconstruction : 88.19091796875 Val_KL : 3.475063443183899\n","Epoch: 4998/8000  Traning Loss: 91.55349159240723  Train_Reconstruction: 88.02319431304932  Train_KL: 3.5302981436252594  Validation Loss : 91.62418365478516 Val_Reconstruction : 88.12824630737305 Val_KL : 3.4959371089935303\n","Epoch: 4999/8000  Traning Loss: 91.45260906219482  Train_Reconstruction: 87.9193229675293  Train_KL: 3.5332854092121124  Validation Loss : 91.28606033325195 Val_Reconstruction : 87.7987174987793 Val_KL : 3.4873448610305786\n","Epoch: 5000/8000  Traning Loss: 91.17109394073486  Train_Reconstruction: 87.65201568603516  Train_KL: 3.519079089164734  Validation Loss : 91.22417831420898 Val_Reconstruction : 87.75452423095703 Val_KL : 3.469655156135559\n","Epoch: 5001/8000  Traning Loss: 91.39540481567383  Train_Reconstruction: 87.87969779968262  Train_KL: 3.5157070755958557  Validation Loss : 91.61941909790039 Val_Reconstruction : 88.13843154907227 Val_KL : 3.4809874296188354\n","Epoch: 5002/8000  Traning Loss: 91.43161201477051  Train_Reconstruction: 87.90647411346436  Train_KL: 3.525137722492218  Validation Loss : 91.14314651489258 Val_Reconstruction : 87.66176223754883 Val_KL : 3.481384515762329\n","Epoch: 5003/8000  Traning Loss: 91.36943531036377  Train_Reconstruction: 87.85184383392334  Train_KL: 3.517590582370758  Validation Loss : 91.93151473999023 Val_Reconstruction : 88.45866012573242 Val_KL : 3.472854495048523\n","Epoch: 5004/8000  Traning Loss: 91.50283336639404  Train_Reconstruction: 87.987868309021  Train_KL: 3.514964997768402  Validation Loss : 91.35795211791992 Val_Reconstruction : 87.8785400390625 Val_KL : 3.4794139862060547\n","Epoch: 5005/8000  Traning Loss: 91.32470035552979  Train_Reconstruction: 87.80339813232422  Train_KL: 3.5213030576705933  Validation Loss : 91.36732864379883 Val_Reconstruction : 87.88112258911133 Val_KL : 3.4862060546875\n","Epoch: 5006/8000  Traning Loss: 91.69636726379395  Train_Reconstruction: 88.17443656921387  Train_KL: 3.5219293236732483  Validation Loss : 92.11191177368164 Val_Reconstruction : 88.62975692749023 Val_KL : 3.4821555614471436\n","Epoch: 5007/8000  Traning Loss: 91.55179405212402  Train_Reconstruction: 88.03109169006348  Train_KL: 3.520701229572296  Validation Loss : 91.28114700317383 Val_Reconstruction : 87.79159545898438 Val_KL : 3.489551305770874\n","Epoch: 5008/8000  Traning Loss: 91.5453405380249  Train_Reconstruction: 88.01894283294678  Train_KL: 3.5263980627059937  Validation Loss : 91.74097442626953 Val_Reconstruction : 88.25757217407227 Val_KL : 3.483402729034424\n","Epoch: 5009/8000  Traning Loss: 91.8185510635376  Train_Reconstruction: 88.29787158966064  Train_KL: 3.520679861307144  Validation Loss : 91.46073532104492 Val_Reconstruction : 87.97688293457031 Val_KL : 3.483850121498108\n","Epoch: 5010/8000  Traning Loss: 91.5097427368164  Train_Reconstruction: 87.9802360534668  Train_KL: 3.5295069813728333  Validation Loss : 91.24255752563477 Val_Reconstruction : 87.75546264648438 Val_KL : 3.487091302871704\n","Epoch: 5011/8000  Traning Loss: 91.44408893585205  Train_Reconstruction: 87.92671203613281  Train_KL: 3.5173772275447845  Validation Loss : 91.57533645629883 Val_Reconstruction : 88.10402297973633 Val_KL : 3.471315026283264\n","Epoch: 5012/8000  Traning Loss: 91.29467105865479  Train_Reconstruction: 87.78699207305908  Train_KL: 3.5076777040958405  Validation Loss : 90.99797821044922 Val_Reconstruction : 87.52218627929688 Val_KL : 3.475792407989502\n","Epoch: 5013/8000  Traning Loss: 91.11731624603271  Train_Reconstruction: 87.59984874725342  Train_KL: 3.5174673795700073  Validation Loss : 91.11011505126953 Val_Reconstruction : 87.63570785522461 Val_KL : 3.4744083881378174\n","Epoch: 5014/8000  Traning Loss: 91.04648876190186  Train_Reconstruction: 87.52456283569336  Train_KL: 3.5219269394874573  Validation Loss : 91.1463508605957 Val_Reconstruction : 87.66408920288086 Val_KL : 3.4822601079940796\n","Epoch: 5015/8000  Traning Loss: 91.25194263458252  Train_Reconstruction: 87.72373008728027  Train_KL: 3.5282125174999237  Validation Loss : 91.51572036743164 Val_Reconstruction : 88.03644943237305 Val_KL : 3.4792720079421997\n","Epoch: 5016/8000  Traning Loss: 91.54218673706055  Train_Reconstruction: 88.02370834350586  Train_KL: 3.518477827310562  Validation Loss : 91.36570358276367 Val_Reconstruction : 87.90077209472656 Val_KL : 3.4649308919906616\n","Epoch: 5017/8000  Traning Loss: 91.57157897949219  Train_Reconstruction: 88.05522441864014  Train_KL: 3.5163544714450836  Validation Loss : 91.53797912597656 Val_Reconstruction : 88.0583381652832 Val_KL : 3.4796392917633057\n","Epoch: 5018/8000  Traning Loss: 91.7130355834961  Train_Reconstruction: 88.18166160583496  Train_KL: 3.5313746631145477  Validation Loss : 91.61834335327148 Val_Reconstruction : 88.1216926574707 Val_KL : 3.49665105342865\n","Epoch: 5019/8000  Traning Loss: 91.37356948852539  Train_Reconstruction: 87.85253620147705  Train_KL: 3.5210337936878204  Validation Loss : 91.23322296142578 Val_Reconstruction : 87.76579666137695 Val_KL : 3.4674293994903564\n","Epoch: 5020/8000  Traning Loss: 91.40923404693604  Train_Reconstruction: 87.90192413330078  Train_KL: 3.5073104798793793  Validation Loss : 91.34292602539062 Val_Reconstruction : 87.87272644042969 Val_KL : 3.4702024459838867\n","Epoch: 5021/8000  Traning Loss: 91.33220100402832  Train_Reconstruction: 87.81100273132324  Train_KL: 3.521198093891144  Validation Loss : 91.2346305847168 Val_Reconstruction : 87.7470703125 Val_KL : 3.4875625371932983\n","Epoch: 5022/8000  Traning Loss: 91.3486738204956  Train_Reconstruction: 87.82524299621582  Train_KL: 3.5234311521053314  Validation Loss : 91.3011703491211 Val_Reconstruction : 87.81167602539062 Val_KL : 3.4894951581954956\n","Epoch: 5023/8000  Traning Loss: 91.82640743255615  Train_Reconstruction: 88.30025577545166  Train_KL: 3.526151567697525  Validation Loss : 91.8290023803711 Val_Reconstruction : 88.34440612792969 Val_KL : 3.484597325325012\n","Epoch: 5024/8000  Traning Loss: 92.1049337387085  Train_Reconstruction: 88.58530712127686  Train_KL: 3.5196273624897003  Validation Loss : 92.11725616455078 Val_Reconstruction : 88.63733291625977 Val_KL : 3.4799212217330933\n","Epoch: 5025/8000  Traning Loss: 91.93215847015381  Train_Reconstruction: 88.42039203643799  Train_KL: 3.5117682218551636  Validation Loss : 92.52947235107422 Val_Reconstruction : 89.0557861328125 Val_KL : 3.473684072494507\n","Epoch: 5026/8000  Traning Loss: 92.13123035430908  Train_Reconstruction: 88.61767196655273  Train_KL: 3.5135589241981506  Validation Loss : 91.89385986328125 Val_Reconstruction : 88.41674041748047 Val_KL : 3.4771182537078857\n","Epoch: 5027/8000  Traning Loss: 91.99036598205566  Train_Reconstruction: 88.46862030029297  Train_KL: 3.5217457115650177  Validation Loss : 92.24903869628906 Val_Reconstruction : 88.76017379760742 Val_KL : 3.488864541053772\n","Epoch: 5028/8000  Traning Loss: 91.88865375518799  Train_Reconstruction: 88.36585998535156  Train_KL: 3.5227940380573273  Validation Loss : 91.5821533203125 Val_Reconstruction : 88.1054458618164 Val_KL : 3.476707935333252\n","Epoch: 5029/8000  Traning Loss: 91.7823257446289  Train_Reconstruction: 88.27056503295898  Train_KL: 3.5117612779140472  Validation Loss : 91.61532211303711 Val_Reconstruction : 88.14667892456055 Val_KL : 3.4686434268951416\n","Epoch: 5030/8000  Traning Loss: 92.0945291519165  Train_Reconstruction: 88.58455562591553  Train_KL: 3.509972780942917  Validation Loss : 91.68492126464844 Val_Reconstruction : 88.2118148803711 Val_KL : 3.473109006881714\n","Epoch: 5031/8000  Traning Loss: 91.61648845672607  Train_Reconstruction: 88.09492874145508  Train_KL: 3.5215592682361603  Validation Loss : 91.38208389282227 Val_Reconstruction : 87.89971160888672 Val_KL : 3.482372283935547\n","Epoch: 5032/8000  Traning Loss: 91.81966876983643  Train_Reconstruction: 88.30280876159668  Train_KL: 3.516859620809555  Validation Loss : 91.64957809448242 Val_Reconstruction : 88.17001342773438 Val_KL : 3.4795639514923096\n","Epoch: 5033/8000  Traning Loss: 91.96579551696777  Train_Reconstruction: 88.43917751312256  Train_KL: 3.5266171395778656  Validation Loss : 91.67541122436523 Val_Reconstruction : 88.18707656860352 Val_KL : 3.4883363246917725\n","Epoch: 5034/8000  Traning Loss: 91.67901420593262  Train_Reconstruction: 88.16162967681885  Train_KL: 3.5173840522766113  Validation Loss : 91.27469253540039 Val_Reconstruction : 87.80291366577148 Val_KL : 3.471779704093933\n","Epoch: 5035/8000  Traning Loss: 91.6288070678711  Train_Reconstruction: 88.10885238647461  Train_KL: 3.5199547111988068  Validation Loss : 91.7446517944336 Val_Reconstruction : 88.26359558105469 Val_KL : 3.4810580015182495\n","Epoch: 5036/8000  Traning Loss: 91.66113185882568  Train_Reconstruction: 88.1289119720459  Train_KL: 3.53221994638443  Validation Loss : 91.45367431640625 Val_Reconstruction : 87.96060943603516 Val_KL : 3.4930648803710938\n","Epoch: 5037/8000  Traning Loss: 91.30622959136963  Train_Reconstruction: 87.78270149230957  Train_KL: 3.523529201745987  Validation Loss : 91.27859878540039 Val_Reconstruction : 87.80615234375 Val_KL : 3.4724477529525757\n","Epoch: 5038/8000  Traning Loss: 91.58591747283936  Train_Reconstruction: 88.07685375213623  Train_KL: 3.5090653598308563  Validation Loss : 91.40962219238281 Val_Reconstruction : 87.93753433227539 Val_KL : 3.4720886945724487\n","Epoch: 5039/8000  Traning Loss: 91.17404556274414  Train_Reconstruction: 87.6540060043335  Train_KL: 3.5200394988059998  Validation Loss : 90.79541397094727 Val_Reconstruction : 87.31587600708008 Val_KL : 3.4795360565185547\n","Epoch: 5040/8000  Traning Loss: 91.2724666595459  Train_Reconstruction: 87.75444412231445  Train_KL: 3.518020838499069  Validation Loss : 91.296875 Val_Reconstruction : 87.82805633544922 Val_KL : 3.468817114830017\n","Epoch: 5041/8000  Traning Loss: 91.46854209899902  Train_Reconstruction: 87.95962715148926  Train_KL: 3.5089144706726074  Validation Loss : 91.17454528808594 Val_Reconstruction : 87.70656204223633 Val_KL : 3.467985153198242\n","Epoch: 5042/8000  Traning Loss: 91.57993698120117  Train_Reconstruction: 88.0564546585083  Train_KL: 3.5234822630882263  Validation Loss : 91.4439582824707 Val_Reconstruction : 87.96377182006836 Val_KL : 3.4801872968673706\n","Epoch: 5043/8000  Traning Loss: 91.64648151397705  Train_Reconstruction: 88.1253252029419  Train_KL: 3.521156817674637  Validation Loss : 91.59073257446289 Val_Reconstruction : 88.11406326293945 Val_KL : 3.4766719341278076\n","Epoch: 5044/8000  Traning Loss: 91.55643939971924  Train_Reconstruction: 88.04339408874512  Train_KL: 3.5130452513694763  Validation Loss : 91.27655029296875 Val_Reconstruction : 87.80356216430664 Val_KL : 3.472987651824951\n","Epoch: 5045/8000  Traning Loss: 91.25727653503418  Train_Reconstruction: 87.73606586456299  Train_KL: 3.521208107471466  Validation Loss : 90.70269775390625 Val_Reconstruction : 87.21709823608398 Val_KL : 3.4855997562408447\n","Epoch: 5046/8000  Traning Loss: 90.9426212310791  Train_Reconstruction: 87.41656398773193  Train_KL: 3.5260567367076874  Validation Loss : 90.87939834594727 Val_Reconstruction : 87.39151382446289 Val_KL : 3.4878835678100586\n","Epoch: 5047/8000  Traning Loss: 91.04644393920898  Train_Reconstruction: 87.51848411560059  Train_KL: 3.527960777282715  Validation Loss : 91.19025421142578 Val_Reconstruction : 87.70528411865234 Val_KL : 3.484967827796936\n","Epoch: 5048/8000  Traning Loss: 91.19563961029053  Train_Reconstruction: 87.67052268981934  Train_KL: 3.5251172184944153  Validation Loss : 91.09851455688477 Val_Reconstruction : 87.61481094360352 Val_KL : 3.4837042093276978\n","Epoch: 5049/8000  Traning Loss: 91.27295017242432  Train_Reconstruction: 87.74836921691895  Train_KL: 3.524580717086792  Validation Loss : 91.21177673339844 Val_Reconstruction : 87.74365997314453 Val_KL : 3.4681164026260376\n","Epoch: 5050/8000  Traning Loss: 91.13306522369385  Train_Reconstruction: 87.6179666519165  Train_KL: 3.515099138021469  Validation Loss : 90.92691802978516 Val_Reconstruction : 87.45420837402344 Val_KL : 3.472707152366638\n","Epoch: 5051/8000  Traning Loss: 91.57481384277344  Train_Reconstruction: 88.04972743988037  Train_KL: 3.525086373090744  Validation Loss : 91.54037094116211 Val_Reconstruction : 88.05899810791016 Val_KL : 3.4813737869262695\n","Epoch: 5052/8000  Traning Loss: 92.07557582855225  Train_Reconstruction: 88.55564403533936  Train_KL: 3.5199313163757324  Validation Loss : 91.81690979003906 Val_Reconstruction : 88.3443603515625 Val_KL : 3.472550630569458\n","Epoch: 5053/8000  Traning Loss: 91.32886505126953  Train_Reconstruction: 87.8090648651123  Train_KL: 3.519800364971161  Validation Loss : 91.12477111816406 Val_Reconstruction : 87.63723754882812 Val_KL : 3.487533211708069\n","Epoch: 5054/8000  Traning Loss: 91.49082565307617  Train_Reconstruction: 87.95987033843994  Train_KL: 3.530956506729126  Validation Loss : 91.32903671264648 Val_Reconstruction : 87.84027862548828 Val_KL : 3.4887595176696777\n","Epoch: 5055/8000  Traning Loss: 91.4691972732544  Train_Reconstruction: 87.94268226623535  Train_KL: 3.5265150666236877  Validation Loss : 91.1341323852539 Val_Reconstruction : 87.66057586669922 Val_KL : 3.4735569953918457\n","Epoch: 5056/8000  Traning Loss: 91.3989372253418  Train_Reconstruction: 87.89205265045166  Train_KL: 3.5068851113319397  Validation Loss : 91.25030136108398 Val_Reconstruction : 87.78311538696289 Val_KL : 3.4671854972839355\n","Epoch: 5057/8000  Traning Loss: 91.7061882019043  Train_Reconstruction: 88.19414234161377  Train_KL: 3.512047052383423  Validation Loss : 91.81239700317383 Val_Reconstruction : 88.34369277954102 Val_KL : 3.4687050580978394\n","Epoch: 5058/8000  Traning Loss: 91.96331405639648  Train_Reconstruction: 88.44909572601318  Train_KL: 3.514217048883438  Validation Loss : 91.73175048828125 Val_Reconstruction : 88.26322555541992 Val_KL : 3.4685226678848267\n","Epoch: 5059/8000  Traning Loss: 92.37586975097656  Train_Reconstruction: 88.86593914031982  Train_KL: 3.509929269552231  Validation Loss : 92.61568832397461 Val_Reconstruction : 89.1408805847168 Val_KL : 3.474809408187866\n","Epoch: 5060/8000  Traning Loss: 92.9127779006958  Train_Reconstruction: 89.40104579925537  Train_KL: 3.5117321014404297  Validation Loss : 93.44648361206055 Val_Reconstruction : 89.98097610473633 Val_KL : 3.465506672859192\n","Epoch: 5061/8000  Traning Loss: 92.63218307495117  Train_Reconstruction: 89.11132049560547  Train_KL: 3.520861566066742  Validation Loss : 91.8349609375 Val_Reconstruction : 88.34639358520508 Val_KL : 3.488566279411316\n","Epoch: 5062/8000  Traning Loss: 92.0878210067749  Train_Reconstruction: 88.55446147918701  Train_KL: 3.53336164355278  Validation Loss : 91.6843376159668 Val_Reconstruction : 88.1882095336914 Val_KL : 3.4961293935775757\n","Epoch: 5063/8000  Traning Loss: 91.68326091766357  Train_Reconstruction: 88.15911674499512  Train_KL: 3.524143695831299  Validation Loss : 91.34584045410156 Val_Reconstruction : 87.87235260009766 Val_KL : 3.4734898805618286\n","Epoch: 5064/8000  Traning Loss: 91.44693088531494  Train_Reconstruction: 87.93295574188232  Train_KL: 3.5139759480953217  Validation Loss : 91.25073623657227 Val_Reconstruction : 87.77884674072266 Val_KL : 3.4718892574310303\n","Epoch: 5065/8000  Traning Loss: 91.15460395812988  Train_Reconstruction: 87.63305473327637  Train_KL: 3.5215485095977783  Validation Loss : 90.73551940917969 Val_Reconstruction : 87.25385665893555 Val_KL : 3.4816625118255615\n","Epoch: 5066/8000  Traning Loss: 91.03795433044434  Train_Reconstruction: 87.51595973968506  Train_KL: 3.5219934284687042  Validation Loss : 90.7641487121582 Val_Reconstruction : 87.2796516418457 Val_KL : 3.484498381614685\n","Epoch: 5067/8000  Traning Loss: 91.0671501159668  Train_Reconstruction: 87.5440444946289  Train_KL: 3.5231058299541473  Validation Loss : 91.01754379272461 Val_Reconstruction : 87.53998565673828 Val_KL : 3.4775586128234863\n","Epoch: 5068/8000  Traning Loss: 90.9324426651001  Train_Reconstruction: 87.41125106811523  Train_KL: 3.521192878484726  Validation Loss : 90.88116073608398 Val_Reconstruction : 87.39458465576172 Val_KL : 3.486576795578003\n","Epoch: 5069/8000  Traning Loss: 91.09610557556152  Train_Reconstruction: 87.56652355194092  Train_KL: 3.529581695795059  Validation Loss : 90.99793243408203 Val_Reconstruction : 87.5140495300293 Val_KL : 3.483882188796997\n","Epoch: 5070/8000  Traning Loss: 91.43106842041016  Train_Reconstruction: 87.90400314331055  Train_KL: 3.5270641446113586  Validation Loss : 91.3159408569336 Val_Reconstruction : 87.82242965698242 Val_KL : 3.4935102462768555\n","Epoch: 5071/8000  Traning Loss: 91.57883358001709  Train_Reconstruction: 88.05495262145996  Train_KL: 3.5238817632198334  Validation Loss : 91.56585311889648 Val_Reconstruction : 88.0918197631836 Val_KL : 3.4740312099456787\n","Epoch: 5072/8000  Traning Loss: 91.64013671875  Train_Reconstruction: 88.12932777404785  Train_KL: 3.5108093321323395  Validation Loss : 91.14042663574219 Val_Reconstruction : 87.66707992553711 Val_KL : 3.4733468294143677\n","Epoch: 5073/8000  Traning Loss: 91.63872337341309  Train_Reconstruction: 88.10661697387695  Train_KL: 3.532106935977936  Validation Loss : 91.8501968383789 Val_Reconstruction : 88.34846115112305 Val_KL : 3.5017343759536743\n","Epoch: 5074/8000  Traning Loss: 92.2073917388916  Train_Reconstruction: 88.66732025146484  Train_KL: 3.540072113275528  Validation Loss : 92.54719161987305 Val_Reconstruction : 89.06413650512695 Val_KL : 3.483054280281067\n","Epoch: 5075/8000  Traning Loss: 92.41251277923584  Train_Reconstruction: 88.90209007263184  Train_KL: 3.510423630475998  Validation Loss : 92.11467361450195 Val_Reconstruction : 88.65067672729492 Val_KL : 3.463997960090637\n","Epoch: 5076/8000  Traning Loss: 92.28828811645508  Train_Reconstruction: 88.76628017425537  Train_KL: 3.5220080614089966  Validation Loss : 91.69023513793945 Val_Reconstruction : 88.2035903930664 Val_KL : 3.486644983291626\n","Epoch: 5077/8000  Traning Loss: 91.48049926757812  Train_Reconstruction: 87.96035575866699  Train_KL: 3.5201434791088104  Validation Loss : 91.20762252807617 Val_Reconstruction : 87.73322296142578 Val_KL : 3.474398374557495\n","Epoch: 5078/8000  Traning Loss: 91.17268943786621  Train_Reconstruction: 87.65193367004395  Train_KL: 3.520754873752594  Validation Loss : 90.83238983154297 Val_Reconstruction : 87.34809875488281 Val_KL : 3.484290838241577\n","Epoch: 5079/8000  Traning Loss: 91.32558631896973  Train_Reconstruction: 87.7994384765625  Train_KL: 3.5261481404304504  Validation Loss : 91.18945693969727 Val_Reconstruction : 87.70395278930664 Val_KL : 3.4855008125305176\n","Epoch: 5080/8000  Traning Loss: 91.53068542480469  Train_Reconstruction: 87.9955244064331  Train_KL: 3.5351613759994507  Validation Loss : 91.67086029052734 Val_Reconstruction : 88.17540740966797 Val_KL : 3.495453953742981\n","Epoch: 5081/8000  Traning Loss: 91.6108627319336  Train_Reconstruction: 88.07659721374512  Train_KL: 3.534265875816345  Validation Loss : 91.3030776977539 Val_Reconstruction : 87.81693649291992 Val_KL : 3.4861433506011963\n","Epoch: 5082/8000  Traning Loss: 91.2446060180664  Train_Reconstruction: 87.72092819213867  Train_KL: 3.5236782133579254  Validation Loss : 91.08672714233398 Val_Reconstruction : 87.60833740234375 Val_KL : 3.478390097618103\n","Epoch: 5083/8000  Traning Loss: 91.56469249725342  Train_Reconstruction: 88.04224491119385  Train_KL: 3.5224488973617554  Validation Loss : 91.76232147216797 Val_Reconstruction : 88.27637481689453 Val_KL : 3.4859447479248047\n","Epoch: 5084/8000  Traning Loss: 91.67318630218506  Train_Reconstruction: 88.14827728271484  Train_KL: 3.524909108877182  Validation Loss : 91.09462356567383 Val_Reconstruction : 87.61366271972656 Val_KL : 3.4809614419937134\n","Epoch: 5085/8000  Traning Loss: 91.21592044830322  Train_Reconstruction: 87.70706176757812  Train_KL: 3.5088594257831573  Validation Loss : 91.27688598632812 Val_Reconstruction : 87.80968856811523 Val_KL : 3.4671982526779175\n","Epoch: 5086/8000  Traning Loss: 91.13601016998291  Train_Reconstruction: 87.619065284729  Train_KL: 3.5169449150562286  Validation Loss : 91.04541015625 Val_Reconstruction : 87.56588745117188 Val_KL : 3.479522943496704\n","Epoch: 5087/8000  Traning Loss: 91.43364429473877  Train_Reconstruction: 87.91275787353516  Train_KL: 3.5208877325057983  Validation Loss : 91.8289680480957 Val_Reconstruction : 88.35502243041992 Val_KL : 3.4739471673965454\n","Epoch: 5088/8000  Traning Loss: 91.58193683624268  Train_Reconstruction: 88.06566619873047  Train_KL: 3.516270697116852  Validation Loss : 91.62646865844727 Val_Reconstruction : 88.1399154663086 Val_KL : 3.48655366897583\n","Epoch: 5089/8000  Traning Loss: 91.84316349029541  Train_Reconstruction: 88.31425094604492  Train_KL: 3.5289124846458435  Validation Loss : 91.94868087768555 Val_Reconstruction : 88.46524047851562 Val_KL : 3.483438730239868\n","Epoch: 5090/8000  Traning Loss: 91.87128448486328  Train_Reconstruction: 88.35704898834229  Train_KL: 3.514235734939575  Validation Loss : 91.67427062988281 Val_Reconstruction : 88.19926071166992 Val_KL : 3.475010395050049\n","Epoch: 5091/8000  Traning Loss: 91.53975677490234  Train_Reconstruction: 88.01866626739502  Train_KL: 3.5210909843444824  Validation Loss : 91.64939880371094 Val_Reconstruction : 88.16460037231445 Val_KL : 3.4847981929779053\n","Epoch: 5092/8000  Traning Loss: 91.71745872497559  Train_Reconstruction: 88.1936388015747  Train_KL: 3.523819148540497  Validation Loss : 91.69794464111328 Val_Reconstruction : 88.21879196166992 Val_KL : 3.479155421257019\n","Epoch: 5093/8000  Traning Loss: 91.37643051147461  Train_Reconstruction: 87.85173988342285  Train_KL: 3.5246913731098175  Validation Loss : 90.9455795288086 Val_Reconstruction : 87.46903991699219 Val_KL : 3.4765390157699585\n","Epoch: 5094/8000  Traning Loss: 91.24668407440186  Train_Reconstruction: 87.72735214233398  Train_KL: 3.5193320214748383  Validation Loss : 91.4296646118164 Val_Reconstruction : 87.9462661743164 Val_KL : 3.4833984375\n","Epoch: 5095/8000  Traning Loss: 91.96733379364014  Train_Reconstruction: 88.43898487091064  Train_KL: 3.5283496379852295  Validation Loss : 92.47821426391602 Val_Reconstruction : 88.99333572387695 Val_KL : 3.4848804473876953\n","Epoch: 5096/8000  Traning Loss: 92.62135314941406  Train_Reconstruction: 89.09994220733643  Train_KL: 3.521410256624222  Validation Loss : 92.70534133911133 Val_Reconstruction : 89.23163986206055 Val_KL : 3.473702549934387\n","Epoch: 5097/8000  Traning Loss: 92.48702144622803  Train_Reconstruction: 88.97399806976318  Train_KL: 3.513024687767029  Validation Loss : 92.39030838012695 Val_Reconstruction : 88.91787338256836 Val_KL : 3.4724345207214355\n","Epoch: 5098/8000  Traning Loss: 91.73882389068604  Train_Reconstruction: 88.22387504577637  Train_KL: 3.514947861433029  Validation Loss : 91.42225646972656 Val_Reconstruction : 87.94356918334961 Val_KL : 3.4786850214004517\n","Epoch: 5099/8000  Traning Loss: 91.4646110534668  Train_Reconstruction: 87.9386157989502  Train_KL: 3.525994747877121  Validation Loss : 91.08681869506836 Val_Reconstruction : 87.60273742675781 Val_KL : 3.4840822219848633\n","Epoch: 5100/8000  Traning Loss: 91.08432102203369  Train_Reconstruction: 87.5612268447876  Train_KL: 3.523094117641449  Validation Loss : 90.8529052734375 Val_Reconstruction : 87.37537384033203 Val_KL : 3.4775331020355225\n","Epoch: 5101/8000  Traning Loss: 90.95033740997314  Train_Reconstruction: 87.43356227874756  Train_KL: 3.5167751014232635  Validation Loss : 90.89462280273438 Val_Reconstruction : 87.41799926757812 Val_KL : 3.4766238927841187\n","Epoch: 5102/8000  Traning Loss: 91.21061611175537  Train_Reconstruction: 87.68638038635254  Train_KL: 3.5242347717285156  Validation Loss : 91.57782363891602 Val_Reconstruction : 88.08981323242188 Val_KL : 3.488008499145508\n","Epoch: 5103/8000  Traning Loss: 91.99374961853027  Train_Reconstruction: 88.46621799468994  Train_KL: 3.527531772851944  Validation Loss : 92.00581741333008 Val_Reconstruction : 88.51739501953125 Val_KL : 3.4884226322174072\n","Epoch: 5104/8000  Traning Loss: 91.46228790283203  Train_Reconstruction: 87.94583034515381  Train_KL: 3.5164582431316376  Validation Loss : 91.11715698242188 Val_Reconstruction : 87.6492919921875 Val_KL : 3.4678627252578735\n","Epoch: 5105/8000  Traning Loss: 91.07014465332031  Train_Reconstruction: 87.5698471069336  Train_KL: 3.500298172235489  Validation Loss : 91.28751754760742 Val_Reconstruction : 87.8199348449707 Val_KL : 3.4675835371017456\n","Epoch: 5106/8000  Traning Loss: 91.56768131256104  Train_Reconstruction: 88.04296398162842  Train_KL: 3.524717003107071  Validation Loss : 91.62028884887695 Val_Reconstruction : 88.1270637512207 Val_KL : 3.4932256937026978\n","Epoch: 5107/8000  Traning Loss: 91.20797538757324  Train_Reconstruction: 87.67735385894775  Train_KL: 3.5306218564510345  Validation Loss : 91.07039642333984 Val_Reconstruction : 87.59099197387695 Val_KL : 3.47940731048584\n","Epoch: 5108/8000  Traning Loss: 91.28170394897461  Train_Reconstruction: 87.76558685302734  Train_KL: 3.516116589307785  Validation Loss : 91.38113784790039 Val_Reconstruction : 87.90752029418945 Val_KL : 3.4736162424087524\n","Epoch: 5109/8000  Traning Loss: 91.05256080627441  Train_Reconstruction: 87.52651882171631  Train_KL: 3.526042640209198  Validation Loss : 90.93590545654297 Val_Reconstruction : 87.44880676269531 Val_KL : 3.4870986938476562\n","Epoch: 5110/8000  Traning Loss: 91.21056842803955  Train_Reconstruction: 87.68435955047607  Train_KL: 3.526208996772766  Validation Loss : 91.1949348449707 Val_Reconstruction : 87.71379089355469 Val_KL : 3.481142997741699\n","Epoch: 5111/8000  Traning Loss: 91.2381591796875  Train_Reconstruction: 87.71541023254395  Train_KL: 3.522748500108719  Validation Loss : 91.13004302978516 Val_Reconstruction : 87.65057373046875 Val_KL : 3.479468822479248\n","Epoch: 5112/8000  Traning Loss: 90.95441722869873  Train_Reconstruction: 87.4258623123169  Train_KL: 3.5285546481609344  Validation Loss : 90.90058517456055 Val_Reconstruction : 87.40839767456055 Val_KL : 3.492188811302185\n","Epoch: 5113/8000  Traning Loss: 91.33491611480713  Train_Reconstruction: 87.80742168426514  Train_KL: 3.527493476867676  Validation Loss : 91.72618103027344 Val_Reconstruction : 88.24788284301758 Val_KL : 3.4782971143722534\n","Epoch: 5114/8000  Traning Loss: 91.25819683074951  Train_Reconstruction: 87.74102020263672  Train_KL: 3.517177939414978  Validation Loss : 91.26690673828125 Val_Reconstruction : 87.7904052734375 Val_KL : 3.4764994382858276\n","Epoch: 5115/8000  Traning Loss: 91.1593656539917  Train_Reconstruction: 87.63370704650879  Train_KL: 3.5256578624248505  Validation Loss : 91.04761123657227 Val_Reconstruction : 87.55424118041992 Val_KL : 3.4933717250823975\n","Epoch: 5116/8000  Traning Loss: 91.30250835418701  Train_Reconstruction: 87.76831817626953  Train_KL: 3.5341904163360596  Validation Loss : 91.17609786987305 Val_Reconstruction : 87.68822860717773 Val_KL : 3.487870454788208\n","Epoch: 5117/8000  Traning Loss: 91.30765056610107  Train_Reconstruction: 87.79198837280273  Train_KL: 3.515661835670471  Validation Loss : 91.01391983032227 Val_Reconstruction : 87.54631423950195 Val_KL : 3.4676029682159424\n","Epoch: 5118/8000  Traning Loss: 91.18905448913574  Train_Reconstruction: 87.66956043243408  Train_KL: 3.5194950699806213  Validation Loss : 91.09463882446289 Val_Reconstruction : 87.61526489257812 Val_KL : 3.4793750047683716\n","Epoch: 5119/8000  Traning Loss: 91.15747356414795  Train_Reconstruction: 87.64247035980225  Train_KL: 3.515002727508545  Validation Loss : 90.9172477722168 Val_Reconstruction : 87.4412612915039 Val_KL : 3.475985884666443\n","Epoch: 5120/8000  Traning Loss: 91.1093168258667  Train_Reconstruction: 87.59431171417236  Train_KL: 3.5150057673454285  Validation Loss : 90.89395141601562 Val_Reconstruction : 87.41743469238281 Val_KL : 3.476515293121338\n","Epoch: 5121/8000  Traning Loss: 91.29039859771729  Train_Reconstruction: 87.76860523223877  Train_KL: 3.521792322397232  Validation Loss : 91.10437774658203 Val_Reconstruction : 87.61601638793945 Val_KL : 3.488362669944763\n","Epoch: 5122/8000  Traning Loss: 91.70199012756348  Train_Reconstruction: 88.17405891418457  Train_KL: 3.5279321670532227  Validation Loss : 92.08960342407227 Val_Reconstruction : 88.60358047485352 Val_KL : 3.4860247373580933\n","Epoch: 5123/8000  Traning Loss: 91.98530578613281  Train_Reconstruction: 88.4672794342041  Train_KL: 3.5180261731147766  Validation Loss : 91.62462615966797 Val_Reconstruction : 88.15132522583008 Val_KL : 3.473299741744995\n","Epoch: 5124/8000  Traning Loss: 91.22721862792969  Train_Reconstruction: 87.70790004730225  Train_KL: 3.519317388534546  Validation Loss : 91.4347915649414 Val_Reconstruction : 87.95251083374023 Val_KL : 3.482279896736145\n","Epoch: 5125/8000  Traning Loss: 91.22984790802002  Train_Reconstruction: 87.70295715332031  Train_KL: 3.52688992023468  Validation Loss : 91.19105911254883 Val_Reconstruction : 87.70347595214844 Val_KL : 3.487584114074707\n","Epoch: 5126/8000  Traning Loss: 91.19348621368408  Train_Reconstruction: 87.66976547241211  Train_KL: 3.5237199664115906  Validation Loss : 91.21538925170898 Val_Reconstruction : 87.7361068725586 Val_KL : 3.479284882545471\n","Epoch: 5127/8000  Traning Loss: 90.9972448348999  Train_Reconstruction: 87.47644901275635  Train_KL: 3.5207960307598114  Validation Loss : 90.88445663452148 Val_Reconstruction : 87.40889739990234 Val_KL : 3.475558876991272\n","Epoch: 5128/8000  Traning Loss: 90.91063976287842  Train_Reconstruction: 87.38409423828125  Train_KL: 3.526544898748398  Validation Loss : 90.79977035522461 Val_Reconstruction : 87.3123550415039 Val_KL : 3.487414240837097\n","Epoch: 5129/8000  Traning Loss: 91.2877550125122  Train_Reconstruction: 87.76721572875977  Train_KL: 3.5205380022525787  Validation Loss : 91.71827697753906 Val_Reconstruction : 88.23642349243164 Val_KL : 3.481852650642395\n","Epoch: 5130/8000  Traning Loss: 91.81123638153076  Train_Reconstruction: 88.29463481903076  Train_KL: 3.5166019797325134  Validation Loss : 91.81546020507812 Val_Reconstruction : 88.33708953857422 Val_KL : 3.4783703088760376\n","Epoch: 5131/8000  Traning Loss: 92.04919528961182  Train_Reconstruction: 88.52955722808838  Train_KL: 3.5196370780467987  Validation Loss : 92.39867782592773 Val_Reconstruction : 88.9119758605957 Val_KL : 3.486704707145691\n","Epoch: 5132/8000  Traning Loss: 92.06821155548096  Train_Reconstruction: 88.55284023284912  Train_KL: 3.515370160341263  Validation Loss : 91.76145935058594 Val_Reconstruction : 88.2911491394043 Val_KL : 3.4703094959259033\n","Epoch: 5133/8000  Traning Loss: 91.63900184631348  Train_Reconstruction: 88.12273597717285  Train_KL: 3.5162667632102966  Validation Loss : 91.14303970336914 Val_Reconstruction : 87.65998840332031 Val_KL : 3.4830520153045654\n","Epoch: 5134/8000  Traning Loss: 91.3657579421997  Train_Reconstruction: 87.8444938659668  Train_KL: 3.5212648808956146  Validation Loss : 91.26327514648438 Val_Reconstruction : 87.78528594970703 Val_KL : 3.4779889583587646\n","Epoch: 5135/8000  Traning Loss: 91.43886184692383  Train_Reconstruction: 87.92086029052734  Train_KL: 3.518000543117523  Validation Loss : 91.28210830688477 Val_Reconstruction : 87.80453491210938 Val_KL : 3.477570652961731\n","Epoch: 5136/8000  Traning Loss: 91.65924167633057  Train_Reconstruction: 88.13685512542725  Train_KL: 3.5223866403102875  Validation Loss : 91.73402786254883 Val_Reconstruction : 88.2540283203125 Val_KL : 3.4800002574920654\n","Epoch: 5137/8000  Traning Loss: 91.69930171966553  Train_Reconstruction: 88.17987442016602  Train_KL: 3.519428789615631  Validation Loss : 91.51451110839844 Val_Reconstruction : 88.04021453857422 Val_KL : 3.474297285079956\n","Epoch: 5138/8000  Traning Loss: 91.44225883483887  Train_Reconstruction: 87.92030143737793  Train_KL: 3.521957963705063  Validation Loss : 91.27276611328125 Val_Reconstruction : 87.78623580932617 Val_KL : 3.486527919769287\n","Epoch: 5139/8000  Traning Loss: 91.02303504943848  Train_Reconstruction: 87.5068359375  Train_KL: 3.5161988735198975  Validation Loss : 91.1451416015625 Val_Reconstruction : 87.6724624633789 Val_KL : 3.4726765155792236\n","Epoch: 5140/8000  Traning Loss: 91.28203678131104  Train_Reconstruction: 87.75855827331543  Train_KL: 3.5234793424606323  Validation Loss : 91.36721420288086 Val_Reconstruction : 87.88102340698242 Val_KL : 3.486193537712097\n","Epoch: 5141/8000  Traning Loss: 91.8430233001709  Train_Reconstruction: 88.32169151306152  Train_KL: 3.5213319063186646  Validation Loss : 91.47036743164062 Val_Reconstruction : 87.9935531616211 Val_KL : 3.4768128395080566\n","Epoch: 5142/8000  Traning Loss: 91.30411338806152  Train_Reconstruction: 87.78930473327637  Train_KL: 3.514807552099228  Validation Loss : 91.52394485473633 Val_Reconstruction : 88.0514144897461 Val_KL : 3.4725306034088135\n","Epoch: 5143/8000  Traning Loss: 91.4763355255127  Train_Reconstruction: 87.95732879638672  Train_KL: 3.5190068781375885  Validation Loss : 91.34277725219727 Val_Reconstruction : 87.87395477294922 Val_KL : 3.468822717666626\n","Epoch: 5144/8000  Traning Loss: 92.23971557617188  Train_Reconstruction: 88.73523807525635  Train_KL: 3.5044763684272766  Validation Loss : 92.7145004272461 Val_Reconstruction : 89.25253677368164 Val_KL : 3.461964964866638\n","Epoch: 5145/8000  Traning Loss: 92.1516637802124  Train_Reconstruction: 88.63578033447266  Train_KL: 3.515882819890976  Validation Loss : 91.70686721801758 Val_Reconstruction : 88.22351455688477 Val_KL : 3.483351230621338\n","Epoch: 5146/8000  Traning Loss: 91.74503135681152  Train_Reconstruction: 88.21938037872314  Train_KL: 3.525651454925537  Validation Loss : 91.5754508972168 Val_Reconstruction : 88.08746719360352 Val_KL : 3.487984299659729\n","Epoch: 5147/8000  Traning Loss: 91.51180839538574  Train_Reconstruction: 87.98859119415283  Train_KL: 3.523218423128128  Validation Loss : 91.49288558959961 Val_Reconstruction : 88.01975631713867 Val_KL : 3.4731298685073853\n","Epoch: 5148/8000  Traning Loss: 91.65987014770508  Train_Reconstruction: 88.1527738571167  Train_KL: 3.5070964694023132  Validation Loss : 91.3652114868164 Val_Reconstruction : 87.89445877075195 Val_KL : 3.4707525968551636\n","Epoch: 5149/8000  Traning Loss: 91.45097827911377  Train_Reconstruction: 87.92994117736816  Train_KL: 3.52103790640831  Validation Loss : 91.18326950073242 Val_Reconstruction : 87.70450592041016 Val_KL : 3.478762149810791\n","Epoch: 5150/8000  Traning Loss: 91.26885032653809  Train_Reconstruction: 87.74274253845215  Train_KL: 3.526106983423233  Validation Loss : 90.91025924682617 Val_Reconstruction : 87.42760848999023 Val_KL : 3.48265278339386\n","Epoch: 5151/8000  Traning Loss: 91.07107067108154  Train_Reconstruction: 87.55839824676514  Train_KL: 3.5126725137233734  Validation Loss : 91.0009994506836 Val_Reconstruction : 87.53675079345703 Val_KL : 3.4642493724823\n","Epoch: 5152/8000  Traning Loss: 91.3964786529541  Train_Reconstruction: 87.88006973266602  Train_KL: 3.5164093673229218  Validation Loss : 91.18329620361328 Val_Reconstruction : 87.70190811157227 Val_KL : 3.4813891649246216\n","Epoch: 5153/8000  Traning Loss: 91.46524238586426  Train_Reconstruction: 87.931640625  Train_KL: 3.533601313829422  Validation Loss : 91.32315063476562 Val_Reconstruction : 87.83581924438477 Val_KL : 3.487331986427307\n","Epoch: 5154/8000  Traning Loss: 91.18501281738281  Train_Reconstruction: 87.66740131378174  Train_KL: 3.5176109075546265  Validation Loss : 91.04159927368164 Val_Reconstruction : 87.57900619506836 Val_KL : 3.4625916481018066\n","Epoch: 5155/8000  Traning Loss: 91.17861652374268  Train_Reconstruction: 87.67960834503174  Train_KL: 3.4990094006061554  Validation Loss : 90.93151092529297 Val_Reconstruction : 87.47290420532227 Val_KL : 3.4586068391799927\n","Epoch: 5156/8000  Traning Loss: 91.20284652709961  Train_Reconstruction: 87.68580913543701  Train_KL: 3.517037868499756  Validation Loss : 91.03057861328125 Val_Reconstruction : 87.54986572265625 Val_KL : 3.480712890625\n","Epoch: 5157/8000  Traning Loss: 91.03236675262451  Train_Reconstruction: 87.50114917755127  Train_KL: 3.531217247247696  Validation Loss : 91.1407356262207 Val_Reconstruction : 87.65092849731445 Val_KL : 3.4898085594177246\n","Epoch: 5158/8000  Traning Loss: 91.03447151184082  Train_Reconstruction: 87.5129451751709  Train_KL: 3.5215267539024353  Validation Loss : 90.7584228515625 Val_Reconstruction : 87.27716445922852 Val_KL : 3.481259822845459\n","Epoch: 5159/8000  Traning Loss: 91.43083763122559  Train_Reconstruction: 87.91019630432129  Train_KL: 3.5206401646137238  Validation Loss : 91.68773651123047 Val_Reconstruction : 88.2108268737793 Val_KL : 3.47691011428833\n","Epoch: 5160/8000  Traning Loss: 92.09138774871826  Train_Reconstruction: 88.57075214385986  Train_KL: 3.520636260509491  Validation Loss : 92.23102951049805 Val_Reconstruction : 88.75453186035156 Val_KL : 3.476497173309326\n","Epoch: 5161/8000  Traning Loss: 92.19818592071533  Train_Reconstruction: 88.6831226348877  Train_KL: 3.515062600374222  Validation Loss : 92.0478286743164 Val_Reconstruction : 88.57427215576172 Val_KL : 3.47355854511261\n","Epoch: 5162/8000  Traning Loss: 92.19155788421631  Train_Reconstruction: 88.67769432067871  Train_KL: 3.5138650238513947  Validation Loss : 91.76141357421875 Val_Reconstruction : 88.29248809814453 Val_KL : 3.4689252376556396\n","Epoch: 5163/8000  Traning Loss: 91.54944229125977  Train_Reconstruction: 88.04496192932129  Train_KL: 3.5044794380664825  Validation Loss : 91.4156265258789 Val_Reconstruction : 87.9502067565918 Val_KL : 3.4654171466827393\n","Epoch: 5164/8000  Traning Loss: 91.41718482971191  Train_Reconstruction: 87.90890216827393  Train_KL: 3.5082824528217316  Validation Loss : 91.25481414794922 Val_Reconstruction : 87.77862548828125 Val_KL : 3.476188063621521\n","Epoch: 5165/8000  Traning Loss: 91.46051979064941  Train_Reconstruction: 87.94645404815674  Train_KL: 3.5140653550624847  Validation Loss : 91.21096420288086 Val_Reconstruction : 87.73358154296875 Val_KL : 3.4773823022842407\n","Epoch: 5166/8000  Traning Loss: 91.27240467071533  Train_Reconstruction: 87.75342082977295  Train_KL: 3.5189843475818634  Validation Loss : 91.32859802246094 Val_Reconstruction : 87.84890365600586 Val_KL : 3.47969388961792\n","Epoch: 5167/8000  Traning Loss: 91.14329242706299  Train_Reconstruction: 87.6146936416626  Train_KL: 3.52859890460968  Validation Loss : 90.99026870727539 Val_Reconstruction : 87.50135040283203 Val_KL : 3.4889185428619385\n","Epoch: 5168/8000  Traning Loss: 91.07672309875488  Train_Reconstruction: 87.5504093170166  Train_KL: 3.526312977075577  Validation Loss : 90.84004211425781 Val_Reconstruction : 87.36557006835938 Val_KL : 3.474473714828491\n","Epoch: 5169/8000  Traning Loss: 91.22239685058594  Train_Reconstruction: 87.70350646972656  Train_KL: 3.518890768289566  Validation Loss : 91.85675048828125 Val_Reconstruction : 88.37586212158203 Val_KL : 3.4808895587921143\n","Epoch: 5170/8000  Traning Loss: 91.56846714019775  Train_Reconstruction: 88.0538854598999  Train_KL: 3.5145793855190277  Validation Loss : 91.39609146118164 Val_Reconstruction : 87.92753601074219 Val_KL : 3.468554377555847\n","Epoch: 5171/8000  Traning Loss: 91.39177703857422  Train_Reconstruction: 87.88399982452393  Train_KL: 3.5077778697013855  Validation Loss : 91.39920043945312 Val_Reconstruction : 87.92473220825195 Val_KL : 3.474468469619751\n","Epoch: 5172/8000  Traning Loss: 91.23429870605469  Train_Reconstruction: 87.70963096618652  Train_KL: 3.5246669948101044  Validation Loss : 91.34712600708008 Val_Reconstruction : 87.85875701904297 Val_KL : 3.4883687496185303\n","Epoch: 5173/8000  Traning Loss: 91.05131340026855  Train_Reconstruction: 87.52626514434814  Train_KL: 3.5250481367111206  Validation Loss : 91.14291763305664 Val_Reconstruction : 87.66371154785156 Val_KL : 3.479205846786499\n","Epoch: 5174/8000  Traning Loss: 91.18445301055908  Train_Reconstruction: 87.65434646606445  Train_KL: 3.5301058888435364  Validation Loss : 91.24469757080078 Val_Reconstruction : 87.7559928894043 Val_KL : 3.488703966140747\n","Epoch: 5175/8000  Traning Loss: 91.60199737548828  Train_Reconstruction: 88.07393169403076  Train_KL: 3.5280641317367554  Validation Loss : 91.69423294067383 Val_Reconstruction : 88.20906829833984 Val_KL : 3.485163450241089\n","Epoch: 5176/8000  Traning Loss: 91.67359447479248  Train_Reconstruction: 88.14420509338379  Train_KL: 3.529389441013336  Validation Loss : 91.60284805297852 Val_Reconstruction : 88.10568618774414 Val_KL : 3.497159719467163\n","Epoch: 5177/8000  Traning Loss: 91.44341659545898  Train_Reconstruction: 87.9206771850586  Train_KL: 3.5227399468421936  Validation Loss : 91.58210754394531 Val_Reconstruction : 88.10395431518555 Val_KL : 3.4781553745269775\n","Epoch: 5178/8000  Traning Loss: 91.23525333404541  Train_Reconstruction: 87.71665477752686  Train_KL: 3.5185986757278442  Validation Loss : 91.00346374511719 Val_Reconstruction : 87.52787780761719 Val_KL : 3.475582480430603\n","Epoch: 5179/8000  Traning Loss: 91.41940307617188  Train_Reconstruction: 87.9012041091919  Train_KL: 3.518198937177658  Validation Loss : 91.03826141357422 Val_Reconstruction : 87.5649528503418 Val_KL : 3.473305106163025\n","Epoch: 5180/8000  Traning Loss: 91.20056247711182  Train_Reconstruction: 87.68175506591797  Train_KL: 3.518807500600815  Validation Loss : 90.76854705810547 Val_Reconstruction : 87.29401779174805 Val_KL : 3.4745311737060547\n","Epoch: 5181/8000  Traning Loss: 90.93993186950684  Train_Reconstruction: 87.41938018798828  Train_KL: 3.5205511152744293  Validation Loss : 90.66873168945312 Val_Reconstruction : 87.19162368774414 Val_KL : 3.477108955383301\n","Epoch: 5182/8000  Traning Loss: 90.81816673278809  Train_Reconstruction: 87.2987871170044  Train_KL: 3.51937797665596  Validation Loss : 91.04420852661133 Val_Reconstruction : 87.57011032104492 Val_KL : 3.4740973711013794\n","Epoch: 5183/8000  Traning Loss: 91.031174659729  Train_Reconstruction: 87.51025676727295  Train_KL: 3.520917922258377  Validation Loss : 91.26108932495117 Val_Reconstruction : 87.78342819213867 Val_KL : 3.4776612520217896\n","Epoch: 5184/8000  Traning Loss: 91.17138004302979  Train_Reconstruction: 87.65389347076416  Train_KL: 3.5174868404865265  Validation Loss : 90.97502136230469 Val_Reconstruction : 87.49908065795898 Val_KL : 3.47594153881073\n","Epoch: 5185/8000  Traning Loss: 91.24346828460693  Train_Reconstruction: 87.71961688995361  Train_KL: 3.523851066827774  Validation Loss : 91.24536895751953 Val_Reconstruction : 87.7633171081543 Val_KL : 3.482050895690918\n","Epoch: 5186/8000  Traning Loss: 91.54928874969482  Train_Reconstruction: 88.03109931945801  Train_KL: 3.5181902647018433  Validation Loss : 91.42054748535156 Val_Reconstruction : 87.94432830810547 Val_KL : 3.4762206077575684\n","Epoch: 5187/8000  Traning Loss: 91.3949146270752  Train_Reconstruction: 87.87739276885986  Train_KL: 3.5175223648548126  Validation Loss : 91.21149063110352 Val_Reconstruction : 87.7315788269043 Val_KL : 3.4799118041992188\n","Epoch: 5188/8000  Traning Loss: 91.33584213256836  Train_Reconstruction: 87.81283187866211  Train_KL: 3.5230115354061127  Validation Loss : 91.19679260253906 Val_Reconstruction : 87.7115249633789 Val_KL : 3.485267400741577\n","Epoch: 5189/8000  Traning Loss: 91.17213249206543  Train_Reconstruction: 87.64600658416748  Train_KL: 3.526125282049179  Validation Loss : 91.19116592407227 Val_Reconstruction : 87.71426010131836 Val_KL : 3.476906657218933\n","Epoch: 5190/8000  Traning Loss: 91.5533504486084  Train_Reconstruction: 88.04338550567627  Train_KL: 3.509965240955353  Validation Loss : 91.82531356811523 Val_Reconstruction : 88.36064529418945 Val_KL : 3.4646672010421753\n","Epoch: 5191/8000  Traning Loss: 91.4822244644165  Train_Reconstruction: 87.96987247467041  Train_KL: 3.5123529732227325  Validation Loss : 91.36261367797852 Val_Reconstruction : 87.88005065917969 Val_KL : 3.4825607538223267\n","Epoch: 5192/8000  Traning Loss: 91.28019523620605  Train_Reconstruction: 87.75343132019043  Train_KL: 3.5267640948295593  Validation Loss : 91.06503295898438 Val_Reconstruction : 87.58024978637695 Val_KL : 3.4847817420959473\n","Epoch: 5193/8000  Traning Loss: 91.31702136993408  Train_Reconstruction: 87.79434204101562  Train_KL: 3.522679626941681  Validation Loss : 91.4694595336914 Val_Reconstruction : 87.99522018432617 Val_KL : 3.474239468574524\n","Epoch: 5194/8000  Traning Loss: 91.27293586730957  Train_Reconstruction: 87.77095127105713  Train_KL: 3.5019845068454742  Validation Loss : 91.42646408081055 Val_Reconstruction : 87.97262191772461 Val_KL : 3.4538387060165405\n","Epoch: 5195/8000  Traning Loss: 91.22525596618652  Train_Reconstruction: 87.70719718933105  Train_KL: 3.5180585980415344  Validation Loss : 90.78627014160156 Val_Reconstruction : 87.30301666259766 Val_KL : 3.483254551887512\n","Epoch: 5196/8000  Traning Loss: 91.01738262176514  Train_Reconstruction: 87.49839687347412  Train_KL: 3.5189857482910156  Validation Loss : 90.74052810668945 Val_Reconstruction : 87.27151107788086 Val_KL : 3.4690195322036743\n","Epoch: 5197/8000  Traning Loss: 90.92587947845459  Train_Reconstruction: 87.41853332519531  Train_KL: 3.5073449313640594  Validation Loss : 91.2357406616211 Val_Reconstruction : 87.76957702636719 Val_KL : 3.4661660194396973\n","Epoch: 5198/8000  Traning Loss: 91.19220352172852  Train_Reconstruction: 87.67818450927734  Train_KL: 3.5140192210674286  Validation Loss : 91.24666213989258 Val_Reconstruction : 87.77085494995117 Val_KL : 3.47580885887146\n","Epoch: 5199/8000  Traning Loss: 91.35692405700684  Train_Reconstruction: 87.84000396728516  Train_KL: 3.5169190764427185  Validation Loss : 91.27096557617188 Val_Reconstruction : 87.80398941040039 Val_KL : 3.4669764041900635\n","Epoch: 5200/8000  Traning Loss: 91.06924438476562  Train_Reconstruction: 87.55431175231934  Train_KL: 3.514932632446289  Validation Loss : 91.27415084838867 Val_Reconstruction : 87.79998779296875 Val_KL : 3.4741623401641846\n","Epoch: 5201/8000  Traning Loss: 91.33180618286133  Train_Reconstruction: 87.8225212097168  Train_KL: 3.5092852413654327  Validation Loss : 91.38436126708984 Val_Reconstruction : 87.92621612548828 Val_KL : 3.4581432342529297\n","Epoch: 5202/8000  Traning Loss: 91.51474475860596  Train_Reconstruction: 88.00297451019287  Train_KL: 3.511769711971283  Validation Loss : 91.50704956054688 Val_Reconstruction : 88.02183532714844 Val_KL : 3.4852126836776733\n","Epoch: 5203/8000  Traning Loss: 91.30167484283447  Train_Reconstruction: 87.77462768554688  Train_KL: 3.5270467698574066  Validation Loss : 91.19579696655273 Val_Reconstruction : 87.6989517211914 Val_KL : 3.4968446493148804\n","Epoch: 5204/8000  Traning Loss: 91.45238590240479  Train_Reconstruction: 87.9192247390747  Train_KL: 3.533161461353302  Validation Loss : 91.2045783996582 Val_Reconstruction : 87.71404647827148 Val_KL : 3.490530848503113\n","Epoch: 5205/8000  Traning Loss: 91.87086200714111  Train_Reconstruction: 88.34711742401123  Train_KL: 3.523743152618408  Validation Loss : 92.18608474731445 Val_Reconstruction : 88.70378112792969 Val_KL : 3.482302665710449\n","Epoch: 5206/8000  Traning Loss: 92.37345027923584  Train_Reconstruction: 88.84270000457764  Train_KL: 3.5307497084140778  Validation Loss : 92.0464973449707 Val_Reconstruction : 88.55714797973633 Val_KL : 3.4893505573272705\n","Epoch: 5207/8000  Traning Loss: 92.31784439086914  Train_Reconstruction: 88.78742408752441  Train_KL: 3.5304196774959564  Validation Loss : 92.04648208618164 Val_Reconstruction : 88.56359100341797 Val_KL : 3.482890009880066\n","Epoch: 5208/8000  Traning Loss: 92.3560266494751  Train_Reconstruction: 88.84304714202881  Train_KL: 3.5129792392253876  Validation Loss : 92.34465408325195 Val_Reconstruction : 88.87135696411133 Val_KL : 3.4732978343963623\n","Epoch: 5209/8000  Traning Loss: 92.08888721466064  Train_Reconstruction: 88.56901931762695  Train_KL: 3.5198671221733093  Validation Loss : 91.6545295715332 Val_Reconstruction : 88.17119598388672 Val_KL : 3.483333468437195\n","Epoch: 5210/8000  Traning Loss: 91.82801151275635  Train_Reconstruction: 88.30013847351074  Train_KL: 3.527871996164322  Validation Loss : 91.93137741088867 Val_Reconstruction : 88.44473648071289 Val_KL : 3.4866418838500977\n","Epoch: 5211/8000  Traning Loss: 91.53577995300293  Train_Reconstruction: 88.02438259124756  Train_KL: 3.511398196220398  Validation Loss : 91.25904846191406 Val_Reconstruction : 87.79400253295898 Val_KL : 3.465043783187866\n","Epoch: 5212/8000  Traning Loss: 91.16490459442139  Train_Reconstruction: 87.65699863433838  Train_KL: 3.507905811071396  Validation Loss : 91.10074234008789 Val_Reconstruction : 87.62277221679688 Val_KL : 3.477971315383911\n","Epoch: 5213/8000  Traning Loss: 91.58322429656982  Train_Reconstruction: 88.0635633468628  Train_KL: 3.5196606814861298  Validation Loss : 92.37337875366211 Val_Reconstruction : 88.88949966430664 Val_KL : 3.4838796854019165\n","Epoch: 5214/8000  Traning Loss: 91.79688739776611  Train_Reconstruction: 88.27255058288574  Train_KL: 3.5243380069732666  Validation Loss : 92.0947380065918 Val_Reconstruction : 88.60569381713867 Val_KL : 3.489042282104492\n","Epoch: 5215/8000  Traning Loss: 91.59221363067627  Train_Reconstruction: 88.06982231140137  Train_KL: 3.5223923325538635  Validation Loss : 91.30315017700195 Val_Reconstruction : 87.82373046875 Val_KL : 3.479421615600586\n","Epoch: 5216/8000  Traning Loss: 91.12946033477783  Train_Reconstruction: 87.60700035095215  Train_KL: 3.5224601328372955  Validation Loss : 90.80435943603516 Val_Reconstruction : 87.31775283813477 Val_KL : 3.486607074737549\n","Epoch: 5217/8000  Traning Loss: 91.17432975769043  Train_Reconstruction: 87.64609622955322  Train_KL: 3.5282330214977264  Validation Loss : 91.43675994873047 Val_Reconstruction : 87.95136642456055 Val_KL : 3.485395312309265\n","Epoch: 5218/8000  Traning Loss: 91.29844188690186  Train_Reconstruction: 87.78340721130371  Train_KL: 3.515033483505249  Validation Loss : 91.66457748413086 Val_Reconstruction : 88.19787979125977 Val_KL : 3.466696262359619\n","Epoch: 5219/8000  Traning Loss: 91.34411907196045  Train_Reconstruction: 87.83088493347168  Train_KL: 3.513234496116638  Validation Loss : 90.96947860717773 Val_Reconstruction : 87.49161911010742 Val_KL : 3.4778594970703125\n","Epoch: 5220/8000  Traning Loss: 90.94232559204102  Train_Reconstruction: 87.419921875  Train_KL: 3.5224045515060425  Validation Loss : 91.16204071044922 Val_Reconstruction : 87.6782341003418 Val_KL : 3.4838062524795532\n","Epoch: 5221/8000  Traning Loss: 90.99949645996094  Train_Reconstruction: 87.48566627502441  Train_KL: 3.513829380273819  Validation Loss : 91.1060905456543 Val_Reconstruction : 87.63436508178711 Val_KL : 3.471725106239319\n","Epoch: 5222/8000  Traning Loss: 91.15837860107422  Train_Reconstruction: 87.64239883422852  Train_KL: 3.515979379415512  Validation Loss : 91.22010040283203 Val_Reconstruction : 87.73676681518555 Val_KL : 3.4833343029022217\n","Epoch: 5223/8000  Traning Loss: 91.52213287353516  Train_Reconstruction: 88.00082302093506  Train_KL: 3.5213093161582947  Validation Loss : 91.83162689208984 Val_Reconstruction : 88.34899139404297 Val_KL : 3.4826347827911377\n","Epoch: 5224/8000  Traning Loss: 91.43723106384277  Train_Reconstruction: 87.92348098754883  Train_KL: 3.5137492418289185  Validation Loss : 90.96871566772461 Val_Reconstruction : 87.49746704101562 Val_KL : 3.4712467193603516\n","Epoch: 5225/8000  Traning Loss: 91.10463333129883  Train_Reconstruction: 87.58161735534668  Train_KL: 3.5230155885219574  Validation Loss : 90.986328125 Val_Reconstruction : 87.49271011352539 Val_KL : 3.4936193227767944\n","Epoch: 5226/8000  Traning Loss: 91.34350395202637  Train_Reconstruction: 87.81054973602295  Train_KL: 3.532954841852188  Validation Loss : 90.91574478149414 Val_Reconstruction : 87.423828125 Val_KL : 3.491914987564087\n","Epoch: 5227/8000  Traning Loss: 90.72499465942383  Train_Reconstruction: 87.20933818817139  Train_KL: 3.5156560838222504  Validation Loss : 90.78096389770508 Val_Reconstruction : 87.30881881713867 Val_KL : 3.4721431732177734\n","Epoch: 5228/8000  Traning Loss: 91.04512310028076  Train_Reconstruction: 87.52989387512207  Train_KL: 3.5152283906936646  Validation Loss : 91.12965393066406 Val_Reconstruction : 87.64028549194336 Val_KL : 3.4893680810928345\n","Epoch: 5229/8000  Traning Loss: 91.64159870147705  Train_Reconstruction: 88.11651802062988  Train_KL: 3.525079697370529  Validation Loss : 91.8299789428711 Val_Reconstruction : 88.34365844726562 Val_KL : 3.486318588256836\n","Epoch: 5230/8000  Traning Loss: 92.06307792663574  Train_Reconstruction: 88.54382610321045  Train_KL: 3.5192523896694183  Validation Loss : 92.37652969360352 Val_Reconstruction : 88.89874649047852 Val_KL : 3.477785348892212\n","Epoch: 5231/8000  Traning Loss: 91.77297496795654  Train_Reconstruction: 88.2557897567749  Train_KL: 3.5171848833560944  Validation Loss : 91.85689163208008 Val_Reconstruction : 88.37934875488281 Val_KL : 3.4775447845458984\n","Epoch: 5232/8000  Traning Loss: 91.29736804962158  Train_Reconstruction: 87.77478313446045  Train_KL: 3.5225848257541656  Validation Loss : 90.95270538330078 Val_Reconstruction : 87.47411727905273 Val_KL : 3.478588104248047\n","Epoch: 5233/8000  Traning Loss: 91.03430461883545  Train_Reconstruction: 87.5114860534668  Train_KL: 3.5228174924850464  Validation Loss : 91.06278991699219 Val_Reconstruction : 87.57979965209961 Val_KL : 3.482993006706238\n","Epoch: 5234/8000  Traning Loss: 91.21176815032959  Train_Reconstruction: 87.69137477874756  Train_KL: 3.5203921496868134  Validation Loss : 91.0512466430664 Val_Reconstruction : 87.57114791870117 Val_KL : 3.4800992012023926\n","Epoch: 5235/8000  Traning Loss: 91.02900981903076  Train_Reconstruction: 87.50410270690918  Train_KL: 3.5249066054821014  Validation Loss : 90.66002655029297 Val_Reconstruction : 87.17457962036133 Val_KL : 3.485447883605957\n","Epoch: 5236/8000  Traning Loss: 91.04129600524902  Train_Reconstruction: 87.51391887664795  Train_KL: 3.5273769199848175  Validation Loss : 91.1557388305664 Val_Reconstruction : 87.66460037231445 Val_KL : 3.491139531135559\n","Epoch: 5237/8000  Traning Loss: 91.33292865753174  Train_Reconstruction: 87.81201839447021  Train_KL: 3.5209115743637085  Validation Loss : 91.10283660888672 Val_Reconstruction : 87.62601852416992 Val_KL : 3.476817011833191\n","Epoch: 5238/8000  Traning Loss: 91.14178276062012  Train_Reconstruction: 87.62613773345947  Train_KL: 3.5156460106372833  Validation Loss : 91.2025146484375 Val_Reconstruction : 87.7186508178711 Val_KL : 3.4838632345199585\n","Epoch: 5239/8000  Traning Loss: 91.03893375396729  Train_Reconstruction: 87.5135087966919  Train_KL: 3.525424301624298  Validation Loss : 90.83992385864258 Val_Reconstruction : 87.35655212402344 Val_KL : 3.483373999595642\n","Epoch: 5240/8000  Traning Loss: 91.08113956451416  Train_Reconstruction: 87.56057643890381  Train_KL: 3.520563155412674  Validation Loss : 91.20032501220703 Val_Reconstruction : 87.7215690612793 Val_KL : 3.47875440120697\n","Epoch: 5241/8000  Traning Loss: 91.30632209777832  Train_Reconstruction: 87.78637027740479  Train_KL: 3.519951492547989  Validation Loss : 91.11975860595703 Val_Reconstruction : 87.64305877685547 Val_KL : 3.4766993522644043\n","Epoch: 5242/8000  Traning Loss: 91.5723352432251  Train_Reconstruction: 88.04681491851807  Train_KL: 3.525520443916321  Validation Loss : 91.32461547851562 Val_Reconstruction : 87.83872985839844 Val_KL : 3.4858886003494263\n","Epoch: 5243/8000  Traning Loss: 91.43473243713379  Train_Reconstruction: 87.91349220275879  Train_KL: 3.521240770816803  Validation Loss : 91.4500503540039 Val_Reconstruction : 87.97411346435547 Val_KL : 3.475938081741333\n","Epoch: 5244/8000  Traning Loss: 91.10971927642822  Train_Reconstruction: 87.59515285491943  Train_KL: 3.5145652294158936  Validation Loss : 90.77897644042969 Val_Reconstruction : 87.30315017700195 Val_KL : 3.475828766822815\n","Epoch: 5245/8000  Traning Loss: 91.2425479888916  Train_Reconstruction: 87.72133350372314  Train_KL: 3.521214187145233  Validation Loss : 91.40444564819336 Val_Reconstruction : 87.9173469543457 Val_KL : 3.4870970249176025\n","Epoch: 5246/8000  Traning Loss: 91.59861850738525  Train_Reconstruction: 88.07227993011475  Train_KL: 3.526338905096054  Validation Loss : 91.29757690429688 Val_Reconstruction : 87.80671691894531 Val_KL : 3.4908623695373535\n","Epoch: 5247/8000  Traning Loss: 91.21371650695801  Train_Reconstruction: 87.67644119262695  Train_KL: 3.5372745990753174  Validation Loss : 91.03833389282227 Val_Reconstruction : 87.54607772827148 Val_KL : 3.492255687713623\n","Epoch: 5248/8000  Traning Loss: 90.9750452041626  Train_Reconstruction: 87.45151901245117  Train_KL: 3.523526281118393  Validation Loss : 90.85291290283203 Val_Reconstruction : 87.37309265136719 Val_KL : 3.4798234701156616\n","Epoch: 5249/8000  Traning Loss: 90.980788230896  Train_Reconstruction: 87.4670581817627  Train_KL: 3.5137301981449127  Validation Loss : 90.72653579711914 Val_Reconstruction : 87.2487678527832 Val_KL : 3.4777672290802\n","Epoch: 5250/8000  Traning Loss: 91.00855159759521  Train_Reconstruction: 87.49067878723145  Train_KL: 3.5178731977939606  Validation Loss : 90.65476608276367 Val_Reconstruction : 87.17564392089844 Val_KL : 3.4791245460510254\n","Epoch: 5251/8000  Traning Loss: 90.96382331848145  Train_Reconstruction: 87.44796848297119  Train_KL: 3.5158539414405823  Validation Loss : 90.82585525512695 Val_Reconstruction : 87.34916687011719 Val_KL : 3.4766892194747925\n","Epoch: 5252/8000  Traning Loss: 90.9853048324585  Train_Reconstruction: 87.46497821807861  Train_KL: 3.5203278064727783  Validation Loss : 91.09687042236328 Val_Reconstruction : 87.61741256713867 Val_KL : 3.4794578552246094\n","Epoch: 5253/8000  Traning Loss: 91.04523372650146  Train_Reconstruction: 87.51995658874512  Train_KL: 3.525277078151703  Validation Loss : 90.96243667602539 Val_Reconstruction : 87.47937393188477 Val_KL : 3.483063578605652\n","Epoch: 5254/8000  Traning Loss: 91.43730068206787  Train_Reconstruction: 87.91289710998535  Train_KL: 3.52440145611763  Validation Loss : 91.71027374267578 Val_Reconstruction : 88.21888732910156 Val_KL : 3.4913856983184814\n","Epoch: 5255/8000  Traning Loss: 91.37963771820068  Train_Reconstruction: 87.84997177124023  Train_KL: 3.529665470123291  Validation Loss : 91.07561874389648 Val_Reconstruction : 87.591552734375 Val_KL : 3.4840662479400635\n","Epoch: 5256/8000  Traning Loss: 91.12637519836426  Train_Reconstruction: 87.61588478088379  Train_KL: 3.510489195585251  Validation Loss : 91.06275177001953 Val_Reconstruction : 87.58999633789062 Val_KL : 3.4727554321289062\n","Epoch: 5257/8000  Traning Loss: 91.53104400634766  Train_Reconstruction: 88.01211833953857  Train_KL: 3.518926203250885  Validation Loss : 91.28775787353516 Val_Reconstruction : 87.80785751342773 Val_KL : 3.4799000024795532\n","Epoch: 5258/8000  Traning Loss: 91.6256160736084  Train_Reconstruction: 88.10496711730957  Train_KL: 3.520648330450058  Validation Loss : 91.34468460083008 Val_Reconstruction : 87.86533737182617 Val_KL : 3.4793492555618286\n","Epoch: 5259/8000  Traning Loss: 91.62702178955078  Train_Reconstruction: 88.100998878479  Train_KL: 3.5260222256183624  Validation Loss : 91.5886001586914 Val_Reconstruction : 88.10282897949219 Val_KL : 3.4857702255249023\n","Epoch: 5260/8000  Traning Loss: 91.45889377593994  Train_Reconstruction: 87.94116306304932  Train_KL: 3.517729341983795  Validation Loss : 91.55366897583008 Val_Reconstruction : 88.0845718383789 Val_KL : 3.4690961837768555\n","Epoch: 5261/8000  Traning Loss: 91.66508293151855  Train_Reconstruction: 88.15868091583252  Train_KL: 3.5064015984535217  Validation Loss : 91.76760482788086 Val_Reconstruction : 88.30122756958008 Val_KL : 3.466374158859253\n","Epoch: 5262/8000  Traning Loss: 91.58787441253662  Train_Reconstruction: 88.06660175323486  Train_KL: 3.5212728083133698  Validation Loss : 91.42041397094727 Val_Reconstruction : 87.93860244750977 Val_KL : 3.4818116426467896\n","Epoch: 5263/8000  Traning Loss: 91.48364639282227  Train_Reconstruction: 87.95803356170654  Train_KL: 3.525612562894821  Validation Loss : 91.5885009765625 Val_Reconstruction : 88.10613632202148 Val_KL : 3.4823625087738037\n","Epoch: 5264/8000  Traning Loss: 91.2864122390747  Train_Reconstruction: 87.77269840240479  Train_KL: 3.5137141942977905  Validation Loss : 91.3510971069336 Val_Reconstruction : 87.88112258911133 Val_KL : 3.4699771404266357\n","Epoch: 5265/8000  Traning Loss: 91.3749008178711  Train_Reconstruction: 87.86159706115723  Train_KL: 3.5133035480976105  Validation Loss : 91.32743835449219 Val_Reconstruction : 87.84521484375 Val_KL : 3.48222279548645\n","Epoch: 5266/8000  Traning Loss: 91.25358200073242  Train_Reconstruction: 87.7351713180542  Train_KL: 3.518410861492157  Validation Loss : 90.91962432861328 Val_Reconstruction : 87.44432830810547 Val_KL : 3.475296378135681\n","Epoch: 5267/8000  Traning Loss: 91.02362632751465  Train_Reconstruction: 87.51019763946533  Train_KL: 3.513428032398224  Validation Loss : 91.2420654296875 Val_Reconstruction : 87.76295471191406 Val_KL : 3.479111075401306\n","Epoch: 5268/8000  Traning Loss: 91.08439254760742  Train_Reconstruction: 87.5689468383789  Train_KL: 3.5154450833797455  Validation Loss : 90.60770416259766 Val_Reconstruction : 87.13507843017578 Val_KL : 3.47262442111969\n","Epoch: 5269/8000  Traning Loss: 90.75335693359375  Train_Reconstruction: 87.23284530639648  Train_KL: 3.520511746406555  Validation Loss : 90.70647048950195 Val_Reconstruction : 87.21441650390625 Val_KL : 3.4920512437820435\n","Epoch: 5270/8000  Traning Loss: 91.13313388824463  Train_Reconstruction: 87.59706974029541  Train_KL: 3.5360638201236725  Validation Loss : 91.1896743774414 Val_Reconstruction : 87.686767578125 Val_KL : 3.502907156944275\n","Epoch: 5271/8000  Traning Loss: 91.34404563903809  Train_Reconstruction: 87.81455898284912  Train_KL: 3.529486060142517  Validation Loss : 91.15382385253906 Val_Reconstruction : 87.67578887939453 Val_KL : 3.4780343770980835\n","Epoch: 5272/8000  Traning Loss: 91.53664970397949  Train_Reconstruction: 88.01796245574951  Train_KL: 3.518687665462494  Validation Loss : 91.92618560791016 Val_Reconstruction : 88.4421272277832 Val_KL : 3.4840586185455322\n","Epoch: 5273/8000  Traning Loss: 91.50584888458252  Train_Reconstruction: 87.98078536987305  Train_KL: 3.5250627994537354  Validation Loss : 91.59276580810547 Val_Reconstruction : 88.10700607299805 Val_KL : 3.4857609272003174\n","Epoch: 5274/8000  Traning Loss: 91.47003650665283  Train_Reconstruction: 87.943359375  Train_KL: 3.526677966117859  Validation Loss : 91.0826187133789 Val_Reconstruction : 87.59692764282227 Val_KL : 3.4856913089752197\n","Epoch: 5275/8000  Traning Loss: 90.9984302520752  Train_Reconstruction: 87.47317028045654  Train_KL: 3.525259852409363  Validation Loss : 90.8776969909668 Val_Reconstruction : 87.39877319335938 Val_KL : 3.4789249897003174\n","Epoch: 5276/8000  Traning Loss: 90.93520069122314  Train_Reconstruction: 87.42105293273926  Train_KL: 3.5141482651233673  Validation Loss : 90.81449127197266 Val_Reconstruction : 87.34105682373047 Val_KL : 3.473433494567871\n","Epoch: 5277/8000  Traning Loss: 90.86201667785645  Train_Reconstruction: 87.34267044067383  Train_KL: 3.5193471014499664  Validation Loss : 90.9528923034668 Val_Reconstruction : 87.47309494018555 Val_KL : 3.47979474067688\n","Epoch: 5278/8000  Traning Loss: 91.06129837036133  Train_Reconstruction: 87.54311847686768  Train_KL: 3.5181797444820404  Validation Loss : 90.82625579833984 Val_Reconstruction : 87.33924865722656 Val_KL : 3.4870071411132812\n","Epoch: 5279/8000  Traning Loss: 91.10381698608398  Train_Reconstruction: 87.58679485321045  Train_KL: 3.5170221626758575  Validation Loss : 91.33527755737305 Val_Reconstruction : 87.85995101928711 Val_KL : 3.475327968597412\n","Epoch: 5280/8000  Traning Loss: 91.43345832824707  Train_Reconstruction: 87.92022895812988  Train_KL: 3.513228327035904  Validation Loss : 91.53386688232422 Val_Reconstruction : 88.06265258789062 Val_KL : 3.4712144136428833\n","Epoch: 5281/8000  Traning Loss: 91.63794803619385  Train_Reconstruction: 88.12472820281982  Train_KL: 3.5132185518741608  Validation Loss : 91.40564346313477 Val_Reconstruction : 87.93172836303711 Val_KL : 3.4739142656326294\n","Epoch: 5282/8000  Traning Loss: 91.21036624908447  Train_Reconstruction: 87.6959924697876  Train_KL: 3.5143729150295258  Validation Loss : 91.25528717041016 Val_Reconstruction : 87.77511978149414 Val_KL : 3.480164885520935\n","Epoch: 5283/8000  Traning Loss: 91.21465587615967  Train_Reconstruction: 87.69179058074951  Train_KL: 3.52286559343338  Validation Loss : 91.3034553527832 Val_Reconstruction : 87.8239517211914 Val_KL : 3.479501962661743\n","Epoch: 5284/8000  Traning Loss: 91.45986270904541  Train_Reconstruction: 87.94415664672852  Train_KL: 3.515705406665802  Validation Loss : 91.31336212158203 Val_Reconstruction : 87.84121322631836 Val_KL : 3.472147583961487\n","Epoch: 5285/8000  Traning Loss: 91.8680305480957  Train_Reconstruction: 88.3527717590332  Train_KL: 3.5152582228183746  Validation Loss : 92.1368408203125 Val_Reconstruction : 88.67344665527344 Val_KL : 3.4633944034576416\n","Epoch: 5286/8000  Traning Loss: 91.55516338348389  Train_Reconstruction: 88.03994941711426  Train_KL: 3.5152143836021423  Validation Loss : 91.11310958862305 Val_Reconstruction : 87.64127349853516 Val_KL : 3.4718340635299683\n","Epoch: 5287/8000  Traning Loss: 91.35289669036865  Train_Reconstruction: 87.83625030517578  Train_KL: 3.516646146774292  Validation Loss : 91.26177597045898 Val_Reconstruction : 87.78507232666016 Val_KL : 3.476703643798828\n","Epoch: 5288/8000  Traning Loss: 91.97878360748291  Train_Reconstruction: 88.46027278900146  Train_KL: 3.518511086702347  Validation Loss : 92.44197082519531 Val_Reconstruction : 88.96824645996094 Val_KL : 3.473724842071533\n","Epoch: 5289/8000  Traning Loss: 92.36366081237793  Train_Reconstruction: 88.85100078582764  Train_KL: 3.5126596689224243  Validation Loss : 92.54446029663086 Val_Reconstruction : 89.06721496582031 Val_KL : 3.4772439002990723\n","Epoch: 5290/8000  Traning Loss: 92.40122032165527  Train_Reconstruction: 88.88384246826172  Train_KL: 3.5173791348934174  Validation Loss : 92.17939758300781 Val_Reconstruction : 88.70517349243164 Val_KL : 3.4742244482040405\n","Epoch: 5291/8000  Traning Loss: 92.60327529907227  Train_Reconstruction: 89.08879280090332  Train_KL: 3.5144838094711304  Validation Loss : 93.00264739990234 Val_Reconstruction : 89.52811050415039 Val_KL : 3.4745341539382935\n","Epoch: 5292/8000  Traning Loss: 91.99844455718994  Train_Reconstruction: 88.47966861724854  Train_KL: 3.518775910139084  Validation Loss : 90.99747848510742 Val_Reconstruction : 87.51683044433594 Val_KL : 3.480649471282959\n","Epoch: 5293/8000  Traning Loss: 91.07302474975586  Train_Reconstruction: 87.55576133728027  Train_KL: 3.5172646939754486  Validation Loss : 91.19670486450195 Val_Reconstruction : 87.72269821166992 Val_KL : 3.4740062952041626\n","Epoch: 5294/8000  Traning Loss: 91.05551719665527  Train_Reconstruction: 87.54311656951904  Train_KL: 3.5124005377292633  Validation Loss : 91.15867614746094 Val_Reconstruction : 87.69204330444336 Val_KL : 3.4666337966918945\n","Epoch: 5295/8000  Traning Loss: 90.92273426055908  Train_Reconstruction: 87.41379070281982  Train_KL: 3.508943885564804  Validation Loss : 90.64251708984375 Val_Reconstruction : 87.16433715820312 Val_KL : 3.4781771898269653\n","Epoch: 5296/8000  Traning Loss: 91.15910816192627  Train_Reconstruction: 87.63210391998291  Train_KL: 3.527004510164261  Validation Loss : 90.94477462768555 Val_Reconstruction : 87.44486999511719 Val_KL : 3.499905228614807\n","Epoch: 5297/8000  Traning Loss: 91.33763217926025  Train_Reconstruction: 87.80890464782715  Train_KL: 3.5287277698516846  Validation Loss : 91.6191520690918 Val_Reconstruction : 88.13523864746094 Val_KL : 3.483912944793701\n","Epoch: 5298/8000  Traning Loss: 91.46090984344482  Train_Reconstruction: 87.94523525238037  Train_KL: 3.515674978494644  Validation Loss : 91.33028793334961 Val_Reconstruction : 87.85566329956055 Val_KL : 3.4746224880218506\n","Epoch: 5299/8000  Traning Loss: 91.08250999450684  Train_Reconstruction: 87.55659484863281  Train_KL: 3.5259146690368652  Validation Loss : 90.93426513671875 Val_Reconstruction : 87.4442024230957 Val_KL : 3.490065097808838\n","Epoch: 5300/8000  Traning Loss: 90.8669261932373  Train_Reconstruction: 87.34486389160156  Train_KL: 3.5220614671707153  Validation Loss : 90.67765808105469 Val_Reconstruction : 87.19635009765625 Val_KL : 3.481307864189148\n","Epoch: 5301/8000  Traning Loss: 90.88996887207031  Train_Reconstruction: 87.37196350097656  Train_KL: 3.51800537109375  Validation Loss : 90.69357299804688 Val_Reconstruction : 87.2120132446289 Val_KL : 3.4815614223480225\n","Epoch: 5302/8000  Traning Loss: 90.95769214630127  Train_Reconstruction: 87.43487167358398  Train_KL: 3.522819459438324  Validation Loss : 90.69135284423828 Val_Reconstruction : 87.2064323425293 Val_KL : 3.484921097755432\n","Epoch: 5303/8000  Traning Loss: 91.32268905639648  Train_Reconstruction: 87.7960786819458  Train_KL: 3.5266098380088806  Validation Loss : 91.33756637573242 Val_Reconstruction : 87.85387802124023 Val_KL : 3.4836888313293457\n","Epoch: 5304/8000  Traning Loss: 91.37733554840088  Train_Reconstruction: 87.86042022705078  Train_KL: 3.516914188861847  Validation Loss : 91.15586471557617 Val_Reconstruction : 87.67587280273438 Val_KL : 3.4799939393997192\n","Epoch: 5305/8000  Traning Loss: 90.979811668396  Train_Reconstruction: 87.46533012390137  Train_KL: 3.514482617378235  Validation Loss : 90.84701919555664 Val_Reconstruction : 87.37792205810547 Val_KL : 3.4690966606140137\n","Epoch: 5306/8000  Traning Loss: 90.80068778991699  Train_Reconstruction: 87.28810214996338  Train_KL: 3.5125852227211  Validation Loss : 90.65316772460938 Val_Reconstruction : 87.17670822143555 Val_KL : 3.4764589071273804\n","Epoch: 5307/8000  Traning Loss: 90.71598529815674  Train_Reconstruction: 87.19167900085449  Train_KL: 3.5243056416511536  Validation Loss : 90.83335494995117 Val_Reconstruction : 87.34326553344727 Val_KL : 3.4900920391082764\n","Epoch: 5308/8000  Traning Loss: 90.84364604949951  Train_Reconstruction: 87.31470394134521  Train_KL: 3.5289406180381775  Validation Loss : 91.05469512939453 Val_Reconstruction : 87.57368469238281 Val_KL : 3.481008529663086\n","Epoch: 5309/8000  Traning Loss: 91.2638292312622  Train_Reconstruction: 87.74995136260986  Train_KL: 3.5138776898384094  Validation Loss : 91.33227157592773 Val_Reconstruction : 87.861572265625 Val_KL : 3.4706982374191284\n","Epoch: 5310/8000  Traning Loss: 91.97563171386719  Train_Reconstruction: 88.46302700042725  Train_KL: 3.5126051902770996  Validation Loss : 92.20155334472656 Val_Reconstruction : 88.72653198242188 Val_KL : 3.47502064704895\n","Epoch: 5311/8000  Traning Loss: 91.99531841278076  Train_Reconstruction: 88.47375679016113  Train_KL: 3.521561324596405  Validation Loss : 91.76872634887695 Val_Reconstruction : 88.28224563598633 Val_KL : 3.486479878425598\n","Epoch: 5312/8000  Traning Loss: 91.40490913391113  Train_Reconstruction: 87.88465023040771  Train_KL: 3.5202579498291016  Validation Loss : 90.99203491210938 Val_Reconstruction : 87.51280975341797 Val_KL : 3.4792261123657227\n","Epoch: 5313/8000  Traning Loss: 91.13207149505615  Train_Reconstruction: 87.61275959014893  Train_KL: 3.51931032538414  Validation Loss : 90.83619689941406 Val_Reconstruction : 87.35118865966797 Val_KL : 3.4850088357925415\n","Epoch: 5314/8000  Traning Loss: 90.83394718170166  Train_Reconstruction: 87.30024337768555  Train_KL: 3.5337029695510864  Validation Loss : 90.79080963134766 Val_Reconstruction : 87.2929916381836 Val_KL : 3.4978175163269043\n","Epoch: 5315/8000  Traning Loss: 90.84309959411621  Train_Reconstruction: 87.31686878204346  Train_KL: 3.5262302458286285  Validation Loss : 91.03836822509766 Val_Reconstruction : 87.55979919433594 Val_KL : 3.4785690307617188\n","Epoch: 5316/8000  Traning Loss: 91.35836696624756  Train_Reconstruction: 87.83888053894043  Train_KL: 3.519487977027893  Validation Loss : 91.96924591064453 Val_Reconstruction : 88.4816665649414 Val_KL : 3.487578511238098\n","Epoch: 5317/8000  Traning Loss: 91.73767948150635  Train_Reconstruction: 88.20890808105469  Train_KL: 3.5287704467773438  Validation Loss : 91.65100479125977 Val_Reconstruction : 88.16362380981445 Val_KL : 3.4873818159103394\n","Epoch: 5318/8000  Traning Loss: 91.04774570465088  Train_Reconstruction: 87.52589511871338  Train_KL: 3.5218503177165985  Validation Loss : 90.69498443603516 Val_Reconstruction : 87.21270751953125 Val_KL : 3.4822763204574585\n","Epoch: 5319/8000  Traning Loss: 90.69846820831299  Train_Reconstruction: 87.17574119567871  Train_KL: 3.5227280259132385  Validation Loss : 90.84356307983398 Val_Reconstruction : 87.35794448852539 Val_KL : 3.4856189489364624\n","Epoch: 5320/8000  Traning Loss: 91.09128475189209  Train_Reconstruction: 87.56304359436035  Train_KL: 3.528240829706192  Validation Loss : 91.09895706176758 Val_Reconstruction : 87.60457611083984 Val_KL : 3.4943830966949463\n","Epoch: 5321/8000  Traning Loss: 91.57312297821045  Train_Reconstruction: 88.04340553283691  Train_KL: 3.5297180712223053  Validation Loss : 91.50347900390625 Val_Reconstruction : 88.01725769042969 Val_KL : 3.4862220287323\n","Epoch: 5322/8000  Traning Loss: 91.5665283203125  Train_Reconstruction: 88.04898452758789  Train_KL: 3.517543762922287  Validation Loss : 91.10638046264648 Val_Reconstruction : 87.62262725830078 Val_KL : 3.483752131462097\n","Epoch: 5323/8000  Traning Loss: 91.36898803710938  Train_Reconstruction: 87.85163879394531  Train_KL: 3.5173495411872864  Validation Loss : 90.9668197631836 Val_Reconstruction : 87.49249267578125 Val_KL : 3.4743258953094482\n","Epoch: 5324/8000  Traning Loss: 90.91099548339844  Train_Reconstruction: 87.4018383026123  Train_KL: 3.5091554522514343  Validation Loss : 90.85936737060547 Val_Reconstruction : 87.38816452026367 Val_KL : 3.471203088760376\n","Epoch: 5325/8000  Traning Loss: 91.03342247009277  Train_Reconstruction: 87.5013780593872  Train_KL: 3.5320446491241455  Validation Loss : 91.01939010620117 Val_Reconstruction : 87.51359558105469 Val_KL : 3.5057926177978516\n","Epoch: 5326/8000  Traning Loss: 90.99628257751465  Train_Reconstruction: 87.46733856201172  Train_KL: 3.528943806886673  Validation Loss : 91.2943229675293 Val_Reconstruction : 87.81684112548828 Val_KL : 3.477482318878174\n","Epoch: 5327/8000  Traning Loss: 91.34062480926514  Train_Reconstruction: 87.83166313171387  Train_KL: 3.5089626908302307  Validation Loss : 91.69866561889648 Val_Reconstruction : 88.2271842956543 Val_KL : 3.471480965614319\n","Epoch: 5328/8000  Traning Loss: 92.11317253112793  Train_Reconstruction: 88.5986557006836  Train_KL: 3.514516234397888  Validation Loss : 92.6666374206543 Val_Reconstruction : 89.18699645996094 Val_KL : 3.479639768600464\n","Epoch: 5329/8000  Traning Loss: 92.1074857711792  Train_Reconstruction: 88.59172534942627  Train_KL: 3.5157608687877655  Validation Loss : 92.19369125366211 Val_Reconstruction : 88.72053909301758 Val_KL : 3.473150134086609\n","Epoch: 5330/8000  Traning Loss: 91.22074031829834  Train_Reconstruction: 87.7025375366211  Train_KL: 3.5182020366191864  Validation Loss : 90.8047981262207 Val_Reconstruction : 87.32365417480469 Val_KL : 3.481142997741699\n","Epoch: 5331/8000  Traning Loss: 90.74586868286133  Train_Reconstruction: 87.2199935913086  Train_KL: 3.5258761942386627  Validation Loss : 90.75209045410156 Val_Reconstruction : 87.2706413269043 Val_KL : 3.4814486503601074\n","Epoch: 5332/8000  Traning Loss: 90.70689487457275  Train_Reconstruction: 87.1741533279419  Train_KL: 3.5327413082122803  Validation Loss : 90.46845626831055 Val_Reconstruction : 86.9786605834961 Val_KL : 3.489795446395874\n","Epoch: 5333/8000  Traning Loss: 90.69034957885742  Train_Reconstruction: 87.16764640808105  Train_KL: 3.522704631090164  Validation Loss : 90.82363891601562 Val_Reconstruction : 87.35075759887695 Val_KL : 3.472882866859436\n","Epoch: 5334/8000  Traning Loss: 90.91696166992188  Train_Reconstruction: 87.40521907806396  Train_KL: 3.5117413997650146  Validation Loss : 91.0943374633789 Val_Reconstruction : 87.62233352661133 Val_KL : 3.472006678581238\n","Epoch: 5335/8000  Traning Loss: 91.35480880737305  Train_Reconstruction: 87.82666015625  Train_KL: 3.528149724006653  Validation Loss : 91.15501022338867 Val_Reconstruction : 87.65307235717773 Val_KL : 3.501936674118042\n","Epoch: 5336/8000  Traning Loss: 91.33171272277832  Train_Reconstruction: 87.7998218536377  Train_KL: 3.5318897664546967  Validation Loss : 90.81034088134766 Val_Reconstruction : 87.32449722290039 Val_KL : 3.4858402013778687\n","Epoch: 5337/8000  Traning Loss: 90.85282039642334  Train_Reconstruction: 87.34182262420654  Train_KL: 3.510997325181961  Validation Loss : 90.64199829101562 Val_Reconstruction : 87.17916488647461 Val_KL : 3.462835192680359\n","Epoch: 5338/8000  Traning Loss: 91.02903747558594  Train_Reconstruction: 87.51654815673828  Train_KL: 3.512487977743149  Validation Loss : 91.39274215698242 Val_Reconstruction : 87.90639114379883 Val_KL : 3.486353278160095\n","Epoch: 5339/8000  Traning Loss: 90.91415977478027  Train_Reconstruction: 87.38609981536865  Train_KL: 3.528059870004654  Validation Loss : 90.65608215332031 Val_Reconstruction : 87.15718841552734 Val_KL : 3.498895287513733\n","Epoch: 5340/8000  Traning Loss: 90.84242057800293  Train_Reconstruction: 87.31401538848877  Train_KL: 3.5284058153629303  Validation Loss : 90.87682342529297 Val_Reconstruction : 87.39709854125977 Val_KL : 3.47972309589386\n","Epoch: 5341/8000  Traning Loss: 91.18163681030273  Train_Reconstruction: 87.67331314086914  Train_KL: 3.508323699235916  Validation Loss : 91.26279830932617 Val_Reconstruction : 87.79468536376953 Val_KL : 3.4681127071380615\n","Epoch: 5342/8000  Traning Loss: 91.44072532653809  Train_Reconstruction: 87.92725849151611  Train_KL: 3.513467162847519  Validation Loss : 91.27614212036133 Val_Reconstruction : 87.79439544677734 Val_KL : 3.4817484617233276\n","Epoch: 5343/8000  Traning Loss: 91.29738521575928  Train_Reconstruction: 87.76565837860107  Train_KL: 3.531726062297821  Validation Loss : 91.5374984741211 Val_Reconstruction : 88.04803466796875 Val_KL : 3.489465355873108\n","Epoch: 5344/8000  Traning Loss: 91.1089973449707  Train_Reconstruction: 87.58489322662354  Train_KL: 3.524104058742523  Validation Loss : 91.04040145874023 Val_Reconstruction : 87.55982971191406 Val_KL : 3.480573296546936\n","Epoch: 5345/8000  Traning Loss: 90.82839870452881  Train_Reconstruction: 87.31016826629639  Train_KL: 3.5182297825813293  Validation Loss : 90.70636367797852 Val_Reconstruction : 87.21963500976562 Val_KL : 3.4867299795150757\n","Epoch: 5346/8000  Traning Loss: 90.88372802734375  Train_Reconstruction: 87.36099529266357  Train_KL: 3.522733509540558  Validation Loss : 91.34804153442383 Val_Reconstruction : 87.86369705200195 Val_KL : 3.4843441247940063\n","Epoch: 5347/8000  Traning Loss: 91.16717720031738  Train_Reconstruction: 87.65141868591309  Train_KL: 3.515758126974106  Validation Loss : 90.89102935791016 Val_Reconstruction : 87.42509460449219 Val_KL : 3.4659347534179688\n","Epoch: 5348/8000  Traning Loss: 91.05667209625244  Train_Reconstruction: 87.55001831054688  Train_KL: 3.506653279066086  Validation Loss : 91.0833740234375 Val_Reconstruction : 87.61624145507812 Val_KL : 3.4671329259872437\n","Epoch: 5349/8000  Traning Loss: 90.8233699798584  Train_Reconstruction: 87.31339359283447  Train_KL: 3.509976327419281  Validation Loss : 90.57672882080078 Val_Reconstruction : 87.09384536743164 Val_KL : 3.4828832149505615\n","Epoch: 5350/8000  Traning Loss: 90.71215534210205  Train_Reconstruction: 87.1814193725586  Train_KL: 3.530734956264496  Validation Loss : 90.63371658325195 Val_Reconstruction : 87.13163375854492 Val_KL : 3.502081513404846\n","Epoch: 5351/8000  Traning Loss: 90.71484756469727  Train_Reconstruction: 87.1791181564331  Train_KL: 3.535729318857193  Validation Loss : 90.69239807128906 Val_Reconstruction : 87.20406723022461 Val_KL : 3.488331913948059\n","Epoch: 5352/8000  Traning Loss: 90.90530014038086  Train_Reconstruction: 87.37978267669678  Train_KL: 3.5255175828933716  Validation Loss : 90.62438583374023 Val_Reconstruction : 87.14286422729492 Val_KL : 3.4815194606781006\n","Epoch: 5353/8000  Traning Loss: 90.6707353591919  Train_Reconstruction: 87.15160751342773  Train_KL: 3.5191280245780945  Validation Loss : 90.5564956665039 Val_Reconstruction : 87.0678825378418 Val_KL : 3.488613486289978\n","Epoch: 5354/8000  Traning Loss: 90.91207885742188  Train_Reconstruction: 87.38476848602295  Train_KL: 3.527310222387314  Validation Loss : 91.03767395019531 Val_Reconstruction : 87.54241561889648 Val_KL : 3.4952611923217773\n","Epoch: 5355/8000  Traning Loss: 91.05239486694336  Train_Reconstruction: 87.53026103973389  Train_KL: 3.5221339762210846  Validation Loss : 91.09975814819336 Val_Reconstruction : 87.62140655517578 Val_KL : 3.478351593017578\n","Epoch: 5356/8000  Traning Loss: 91.72096061706543  Train_Reconstruction: 88.20234298706055  Train_KL: 3.518617868423462  Validation Loss : 91.62171173095703 Val_Reconstruction : 88.13761520385742 Val_KL : 3.4840999841690063\n","Epoch: 5357/8000  Traning Loss: 91.40217971801758  Train_Reconstruction: 87.87593364715576  Train_KL: 3.5262447893619537  Validation Loss : 91.38504028320312 Val_Reconstruction : 87.89569091796875 Val_KL : 3.489347457885742\n","Epoch: 5358/8000  Traning Loss: 91.05671691894531  Train_Reconstruction: 87.52854347229004  Train_KL: 3.528173178434372  Validation Loss : 90.80636215209961 Val_Reconstruction : 87.32427215576172 Val_KL : 3.482092261314392\n","Epoch: 5359/8000  Traning Loss: 90.78474998474121  Train_Reconstruction: 87.26139163970947  Train_KL: 3.5233566761016846  Validation Loss : 90.70721435546875 Val_Reconstruction : 87.2283706665039 Val_KL : 3.4788453578948975\n","Epoch: 5360/8000  Traning Loss: 91.10043525695801  Train_Reconstruction: 87.58280563354492  Train_KL: 3.517629384994507  Validation Loss : 91.49258041381836 Val_Reconstruction : 88.01712417602539 Val_KL : 3.475455403327942\n","Epoch: 5361/8000  Traning Loss: 91.60519027709961  Train_Reconstruction: 88.07874870300293  Train_KL: 3.526442289352417  Validation Loss : 91.90625762939453 Val_Reconstruction : 88.41294860839844 Val_KL : 3.4933085441589355\n","Epoch: 5362/8000  Traning Loss: 91.81121063232422  Train_Reconstruction: 88.29107093811035  Train_KL: 3.520139694213867  Validation Loss : 91.85186386108398 Val_Reconstruction : 88.37417984008789 Val_KL : 3.4776840209960938\n","Epoch: 5363/8000  Traning Loss: 92.97751331329346  Train_Reconstruction: 89.46618938446045  Train_KL: 3.5113241374492645  Validation Loss : 93.55858993530273 Val_Reconstruction : 90.08275985717773 Val_KL : 3.475829005241394\n","Epoch: 5364/8000  Traning Loss: 92.73868465423584  Train_Reconstruction: 89.2199182510376  Train_KL: 3.5187662839889526  Validation Loss : 92.6200942993164 Val_Reconstruction : 89.13267517089844 Val_KL : 3.487418532371521\n","Epoch: 5365/8000  Traning Loss: 91.80234146118164  Train_Reconstruction: 88.27257633209229  Train_KL: 3.5297646522521973  Validation Loss : 91.01030349731445 Val_Reconstruction : 87.5173110961914 Val_KL : 3.4929920434951782\n","Epoch: 5366/8000  Traning Loss: 90.80662155151367  Train_Reconstruction: 87.278244972229  Train_KL: 3.5283758640289307  Validation Loss : 91.21751403808594 Val_Reconstruction : 87.73306274414062 Val_KL : 3.484450101852417\n","Epoch: 5367/8000  Traning Loss: 90.96622562408447  Train_Reconstruction: 87.45143604278564  Train_KL: 3.5147902369499207  Validation Loss : 90.82307434082031 Val_Reconstruction : 87.35275268554688 Val_KL : 3.4703227281570435\n","Epoch: 5368/8000  Traning Loss: 90.72002506256104  Train_Reconstruction: 87.20246124267578  Train_KL: 3.517563074827194  Validation Loss : 90.623779296875 Val_Reconstruction : 87.14675521850586 Val_KL : 3.4770227670669556\n","Epoch: 5369/8000  Traning Loss: 90.82546806335449  Train_Reconstruction: 87.3041467666626  Train_KL: 3.521322011947632  Validation Loss : 91.09103012084961 Val_Reconstruction : 87.61222076416016 Val_KL : 3.47880756855011\n","Epoch: 5370/8000  Traning Loss: 91.69901847839355  Train_Reconstruction: 88.18691635131836  Train_KL: 3.512102574110031  Validation Loss : 92.13944244384766 Val_Reconstruction : 88.67154693603516 Val_KL : 3.467894673347473\n","Epoch: 5371/8000  Traning Loss: 93.51363945007324  Train_Reconstruction: 89.9954605102539  Train_KL: 3.518179774284363  Validation Loss : 93.73891067504883 Val_Reconstruction : 90.25213241577148 Val_KL : 3.4867801666259766\n","Epoch: 5372/8000  Traning Loss: 92.50875949859619  Train_Reconstruction: 88.97954654693604  Train_KL: 3.5292125940322876  Validation Loss : 91.79088592529297 Val_Reconstruction : 88.29862213134766 Val_KL : 3.492264151573181\n","Epoch: 5373/8000  Traning Loss: 91.83601570129395  Train_Reconstruction: 88.31256675720215  Train_KL: 3.523448199033737  Validation Loss : 91.5297966003418 Val_Reconstruction : 88.05390167236328 Val_KL : 3.475895881652832\n","Epoch: 5374/8000  Traning Loss: 91.11768436431885  Train_Reconstruction: 87.59654998779297  Train_KL: 3.521133244037628  Validation Loss : 90.84614944458008 Val_Reconstruction : 87.36167907714844 Val_KL : 3.4844717979431152\n","Epoch: 5375/8000  Traning Loss: 90.86208629608154  Train_Reconstruction: 87.33720016479492  Train_KL: 3.524886727333069  Validation Loss : 90.84712219238281 Val_Reconstruction : 87.3714599609375 Val_KL : 3.47566294670105\n","Epoch: 5376/8000  Traning Loss: 91.06610298156738  Train_Reconstruction: 87.54782962799072  Train_KL: 3.5182733833789825  Validation Loss : 91.3838996887207 Val_Reconstruction : 87.90405654907227 Val_KL : 3.4798457622528076\n","Epoch: 5377/8000  Traning Loss: 91.40960884094238  Train_Reconstruction: 87.89254951477051  Train_KL: 3.5170594453811646  Validation Loss : 91.2080307006836 Val_Reconstruction : 87.73230361938477 Val_KL : 3.4757280349731445\n","Epoch: 5378/8000  Traning Loss: 91.21977424621582  Train_Reconstruction: 87.70458030700684  Train_KL: 3.5151946246623993  Validation Loss : 91.16501235961914 Val_Reconstruction : 87.68842315673828 Val_KL : 3.476589560508728\n","Epoch: 5379/8000  Traning Loss: 90.91552734375  Train_Reconstruction: 87.40390396118164  Train_KL: 3.511622667312622  Validation Loss : 90.82398223876953 Val_Reconstruction : 87.34976959228516 Val_KL : 3.474210023880005\n","Epoch: 5380/8000  Traning Loss: 90.83691024780273  Train_Reconstruction: 87.32099151611328  Train_KL: 3.515919268131256  Validation Loss : 90.75992965698242 Val_Reconstruction : 87.28254318237305 Val_KL : 3.4773874282836914\n","Epoch: 5381/8000  Traning Loss: 90.89027500152588  Train_Reconstruction: 87.37015724182129  Train_KL: 3.520117402076721  Validation Loss : 91.14416885375977 Val_Reconstruction : 87.65803146362305 Val_KL : 3.4861356019973755\n","Epoch: 5382/8000  Traning Loss: 91.09727001190186  Train_Reconstruction: 87.57438468933105  Train_KL: 3.5228854715824127  Validation Loss : 91.10479354858398 Val_Reconstruction : 87.62434005737305 Val_KL : 3.480454683303833\n","Epoch: 5383/8000  Traning Loss: 91.06019878387451  Train_Reconstruction: 87.5322961807251  Train_KL: 3.527900844812393  Validation Loss : 90.94406509399414 Val_Reconstruction : 87.4595947265625 Val_KL : 3.484472393989563\n","Epoch: 5384/8000  Traning Loss: 90.96282863616943  Train_Reconstruction: 87.43747043609619  Train_KL: 3.5253573060035706  Validation Loss : 90.75775909423828 Val_Reconstruction : 87.28446578979492 Val_KL : 3.4732946157455444\n","Epoch: 5385/8000  Traning Loss: 90.86793613433838  Train_Reconstruction: 87.34727191925049  Train_KL: 3.520664393901825  Validation Loss : 90.56112670898438 Val_Reconstruction : 87.0828971862793 Val_KL : 3.4782310724258423\n","Epoch: 5386/8000  Traning Loss: 90.73829078674316  Train_Reconstruction: 87.20940494537354  Train_KL: 3.528885841369629  Validation Loss : 90.71943664550781 Val_Reconstruction : 87.23290634155273 Val_KL : 3.4865282773971558\n","Epoch: 5387/8000  Traning Loss: 90.93018436431885  Train_Reconstruction: 87.40464973449707  Train_KL: 3.5255337357521057  Validation Loss : 90.81650161743164 Val_Reconstruction : 87.33737182617188 Val_KL : 3.4791282415390015\n","Epoch: 5388/8000  Traning Loss: 91.15635299682617  Train_Reconstruction: 87.6403980255127  Train_KL: 3.515956223011017  Validation Loss : 91.07878875732422 Val_Reconstruction : 87.60968017578125 Val_KL : 3.4691078662872314\n","Epoch: 5389/8000  Traning Loss: 91.74018955230713  Train_Reconstruction: 88.22026443481445  Train_KL: 3.519925206899643  Validation Loss : 91.67493057250977 Val_Reconstruction : 88.19100189208984 Val_KL : 3.4839266538619995\n","Epoch: 5390/8000  Traning Loss: 92.92428493499756  Train_Reconstruction: 89.39671611785889  Train_KL: 3.527568757534027  Validation Loss : 93.02447128295898 Val_Reconstruction : 89.53637313842773 Val_KL : 3.4880964756011963\n","Epoch: 5391/8000  Traning Loss: 92.5733871459961  Train_Reconstruction: 89.05316925048828  Train_KL: 3.5202181935310364  Validation Loss : 92.27149200439453 Val_Reconstruction : 88.8118896484375 Val_KL : 3.4596002101898193\n","Epoch: 5392/8000  Traning Loss: 92.83969116210938  Train_Reconstruction: 89.3317346572876  Train_KL: 3.507957011461258  Validation Loss : 92.7584457397461 Val_Reconstruction : 89.28950500488281 Val_KL : 3.4689431190490723\n","Epoch: 5393/8000  Traning Loss: 92.94642543792725  Train_Reconstruction: 89.42172527313232  Train_KL: 3.5247000455856323  Validation Loss : 92.75541305541992 Val_Reconstruction : 89.27005004882812 Val_KL : 3.485363245010376\n","Epoch: 5394/8000  Traning Loss: 92.56999969482422  Train_Reconstruction: 89.04036617279053  Train_KL: 3.52963188290596  Validation Loss : 92.82757186889648 Val_Reconstruction : 89.34334564208984 Val_KL : 3.4842278957366943\n","Epoch: 5395/8000  Traning Loss: 92.37873935699463  Train_Reconstruction: 88.85825443267822  Train_KL: 3.520484834909439  Validation Loss : 91.87126541137695 Val_Reconstruction : 88.39200592041016 Val_KL : 3.4792580604553223\n","Epoch: 5396/8000  Traning Loss: 91.23547077178955  Train_Reconstruction: 87.72282600402832  Train_KL: 3.5126456320285797  Validation Loss : 91.10352325439453 Val_Reconstruction : 87.6284065246582 Val_KL : 3.4751189947128296\n","Epoch: 5397/8000  Traning Loss: 91.03203010559082  Train_Reconstruction: 87.50763988494873  Train_KL: 3.5243909060955048  Validation Loss : 90.86992645263672 Val_Reconstruction : 87.3824577331543 Val_KL : 3.4874680042266846\n","Epoch: 5398/8000  Traning Loss: 91.05290985107422  Train_Reconstruction: 87.53568840026855  Train_KL: 3.5172199606895447  Validation Loss : 91.11105728149414 Val_Reconstruction : 87.64130020141602 Val_KL : 3.4697580337524414\n","Epoch: 5399/8000  Traning Loss: 90.78723335266113  Train_Reconstruction: 87.27048683166504  Train_KL: 3.5167455673217773  Validation Loss : 90.83639144897461 Val_Reconstruction : 87.35733413696289 Val_KL : 3.479056239128113\n","Epoch: 5400/8000  Traning Loss: 91.08970165252686  Train_Reconstruction: 87.56866455078125  Train_KL: 3.52103653550148  Validation Loss : 91.11193084716797 Val_Reconstruction : 87.62908554077148 Val_KL : 3.482846975326538\n","Epoch: 5401/8000  Traning Loss: 91.29852771759033  Train_Reconstruction: 87.77138710021973  Train_KL: 3.527140885591507  Validation Loss : 91.29295349121094 Val_Reconstruction : 87.81150436401367 Val_KL : 3.48144793510437\n","Epoch: 5402/8000  Traning Loss: 91.03957080841064  Train_Reconstruction: 87.51289463043213  Train_KL: 3.526675134897232  Validation Loss : 90.84508895874023 Val_Reconstruction : 87.36404800415039 Val_KL : 3.481042265892029\n","Epoch: 5403/8000  Traning Loss: 90.95724964141846  Train_Reconstruction: 87.43803691864014  Train_KL: 3.5192128121852875  Validation Loss : 90.68341827392578 Val_Reconstruction : 87.20929336547852 Val_KL : 3.4741240739822388\n","Epoch: 5404/8000  Traning Loss: 90.83800029754639  Train_Reconstruction: 87.31883239746094  Train_KL: 3.5191673040390015  Validation Loss : 90.95510864257812 Val_Reconstruction : 87.47723007202148 Val_KL : 3.4778788089752197\n","Epoch: 5405/8000  Traning Loss: 91.04902267456055  Train_Reconstruction: 87.52821826934814  Train_KL: 3.5208044946193695  Validation Loss : 91.28252029418945 Val_Reconstruction : 87.80306243896484 Val_KL : 3.479459047317505\n","Epoch: 5406/8000  Traning Loss: 91.00334548950195  Train_Reconstruction: 87.4801836013794  Train_KL: 3.5231614112854004  Validation Loss : 91.19124984741211 Val_Reconstruction : 87.70883178710938 Val_KL : 3.4824174642562866\n","Epoch: 5407/8000  Traning Loss: 91.03048706054688  Train_Reconstruction: 87.50625514984131  Train_KL: 3.5242320597171783  Validation Loss : 91.05244064331055 Val_Reconstruction : 87.56566619873047 Val_KL : 3.4867764711380005\n","Epoch: 5408/8000  Traning Loss: 91.11939716339111  Train_Reconstruction: 87.59466457366943  Train_KL: 3.524733394384384  Validation Loss : 90.9248046875 Val_Reconstruction : 87.44783782958984 Val_KL : 3.476966142654419\n","Epoch: 5409/8000  Traning Loss: 90.96721076965332  Train_Reconstruction: 87.45073223114014  Train_KL: 3.516479194164276  Validation Loss : 90.96960067749023 Val_Reconstruction : 87.50078201293945 Val_KL : 3.468819499015808\n","Epoch: 5410/8000  Traning Loss: 91.03585910797119  Train_Reconstruction: 87.52090549468994  Train_KL: 3.514954626560211  Validation Loss : 90.98428726196289 Val_Reconstruction : 87.50736999511719 Val_KL : 3.476918935775757\n","Epoch: 5411/8000  Traning Loss: 91.06916046142578  Train_Reconstruction: 87.54720687866211  Train_KL: 3.5219543278217316  Validation Loss : 91.03055953979492 Val_Reconstruction : 87.55489730834961 Val_KL : 3.4756624698638916\n","Epoch: 5412/8000  Traning Loss: 91.00072288513184  Train_Reconstruction: 87.48030376434326  Train_KL: 3.5204195082187653  Validation Loss : 91.11814498901367 Val_Reconstruction : 87.63146209716797 Val_KL : 3.4866809844970703\n","Epoch: 5413/8000  Traning Loss: 91.34955501556396  Train_Reconstruction: 87.82599830627441  Train_KL: 3.5235560834407806  Validation Loss : 91.2689437866211 Val_Reconstruction : 87.79015350341797 Val_KL : 3.4787914752960205\n","Epoch: 5414/8000  Traning Loss: 91.12668704986572  Train_Reconstruction: 87.61435794830322  Train_KL: 3.5123294293880463  Validation Loss : 91.02413940429688 Val_Reconstruction : 87.54905319213867 Val_KL : 3.475083112716675\n","Epoch: 5415/8000  Traning Loss: 91.49038219451904  Train_Reconstruction: 87.96449089050293  Train_KL: 3.5258910953998566  Validation Loss : 91.42646408081055 Val_Reconstruction : 87.93880081176758 Val_KL : 3.487662434577942\n","Epoch: 5416/8000  Traning Loss: 91.43187046051025  Train_Reconstruction: 87.9106616973877  Train_KL: 3.5212088525295258  Validation Loss : 91.1802978515625 Val_Reconstruction : 87.70906448364258 Val_KL : 3.4712347984313965\n","Epoch: 5417/8000  Traning Loss: 90.91326808929443  Train_Reconstruction: 87.39990234375  Train_KL: 3.513365298509598  Validation Loss : 90.66672134399414 Val_Reconstruction : 87.1932144165039 Val_KL : 3.4735058546066284\n","Epoch: 5418/8000  Traning Loss: 90.85328006744385  Train_Reconstruction: 87.33671283721924  Train_KL: 3.516565680503845  Validation Loss : 91.26312637329102 Val_Reconstruction : 87.78741836547852 Val_KL : 3.4757102727890015\n","Epoch: 5419/8000  Traning Loss: 91.03921699523926  Train_Reconstruction: 87.5132303237915  Train_KL: 3.5259880125522614  Validation Loss : 90.97747039794922 Val_Reconstruction : 87.49428176879883 Val_KL : 3.483188271522522\n","Epoch: 5420/8000  Traning Loss: 91.1776990890503  Train_Reconstruction: 87.65654277801514  Train_KL: 3.521155208349228  Validation Loss : 91.45978164672852 Val_Reconstruction : 87.98080825805664 Val_KL : 3.4789737462997437\n","Epoch: 5421/8000  Traning Loss: 91.13871192932129  Train_Reconstruction: 87.61716365814209  Train_KL: 3.521548569202423  Validation Loss : 91.0245132446289 Val_Reconstruction : 87.5417709350586 Val_KL : 3.4827418327331543\n","Epoch: 5422/8000  Traning Loss: 91.0488452911377  Train_Reconstruction: 87.53598308563232  Train_KL: 3.512862592935562  Validation Loss : 91.15227508544922 Val_Reconstruction : 87.68037796020508 Val_KL : 3.471898913383484\n","Epoch: 5423/8000  Traning Loss: 90.94525051116943  Train_Reconstruction: 87.427414894104  Train_KL: 3.5178359150886536  Validation Loss : 90.92224884033203 Val_Reconstruction : 87.43021011352539 Val_KL : 3.492036819458008\n","Epoch: 5424/8000  Traning Loss: 91.0089225769043  Train_Reconstruction: 87.47912311553955  Train_KL: 3.5297984778881073  Validation Loss : 90.9248161315918 Val_Reconstruction : 87.43230056762695 Val_KL : 3.4925177097320557\n","Epoch: 5425/8000  Traning Loss: 90.82015895843506  Train_Reconstruction: 87.28796100616455  Train_KL: 3.532197594642639  Validation Loss : 90.69161605834961 Val_Reconstruction : 87.2037239074707 Val_KL : 3.4878939390182495\n","Epoch: 5426/8000  Traning Loss: 90.85445880889893  Train_Reconstruction: 87.33313941955566  Train_KL: 3.521318703889847  Validation Loss : 91.10277557373047 Val_Reconstruction : 87.62838745117188 Val_KL : 3.474391460418701\n","Epoch: 5427/8000  Traning Loss: 90.98420333862305  Train_Reconstruction: 87.46814727783203  Train_KL: 3.516055941581726  Validation Loss : 91.2291374206543 Val_Reconstruction : 87.74796676635742 Val_KL : 3.481172204017639\n","Epoch: 5428/8000  Traning Loss: 90.89792442321777  Train_Reconstruction: 87.36581039428711  Train_KL: 3.5321137607097626  Validation Loss : 90.93399810791016 Val_Reconstruction : 87.4356460571289 Val_KL : 3.49835205078125\n","Epoch: 5429/8000  Traning Loss: 91.10096836090088  Train_Reconstruction: 87.57909107208252  Train_KL: 3.5218769907951355  Validation Loss : 91.05594635009766 Val_Reconstruction : 87.58846282958984 Val_KL : 3.4674863815307617\n","Epoch: 5430/8000  Traning Loss: 91.19159030914307  Train_Reconstruction: 87.68669033050537  Train_KL: 3.5048989057540894  Validation Loss : 91.10313415527344 Val_Reconstruction : 87.64177322387695 Val_KL : 3.4613606929779053\n","Epoch: 5431/8000  Traning Loss: 91.41420841217041  Train_Reconstruction: 87.89882850646973  Train_KL: 3.515379786491394  Validation Loss : 91.19094467163086 Val_Reconstruction : 87.70869827270508 Val_KL : 3.4822458028793335\n","Epoch: 5432/8000  Traning Loss: 90.9418773651123  Train_Reconstruction: 87.41623210906982  Train_KL: 3.5256455540657043  Validation Loss : 90.93031692504883 Val_Reconstruction : 87.44430541992188 Val_KL : 3.486010193824768\n","Epoch: 5433/8000  Traning Loss: 90.97465991973877  Train_Reconstruction: 87.45145511627197  Train_KL: 3.523204982280731  Validation Loss : 90.93084716796875 Val_Reconstruction : 87.45592498779297 Val_KL : 3.47492253780365\n","Epoch: 5434/8000  Traning Loss: 91.22177982330322  Train_Reconstruction: 87.71410369873047  Train_KL: 3.507675141096115  Validation Loss : 91.29022598266602 Val_Reconstruction : 87.82201766967773 Val_KL : 3.468207836151123\n","Epoch: 5435/8000  Traning Loss: 90.93466377258301  Train_Reconstruction: 87.42204666137695  Train_KL: 3.512617915868759  Validation Loss : 90.94065475463867 Val_Reconstruction : 87.45798110961914 Val_KL : 3.482673168182373\n","Epoch: 5436/8000  Traning Loss: 91.09825038909912  Train_Reconstruction: 87.55900764465332  Train_KL: 3.539242774248123  Validation Loss : 91.32929611206055 Val_Reconstruction : 87.82646179199219 Val_KL : 3.5028364658355713\n","Epoch: 5437/8000  Traning Loss: 91.21303844451904  Train_Reconstruction: 87.68775939941406  Train_KL: 3.525280326604843  Validation Loss : 91.36837387084961 Val_Reconstruction : 87.89506149291992 Val_KL : 3.473314046859741\n","Epoch: 5438/8000  Traning Loss: 90.83205318450928  Train_Reconstruction: 87.32403373718262  Train_KL: 3.50801882147789  Validation Loss : 90.5473861694336 Val_Reconstruction : 87.08229446411133 Val_KL : 3.465092897415161\n","Epoch: 5439/8000  Traning Loss: 90.70188617706299  Train_Reconstruction: 87.18643283843994  Train_KL: 3.5154542326927185  Validation Loss : 90.86465835571289 Val_Reconstruction : 87.37874221801758 Val_KL : 3.4859185218811035\n","Epoch: 5440/8000  Traning Loss: 90.64029788970947  Train_Reconstruction: 87.1089391708374  Train_KL: 3.531359523534775  Validation Loss : 90.62645721435547 Val_Reconstruction : 87.13384628295898 Val_KL : 3.4926135540008545\n","Epoch: 5441/8000  Traning Loss: 90.73028469085693  Train_Reconstruction: 87.20578479766846  Train_KL: 3.524500250816345  Validation Loss : 91.0423812866211 Val_Reconstruction : 87.56851959228516 Val_KL : 3.4738619327545166\n","Epoch: 5442/8000  Traning Loss: 91.18370342254639  Train_Reconstruction: 87.67233848571777  Train_KL: 3.5113640427589417  Validation Loss : 91.20198440551758 Val_Reconstruction : 87.73168182373047 Val_KL : 3.4703041315078735\n","Epoch: 5443/8000  Traning Loss: 91.23254776000977  Train_Reconstruction: 87.71114349365234  Train_KL: 3.5214041769504547  Validation Loss : 91.4053955078125 Val_Reconstruction : 87.91779327392578 Val_KL : 3.4876036643981934\n","Epoch: 5444/8000  Traning Loss: 91.21750736236572  Train_Reconstruction: 87.69009780883789  Train_KL: 3.5274108946323395  Validation Loss : 90.76640701293945 Val_Reconstruction : 87.28947448730469 Val_KL : 3.476933479309082\n","Epoch: 5445/8000  Traning Loss: 91.26100635528564  Train_Reconstruction: 87.74939823150635  Train_KL: 3.511607974767685  Validation Loss : 91.35837173461914 Val_Reconstruction : 87.89349365234375 Val_KL : 3.464877128601074\n","Epoch: 5446/8000  Traning Loss: 91.46690845489502  Train_Reconstruction: 87.95206069946289  Train_KL: 3.5148483514785767  Validation Loss : 91.62556457519531 Val_Reconstruction : 88.13879013061523 Val_KL : 3.4867721796035767\n","Epoch: 5447/8000  Traning Loss: 91.87479591369629  Train_Reconstruction: 88.34942245483398  Train_KL: 3.5253722965717316  Validation Loss : 91.44709014892578 Val_Reconstruction : 87.96368408203125 Val_KL : 3.4834084510803223\n","Epoch: 5448/8000  Traning Loss: 91.18097686767578  Train_Reconstruction: 87.66099834442139  Train_KL: 3.519978880882263  Validation Loss : 90.9459457397461 Val_Reconstruction : 87.4626235961914 Val_KL : 3.483320951461792\n","Epoch: 5449/8000  Traning Loss: 90.85279655456543  Train_Reconstruction: 87.34063148498535  Train_KL: 3.5121647715568542  Validation Loss : 90.76805877685547 Val_Reconstruction : 87.2991714477539 Val_KL : 3.4688867330551147\n","Epoch: 5450/8000  Traning Loss: 90.91141605377197  Train_Reconstruction: 87.39783191680908  Train_KL: 3.5135832726955414  Validation Loss : 90.99689865112305 Val_Reconstruction : 87.51962661743164 Val_KL : 3.477272868156433\n","Epoch: 5451/8000  Traning Loss: 90.99749565124512  Train_Reconstruction: 87.48033332824707  Train_KL: 3.5171628296375275  Validation Loss : 90.74938583374023 Val_Reconstruction : 87.27286911010742 Val_KL : 3.476515531539917\n","Epoch: 5452/8000  Traning Loss: 91.1511402130127  Train_Reconstruction: 87.633225440979  Train_KL: 3.5179154872894287  Validation Loss : 91.11628341674805 Val_Reconstruction : 87.64266204833984 Val_KL : 3.473620057106018\n","Epoch: 5453/8000  Traning Loss: 91.43030166625977  Train_Reconstruction: 87.9072093963623  Train_KL: 3.5230930745601654  Validation Loss : 91.58977890014648 Val_Reconstruction : 88.1034049987793 Val_KL : 3.4863736629486084\n","Epoch: 5454/8000  Traning Loss: 91.6895809173584  Train_Reconstruction: 88.17001438140869  Train_KL: 3.519567847251892  Validation Loss : 91.73273849487305 Val_Reconstruction : 88.2477912902832 Val_KL : 3.484949827194214\n","Epoch: 5455/8000  Traning Loss: 91.98767471313477  Train_Reconstruction: 88.46585750579834  Train_KL: 3.5218165814876556  Validation Loss : 91.4449462890625 Val_Reconstruction : 87.96048355102539 Val_KL : 3.4844608306884766\n","Epoch: 5456/8000  Traning Loss: 91.35451984405518  Train_Reconstruction: 87.82744121551514  Train_KL: 3.527079403400421  Validation Loss : 91.13445281982422 Val_Reconstruction : 87.64818572998047 Val_KL : 3.486264944076538\n","Epoch: 5457/8000  Traning Loss: 91.1315860748291  Train_Reconstruction: 87.61358165740967  Train_KL: 3.5180046260356903  Validation Loss : 91.11069107055664 Val_Reconstruction : 87.62981796264648 Val_KL : 3.4808714389801025\n","Epoch: 5458/8000  Traning Loss: 91.50194931030273  Train_Reconstruction: 87.9815034866333  Train_KL: 3.5204455852508545  Validation Loss : 91.4205093383789 Val_Reconstruction : 87.94009017944336 Val_KL : 3.4804199934005737\n","Epoch: 5459/8000  Traning Loss: 91.02024936676025  Train_Reconstruction: 87.50485229492188  Train_KL: 3.5153965055942535  Validation Loss : 90.90695571899414 Val_Reconstruction : 87.44568252563477 Val_KL : 3.461273670196533\n","Epoch: 5460/8000  Traning Loss: 91.2255163192749  Train_Reconstruction: 87.7113561630249  Train_KL: 3.5141602158546448  Validation Loss : 90.89717864990234 Val_Reconstruction : 87.42227172851562 Val_KL : 3.4749081134796143\n","Epoch: 5461/8000  Traning Loss: 90.74468994140625  Train_Reconstruction: 87.2280101776123  Train_KL: 3.5166799128055573  Validation Loss : 90.74347305297852 Val_Reconstruction : 87.26885604858398 Val_KL : 3.4746173620224\n","Epoch: 5462/8000  Traning Loss: 90.86974430084229  Train_Reconstruction: 87.34652996063232  Train_KL: 3.5232138633728027  Validation Loss : 91.16688537597656 Val_Reconstruction : 87.68181228637695 Val_KL : 3.485070586204529\n","Epoch: 5463/8000  Traning Loss: 91.01577091217041  Train_Reconstruction: 87.49197483062744  Train_KL: 3.523795425891876  Validation Loss : 90.93436050415039 Val_Reconstruction : 87.44951248168945 Val_KL : 3.4848480224609375\n","Epoch: 5464/8000  Traning Loss: 90.9677963256836  Train_Reconstruction: 87.44069385528564  Train_KL: 3.5271036326885223  Validation Loss : 91.04117202758789 Val_Reconstruction : 87.55701446533203 Val_KL : 3.484156847000122\n","Epoch: 5465/8000  Traning Loss: 90.97334003448486  Train_Reconstruction: 87.44962501525879  Train_KL: 3.5237148702144623  Validation Loss : 90.7951774597168 Val_Reconstruction : 87.31805419921875 Val_KL : 3.477125883102417\n","Epoch: 5466/8000  Traning Loss: 90.81956577301025  Train_Reconstruction: 87.29863739013672  Train_KL: 3.5209276378154755  Validation Loss : 90.91582107543945 Val_Reconstruction : 87.43939971923828 Val_KL : 3.476420044898987\n","Epoch: 5467/8000  Traning Loss: 90.8683090209961  Train_Reconstruction: 87.35370254516602  Train_KL: 3.5146077275276184  Validation Loss : 90.89643478393555 Val_Reconstruction : 87.42296981811523 Val_KL : 3.4734667539596558\n","Epoch: 5468/8000  Traning Loss: 90.8970308303833  Train_Reconstruction: 87.38320541381836  Train_KL: 3.513825833797455  Validation Loss : 90.9006462097168 Val_Reconstruction : 87.43002700805664 Val_KL : 3.470618724822998\n","Epoch: 5469/8000  Traning Loss: 91.11655044555664  Train_Reconstruction: 87.60029697418213  Train_KL: 3.5162529945373535  Validation Loss : 90.91756439208984 Val_Reconstruction : 87.42726516723633 Val_KL : 3.4902983903884888\n","Epoch: 5470/8000  Traning Loss: 90.84197807312012  Train_Reconstruction: 87.3206148147583  Train_KL: 3.521363526582718  Validation Loss : 90.6057357788086 Val_Reconstruction : 87.1214714050293 Val_KL : 3.4842623472213745\n","Epoch: 5471/8000  Traning Loss: 90.66555786132812  Train_Reconstruction: 87.14189910888672  Train_KL: 3.523658126592636  Validation Loss : 90.70863723754883 Val_Reconstruction : 87.21696472167969 Val_KL : 3.4916738271713257\n","Epoch: 5472/8000  Traning Loss: 90.92758178710938  Train_Reconstruction: 87.40691661834717  Train_KL: 3.520663797855377  Validation Loss : 90.97564315795898 Val_Reconstruction : 87.49346160888672 Val_KL : 3.482179641723633\n","Epoch: 5473/8000  Traning Loss: 90.7573938369751  Train_Reconstruction: 87.24377059936523  Train_KL: 3.5136236548423767  Validation Loss : 90.88274765014648 Val_Reconstruction : 87.41130065917969 Val_KL : 3.4714447259902954\n","Epoch: 5474/8000  Traning Loss: 90.62930393218994  Train_Reconstruction: 87.11714267730713  Train_KL: 3.51216059923172  Validation Loss : 90.76070022583008 Val_Reconstruction : 87.27656555175781 Val_KL : 3.4841352701187134\n","Epoch: 5475/8000  Traning Loss: 90.78974533081055  Train_Reconstruction: 87.26183605194092  Train_KL: 3.5279106497764587  Validation Loss : 91.04439544677734 Val_Reconstruction : 87.55333709716797 Val_KL : 3.491056799888611\n","Epoch: 5476/8000  Traning Loss: 90.70446872711182  Train_Reconstruction: 87.17848205566406  Train_KL: 3.5259870886802673  Validation Loss : 90.5534896850586 Val_Reconstruction : 87.07071304321289 Val_KL : 3.4827792644500732\n","Epoch: 5477/8000  Traning Loss: 90.7664852142334  Train_Reconstruction: 87.25056838989258  Train_KL: 3.515917420387268  Validation Loss : 90.73175430297852 Val_Reconstruction : 87.25955963134766 Val_KL : 3.4721962213516235\n","Epoch: 5478/8000  Traning Loss: 90.91745471954346  Train_Reconstruction: 87.40097904205322  Train_KL: 3.5164763629436493  Validation Loss : 91.04640197753906 Val_Reconstruction : 87.56427001953125 Val_KL : 3.4821345806121826\n","Epoch: 5479/8000  Traning Loss: 91.02011013031006  Train_Reconstruction: 87.49922370910645  Train_KL: 3.5208868384361267  Validation Loss : 90.78428268432617 Val_Reconstruction : 87.29508590698242 Val_KL : 3.4891966581344604\n","Epoch: 5480/8000  Traning Loss: 91.3302354812622  Train_Reconstruction: 87.81232643127441  Train_KL: 3.5179069340229034  Validation Loss : 91.3149642944336 Val_Reconstruction : 87.8368148803711 Val_KL : 3.4781497716903687\n","Epoch: 5481/8000  Traning Loss: 90.9077033996582  Train_Reconstruction: 87.38437461853027  Train_KL: 3.523329436779022  Validation Loss : 90.74407958984375 Val_Reconstruction : 87.25189208984375 Val_KL : 3.492187261581421\n","Epoch: 5482/8000  Traning Loss: 91.00651931762695  Train_Reconstruction: 87.4828634262085  Train_KL: 3.5236559212207794  Validation Loss : 91.26042938232422 Val_Reconstruction : 87.78206634521484 Val_KL : 3.4783618450164795\n","Epoch: 5483/8000  Traning Loss: 90.97947216033936  Train_Reconstruction: 87.4655294418335  Train_KL: 3.513942688703537  Validation Loss : 90.90139770507812 Val_Reconstruction : 87.422607421875 Val_KL : 3.4787893295288086\n","Epoch: 5484/8000  Traning Loss: 90.84124660491943  Train_Reconstruction: 87.3162899017334  Train_KL: 3.5249566733837128  Validation Loss : 91.06714630126953 Val_Reconstruction : 87.57936477661133 Val_KL : 3.487780809402466\n","Epoch: 5485/8000  Traning Loss: 91.10819053649902  Train_Reconstruction: 87.58504486083984  Train_KL: 3.5231460332870483  Validation Loss : 91.58069229125977 Val_Reconstruction : 88.10186004638672 Val_KL : 3.478834629058838\n","Epoch: 5486/8000  Traning Loss: 91.29408550262451  Train_Reconstruction: 87.77895641326904  Train_KL: 3.5151285231113434  Validation Loss : 91.05136489868164 Val_Reconstruction : 87.5715217590332 Val_KL : 3.4798452854156494\n","Epoch: 5487/8000  Traning Loss: 91.34629821777344  Train_Reconstruction: 87.83002471923828  Train_KL: 3.5162727534770966  Validation Loss : 91.50833129882812 Val_Reconstruction : 88.03185272216797 Val_KL : 3.4764771461486816\n","Epoch: 5488/8000  Traning Loss: 91.31907176971436  Train_Reconstruction: 87.79478359222412  Train_KL: 3.5242891907691956  Validation Loss : 91.22763442993164 Val_Reconstruction : 87.7376480102539 Val_KL : 3.489985704421997\n","Epoch: 5489/8000  Traning Loss: 90.92489051818848  Train_Reconstruction: 87.40447902679443  Train_KL: 3.520412415266037  Validation Loss : 90.85174560546875 Val_Reconstruction : 87.37320327758789 Val_KL : 3.478541851043701\n","Epoch: 5490/8000  Traning Loss: 91.02594757080078  Train_Reconstruction: 87.51168155670166  Train_KL: 3.5142648816108704  Validation Loss : 91.18180465698242 Val_Reconstruction : 87.70608520507812 Val_KL : 3.4757206439971924\n","Epoch: 5491/8000  Traning Loss: 91.15194606781006  Train_Reconstruction: 87.62692356109619  Train_KL: 3.525022029876709  Validation Loss : 90.8568344116211 Val_Reconstruction : 87.36552810668945 Val_KL : 3.491305947303772\n","Epoch: 5492/8000  Traning Loss: 91.034912109375  Train_Reconstruction: 87.51432132720947  Train_KL: 3.5205920338630676  Validation Loss : 91.00284576416016 Val_Reconstruction : 87.52558135986328 Val_KL : 3.4772634506225586\n","Epoch: 5493/8000  Traning Loss: 91.34197330474854  Train_Reconstruction: 87.82608222961426  Train_KL: 3.515890806913376  Validation Loss : 91.34444808959961 Val_Reconstruction : 87.86521530151367 Val_KL : 3.4792340993881226\n","Epoch: 5494/8000  Traning Loss: 91.28727912902832  Train_Reconstruction: 87.77358627319336  Train_KL: 3.5136934518814087  Validation Loss : 90.89728164672852 Val_Reconstruction : 87.42152404785156 Val_KL : 3.4757580757141113\n","Epoch: 5495/8000  Traning Loss: 90.96790790557861  Train_Reconstruction: 87.45470142364502  Train_KL: 3.513205885887146  Validation Loss : 90.89080810546875 Val_Reconstruction : 87.41804504394531 Val_KL : 3.472764015197754\n","Epoch: 5496/8000  Traning Loss: 91.19378471374512  Train_Reconstruction: 87.6709394454956  Train_KL: 3.5228436291217804  Validation Loss : 91.44272232055664 Val_Reconstruction : 87.9616470336914 Val_KL : 3.4810770750045776\n","Epoch: 5497/8000  Traning Loss: 91.50064182281494  Train_Reconstruction: 87.9755334854126  Train_KL: 3.5251100659370422  Validation Loss : 91.05144882202148 Val_Reconstruction : 87.56481170654297 Val_KL : 3.4866392612457275\n","Epoch: 5498/8000  Traning Loss: 91.12525272369385  Train_Reconstruction: 87.60164070129395  Train_KL: 3.5236129462718964  Validation Loss : 91.18985366821289 Val_Reconstruction : 87.7135238647461 Val_KL : 3.4763295650482178\n","Epoch: 5499/8000  Traning Loss: 91.15842914581299  Train_Reconstruction: 87.64586639404297  Train_KL: 3.5125631392002106  Validation Loss : 91.11001586914062 Val_Reconstruction : 87.63086318969727 Val_KL : 3.4791547060012817\n","Epoch: 5500/8000  Traning Loss: 91.47040939331055  Train_Reconstruction: 87.9507646560669  Train_KL: 3.5196457505226135  Validation Loss : 91.57016372680664 Val_Reconstruction : 88.08784103393555 Val_KL : 3.4823241233825684\n","Epoch: 5501/8000  Traning Loss: 91.73687076568604  Train_Reconstruction: 88.22288131713867  Train_KL: 3.513989269733429  Validation Loss : 91.77085494995117 Val_Reconstruction : 88.28950119018555 Val_KL : 3.4813551902770996\n","Epoch: 5502/8000  Traning Loss: 91.41380882263184  Train_Reconstruction: 87.89902782440186  Train_KL: 3.514781653881073  Validation Loss : 91.3068962097168 Val_Reconstruction : 87.8264045715332 Val_KL : 3.4804913997650146\n","Epoch: 5503/8000  Traning Loss: 91.32730865478516  Train_Reconstruction: 87.80368041992188  Train_KL: 3.5236282646656036  Validation Loss : 91.04434585571289 Val_Reconstruction : 87.5491828918457 Val_KL : 3.49516224861145\n","Epoch: 5504/8000  Traning Loss: 90.99702167510986  Train_Reconstruction: 87.47402763366699  Train_KL: 3.522993892431259  Validation Loss : 91.02360916137695 Val_Reconstruction : 87.55368423461914 Val_KL : 3.4699231386184692\n","Epoch: 5505/8000  Traning Loss: 91.22677803039551  Train_Reconstruction: 87.72480773925781  Train_KL: 3.5019706189632416  Validation Loss : 90.75529861450195 Val_Reconstruction : 87.2911376953125 Val_KL : 3.464162230491638\n","Epoch: 5506/8000  Traning Loss: 91.4398136138916  Train_Reconstruction: 87.91671180725098  Train_KL: 3.5231029093265533  Validation Loss : 91.49919509887695 Val_Reconstruction : 88.01485824584961 Val_KL : 3.484337091445923\n","Epoch: 5507/8000  Traning Loss: 91.28978061676025  Train_Reconstruction: 87.7631425857544  Train_KL: 3.526638448238373  Validation Loss : 91.23409271240234 Val_Reconstruction : 87.7553482055664 Val_KL : 3.478742480278015\n","Epoch: 5508/8000  Traning Loss: 91.26784324645996  Train_Reconstruction: 87.74675273895264  Train_KL: 3.5210905969142914  Validation Loss : 91.05782318115234 Val_Reconstruction : 87.58250427246094 Val_KL : 3.475318193435669\n","Epoch: 5509/8000  Traning Loss: 90.80208206176758  Train_Reconstruction: 87.27977275848389  Train_KL: 3.52231028676033  Validation Loss : 90.93862915039062 Val_Reconstruction : 87.44978713989258 Val_KL : 3.4888439178466797\n","Epoch: 5510/8000  Traning Loss: 90.79952716827393  Train_Reconstruction: 87.27740287780762  Train_KL: 3.5221242904663086  Validation Loss : 90.85843276977539 Val_Reconstruction : 87.37589645385742 Val_KL : 3.4825366735458374\n","Epoch: 5511/8000  Traning Loss: 91.02173709869385  Train_Reconstruction: 87.49578475952148  Train_KL: 3.5259521305561066  Validation Loss : 91.17602920532227 Val_Reconstruction : 87.69408798217773 Val_KL : 3.4819415807724\n","Epoch: 5512/8000  Traning Loss: 91.0285177230835  Train_Reconstruction: 87.50670433044434  Train_KL: 3.5218135714530945  Validation Loss : 91.20150375366211 Val_Reconstruction : 87.72076797485352 Val_KL : 3.4807374477386475\n","Epoch: 5513/8000  Traning Loss: 91.15720176696777  Train_Reconstruction: 87.63403701782227  Train_KL: 3.523164004087448  Validation Loss : 90.83351516723633 Val_Reconstruction : 87.34671020507812 Val_KL : 3.4868050813674927\n","Epoch: 5514/8000  Traning Loss: 90.81556129455566  Train_Reconstruction: 87.2922887802124  Train_KL: 3.5232724845409393  Validation Loss : 91.00765228271484 Val_Reconstruction : 87.5274429321289 Val_KL : 3.480210065841675\n","Epoch: 5515/8000  Traning Loss: 91.03297519683838  Train_Reconstruction: 87.51308727264404  Train_KL: 3.5198879837989807  Validation Loss : 90.92761611938477 Val_Reconstruction : 87.45332336425781 Val_KL : 3.474295973777771\n","Epoch: 5516/8000  Traning Loss: 90.94277381896973  Train_Reconstruction: 87.43714046478271  Train_KL: 3.5056329369544983  Validation Loss : 90.96522521972656 Val_Reconstruction : 87.49065399169922 Val_KL : 3.4745707511901855\n","Epoch: 5517/8000  Traning Loss: 91.43188667297363  Train_Reconstruction: 87.91851043701172  Train_KL: 3.5133771002292633  Validation Loss : 91.89915084838867 Val_Reconstruction : 88.41535186767578 Val_KL : 3.4838002920150757\n","Epoch: 5518/8000  Traning Loss: 91.51845359802246  Train_Reconstruction: 87.99760913848877  Train_KL: 3.520844727754593  Validation Loss : 91.60507583618164 Val_Reconstruction : 88.12220764160156 Val_KL : 3.4828689098358154\n","Epoch: 5519/8000  Traning Loss: 91.45860576629639  Train_Reconstruction: 87.94400882720947  Train_KL: 3.514596253633499  Validation Loss : 91.25385665893555 Val_Reconstruction : 87.77405166625977 Val_KL : 3.4798061847686768\n","Epoch: 5520/8000  Traning Loss: 91.14233779907227  Train_Reconstruction: 87.62525272369385  Train_KL: 3.517084389925003  Validation Loss : 90.79823684692383 Val_Reconstruction : 87.32114791870117 Val_KL : 3.477088212966919\n","Epoch: 5521/8000  Traning Loss: 90.93455123901367  Train_Reconstruction: 87.41356754302979  Train_KL: 3.520983248949051  Validation Loss : 90.8360710144043 Val_Reconstruction : 87.3526611328125 Val_KL : 3.483412504196167\n","Epoch: 5522/8000  Traning Loss: 90.86253070831299  Train_Reconstruction: 87.34061431884766  Train_KL: 3.5219154357910156  Validation Loss : 90.64227294921875 Val_Reconstruction : 87.15439224243164 Val_KL : 3.487883687019348\n","Epoch: 5523/8000  Traning Loss: 90.87669658660889  Train_Reconstruction: 87.3543815612793  Train_KL: 3.5223162174224854  Validation Loss : 90.77850341796875 Val_Reconstruction : 87.30292510986328 Val_KL : 3.4755762815475464\n","Epoch: 5524/8000  Traning Loss: 90.72955989837646  Train_Reconstruction: 87.21843147277832  Train_KL: 3.511128783226013  Validation Loss : 90.6328239440918 Val_Reconstruction : 87.159423828125 Val_KL : 3.473400592803955\n","Epoch: 5525/8000  Traning Loss: 90.78490543365479  Train_Reconstruction: 87.26681137084961  Train_KL: 3.518093526363373  Validation Loss : 90.64373397827148 Val_Reconstruction : 87.16236877441406 Val_KL : 3.4813653230667114\n","Epoch: 5526/8000  Traning Loss: 90.58112525939941  Train_Reconstruction: 87.05686092376709  Train_KL: 3.5242635905742645  Validation Loss : 90.48246765136719 Val_Reconstruction : 87.00436782836914 Val_KL : 3.478099226951599\n","Epoch: 5527/8000  Traning Loss: 90.51799774169922  Train_Reconstruction: 86.9946517944336  Train_KL: 3.523346185684204  Validation Loss : 90.6165771484375 Val_Reconstruction : 87.1360969543457 Val_KL : 3.4804794788360596\n","Epoch: 5528/8000  Traning Loss: 90.78513050079346  Train_Reconstruction: 87.26585102081299  Train_KL: 3.5192800164222717  Validation Loss : 91.05240249633789 Val_Reconstruction : 87.57453155517578 Val_KL : 3.477870464324951\n","Epoch: 5529/8000  Traning Loss: 91.43458938598633  Train_Reconstruction: 87.91052627563477  Train_KL: 3.524061918258667  Validation Loss : 91.44719696044922 Val_Reconstruction : 87.95792007446289 Val_KL : 3.4892752170562744\n","Epoch: 5530/8000  Traning Loss: 91.23094749450684  Train_Reconstruction: 87.70094108581543  Train_KL: 3.5300065875053406  Validation Loss : 90.9405746459961 Val_Reconstruction : 87.45689010620117 Val_KL : 3.483684539794922\n","Epoch: 5531/8000  Traning Loss: 90.92512130737305  Train_Reconstruction: 87.40235233306885  Train_KL: 3.522767812013626  Validation Loss : 90.7239875793457 Val_Reconstruction : 87.24962615966797 Val_KL : 3.47436261177063\n","Epoch: 5532/8000  Traning Loss: 91.19992256164551  Train_Reconstruction: 87.68793106079102  Train_KL: 3.5119916796684265  Validation Loss : 91.3936882019043 Val_Reconstruction : 87.91620254516602 Val_KL : 3.477485775947571\n","Epoch: 5533/8000  Traning Loss: 91.55571746826172  Train_Reconstruction: 88.03955936431885  Train_KL: 3.516157418489456  Validation Loss : 91.64119338989258 Val_Reconstruction : 88.1669807434082 Val_KL : 3.4742106199264526\n","Epoch: 5534/8000  Traning Loss: 91.34606075286865  Train_Reconstruction: 87.83066463470459  Train_KL: 3.5153965055942535  Validation Loss : 91.55442810058594 Val_Reconstruction : 88.06573486328125 Val_KL : 3.488691568374634\n","Epoch: 5535/8000  Traning Loss: 91.08765888214111  Train_Reconstruction: 87.56518459320068  Train_KL: 3.5224743485450745  Validation Loss : 91.05822372436523 Val_Reconstruction : 87.57975769042969 Val_KL : 3.4784690141677856\n","Epoch: 5536/8000  Traning Loss: 90.87909507751465  Train_Reconstruction: 87.36701107025146  Train_KL: 3.5120840966701508  Validation Loss : 90.78881454467773 Val_Reconstruction : 87.3173713684082 Val_KL : 3.4714425802230835\n","Epoch: 5537/8000  Traning Loss: 91.02768230438232  Train_Reconstruction: 87.51157569885254  Train_KL: 3.5161075592041016  Validation Loss : 90.86885452270508 Val_Reconstruction : 87.39128112792969 Val_KL : 3.4775766134262085\n","Epoch: 5538/8000  Traning Loss: 91.33094501495361  Train_Reconstruction: 87.79748058319092  Train_KL: 3.5334635376930237  Validation Loss : 91.37536239624023 Val_Reconstruction : 87.87828063964844 Val_KL : 3.4970797300338745\n","Epoch: 5539/8000  Traning Loss: 91.31640720367432  Train_Reconstruction: 87.78799819946289  Train_KL: 3.5284085273742676  Validation Loss : 91.17033767700195 Val_Reconstruction : 87.6902961730957 Val_KL : 3.480039954185486\n","Epoch: 5540/8000  Traning Loss: 91.0545301437378  Train_Reconstruction: 87.53937149047852  Train_KL: 3.5151579678058624  Validation Loss : 91.2256088256836 Val_Reconstruction : 87.75112533569336 Val_KL : 3.4744831323623657\n","Epoch: 5541/8000  Traning Loss: 91.41500186920166  Train_Reconstruction: 87.89560413360596  Train_KL: 3.5193973779678345  Validation Loss : 92.26292419433594 Val_Reconstruction : 88.77267456054688 Val_KL : 3.490249514579773\n","Epoch: 5542/8000  Traning Loss: 91.4709701538086  Train_Reconstruction: 87.94495391845703  Train_KL: 3.5260149538517  Validation Loss : 91.4752311706543 Val_Reconstruction : 87.98847961425781 Val_KL : 3.4867515563964844\n","Epoch: 5543/8000  Traning Loss: 90.96009635925293  Train_Reconstruction: 87.44019412994385  Train_KL: 3.519901782274246  Validation Loss : 90.82239151000977 Val_Reconstruction : 87.34929656982422 Val_KL : 3.4730920791625977\n","Epoch: 5544/8000  Traning Loss: 90.83981227874756  Train_Reconstruction: 87.33430862426758  Train_KL: 3.5055030584335327  Validation Loss : 90.49469757080078 Val_Reconstruction : 87.02974319458008 Val_KL : 3.4649561643600464\n","Epoch: 5545/8000  Traning Loss: 90.72800159454346  Train_Reconstruction: 87.21387767791748  Train_KL: 3.5141237676143646  Validation Loss : 90.9606704711914 Val_Reconstruction : 87.48115539550781 Val_KL : 3.4795135259628296\n","Epoch: 5546/8000  Traning Loss: 90.87422943115234  Train_Reconstruction: 87.35303497314453  Train_KL: 3.5211939811706543  Validation Loss : 90.78842544555664 Val_Reconstruction : 87.30143356323242 Val_KL : 3.486994981765747\n","Epoch: 5547/8000  Traning Loss: 90.9988145828247  Train_Reconstruction: 87.4755277633667  Train_KL: 3.523285388946533  Validation Loss : 91.01988983154297 Val_Reconstruction : 87.53576278686523 Val_KL : 3.484124541282654\n","Epoch: 5548/8000  Traning Loss: 91.27201557159424  Train_Reconstruction: 87.75403881072998  Train_KL: 3.517976552248001  Validation Loss : 90.80769729614258 Val_Reconstruction : 87.33335494995117 Val_KL : 3.474343419075012\n","Epoch: 5549/8000  Traning Loss: 90.9221773147583  Train_Reconstruction: 87.4036512374878  Train_KL: 3.5185258090496063  Validation Loss : 90.8118782043457 Val_Reconstruction : 87.32913589477539 Val_KL : 3.482744336128235\n","Epoch: 5550/8000  Traning Loss: 90.74311065673828  Train_Reconstruction: 87.22260856628418  Train_KL: 3.520502358675003  Validation Loss : 90.52177429199219 Val_Reconstruction : 87.04556274414062 Val_KL : 3.476209044456482\n","Epoch: 5551/8000  Traning Loss: 90.77835845947266  Train_Reconstruction: 87.26648044586182  Train_KL: 3.5118777453899384  Validation Loss : 90.68061828613281 Val_Reconstruction : 87.21131896972656 Val_KL : 3.469298243522644\n","Epoch: 5552/8000  Traning Loss: 91.03774356842041  Train_Reconstruction: 87.52674293518066  Train_KL: 3.5109995305538177  Validation Loss : 90.69964981079102 Val_Reconstruction : 87.22834777832031 Val_KL : 3.471301794052124\n","Epoch: 5553/8000  Traning Loss: 90.47490787506104  Train_Reconstruction: 86.9603271484375  Train_KL: 3.514581322669983  Validation Loss : 90.33448791503906 Val_Reconstruction : 86.85050201416016 Val_KL : 3.4839850664138794\n","Epoch: 5554/8000  Traning Loss: 90.68462467193604  Train_Reconstruction: 87.15654373168945  Train_KL: 3.528081327676773  Validation Loss : 90.89859390258789 Val_Reconstruction : 87.4085578918457 Val_KL : 3.4900333881378174\n","Epoch: 5555/8000  Traning Loss: 91.0975227355957  Train_Reconstruction: 87.57036113739014  Train_KL: 3.527160495519638  Validation Loss : 91.0921516418457 Val_Reconstruction : 87.60970306396484 Val_KL : 3.482448935508728\n","Epoch: 5556/8000  Traning Loss: 91.14510154724121  Train_Reconstruction: 87.6234769821167  Train_KL: 3.521623969078064  Validation Loss : 91.29928588867188 Val_Reconstruction : 87.8278694152832 Val_KL : 3.471415638923645\n","Epoch: 5557/8000  Traning Loss: 90.89450168609619  Train_Reconstruction: 87.37705612182617  Train_KL: 3.517444282770157  Validation Loss : 90.84910202026367 Val_Reconstruction : 87.37341690063477 Val_KL : 3.475682497024536\n","Epoch: 5558/8000  Traning Loss: 90.77910614013672  Train_Reconstruction: 87.2609453201294  Train_KL: 3.518159419298172  Validation Loss : 90.69147491455078 Val_Reconstruction : 87.21564483642578 Val_KL : 3.4758304357528687\n","Epoch: 5559/8000  Traning Loss: 90.65311241149902  Train_Reconstruction: 87.13390350341797  Train_KL: 3.5192075967788696  Validation Loss : 90.88717651367188 Val_Reconstruction : 87.4120864868164 Val_KL : 3.475089907646179\n","Epoch: 5560/8000  Traning Loss: 90.77355670928955  Train_Reconstruction: 87.25450325012207  Train_KL: 3.519053041934967  Validation Loss : 90.90180969238281 Val_Reconstruction : 87.42118453979492 Val_KL : 3.4806238412857056\n","Epoch: 5561/8000  Traning Loss: 90.54104232788086  Train_Reconstruction: 87.01832008361816  Train_KL: 3.5227208137512207  Validation Loss : 90.54461669921875 Val_Reconstruction : 87.0601806640625 Val_KL : 3.4844350814819336\n","Epoch: 5562/8000  Traning Loss: 90.85543441772461  Train_Reconstruction: 87.3278636932373  Train_KL: 3.5275714695453644  Validation Loss : 91.13636016845703 Val_Reconstruction : 87.64933013916016 Val_KL : 3.48702871799469\n","Epoch: 5563/8000  Traning Loss: 91.13812828063965  Train_Reconstruction: 87.61489772796631  Train_KL: 3.5232317745685577  Validation Loss : 90.98391723632812 Val_Reconstruction : 87.50802612304688 Val_KL : 3.475890278816223\n","Epoch: 5564/8000  Traning Loss: 91.08809852600098  Train_Reconstruction: 87.56671237945557  Train_KL: 3.5213860869407654  Validation Loss : 91.03428268432617 Val_Reconstruction : 87.55571746826172 Val_KL : 3.4785624742507935\n","Epoch: 5565/8000  Traning Loss: 90.91615104675293  Train_Reconstruction: 87.38780784606934  Train_KL: 3.528343826532364  Validation Loss : 91.01028060913086 Val_Reconstruction : 87.5309944152832 Val_KL : 3.4792879819869995\n","Epoch: 5566/8000  Traning Loss: 91.16936588287354  Train_Reconstruction: 87.64954280853271  Train_KL: 3.5198226273059845  Validation Loss : 91.223388671875 Val_Reconstruction : 87.75186157226562 Val_KL : 3.4715276956558228\n","Epoch: 5567/8000  Traning Loss: 90.7930097579956  Train_Reconstruction: 87.28058624267578  Train_KL: 3.5124236941337585  Validation Loss : 90.71012878417969 Val_Reconstruction : 87.23995971679688 Val_KL : 3.4701712131500244\n","Epoch: 5568/8000  Traning Loss: 90.55427837371826  Train_Reconstruction: 87.04096126556396  Train_KL: 3.5133176743984222  Validation Loss : 90.67107391357422 Val_Reconstruction : 87.18830108642578 Val_KL : 3.4827741384506226\n","Epoch: 5569/8000  Traning Loss: 90.92867279052734  Train_Reconstruction: 87.40228843688965  Train_KL: 3.5263847410678864  Validation Loss : 91.15973281860352 Val_Reconstruction : 87.67397689819336 Val_KL : 3.4857542514801025\n","Epoch: 5570/8000  Traning Loss: 91.59596920013428  Train_Reconstruction: 88.07052230834961  Train_KL: 3.525446265935898  Validation Loss : 91.9765510559082 Val_Reconstruction : 88.4907112121582 Val_KL : 3.48583984375\n","Epoch: 5571/8000  Traning Loss: 92.01402950286865  Train_Reconstruction: 88.50085544586182  Train_KL: 3.513174891471863  Validation Loss : 91.94675064086914 Val_Reconstruction : 88.47321319580078 Val_KL : 3.47353732585907\n","Epoch: 5572/8000  Traning Loss: 91.7114028930664  Train_Reconstruction: 88.19567489624023  Train_KL: 3.515728682279587  Validation Loss : 91.31308364868164 Val_Reconstruction : 87.83031845092773 Val_KL : 3.482762098312378\n","Epoch: 5573/8000  Traning Loss: 91.10685062408447  Train_Reconstruction: 87.58565330505371  Train_KL: 3.5211972296237946  Validation Loss : 91.01075744628906 Val_Reconstruction : 87.53023147583008 Val_KL : 3.4805270433425903\n","Epoch: 5574/8000  Traning Loss: 90.88766288757324  Train_Reconstruction: 87.37130928039551  Train_KL: 3.516353130340576  Validation Loss : 90.63862228393555 Val_Reconstruction : 87.17654418945312 Val_KL : 3.4620797634124756\n","Epoch: 5575/8000  Traning Loss: 90.7129259109497  Train_Reconstruction: 87.2083854675293  Train_KL: 3.5045403242111206  Validation Loss : 90.77654647827148 Val_Reconstruction : 87.3120346069336 Val_KL : 3.464513063430786\n","Epoch: 5576/8000  Traning Loss: 91.64775466918945  Train_Reconstruction: 88.122802734375  Train_KL: 3.524951547384262  Validation Loss : 91.70392608642578 Val_Reconstruction : 88.2122573852539 Val_KL : 3.4916683435440063\n","Epoch: 5577/8000  Traning Loss: 91.71567916870117  Train_Reconstruction: 88.18352603912354  Train_KL: 3.532151997089386  Validation Loss : 91.2860221862793 Val_Reconstruction : 87.79919052124023 Val_KL : 3.486830949783325\n","Epoch: 5578/8000  Traning Loss: 91.15528297424316  Train_Reconstruction: 87.63700866699219  Train_KL: 3.518275201320648  Validation Loss : 90.75850677490234 Val_Reconstruction : 87.2824592590332 Val_KL : 3.4760444164276123\n","Epoch: 5579/8000  Traning Loss: 90.97376155853271  Train_Reconstruction: 87.45144271850586  Train_KL: 3.5223195254802704  Validation Loss : 90.88716125488281 Val_Reconstruction : 87.40417098999023 Val_KL : 3.4829903841018677\n","Epoch: 5580/8000  Traning Loss: 90.9718713760376  Train_Reconstruction: 87.45436954498291  Train_KL: 3.517501264810562  Validation Loss : 91.12605285644531 Val_Reconstruction : 87.65203857421875 Val_KL : 3.4740147590637207\n","Epoch: 5581/8000  Traning Loss: 91.21794700622559  Train_Reconstruction: 87.69794273376465  Train_KL: 3.5200049579143524  Validation Loss : 91.25320816040039 Val_Reconstruction : 87.77645874023438 Val_KL : 3.476751446723938\n","Epoch: 5582/8000  Traning Loss: 91.25478744506836  Train_Reconstruction: 87.73468971252441  Train_KL: 3.520098030567169  Validation Loss : 91.32674407958984 Val_Reconstruction : 87.84843063354492 Val_KL : 3.4783118963241577\n","Epoch: 5583/8000  Traning Loss: 91.10525512695312  Train_Reconstruction: 87.58943748474121  Train_KL: 3.5158175826072693  Validation Loss : 90.97958755493164 Val_Reconstruction : 87.51432037353516 Val_KL : 3.4652669429779053\n","Epoch: 5584/8000  Traning Loss: 90.9226303100586  Train_Reconstruction: 87.41345405578613  Train_KL: 3.50917711853981  Validation Loss : 91.16750717163086 Val_Reconstruction : 87.69382095336914 Val_KL : 3.4736870527267456\n","Epoch: 5585/8000  Traning Loss: 91.2883882522583  Train_Reconstruction: 87.76504707336426  Train_KL: 3.5233422815799713  Validation Loss : 91.42138671875 Val_Reconstruction : 87.93782806396484 Val_KL : 3.4835574626922607\n","Epoch: 5586/8000  Traning Loss: 91.26697063446045  Train_Reconstruction: 87.74444580078125  Train_KL: 3.5225249528884888  Validation Loss : 91.43999099731445 Val_Reconstruction : 87.95611190795898 Val_KL : 3.4838777780532837\n","Epoch: 5587/8000  Traning Loss: 91.01163673400879  Train_Reconstruction: 87.48771476745605  Train_KL: 3.523922383785248  Validation Loss : 90.94429397583008 Val_Reconstruction : 87.47593688964844 Val_KL : 3.468357801437378\n","Epoch: 5588/8000  Traning Loss: 91.09174251556396  Train_Reconstruction: 87.57935333251953  Train_KL: 3.51238951086998  Validation Loss : 91.73025512695312 Val_Reconstruction : 88.25363540649414 Val_KL : 3.4766182899475098\n","Epoch: 5589/8000  Traning Loss: 91.28735733032227  Train_Reconstruction: 87.76284313201904  Train_KL: 3.5245143473148346  Validation Loss : 91.05617141723633 Val_Reconstruction : 87.57376480102539 Val_KL : 3.482407808303833\n","Epoch: 5590/8000  Traning Loss: 90.9841136932373  Train_Reconstruction: 87.4650068283081  Train_KL: 3.5191054940223694  Validation Loss : 90.78615951538086 Val_Reconstruction : 87.31663131713867 Val_KL : 3.469528555870056\n","Epoch: 5591/8000  Traning Loss: 90.75127029418945  Train_Reconstruction: 87.23885726928711  Train_KL: 3.512413263320923  Validation Loss : 90.81713485717773 Val_Reconstruction : 87.34343719482422 Val_KL : 3.4737006425857544\n","Epoch: 5592/8000  Traning Loss: 90.86636352539062  Train_Reconstruction: 87.34618091583252  Train_KL: 3.52018404006958  Validation Loss : 90.69354248046875 Val_Reconstruction : 87.20521926879883 Val_KL : 3.488323211669922\n","Epoch: 5593/8000  Traning Loss: 90.70130443572998  Train_Reconstruction: 87.18636417388916  Train_KL: 3.5149398744106293  Validation Loss : 90.71688461303711 Val_Reconstruction : 87.2477798461914 Val_KL : 3.4691051244735718\n","Epoch: 5594/8000  Traning Loss: 90.85010719299316  Train_Reconstruction: 87.33434772491455  Train_KL: 3.515760600566864  Validation Loss : 90.7562026977539 Val_Reconstruction : 87.27355575561523 Val_KL : 3.482648491859436\n","Epoch: 5595/8000  Traning Loss: 90.66854953765869  Train_Reconstruction: 87.14556407928467  Train_KL: 3.522984892129898  Validation Loss : 90.64241027832031 Val_Reconstruction : 87.15865325927734 Val_KL : 3.483754873275757\n","Epoch: 5596/8000  Traning Loss: 90.58613967895508  Train_Reconstruction: 87.06031799316406  Train_KL: 3.5258208513259888  Validation Loss : 90.49168395996094 Val_Reconstruction : 87.01010513305664 Val_KL : 3.481577515602112\n","Epoch: 5597/8000  Traning Loss: 90.39663505554199  Train_Reconstruction: 86.87382411956787  Train_KL: 3.5228101313114166  Validation Loss : 90.28341674804688 Val_Reconstruction : 86.8102912902832 Val_KL : 3.4731245040893555\n","Epoch: 5598/8000  Traning Loss: 90.4846601486206  Train_Reconstruction: 86.96285152435303  Train_KL: 3.52180814743042  Validation Loss : 90.59151458740234 Val_Reconstruction : 87.10913467407227 Val_KL : 3.4823802709579468\n","Epoch: 5599/8000  Traning Loss: 90.86248302459717  Train_Reconstruction: 87.3391056060791  Train_KL: 3.523377239704132  Validation Loss : 91.0494613647461 Val_Reconstruction : 87.5672492980957 Val_KL : 3.4822131395339966\n","Epoch: 5600/8000  Traning Loss: 91.30040454864502  Train_Reconstruction: 87.77969646453857  Train_KL: 3.520707607269287  Validation Loss : 91.23859405517578 Val_Reconstruction : 87.75639343261719 Val_KL : 3.4821990728378296\n","Epoch: 5601/8000  Traning Loss: 91.42647743225098  Train_Reconstruction: 87.90354251861572  Train_KL: 3.5229349434375763  Validation Loss : 91.34590148925781 Val_Reconstruction : 87.86964416503906 Val_KL : 3.476255774497986\n","Epoch: 5602/8000  Traning Loss: 91.15538692474365  Train_Reconstruction: 87.63765525817871  Train_KL: 3.5177309215068817  Validation Loss : 90.77521896362305 Val_Reconstruction : 87.29431915283203 Val_KL : 3.4808980226516724\n","Epoch: 5603/8000  Traning Loss: 91.00544261932373  Train_Reconstruction: 87.48036098480225  Train_KL: 3.525082051753998  Validation Loss : 91.3868522644043 Val_Reconstruction : 87.89434051513672 Val_KL : 3.492509961128235\n","Epoch: 5604/8000  Traning Loss: 91.11024379730225  Train_Reconstruction: 87.58832550048828  Train_KL: 3.521918147802353  Validation Loss : 91.25930786132812 Val_Reconstruction : 87.78152465820312 Val_KL : 3.477782964706421\n","Epoch: 5605/8000  Traning Loss: 91.0489559173584  Train_Reconstruction: 87.52419185638428  Train_KL: 3.5247645676136017  Validation Loss : 90.94795608520508 Val_Reconstruction : 87.45859909057617 Val_KL : 3.489357829093933\n","Epoch: 5606/8000  Traning Loss: 90.93545913696289  Train_Reconstruction: 87.41853427886963  Train_KL: 3.5169256031513214  Validation Loss : 91.18833923339844 Val_Reconstruction : 87.71559524536133 Val_KL : 3.4727442264556885\n","Epoch: 5607/8000  Traning Loss: 91.18232154846191  Train_Reconstruction: 87.65477085113525  Train_KL: 3.5275521278381348  Validation Loss : 91.26852798461914 Val_Reconstruction : 87.78747177124023 Val_KL : 3.481057643890381\n","Epoch: 5608/8000  Traning Loss: 90.8864517211914  Train_Reconstruction: 87.36249542236328  Train_KL: 3.523956298828125  Validation Loss : 91.13700485229492 Val_Reconstruction : 87.66184997558594 Val_KL : 3.4751561880111694\n","Epoch: 5609/8000  Traning Loss: 90.71346664428711  Train_Reconstruction: 87.19727325439453  Train_KL: 3.5161933600902557  Validation Loss : 90.53644180297852 Val_Reconstruction : 87.057373046875 Val_KL : 3.4790695905685425\n","Epoch: 5610/8000  Traning Loss: 90.74478530883789  Train_Reconstruction: 87.22545528411865  Train_KL: 3.519329696893692  Validation Loss : 90.65045547485352 Val_Reconstruction : 87.17198181152344 Val_KL : 3.4784724712371826\n","Epoch: 5611/8000  Traning Loss: 90.72003269195557  Train_Reconstruction: 87.19522666931152  Train_KL: 3.524807035923004  Validation Loss : 90.58773040771484 Val_Reconstruction : 87.10492706298828 Val_KL : 3.4828038215637207\n","Epoch: 5612/8000  Traning Loss: 90.78996276855469  Train_Reconstruction: 87.26017761230469  Train_KL: 3.5297840237617493  Validation Loss : 91.29444885253906 Val_Reconstruction : 87.80835342407227 Val_KL : 3.4860951900482178\n","Epoch: 5613/8000  Traning Loss: 91.0257215499878  Train_Reconstruction: 87.49789524078369  Train_KL: 3.527826577425003  Validation Loss : 91.13478469848633 Val_Reconstruction : 87.64881896972656 Val_KL : 3.4859654903411865\n","Epoch: 5614/8000  Traning Loss: 91.07261085510254  Train_Reconstruction: 87.54830551147461  Train_KL: 3.5243042707443237  Validation Loss : 91.05662155151367 Val_Reconstruction : 87.57768630981445 Val_KL : 3.478933811187744\n","Epoch: 5615/8000  Traning Loss: 90.75553894042969  Train_Reconstruction: 87.23306846618652  Train_KL: 3.5224705040454865  Validation Loss : 90.89094543457031 Val_Reconstruction : 87.40452194213867 Val_KL : 3.4864258766174316\n","Epoch: 5616/8000  Traning Loss: 90.59350109100342  Train_Reconstruction: 87.0719861984253  Train_KL: 3.5215146839618683  Validation Loss : 90.52803039550781 Val_Reconstruction : 87.04270935058594 Val_KL : 3.485321044921875\n","Epoch: 5617/8000  Traning Loss: 90.55083274841309  Train_Reconstruction: 87.02441215515137  Train_KL: 3.5264204144477844  Validation Loss : 90.53117752075195 Val_Reconstruction : 87.04602813720703 Val_KL : 3.4851489067077637\n","Epoch: 5618/8000  Traning Loss: 90.99619579315186  Train_Reconstruction: 87.47586727142334  Train_KL: 3.5203283429145813  Validation Loss : 91.31534957885742 Val_Reconstruction : 87.83454132080078 Val_KL : 3.480806350708008\n","Epoch: 5619/8000  Traning Loss: 91.6539306640625  Train_Reconstruction: 88.1315279006958  Train_KL: 3.5224027931690216  Validation Loss : 91.3935317993164 Val_Reconstruction : 87.9028091430664 Val_KL : 3.4907232522964478\n","Epoch: 5620/8000  Traning Loss: 91.05560398101807  Train_Reconstruction: 87.53497409820557  Train_KL: 3.5206294655799866  Validation Loss : 90.6879653930664 Val_Reconstruction : 87.21188354492188 Val_KL : 3.476083278656006\n","Epoch: 5621/8000  Traning Loss: 90.81347370147705  Train_Reconstruction: 87.30063915252686  Train_KL: 3.512835532426834  Validation Loss : 91.0074462890625 Val_Reconstruction : 87.52909851074219 Val_KL : 3.4783473014831543\n","Epoch: 5622/8000  Traning Loss: 91.60764122009277  Train_Reconstruction: 88.08985233306885  Train_KL: 3.51778843998909  Validation Loss : 91.9916877746582 Val_Reconstruction : 88.51615905761719 Val_KL : 3.4755269289016724\n","Epoch: 5623/8000  Traning Loss: 91.52475070953369  Train_Reconstruction: 88.00267028808594  Train_KL: 3.5220803022384644  Validation Loss : 91.19503784179688 Val_Reconstruction : 87.70690155029297 Val_KL : 3.4881370067596436\n","Epoch: 5624/8000  Traning Loss: 91.06923770904541  Train_Reconstruction: 87.54432773590088  Train_KL: 3.524909645318985  Validation Loss : 90.95812606811523 Val_Reconstruction : 87.47937393188477 Val_KL : 3.4787522554397583\n","Epoch: 5625/8000  Traning Loss: 90.66114044189453  Train_Reconstruction: 87.14134311676025  Train_KL: 3.519797205924988  Validation Loss : 90.79248428344727 Val_Reconstruction : 87.30879592895508 Val_KL : 3.4836881160736084\n","Epoch: 5626/8000  Traning Loss: 90.83447074890137  Train_Reconstruction: 87.30776596069336  Train_KL: 3.5267040133476257  Validation Loss : 90.87505722045898 Val_Reconstruction : 87.38312530517578 Val_KL : 3.4919320344924927\n","Epoch: 5627/8000  Traning Loss: 90.55231380462646  Train_Reconstruction: 87.02302169799805  Train_KL: 3.5292910039424896  Validation Loss : 90.52204513549805 Val_Reconstruction : 87.03609466552734 Val_KL : 3.4859505891799927\n","Epoch: 5628/8000  Traning Loss: 90.79476928710938  Train_Reconstruction: 87.2687873840332  Train_KL: 3.525981307029724  Validation Loss : 90.98258972167969 Val_Reconstruction : 87.49873733520508 Val_KL : 3.4838547706604004\n","Epoch: 5629/8000  Traning Loss: 91.0625171661377  Train_Reconstruction: 87.54008865356445  Train_KL: 3.522427350282669  Validation Loss : 90.88607788085938 Val_Reconstruction : 87.40058898925781 Val_KL : 3.4854884147644043\n","Epoch: 5630/8000  Traning Loss: 90.9352674484253  Train_Reconstruction: 87.4072437286377  Train_KL: 3.528022825717926  Validation Loss : 90.74882125854492 Val_Reconstruction : 87.26935958862305 Val_KL : 3.479461669921875\n","Epoch: 5631/8000  Traning Loss: 90.7508134841919  Train_Reconstruction: 87.22666454315186  Train_KL: 3.5241492092609406  Validation Loss : 90.78995132446289 Val_Reconstruction : 87.3132209777832 Val_KL : 3.4767287969589233\n","Epoch: 5632/8000  Traning Loss: 91.03368282318115  Train_Reconstruction: 87.51421356201172  Train_KL: 3.519469529390335  Validation Loss : 90.91307830810547 Val_Reconstruction : 87.4304313659668 Val_KL : 3.4826472997665405\n","Epoch: 5633/8000  Traning Loss: 90.92035102844238  Train_Reconstruction: 87.39288806915283  Train_KL: 3.527462661266327  Validation Loss : 90.57777786254883 Val_Reconstruction : 87.0936393737793 Val_KL : 3.484135866165161\n","Epoch: 5634/8000  Traning Loss: 90.44845867156982  Train_Reconstruction: 86.92502403259277  Train_KL: 3.5234343111515045  Validation Loss : 90.51211166381836 Val_Reconstruction : 87.03678894042969 Val_KL : 3.4753226041793823\n","Epoch: 5635/8000  Traning Loss: 90.61245536804199  Train_Reconstruction: 87.09250545501709  Train_KL: 3.519950360059738  Validation Loss : 90.65802001953125 Val_Reconstruction : 87.1751708984375 Val_KL : 3.4828485250473022\n","Epoch: 5636/8000  Traning Loss: 90.76669788360596  Train_Reconstruction: 87.24096488952637  Train_KL: 3.5257323384284973  Validation Loss : 90.79933166503906 Val_Reconstruction : 87.31676483154297 Val_KL : 3.48256778717041\n","Epoch: 5637/8000  Traning Loss: 90.69148063659668  Train_Reconstruction: 87.16817378997803  Train_KL: 3.5233077108860016  Validation Loss : 90.69830703735352 Val_Reconstruction : 87.2189712524414 Val_KL : 3.479336977005005\n","Epoch: 5638/8000  Traning Loss: 90.92814636230469  Train_Reconstruction: 87.41114711761475  Train_KL: 3.5170001089572906  Validation Loss : 91.01322174072266 Val_Reconstruction : 87.54206085205078 Val_KL : 3.4711602926254272\n","Epoch: 5639/8000  Traning Loss: 90.9770278930664  Train_Reconstruction: 87.46765518188477  Train_KL: 3.5093740224838257  Validation Loss : 90.7470588684082 Val_Reconstruction : 87.28308868408203 Val_KL : 3.463970899581909\n","Epoch: 5640/8000  Traning Loss: 90.79324436187744  Train_Reconstruction: 87.27515697479248  Train_KL: 3.518085867166519  Validation Loss : 90.7551155090332 Val_Reconstruction : 87.27352142333984 Val_KL : 3.481594443321228\n","Epoch: 5641/8000  Traning Loss: 91.03277111053467  Train_Reconstruction: 87.50465965270996  Train_KL: 3.5281118750572205  Validation Loss : 91.27240371704102 Val_Reconstruction : 87.79145431518555 Val_KL : 3.480951189994812\n","Epoch: 5642/8000  Traning Loss: 91.0796308517456  Train_Reconstruction: 87.55204677581787  Train_KL: 3.5275829136371613  Validation Loss : 90.62874221801758 Val_Reconstruction : 87.13962936401367 Val_KL : 3.4891103506088257\n","Epoch: 5643/8000  Traning Loss: 90.7313289642334  Train_Reconstruction: 87.20478057861328  Train_KL: 3.5265473425388336  Validation Loss : 90.57416152954102 Val_Reconstruction : 87.0957145690918 Val_KL : 3.478448987007141\n","Epoch: 5644/8000  Traning Loss: 90.76885890960693  Train_Reconstruction: 87.24783134460449  Train_KL: 3.5210278630256653  Validation Loss : 90.54023742675781 Val_Reconstruction : 87.0560188293457 Val_KL : 3.4842182397842407\n","Epoch: 5645/8000  Traning Loss: 90.91144561767578  Train_Reconstruction: 87.38825035095215  Train_KL: 3.523195266723633  Validation Loss : 91.1474838256836 Val_Reconstruction : 87.66321182250977 Val_KL : 3.4842708110809326\n","Epoch: 5646/8000  Traning Loss: 91.13964366912842  Train_Reconstruction: 87.6138391494751  Train_KL: 3.525805115699768  Validation Loss : 91.10784149169922 Val_Reconstruction : 87.62638092041016 Val_KL : 3.4814612865448\n","Epoch: 5647/8000  Traning Loss: 90.89196014404297  Train_Reconstruction: 87.37920951843262  Train_KL: 3.512751668691635  Validation Loss : 90.81587600708008 Val_Reconstruction : 87.34346389770508 Val_KL : 3.472410202026367\n","Epoch: 5648/8000  Traning Loss: 90.8723783493042  Train_Reconstruction: 87.35838603973389  Train_KL: 3.513992190361023  Validation Loss : 90.68898010253906 Val_Reconstruction : 87.20782089233398 Val_KL : 3.4811595678329468\n","Epoch: 5649/8000  Traning Loss: 90.90880966186523  Train_Reconstruction: 87.3977689743042  Train_KL: 3.511039525270462  Validation Loss : 90.89510726928711 Val_Reconstruction : 87.42195129394531 Val_KL : 3.473155975341797\n","Epoch: 5650/8000  Traning Loss: 90.62012195587158  Train_Reconstruction: 87.10306549072266  Train_KL: 3.5170571208000183  Validation Loss : 90.55418014526367 Val_Reconstruction : 87.07964706420898 Val_KL : 3.474531650543213\n","Epoch: 5651/8000  Traning Loss: 90.66588878631592  Train_Reconstruction: 87.14646625518799  Train_KL: 3.519421637058258  Validation Loss : 91.1318588256836 Val_Reconstruction : 87.65384674072266 Val_KL : 3.478013038635254\n","Epoch: 5652/8000  Traning Loss: 91.17269515991211  Train_Reconstruction: 87.6525993347168  Train_KL: 3.52009579539299  Validation Loss : 91.33501434326172 Val_Reconstruction : 87.8562126159668 Val_KL : 3.4788014888763428\n","Epoch: 5653/8000  Traning Loss: 91.2814588546753  Train_Reconstruction: 87.76472568511963  Train_KL: 3.516734093427658  Validation Loss : 91.42634963989258 Val_Reconstruction : 87.9536361694336 Val_KL : 3.472716450691223\n","Epoch: 5654/8000  Traning Loss: 91.74878978729248  Train_Reconstruction: 88.23275947570801  Train_KL: 3.5160292387008667  Validation Loss : 91.26136016845703 Val_Reconstruction : 87.79312896728516 Val_KL : 3.4682310819625854\n","Epoch: 5655/8000  Traning Loss: 91.55588436126709  Train_Reconstruction: 88.03818416595459  Train_KL: 3.5176989436149597  Validation Loss : 91.51876449584961 Val_Reconstruction : 88.03914642333984 Val_KL : 3.4796184301376343\n","Epoch: 5656/8000  Traning Loss: 91.46771812438965  Train_Reconstruction: 87.94776725769043  Train_KL: 3.519951969385147  Validation Loss : 91.06173324584961 Val_Reconstruction : 87.58391571044922 Val_KL : 3.4778189659118652\n","Epoch: 5657/8000  Traning Loss: 90.98004913330078  Train_Reconstruction: 87.46347904205322  Train_KL: 3.5165699124336243  Validation Loss : 91.22064590454102 Val_Reconstruction : 87.7442512512207 Val_KL : 3.4763951301574707\n","Epoch: 5658/8000  Traning Loss: 91.04557418823242  Train_Reconstruction: 87.52533435821533  Train_KL: 3.520238161087036  Validation Loss : 90.8520278930664 Val_Reconstruction : 87.37432479858398 Val_KL : 3.4777055978775024\n","Epoch: 5659/8000  Traning Loss: 90.9210844039917  Train_Reconstruction: 87.39806938171387  Train_KL: 3.5230157375335693  Validation Loss : 90.6947021484375 Val_Reconstruction : 87.2019271850586 Val_KL : 3.492776036262512\n","Epoch: 5660/8000  Traning Loss: 90.5948486328125  Train_Reconstruction: 87.06731796264648  Train_KL: 3.527530938386917  Validation Loss : 90.6364974975586 Val_Reconstruction : 87.1484603881836 Val_KL : 3.4880353212356567\n","Epoch: 5661/8000  Traning Loss: 90.56447219848633  Train_Reconstruction: 87.03702545166016  Train_KL: 3.5274471044540405  Validation Loss : 90.72470474243164 Val_Reconstruction : 87.24167251586914 Val_KL : 3.483033537864685\n","Epoch: 5662/8000  Traning Loss: 90.71656227111816  Train_Reconstruction: 87.19706344604492  Train_KL: 3.5194990932941437  Validation Loss : 91.01657485961914 Val_Reconstruction : 87.54063415527344 Val_KL : 3.4759405851364136\n","Epoch: 5663/8000  Traning Loss: 90.61231136322021  Train_Reconstruction: 87.09842872619629  Train_KL: 3.5138811469078064  Validation Loss : 90.50402450561523 Val_Reconstruction : 87.0235710144043 Val_KL : 3.4804508686065674\n","Epoch: 5664/8000  Traning Loss: 90.78243732452393  Train_Reconstruction: 87.26297569274902  Train_KL: 3.5194608867168427  Validation Loss : 91.09501266479492 Val_Reconstruction : 87.6073226928711 Val_KL : 3.4876922369003296\n","Epoch: 5665/8000  Traning Loss: 90.85940074920654  Train_Reconstruction: 87.33170413970947  Train_KL: 3.527695596218109  Validation Loss : 91.03636169433594 Val_Reconstruction : 87.55339050292969 Val_KL : 3.4829684495925903\n","Epoch: 5666/8000  Traning Loss: 91.32541942596436  Train_Reconstruction: 87.80865478515625  Train_KL: 3.516762852668762  Validation Loss : 91.66324996948242 Val_Reconstruction : 88.19366455078125 Val_KL : 3.4695866107940674\n","Epoch: 5667/8000  Traning Loss: 91.63193988800049  Train_Reconstruction: 88.11667537689209  Train_KL: 3.515265017747879  Validation Loss : 91.71386337280273 Val_Reconstruction : 88.24317169189453 Val_KL : 3.4706932306289673\n","Epoch: 5668/8000  Traning Loss: 91.20893383026123  Train_Reconstruction: 87.68639659881592  Train_KL: 3.52253794670105  Validation Loss : 90.94866943359375 Val_Reconstruction : 87.46812438964844 Val_KL : 3.480546474456787\n","Epoch: 5669/8000  Traning Loss: 91.05949020385742  Train_Reconstruction: 87.52740859985352  Train_KL: 3.5320809185504913  Validation Loss : 91.48144912719727 Val_Reconstruction : 87.98953628540039 Val_KL : 3.491912603378296\n","Epoch: 5670/8000  Traning Loss: 91.0439863204956  Train_Reconstruction: 87.51145839691162  Train_KL: 3.532527983188629  Validation Loss : 90.86880493164062 Val_Reconstruction : 87.38451766967773 Val_KL : 3.4842852354049683\n","Epoch: 5671/8000  Traning Loss: 91.11173439025879  Train_Reconstruction: 87.59296798706055  Train_KL: 3.518767088651657  Validation Loss : 91.68882369995117 Val_Reconstruction : 88.21610641479492 Val_KL : 3.47271990776062\n","Epoch: 5672/8000  Traning Loss: 91.07069492340088  Train_Reconstruction: 87.55085849761963  Train_KL: 3.5198372900485992  Validation Loss : 90.93886184692383 Val_Reconstruction : 87.46463775634766 Val_KL : 3.4742228984832764\n","Epoch: 5673/8000  Traning Loss: 90.76638698577881  Train_Reconstruction: 87.24772071838379  Train_KL: 3.5186664760112762  Validation Loss : 90.7219467163086 Val_Reconstruction : 87.23881149291992 Val_KL : 3.483135223388672\n","Epoch: 5674/8000  Traning Loss: 90.77587032318115  Train_Reconstruction: 87.253173828125  Train_KL: 3.5226952135562897  Validation Loss : 90.82790756225586 Val_Reconstruction : 87.3480224609375 Val_KL : 3.479887008666992\n","Epoch: 5675/8000  Traning Loss: 91.06196117401123  Train_Reconstruction: 87.53182029724121  Train_KL: 3.5301407873630524  Validation Loss : 91.01659774780273 Val_Reconstruction : 87.52120971679688 Val_KL : 3.495386838912964\n","Epoch: 5676/8000  Traning Loss: 91.10554313659668  Train_Reconstruction: 87.58020973205566  Train_KL: 3.5253329277038574  Validation Loss : 91.52317428588867 Val_Reconstruction : 88.04471206665039 Val_KL : 3.478462338447571\n","Epoch: 5677/8000  Traning Loss: 90.79722690582275  Train_Reconstruction: 87.2750473022461  Train_KL: 3.5221784114837646  Validation Loss : 90.7063980102539 Val_Reconstruction : 87.22350692749023 Val_KL : 3.482889413833618\n","Epoch: 5678/8000  Traning Loss: 90.50686740875244  Train_Reconstruction: 86.99199199676514  Train_KL: 3.514875501394272  Validation Loss : 90.48527526855469 Val_Reconstruction : 87.0160026550293 Val_KL : 3.4692718982696533\n","Epoch: 5679/8000  Traning Loss: 90.56874084472656  Train_Reconstruction: 87.05566883087158  Train_KL: 3.513072282075882  Validation Loss : 90.30109405517578 Val_Reconstruction : 86.8152084350586 Val_KL : 3.485883355140686\n","Epoch: 5680/8000  Traning Loss: 90.6397533416748  Train_Reconstruction: 87.11590480804443  Train_KL: 3.5238478779792786  Validation Loss : 90.67619705200195 Val_Reconstruction : 87.19552993774414 Val_KL : 3.480666756629944\n","Epoch: 5681/8000  Traning Loss: 90.86148262023926  Train_Reconstruction: 87.34263038635254  Train_KL: 3.5188517570495605  Validation Loss : 90.9611587524414 Val_Reconstruction : 87.48794555664062 Val_KL : 3.473212718963623\n","Epoch: 5682/8000  Traning Loss: 90.78717136383057  Train_Reconstruction: 87.27159881591797  Train_KL: 3.5155718326568604  Validation Loss : 90.82676315307617 Val_Reconstruction : 87.33806610107422 Val_KL : 3.48870050907135\n","Epoch: 5683/8000  Traning Loss: 90.53382682800293  Train_Reconstruction: 87.0046033859253  Train_KL: 3.5292222797870636  Validation Loss : 90.54684448242188 Val_Reconstruction : 87.06583023071289 Val_KL : 3.4810123443603516\n","Epoch: 5684/8000  Traning Loss: 90.93608093261719  Train_Reconstruction: 87.41485023498535  Train_KL: 3.5212323665618896  Validation Loss : 91.22009658813477 Val_Reconstruction : 87.73978042602539 Val_KL : 3.480316162109375\n","Epoch: 5685/8000  Traning Loss: 90.77172183990479  Train_Reconstruction: 87.25881385803223  Train_KL: 3.5129085779190063  Validation Loss : 90.86084747314453 Val_Reconstruction : 87.39230728149414 Val_KL : 3.4685380458831787\n","Epoch: 5686/8000  Traning Loss: 90.90801334381104  Train_Reconstruction: 87.38466453552246  Train_KL: 3.5233489274978638  Validation Loss : 91.3488883972168 Val_Reconstruction : 87.86190032958984 Val_KL : 3.4869909286499023\n","Epoch: 5687/8000  Traning Loss: 91.16433238983154  Train_Reconstruction: 87.63315105438232  Train_KL: 3.53118172287941  Validation Loss : 90.95755004882812 Val_Reconstruction : 87.47279357910156 Val_KL : 3.484757900238037\n","Epoch: 5688/8000  Traning Loss: 90.90535354614258  Train_Reconstruction: 87.38491249084473  Train_KL: 3.5204412639141083  Validation Loss : 90.68607711791992 Val_Reconstruction : 87.20654678344727 Val_KL : 3.4795295000076294\n","Epoch: 5689/8000  Traning Loss: 90.63906764984131  Train_Reconstruction: 87.12072467803955  Train_KL: 3.5183423161506653  Validation Loss : 90.86813735961914 Val_Reconstruction : 87.38124465942383 Val_KL : 3.486891984939575\n","Epoch: 5690/8000  Traning Loss: 91.07844543457031  Train_Reconstruction: 87.55407619476318  Train_KL: 3.5243686139583588  Validation Loss : 91.14854049682617 Val_Reconstruction : 87.66973876953125 Val_KL : 3.4788005352020264\n","Epoch: 5691/8000  Traning Loss: 90.92192077636719  Train_Reconstruction: 87.40754508972168  Train_KL: 3.5143770277500153  Validation Loss : 90.79695510864258 Val_Reconstruction : 87.32510757446289 Val_KL : 3.471846342086792\n","Epoch: 5692/8000  Traning Loss: 90.80290699005127  Train_Reconstruction: 87.28713893890381  Train_KL: 3.5157682299613953  Validation Loss : 90.71819305419922 Val_Reconstruction : 87.24292373657227 Val_KL : 3.4752696752548218\n","Epoch: 5693/8000  Traning Loss: 90.53482151031494  Train_Reconstruction: 87.01126194000244  Train_KL: 3.523559421300888  Validation Loss : 90.41695785522461 Val_Reconstruction : 86.9351577758789 Val_KL : 3.4818004369735718\n","Epoch: 5694/8000  Traning Loss: 90.51495742797852  Train_Reconstruction: 86.99265956878662  Train_KL: 3.522298753261566  Validation Loss : 90.38003158569336 Val_Reconstruction : 86.90178298950195 Val_KL : 3.478246808052063\n","Epoch: 5695/8000  Traning Loss: 90.47006416320801  Train_Reconstruction: 86.94351482391357  Train_KL: 3.5265514254570007  Validation Loss : 90.4046516418457 Val_Reconstruction : 86.91307067871094 Val_KL : 3.4915801286697388\n","Epoch: 5696/8000  Traning Loss: 90.52069187164307  Train_Reconstruction: 86.9955186843872  Train_KL: 3.525174140930176  Validation Loss : 90.55150985717773 Val_Reconstruction : 87.06847381591797 Val_KL : 3.483034133911133\n","Epoch: 5697/8000  Traning Loss: 90.50364589691162  Train_Reconstruction: 86.98314952850342  Train_KL: 3.5204951465129852  Validation Loss : 90.48632049560547 Val_Reconstruction : 87.01051330566406 Val_KL : 3.4758074283599854\n","Epoch: 5698/8000  Traning Loss: 90.55256938934326  Train_Reconstruction: 87.02711486816406  Train_KL: 3.5254534482955933  Validation Loss : 90.4538688659668 Val_Reconstruction : 86.95792388916016 Val_KL : 3.495943546295166\n","Epoch: 5699/8000  Traning Loss: 90.6616153717041  Train_Reconstruction: 87.13427829742432  Train_KL: 3.5273363888263702  Validation Loss : 90.79574966430664 Val_Reconstruction : 87.31623840332031 Val_KL : 3.4795106649398804\n","Epoch: 5700/8000  Traning Loss: 90.7866153717041  Train_Reconstruction: 87.27205562591553  Train_KL: 3.5145602226257324  Validation Loss : 90.74200439453125 Val_Reconstruction : 87.26788711547852 Val_KL : 3.4741179943084717\n","Epoch: 5701/8000  Traning Loss: 90.96273040771484  Train_Reconstruction: 87.44553756713867  Train_KL: 3.517192989587784  Validation Loss : 90.86993789672852 Val_Reconstruction : 87.39418411254883 Val_KL : 3.475752592086792\n","Epoch: 5702/8000  Traning Loss: 90.74267196655273  Train_Reconstruction: 87.21760368347168  Train_KL: 3.5250673294067383  Validation Loss : 90.75223159790039 Val_Reconstruction : 87.26446914672852 Val_KL : 3.487760901451111\n","Epoch: 5703/8000  Traning Loss: 90.69504261016846  Train_Reconstruction: 87.1673812866211  Train_KL: 3.527662366628647  Validation Loss : 90.60520935058594 Val_Reconstruction : 87.12540435791016 Val_KL : 3.4798043966293335\n","Epoch: 5704/8000  Traning Loss: 90.82380771636963  Train_Reconstruction: 87.30346298217773  Train_KL: 3.520344704389572  Validation Loss : 91.05926132202148 Val_Reconstruction : 87.57541275024414 Val_KL : 3.483849048614502\n","Epoch: 5705/8000  Traning Loss: 90.8725471496582  Train_Reconstruction: 87.34433364868164  Train_KL: 3.528213620185852  Validation Loss : 90.98916625976562 Val_Reconstruction : 87.506591796875 Val_KL : 3.4825724363327026\n","Epoch: 5706/8000  Traning Loss: 91.52208232879639  Train_Reconstruction: 88.00468158721924  Train_KL: 3.517400026321411  Validation Loss : 91.69071197509766 Val_Reconstruction : 88.21309280395508 Val_KL : 3.477622389793396\n","Epoch: 5707/8000  Traning Loss: 91.583740234375  Train_Reconstruction: 88.07105922698975  Train_KL: 3.512679696083069  Validation Loss : 91.25884246826172 Val_Reconstruction : 87.78747940063477 Val_KL : 3.4713618755340576\n","Epoch: 5708/8000  Traning Loss: 91.3523530960083  Train_Reconstruction: 87.8403730392456  Train_KL: 3.5119796991348267  Validation Loss : 91.76030349731445 Val_Reconstruction : 88.28288650512695 Val_KL : 3.4774152040481567\n","Epoch: 5709/8000  Traning Loss: 91.12561225891113  Train_Reconstruction: 87.61023139953613  Train_KL: 3.5153802037239075  Validation Loss : 90.95210647583008 Val_Reconstruction : 87.48010635375977 Val_KL : 3.4720007181167603\n","Epoch: 5710/8000  Traning Loss: 91.00027275085449  Train_Reconstruction: 87.48322772979736  Train_KL: 3.5170445144176483  Validation Loss : 91.45252227783203 Val_Reconstruction : 87.97450256347656 Val_KL : 3.4780181646347046\n","Epoch: 5711/8000  Traning Loss: 91.29871463775635  Train_Reconstruction: 87.7742977142334  Train_KL: 3.524417132139206  Validation Loss : 91.1886100769043 Val_Reconstruction : 87.70569610595703 Val_KL : 3.482913374900818\n","Epoch: 5712/8000  Traning Loss: 91.05463790893555  Train_Reconstruction: 87.52805995941162  Train_KL: 3.5265784561634064  Validation Loss : 90.98388671875 Val_Reconstruction : 87.49983215332031 Val_KL : 3.484057307243347\n","Epoch: 5713/8000  Traning Loss: 90.5332670211792  Train_Reconstruction: 87.01587200164795  Train_KL: 3.517394334077835  Validation Loss : 90.22214889526367 Val_Reconstruction : 86.75471878051758 Val_KL : 3.4674302339553833\n","Epoch: 5714/8000  Traning Loss: 90.91426658630371  Train_Reconstruction: 87.39668655395508  Train_KL: 3.5175803303718567  Validation Loss : 91.21097183227539 Val_Reconstruction : 87.73515701293945 Val_KL : 3.4758139848709106\n","Epoch: 5715/8000  Traning Loss: 90.6600570678711  Train_Reconstruction: 87.14200115203857  Train_KL: 3.5180540680885315  Validation Loss : 90.56764221191406 Val_Reconstruction : 87.09053802490234 Val_KL : 3.4771043062210083\n","Epoch: 5716/8000  Traning Loss: 90.58574104309082  Train_Reconstruction: 87.06249713897705  Train_KL: 3.523244082927704  Validation Loss : 90.85976028442383 Val_Reconstruction : 87.36921310424805 Val_KL : 3.4905478954315186\n","Epoch: 5717/8000  Traning Loss: 91.25377082824707  Train_Reconstruction: 87.72607517242432  Train_KL: 3.5276953279972076  Validation Loss : 91.52477264404297 Val_Reconstruction : 88.04237365722656 Val_KL : 3.482398271560669\n","Epoch: 5718/8000  Traning Loss: 92.26126003265381  Train_Reconstruction: 88.74864292144775  Train_KL: 3.5126168727874756  Validation Loss : 92.77909469604492 Val_Reconstruction : 89.3051643371582 Val_KL : 3.473930597305298\n","Epoch: 5719/8000  Traning Loss: 91.65888118743896  Train_Reconstruction: 88.13961791992188  Train_KL: 3.5192650258541107  Validation Loss : 91.17350769042969 Val_Reconstruction : 87.69402313232422 Val_KL : 3.479485511779785\n","Epoch: 5720/8000  Traning Loss: 90.92691612243652  Train_Reconstruction: 87.41102123260498  Train_KL: 3.515894114971161  Validation Loss : 90.61581802368164 Val_Reconstruction : 87.14273452758789 Val_KL : 3.4730833768844604\n","Epoch: 5721/8000  Traning Loss: 90.47382259368896  Train_Reconstruction: 86.94919872283936  Train_KL: 3.5246239006519318  Validation Loss : 90.55728149414062 Val_Reconstruction : 87.07483291625977 Val_KL : 3.4824479818344116\n","Epoch: 5722/8000  Traning Loss: 90.6846752166748  Train_Reconstruction: 87.16482734680176  Train_KL: 3.5198475420475006  Validation Loss : 90.92106628417969 Val_Reconstruction : 87.44644546508789 Val_KL : 3.474620223045349\n","Epoch: 5723/8000  Traning Loss: 91.01287269592285  Train_Reconstruction: 87.49091815948486  Train_KL: 3.521955281496048  Validation Loss : 91.1690788269043 Val_Reconstruction : 87.68601608276367 Val_KL : 3.4830621480941772\n","Epoch: 5724/8000  Traning Loss: 91.74487495422363  Train_Reconstruction: 88.21701145172119  Train_KL: 3.5278638303279877  Validation Loss : 92.54831314086914 Val_Reconstruction : 89.0613784790039 Val_KL : 3.4869349002838135\n","Epoch: 5725/8000  Traning Loss: 91.7500410079956  Train_Reconstruction: 88.23200130462646  Train_KL: 3.5180384516716003  Validation Loss : 91.41160202026367 Val_Reconstruction : 87.94286727905273 Val_KL : 3.468736410140991\n","Epoch: 5726/8000  Traning Loss: 91.02448558807373  Train_Reconstruction: 87.50343036651611  Train_KL: 3.521056443452835  Validation Loss : 90.86166000366211 Val_Reconstruction : 87.37984466552734 Val_KL : 3.481813430786133\n","Epoch: 5727/8000  Traning Loss: 90.83004951477051  Train_Reconstruction: 87.3105821609497  Train_KL: 3.5194668769836426  Validation Loss : 90.63070678710938 Val_Reconstruction : 87.16190338134766 Val_KL : 3.468805432319641\n","Epoch: 5728/8000  Traning Loss: 90.86914825439453  Train_Reconstruction: 87.35744094848633  Train_KL: 3.511707901954651  Validation Loss : 90.94661331176758 Val_Reconstruction : 87.47231674194336 Val_KL : 3.474296808242798\n","Epoch: 5729/8000  Traning Loss: 90.96380615234375  Train_Reconstruction: 87.45062446594238  Train_KL: 3.5131810009479523  Validation Loss : 91.07242202758789 Val_Reconstruction : 87.6042366027832 Val_KL : 3.468185305595398\n","Epoch: 5730/8000  Traning Loss: 91.67678737640381  Train_Reconstruction: 88.15798950195312  Train_KL: 3.518797278404236  Validation Loss : 91.47224044799805 Val_Reconstruction : 87.99396896362305 Val_KL : 3.478272557258606\n","Epoch: 5731/8000  Traning Loss: 91.60161590576172  Train_Reconstruction: 88.0806884765625  Train_KL: 3.5209275484085083  Validation Loss : 91.07236862182617 Val_Reconstruction : 87.59637451171875 Val_KL : 3.4759910106658936\n","Epoch: 5732/8000  Traning Loss: 90.84839534759521  Train_Reconstruction: 87.32832908630371  Train_KL: 3.5200666189193726  Validation Loss : 90.80179214477539 Val_Reconstruction : 87.33197402954102 Val_KL : 3.469816207885742\n","Epoch: 5733/8000  Traning Loss: 90.7309741973877  Train_Reconstruction: 87.21316909790039  Train_KL: 3.517805427312851  Validation Loss : 90.67098236083984 Val_Reconstruction : 87.18865203857422 Val_KL : 3.482328176498413\n","Epoch: 5734/8000  Traning Loss: 90.70872783660889  Train_Reconstruction: 87.17702579498291  Train_KL: 3.531699985265732  Validation Loss : 90.37760543823242 Val_Reconstruction : 86.89220809936523 Val_KL : 3.485398530960083\n","Epoch: 5735/8000  Traning Loss: 90.28864097595215  Train_Reconstruction: 86.76152038574219  Train_KL: 3.5271206200122833  Validation Loss : 90.22896194458008 Val_Reconstruction : 86.73977279663086 Val_KL : 3.4891905784606934\n","Epoch: 5736/8000  Traning Loss: 90.4740161895752  Train_Reconstruction: 86.96166229248047  Train_KL: 3.5123544931411743  Validation Loss : 90.58955383300781 Val_Reconstruction : 87.12181854248047 Val_KL : 3.4677369594573975\n","Epoch: 5737/8000  Traning Loss: 90.48767566680908  Train_Reconstruction: 86.96819972991943  Train_KL: 3.5194752514362335  Validation Loss : 90.46691131591797 Val_Reconstruction : 86.98385620117188 Val_KL : 3.4830578565597534\n","Epoch: 5738/8000  Traning Loss: 90.48608589172363  Train_Reconstruction: 86.95547008514404  Train_KL: 3.530615955591202  Validation Loss : 90.48809432983398 Val_Reconstruction : 86.99933242797852 Val_KL : 3.4887622594833374\n","Epoch: 5739/8000  Traning Loss: 90.51050662994385  Train_Reconstruction: 86.98927974700928  Train_KL: 3.5212274193763733  Validation Loss : 90.50854110717773 Val_Reconstruction : 87.04083251953125 Val_KL : 3.4677098989486694\n","Epoch: 5740/8000  Traning Loss: 90.74206352233887  Train_Reconstruction: 87.22101879119873  Train_KL: 3.5210455656051636  Validation Loss : 91.02296829223633 Val_Reconstruction : 87.53992080688477 Val_KL : 3.4830474853515625\n","Epoch: 5741/8000  Traning Loss: 90.89233303070068  Train_Reconstruction: 87.3604736328125  Train_KL: 3.531857669353485  Validation Loss : 91.09395217895508 Val_Reconstruction : 87.59709930419922 Val_KL : 3.4968526363372803\n","Epoch: 5742/8000  Traning Loss: 91.32273483276367  Train_Reconstruction: 87.79307556152344  Train_KL: 3.5296586453914642  Validation Loss : 90.88163375854492 Val_Reconstruction : 87.4001579284668 Val_KL : 3.4814741611480713\n","Epoch: 5743/8000  Traning Loss: 90.90109920501709  Train_Reconstruction: 87.3884162902832  Train_KL: 3.5126833617687225  Validation Loss : 90.99481201171875 Val_Reconstruction : 87.51867294311523 Val_KL : 3.476139187812805\n","Epoch: 5744/8000  Traning Loss: 90.94319820404053  Train_Reconstruction: 87.4295129776001  Train_KL: 3.5136856138706207  Validation Loss : 90.86020278930664 Val_Reconstruction : 87.3817024230957 Val_KL : 3.478498935699463\n","Epoch: 5745/8000  Traning Loss: 90.71002101898193  Train_Reconstruction: 87.18290042877197  Train_KL: 3.5271207690238953  Validation Loss : 90.46284484863281 Val_Reconstruction : 86.97898864746094 Val_KL : 3.4838536977767944\n","Epoch: 5746/8000  Traning Loss: 90.59664154052734  Train_Reconstruction: 87.08309936523438  Train_KL: 3.5135419070720673  Validation Loss : 90.59475326538086 Val_Reconstruction : 87.12613677978516 Val_KL : 3.4686150550842285\n","Epoch: 5747/8000  Traning Loss: 90.45597171783447  Train_Reconstruction: 86.94183826446533  Train_KL: 3.5141331553459167  Validation Loss : 90.44223403930664 Val_Reconstruction : 86.96406173706055 Val_KL : 3.478171706199646\n","Epoch: 5748/8000  Traning Loss: 90.63148212432861  Train_Reconstruction: 87.11038970947266  Train_KL: 3.521093100309372  Validation Loss : 90.51812744140625 Val_Reconstruction : 87.03515243530273 Val_KL : 3.4829747676849365\n","Epoch: 5749/8000  Traning Loss: 90.64623928070068  Train_Reconstruction: 87.11365509033203  Train_KL: 3.5325844287872314  Validation Loss : 91.2733383178711 Val_Reconstruction : 87.77600860595703 Val_KL : 3.497330069541931\n","Epoch: 5750/8000  Traning Loss: 91.18030738830566  Train_Reconstruction: 87.65051460266113  Train_KL: 3.5297930538654327  Validation Loss : 91.25297164916992 Val_Reconstruction : 87.7675666809082 Val_KL : 3.485404372215271\n","Epoch: 5751/8000  Traning Loss: 91.03034687042236  Train_Reconstruction: 87.506911277771  Train_KL: 3.5234345495700836  Validation Loss : 91.30693435668945 Val_Reconstruction : 87.82948684692383 Val_KL : 3.477445125579834\n","Epoch: 5752/8000  Traning Loss: 91.08324813842773  Train_Reconstruction: 87.57084941864014  Train_KL: 3.5123987197875977  Validation Loss : 91.15750122070312 Val_Reconstruction : 87.6868896484375 Val_KL : 3.4706106185913086\n","Epoch: 5753/8000  Traning Loss: 90.76715755462646  Train_Reconstruction: 87.25920104980469  Train_KL: 3.5079562962055206  Validation Loss : 90.5616569519043 Val_Reconstruction : 87.08562850952148 Val_KL : 3.4760279655456543\n","Epoch: 5754/8000  Traning Loss: 90.61456775665283  Train_Reconstruction: 87.08545875549316  Train_KL: 3.529108226299286  Validation Loss : 90.50017166137695 Val_Reconstruction : 87.00107192993164 Val_KL : 3.4991003274917603\n","Epoch: 5755/8000  Traning Loss: 90.57863140106201  Train_Reconstruction: 87.0510835647583  Train_KL: 3.527547985315323  Validation Loss : 90.76895523071289 Val_Reconstruction : 87.2893295288086 Val_KL : 3.479626417160034\n","Epoch: 5756/8000  Traning Loss: 90.92899703979492  Train_Reconstruction: 87.40924453735352  Train_KL: 3.519753634929657  Validation Loss : 90.87342071533203 Val_Reconstruction : 87.39894104003906 Val_KL : 3.474479913711548\n","Epoch: 5757/8000  Traning Loss: 91.29111766815186  Train_Reconstruction: 87.76849365234375  Train_KL: 3.5226246416568756  Validation Loss : 91.65774154663086 Val_Reconstruction : 88.17314529418945 Val_KL : 3.484598755836487\n","Epoch: 5758/8000  Traning Loss: 91.7391529083252  Train_Reconstruction: 88.21167373657227  Train_KL: 3.5274793207645416  Validation Loss : 91.02676773071289 Val_Reconstruction : 87.53461074829102 Val_KL : 3.4921542406082153\n","Epoch: 5759/8000  Traning Loss: 91.27981853485107  Train_Reconstruction: 87.75166511535645  Train_KL: 3.5281533002853394  Validation Loss : 91.25296783447266 Val_Reconstruction : 87.76845932006836 Val_KL : 3.4845104217529297\n","Epoch: 5760/8000  Traning Loss: 91.25570106506348  Train_Reconstruction: 87.7375431060791  Train_KL: 3.518158793449402  Validation Loss : 91.43635559082031 Val_Reconstruction : 87.96195983886719 Val_KL : 3.474395275115967\n","Epoch: 5761/8000  Traning Loss: 91.46736145019531  Train_Reconstruction: 87.9491777420044  Train_KL: 3.5181832909584045  Validation Loss : 91.72092819213867 Val_Reconstruction : 88.23885726928711 Val_KL : 3.482069492340088\n","Epoch: 5762/8000  Traning Loss: 91.29901123046875  Train_Reconstruction: 87.77103328704834  Train_KL: 3.5279802083969116  Validation Loss : 91.1481704711914 Val_Reconstruction : 87.66737747192383 Val_KL : 3.480795383453369\n","Epoch: 5763/8000  Traning Loss: 91.14044380187988  Train_Reconstruction: 87.61483097076416  Train_KL: 3.5256137549877167  Validation Loss : 90.88394546508789 Val_Reconstruction : 87.40060043334961 Val_KL : 3.483344554901123\n","Epoch: 5764/8000  Traning Loss: 90.75090312957764  Train_Reconstruction: 87.22096252441406  Train_KL: 3.5299399495124817  Validation Loss : 90.65350723266602 Val_Reconstruction : 87.16542053222656 Val_KL : 3.488089084625244\n","Epoch: 5765/8000  Traning Loss: 90.3559398651123  Train_Reconstruction: 86.83085536956787  Train_KL: 3.5250839293003082  Validation Loss : 90.36384963989258 Val_Reconstruction : 86.88415908813477 Val_KL : 3.4796918630599976\n","Epoch: 5766/8000  Traning Loss: 90.40454769134521  Train_Reconstruction: 86.88243579864502  Train_KL: 3.5221126973629  Validation Loss : 90.67595672607422 Val_Reconstruction : 87.19196701049805 Val_KL : 3.4839905500411987\n","Epoch: 5767/8000  Traning Loss: 90.59370422363281  Train_Reconstruction: 87.06560802459717  Train_KL: 3.5280964374542236  Validation Loss : 90.62906265258789 Val_Reconstruction : 87.13770294189453 Val_KL : 3.4913578033447266\n","Epoch: 5768/8000  Traning Loss: 90.80677795410156  Train_Reconstruction: 87.2753553390503  Train_KL: 3.531423270702362  Validation Loss : 91.0102310180664 Val_Reconstruction : 87.5218505859375 Val_KL : 3.488379955291748\n","Epoch: 5769/8000  Traning Loss: 90.66893768310547  Train_Reconstruction: 87.14910316467285  Train_KL: 3.5198349058628082  Validation Loss : 90.91133499145508 Val_Reconstruction : 87.43497848510742 Val_KL : 3.476356267929077\n","Epoch: 5770/8000  Traning Loss: 90.95518112182617  Train_Reconstruction: 87.4399938583374  Train_KL: 3.5151881873607635  Validation Loss : 91.45694351196289 Val_Reconstruction : 87.97941207885742 Val_KL : 3.4775326251983643\n","Epoch: 5771/8000  Traning Loss: 91.50917434692383  Train_Reconstruction: 87.98007202148438  Train_KL: 3.5291031301021576  Validation Loss : 91.45372772216797 Val_Reconstruction : 87.96590423583984 Val_KL : 3.4878265857696533\n","Epoch: 5772/8000  Traning Loss: 91.26010227203369  Train_Reconstruction: 87.73721599578857  Train_KL: 3.5228868424892426  Validation Loss : 91.47407913208008 Val_Reconstruction : 87.99858093261719 Val_KL : 3.475495219230652\n","Epoch: 5773/8000  Traning Loss: 90.66360187530518  Train_Reconstruction: 87.14736080169678  Train_KL: 3.516240656375885  Validation Loss : 90.49113845825195 Val_Reconstruction : 87.00851821899414 Val_KL : 3.4826180934906006\n","Epoch: 5774/8000  Traning Loss: 90.81218814849854  Train_Reconstruction: 87.27962398529053  Train_KL: 3.5325633585453033  Validation Loss : 91.3379135131836 Val_Reconstruction : 87.84006881713867 Val_KL : 3.4978448152542114\n","Epoch: 5775/8000  Traning Loss: 91.01276874542236  Train_Reconstruction: 87.4787244796753  Train_KL: 3.5340442657470703  Validation Loss : 91.00260162353516 Val_Reconstruction : 87.51364135742188 Val_KL : 3.4889570474624634\n","Epoch: 5776/8000  Traning Loss: 90.82509803771973  Train_Reconstruction: 87.30372524261475  Train_KL: 3.5213714241981506  Validation Loss : 90.85807418823242 Val_Reconstruction : 87.3794937133789 Val_KL : 3.47858202457428\n","Epoch: 5777/8000  Traning Loss: 90.56215572357178  Train_Reconstruction: 87.04431629180908  Train_KL: 3.5178393721580505  Validation Loss : 90.98093032836914 Val_Reconstruction : 87.49382019042969 Val_KL : 3.487112522125244\n","Epoch: 5778/8000  Traning Loss: 90.65509033203125  Train_Reconstruction: 87.13245296478271  Train_KL: 3.5226366817951202  Validation Loss : 90.51702880859375 Val_Reconstruction : 87.02936935424805 Val_KL : 3.4876564741134644\n","Epoch: 5779/8000  Traning Loss: 91.10930633544922  Train_Reconstruction: 87.58809852600098  Train_KL: 3.5212080776691437  Validation Loss : 91.12628555297852 Val_Reconstruction : 87.64360046386719 Val_KL : 3.4826879501342773\n","Epoch: 5780/8000  Traning Loss: 91.01777839660645  Train_Reconstruction: 87.49877071380615  Train_KL: 3.51900714635849  Validation Loss : 90.91576385498047 Val_Reconstruction : 87.43849563598633 Val_KL : 3.4772684574127197\n","Epoch: 5781/8000  Traning Loss: 91.14542961120605  Train_Reconstruction: 87.6226577758789  Train_KL: 3.522772490978241  Validation Loss : 91.2543716430664 Val_Reconstruction : 87.7766342163086 Val_KL : 3.4777356386184692\n","Epoch: 5782/8000  Traning Loss: 91.28644371032715  Train_Reconstruction: 87.7646656036377  Train_KL: 3.521778255701065  Validation Loss : 91.16400527954102 Val_Reconstruction : 87.68335723876953 Val_KL : 3.4806478023529053\n","Epoch: 5783/8000  Traning Loss: 90.889328956604  Train_Reconstruction: 87.35944938659668  Train_KL: 3.5298770368099213  Validation Loss : 90.79472732543945 Val_Reconstruction : 87.30375671386719 Val_KL : 3.490973472595215\n","Epoch: 5784/8000  Traning Loss: 90.66758060455322  Train_Reconstruction: 87.13889694213867  Train_KL: 3.528683990240097  Validation Loss : 90.45199966430664 Val_Reconstruction : 86.96599578857422 Val_KL : 3.4860048294067383\n","Epoch: 5785/8000  Traning Loss: 90.84021472930908  Train_Reconstruction: 87.3175458908081  Train_KL: 3.522668868303299  Validation Loss : 91.09569931030273 Val_Reconstruction : 87.60807418823242 Val_KL : 3.4876232147216797\n","Epoch: 5786/8000  Traning Loss: 90.98225498199463  Train_Reconstruction: 87.45488548278809  Train_KL: 3.527368724346161  Validation Loss : 90.6600227355957 Val_Reconstruction : 87.17217636108398 Val_KL : 3.487846255302429\n","Epoch: 5787/8000  Traning Loss: 90.87280654907227  Train_Reconstruction: 87.34834098815918  Train_KL: 3.524467021226883  Validation Loss : 90.91637802124023 Val_Reconstruction : 87.43536376953125 Val_KL : 3.481016516685486\n","Epoch: 5788/8000  Traning Loss: 90.71629428863525  Train_Reconstruction: 87.19692611694336  Train_KL: 3.5193678438663483  Validation Loss : 91.11945724487305 Val_Reconstruction : 87.63630676269531 Val_KL : 3.483148455619812\n","Epoch: 5789/8000  Traning Loss: 91.5811767578125  Train_Reconstruction: 88.05622291564941  Train_KL: 3.524953842163086  Validation Loss : 91.71904754638672 Val_Reconstruction : 88.24161529541016 Val_KL : 3.4774314165115356\n","Epoch: 5790/8000  Traning Loss: 92.00414848327637  Train_Reconstruction: 88.4833116531372  Train_KL: 3.5208367109298706  Validation Loss : 91.71090316772461 Val_Reconstruction : 88.23108291625977 Val_KL : 3.4798197746276855\n","Epoch: 5791/8000  Traning Loss: 91.17494869232178  Train_Reconstruction: 87.64913654327393  Train_KL: 3.525812089443207  Validation Loss : 90.95337677001953 Val_Reconstruction : 87.46941375732422 Val_KL : 3.483961582183838\n","Epoch: 5792/8000  Traning Loss: 90.77908229827881  Train_Reconstruction: 87.25321006774902  Train_KL: 3.525871694087982  Validation Loss : 90.68486785888672 Val_Reconstruction : 87.19852828979492 Val_KL : 3.48634135723114\n","Epoch: 5793/8000  Traning Loss: 90.78721904754639  Train_Reconstruction: 87.26813507080078  Train_KL: 3.519084632396698  Validation Loss : 90.89599990844727 Val_Reconstruction : 87.42236709594727 Val_KL : 3.473634123802185\n","Epoch: 5794/8000  Traning Loss: 91.05777263641357  Train_Reconstruction: 87.54248332977295  Train_KL: 3.5152887403964996  Validation Loss : 91.06582641601562 Val_Reconstruction : 87.58879089355469 Val_KL : 3.4770357608795166\n","Epoch: 5795/8000  Traning Loss: 90.89061641693115  Train_Reconstruction: 87.36946296691895  Train_KL: 3.521153509616852  Validation Loss : 90.91450500488281 Val_Reconstruction : 87.43457794189453 Val_KL : 3.479925274848938\n","Epoch: 5796/8000  Traning Loss: 90.85948085784912  Train_Reconstruction: 87.33463191986084  Train_KL: 3.524848908185959  Validation Loss : 90.6123046875 Val_Reconstruction : 87.12322998046875 Val_KL : 3.48907732963562\n","Epoch: 5797/8000  Traning Loss: 90.80623817443848  Train_Reconstruction: 87.27722454071045  Train_KL: 3.5290135741233826  Validation Loss : 91.23695755004883 Val_Reconstruction : 87.7543716430664 Val_KL : 3.482588529586792\n","Epoch: 5798/8000  Traning Loss: 91.16412830352783  Train_Reconstruction: 87.63770389556885  Train_KL: 3.526425749063492  Validation Loss : 90.82644271850586 Val_Reconstruction : 87.3373794555664 Val_KL : 3.489064335823059\n","Epoch: 5799/8000  Traning Loss: 90.83842372894287  Train_Reconstruction: 87.30780410766602  Train_KL: 3.5306197702884674  Validation Loss : 90.60430908203125 Val_Reconstruction : 87.12182998657227 Val_KL : 3.4824795722961426\n","Epoch: 5800/8000  Traning Loss: 90.53053569793701  Train_Reconstruction: 87.01105690002441  Train_KL: 3.519477665424347  Validation Loss : 90.65302658081055 Val_Reconstruction : 87.17435073852539 Val_KL : 3.4786784648895264\n","Epoch: 5801/8000  Traning Loss: 90.62911033630371  Train_Reconstruction: 87.10657215118408  Train_KL: 3.522537797689438  Validation Loss : 90.53369522094727 Val_Reconstruction : 87.05708694458008 Val_KL : 3.4766076803207397\n","Epoch: 5802/8000  Traning Loss: 90.75494384765625  Train_Reconstruction: 87.23547267913818  Train_KL: 3.5194705426692963  Validation Loss : 90.70553970336914 Val_Reconstruction : 87.22494506835938 Val_KL : 3.4805922508239746\n","Epoch: 5803/8000  Traning Loss: 90.92284297943115  Train_Reconstruction: 87.39778137207031  Train_KL: 3.52506285905838  Validation Loss : 90.76558685302734 Val_Reconstruction : 87.2852783203125 Val_KL : 3.4803097248077393\n","Epoch: 5804/8000  Traning Loss: 90.94307041168213  Train_Reconstruction: 87.42718410491943  Train_KL: 3.5158864557743073  Validation Loss : 90.80854415893555 Val_Reconstruction : 87.33739852905273 Val_KL : 3.471148371696472\n","Epoch: 5805/8000  Traning Loss: 90.89819717407227  Train_Reconstruction: 87.38259029388428  Train_KL: 3.5156079828739166  Validation Loss : 91.02288436889648 Val_Reconstruction : 87.53689956665039 Val_KL : 3.485983729362488\n","Epoch: 5806/8000  Traning Loss: 90.98480224609375  Train_Reconstruction: 87.46079635620117  Train_KL: 3.5240066051483154  Validation Loss : 91.11473083496094 Val_Reconstruction : 87.63092041015625 Val_KL : 3.483808755874634\n","Epoch: 5807/8000  Traning Loss: 90.59902286529541  Train_Reconstruction: 87.0788631439209  Train_KL: 3.5201608538627625  Validation Loss : 90.46746063232422 Val_Reconstruction : 86.99284362792969 Val_KL : 3.4746168851852417\n","Epoch: 5808/8000  Traning Loss: 90.512939453125  Train_Reconstruction: 87.0030689239502  Train_KL: 3.5098699033260345  Validation Loss : 90.58429336547852 Val_Reconstruction : 87.11086654663086 Val_KL : 3.4734272956848145\n","Epoch: 5809/8000  Traning Loss: 90.85342693328857  Train_Reconstruction: 87.33698272705078  Train_KL: 3.516444206237793  Validation Loss : 91.03513717651367 Val_Reconstruction : 87.5611686706543 Val_KL : 3.4739664793014526\n","Epoch: 5810/8000  Traning Loss: 90.7860860824585  Train_Reconstruction: 87.25802707672119  Train_KL: 3.528058797121048  Validation Loss : 90.93131256103516 Val_Reconstruction : 87.43981170654297 Val_KL : 3.491502285003662\n","Epoch: 5811/8000  Traning Loss: 90.78817558288574  Train_Reconstruction: 87.26358699798584  Train_KL: 3.5245882272720337  Validation Loss : 90.77387237548828 Val_Reconstruction : 87.29966735839844 Val_KL : 3.4742027521133423\n","Epoch: 5812/8000  Traning Loss: 90.53560066223145  Train_Reconstruction: 87.01629543304443  Train_KL: 3.519304722547531  Validation Loss : 90.55127334594727 Val_Reconstruction : 87.07585144042969 Val_KL : 3.4754202365875244\n","Epoch: 5813/8000  Traning Loss: 90.27269172668457  Train_Reconstruction: 86.75419044494629  Train_KL: 3.5184997618198395  Validation Loss : 90.36227798461914 Val_Reconstruction : 86.88035202026367 Val_KL : 3.4819235801696777\n","Epoch: 5814/8000  Traning Loss: 90.68974876403809  Train_Reconstruction: 87.16744136810303  Train_KL: 3.5223071575164795  Validation Loss : 90.85109329223633 Val_Reconstruction : 87.36148834228516 Val_KL : 3.489606261253357\n","Epoch: 5815/8000  Traning Loss: 90.73209476470947  Train_Reconstruction: 87.2075023651123  Train_KL: 3.524592310190201  Validation Loss : 90.54824447631836 Val_Reconstruction : 87.06414413452148 Val_KL : 3.484099507331848\n","Epoch: 5816/8000  Traning Loss: 90.63817310333252  Train_Reconstruction: 87.11549091339111  Train_KL: 3.522681564092636  Validation Loss : 90.77578353881836 Val_Reconstruction : 87.29343032836914 Val_KL : 3.482353687286377\n","Epoch: 5817/8000  Traning Loss: 90.83839702606201  Train_Reconstruction: 87.32119750976562  Train_KL: 3.517198860645294  Validation Loss : 90.90375900268555 Val_Reconstruction : 87.43232727050781 Val_KL : 3.471435070037842\n","Epoch: 5818/8000  Traning Loss: 91.13717460632324  Train_Reconstruction: 87.61771869659424  Train_KL: 3.5194580256938934  Validation Loss : 90.82154846191406 Val_Reconstruction : 87.33847427368164 Val_KL : 3.483072876930237\n","Epoch: 5819/8000  Traning Loss: 90.84916591644287  Train_Reconstruction: 87.32086563110352  Train_KL: 3.528300017118454  Validation Loss : 91.13410568237305 Val_Reconstruction : 87.65067672729492 Val_KL : 3.4834295511245728\n","Epoch: 5820/8000  Traning Loss: 90.9550724029541  Train_Reconstruction: 87.4374475479126  Train_KL: 3.5176260471343994  Validation Loss : 91.4299087524414 Val_Reconstruction : 87.95845794677734 Val_KL : 3.471453070640564\n","Epoch: 5821/8000  Traning Loss: 91.2045087814331  Train_Reconstruction: 87.68506908416748  Train_KL: 3.519440144300461  Validation Loss : 91.35689163208008 Val_Reconstruction : 87.8755989074707 Val_KL : 3.4812921285629272\n","Epoch: 5822/8000  Traning Loss: 90.922926902771  Train_Reconstruction: 87.39489936828613  Train_KL: 3.52802836894989  Validation Loss : 91.03243637084961 Val_Reconstruction : 87.54163360595703 Val_KL : 3.4908018112182617\n","Epoch: 5823/8000  Traning Loss: 90.76366138458252  Train_Reconstruction: 87.24153518676758  Train_KL: 3.5221270322799683  Validation Loss : 90.7840461730957 Val_Reconstruction : 87.30885314941406 Val_KL : 3.475193500518799\n","Epoch: 5824/8000  Traning Loss: 90.89366817474365  Train_Reconstruction: 87.37398433685303  Train_KL: 3.5196827352046967  Validation Loss : 90.66992568969727 Val_Reconstruction : 87.18397903442383 Val_KL : 3.4859461784362793\n","Epoch: 5825/8000  Traning Loss: 90.9559211730957  Train_Reconstruction: 87.42265319824219  Train_KL: 3.533267915248871  Validation Loss : 90.90897750854492 Val_Reconstruction : 87.41410064697266 Val_KL : 3.4948768615722656\n","Epoch: 5826/8000  Traning Loss: 91.01069068908691  Train_Reconstruction: 87.48699760437012  Train_KL: 3.5236953496932983  Validation Loss : 90.94179153442383 Val_Reconstruction : 87.46269226074219 Val_KL : 3.479100227355957\n","Epoch: 5827/8000  Traning Loss: 90.64888286590576  Train_Reconstruction: 87.1276216506958  Train_KL: 3.5212603211402893  Validation Loss : 90.57062530517578 Val_Reconstruction : 87.08900833129883 Val_KL : 3.481616497039795\n","Epoch: 5828/8000  Traning Loss: 90.5116605758667  Train_Reconstruction: 86.99263191223145  Train_KL: 3.5190273225307465  Validation Loss : 90.60562515258789 Val_Reconstruction : 87.12990951538086 Val_KL : 3.4757171869277954\n","Epoch: 5829/8000  Traning Loss: 90.30698013305664  Train_Reconstruction: 86.79232883453369  Train_KL: 3.5146503150463104  Validation Loss : 90.3514289855957 Val_Reconstruction : 86.8787612915039 Val_KL : 3.4726686477661133\n","Epoch: 5830/8000  Traning Loss: 90.50812816619873  Train_Reconstruction: 86.99219703674316  Train_KL: 3.5159323513507843  Validation Loss : 90.58932876586914 Val_Reconstruction : 87.11095428466797 Val_KL : 3.4783754348754883\n","Epoch: 5831/8000  Traning Loss: 90.76160430908203  Train_Reconstruction: 87.2382001876831  Train_KL: 3.523404449224472  Validation Loss : 91.0447006225586 Val_Reconstruction : 87.56612014770508 Val_KL : 3.4785834550857544\n","Epoch: 5832/8000  Traning Loss: 90.60276222229004  Train_Reconstruction: 87.07925224304199  Train_KL: 3.5235097110271454  Validation Loss : 90.63365936279297 Val_Reconstruction : 87.15288925170898 Val_KL : 3.4807708263397217\n","Epoch: 5833/8000  Traning Loss: 90.60242366790771  Train_Reconstruction: 87.07796382904053  Train_KL: 3.5244600772857666  Validation Loss : 90.76945877075195 Val_Reconstruction : 87.27910232543945 Val_KL : 3.490357995033264\n","Epoch: 5834/8000  Traning Loss: 90.46796607971191  Train_Reconstruction: 86.94604778289795  Train_KL: 3.5219185650348663  Validation Loss : 90.19821166992188 Val_Reconstruction : 86.72677993774414 Val_KL : 3.4714313745498657\n","Epoch: 5835/8000  Traning Loss: 90.48044395446777  Train_Reconstruction: 86.95619487762451  Train_KL: 3.5242501199245453  Validation Loss : 90.31582641601562 Val_Reconstruction : 86.8277587890625 Val_KL : 3.488064408302307\n","Epoch: 5836/8000  Traning Loss: 90.3655481338501  Train_Reconstruction: 86.8369140625  Train_KL: 3.528633803129196  Validation Loss : 90.18452835083008 Val_Reconstruction : 86.70867156982422 Val_KL : 3.475857138633728\n","Epoch: 5837/8000  Traning Loss: 90.56326293945312  Train_Reconstruction: 87.0425968170166  Train_KL: 3.520665794610977  Validation Loss : 90.97705459594727 Val_Reconstruction : 87.50149536132812 Val_KL : 3.4755587577819824\n","Epoch: 5838/8000  Traning Loss: 90.97069644927979  Train_Reconstruction: 87.45948600769043  Train_KL: 3.5112096071243286  Validation Loss : 90.86590194702148 Val_Reconstruction : 87.39908981323242 Val_KL : 3.4668129682540894\n","Epoch: 5839/8000  Traning Loss: 90.7278242111206  Train_Reconstruction: 87.20995616912842  Train_KL: 3.5178690552711487  Validation Loss : 90.54621887207031 Val_Reconstruction : 87.05878829956055 Val_KL : 3.487432837486267\n","Epoch: 5840/8000  Traning Loss: 90.27469539642334  Train_Reconstruction: 86.75446605682373  Train_KL: 3.5202307403087616  Validation Loss : 90.34898376464844 Val_Reconstruction : 86.87471771240234 Val_KL : 3.4742668867111206\n","Epoch: 5841/8000  Traning Loss: 90.37522029876709  Train_Reconstruction: 86.86227703094482  Train_KL: 3.5129431784152985  Validation Loss : 90.72809219360352 Val_Reconstruction : 87.25204467773438 Val_KL : 3.4760466814041138\n","Epoch: 5842/8000  Traning Loss: 90.41683006286621  Train_Reconstruction: 86.89320373535156  Train_KL: 3.5236265063285828  Validation Loss : 90.23490142822266 Val_Reconstruction : 86.75238037109375 Val_KL : 3.4825220108032227\n","Epoch: 5843/8000  Traning Loss: 90.37816524505615  Train_Reconstruction: 86.8485689163208  Train_KL: 3.529596120119095  Validation Loss : 90.27175903320312 Val_Reconstruction : 86.79107666015625 Val_KL : 3.4806854724884033\n","Epoch: 5844/8000  Traning Loss: 90.63817501068115  Train_Reconstruction: 87.11219882965088  Train_KL: 3.5259766578674316  Validation Loss : 90.84343338012695 Val_Reconstruction : 87.35753631591797 Val_KL : 3.4858988523483276\n","Epoch: 5845/8000  Traning Loss: 91.0343542098999  Train_Reconstruction: 87.51103401184082  Train_KL: 3.5233210027217865  Validation Loss : 91.1798095703125 Val_Reconstruction : 87.68743896484375 Val_KL : 3.49237060546875\n","Epoch: 5846/8000  Traning Loss: 90.55618190765381  Train_Reconstruction: 87.02477169036865  Train_KL: 3.5314099192619324  Validation Loss : 90.59822463989258 Val_Reconstruction : 87.10148620605469 Val_KL : 3.496738076210022\n","Epoch: 5847/8000  Traning Loss: 90.3976058959961  Train_Reconstruction: 86.86905574798584  Train_KL: 3.528550386428833  Validation Loss : 90.50252532958984 Val_Reconstruction : 87.0106315612793 Val_KL : 3.4918943643569946\n","Epoch: 5848/8000  Traning Loss: 90.98432064056396  Train_Reconstruction: 87.4628734588623  Train_KL: 3.521448463201523  Validation Loss : 91.43064880371094 Val_Reconstruction : 87.94950866699219 Val_KL : 3.4811400175094604\n","Epoch: 5849/8000  Traning Loss: 91.15138912200928  Train_Reconstruction: 87.62879180908203  Train_KL: 3.522597372531891  Validation Loss : 91.58084487915039 Val_Reconstruction : 88.1011848449707 Val_KL : 3.4796621799468994\n","Epoch: 5850/8000  Traning Loss: 90.91838073730469  Train_Reconstruction: 87.40374660491943  Train_KL: 3.5146345496177673  Validation Loss : 90.69569778442383 Val_Reconstruction : 87.22607803344727 Val_KL : 3.4696216583251953\n","Epoch: 5851/8000  Traning Loss: 90.42046928405762  Train_Reconstruction: 86.90465259552002  Train_KL: 3.515815496444702  Validation Loss : 90.42208480834961 Val_Reconstruction : 86.94284439086914 Val_KL : 3.4792407751083374\n","Epoch: 5852/8000  Traning Loss: 90.69056224822998  Train_Reconstruction: 87.16258430480957  Train_KL: 3.527978539466858  Validation Loss : 90.60325622558594 Val_Reconstruction : 87.11169052124023 Val_KL : 3.4915634393692017\n","Epoch: 5853/8000  Traning Loss: 90.54497528076172  Train_Reconstruction: 87.01121807098389  Train_KL: 3.53375706076622  Validation Loss : 90.43133163452148 Val_Reconstruction : 86.94388580322266 Val_KL : 3.487447142601013\n","Epoch: 5854/8000  Traning Loss: 90.48545169830322  Train_Reconstruction: 86.96679306030273  Train_KL: 3.5186585187911987  Validation Loss : 90.33838272094727 Val_Reconstruction : 86.86196899414062 Val_KL : 3.4764134883880615\n","Epoch: 5855/8000  Traning Loss: 91.0090913772583  Train_Reconstruction: 87.48682498931885  Train_KL: 3.5222658812999725  Validation Loss : 91.43501281738281 Val_Reconstruction : 87.95329666137695 Val_KL : 3.481717348098755\n","Epoch: 5856/8000  Traning Loss: 91.5787124633789  Train_Reconstruction: 88.05654048919678  Train_KL: 3.522172272205353  Validation Loss : 91.6695671081543 Val_Reconstruction : 88.19014739990234 Val_KL : 3.479421615600586\n","Epoch: 5857/8000  Traning Loss: 91.06793594360352  Train_Reconstruction: 87.54397869110107  Train_KL: 3.52395761013031  Validation Loss : 90.97274780273438 Val_Reconstruction : 87.49259948730469 Val_KL : 3.480145573616028\n","Epoch: 5858/8000  Traning Loss: 90.77749156951904  Train_Reconstruction: 87.25721549987793  Train_KL: 3.5202767848968506  Validation Loss : 90.4338493347168 Val_Reconstruction : 86.95403671264648 Val_KL : 3.4798152446746826\n","Epoch: 5859/8000  Traning Loss: 90.42523765563965  Train_Reconstruction: 86.90120792388916  Train_KL: 3.5240295827388763  Validation Loss : 90.33099365234375 Val_Reconstruction : 86.84180068969727 Val_KL : 3.489192485809326\n","Epoch: 5860/8000  Traning Loss: 90.37964630126953  Train_Reconstruction: 86.84934997558594  Train_KL: 3.5302963256835938  Validation Loss : 90.4333267211914 Val_Reconstruction : 86.94021987915039 Val_KL : 3.4931055307388306\n","Epoch: 5861/8000  Traning Loss: 90.56797313690186  Train_Reconstruction: 87.04093742370605  Train_KL: 3.5270361602306366  Validation Loss : 90.6254768371582 Val_Reconstruction : 87.14335250854492 Val_KL : 3.482121706008911\n","Epoch: 5862/8000  Traning Loss: 90.52630424499512  Train_Reconstruction: 87.0033950805664  Train_KL: 3.522909849882126  Validation Loss : 90.64391708374023 Val_Reconstruction : 87.15376663208008 Val_KL : 3.490150570869446\n","Epoch: 5863/8000  Traning Loss: 90.62668037414551  Train_Reconstruction: 87.09423732757568  Train_KL: 3.532442420721054  Validation Loss : 90.60686111450195 Val_Reconstruction : 87.10832977294922 Val_KL : 3.498530864715576\n","Epoch: 5864/8000  Traning Loss: 90.79981708526611  Train_Reconstruction: 87.2612533569336  Train_KL: 3.5385624170303345  Validation Loss : 90.9739761352539 Val_Reconstruction : 87.47305297851562 Val_KL : 3.500923275947571\n","Epoch: 5865/8000  Traning Loss: 90.76784801483154  Train_Reconstruction: 87.23645973205566  Train_KL: 3.5313885509967804  Validation Loss : 90.6734390258789 Val_Reconstruction : 87.19596099853516 Val_KL : 3.477474570274353\n","Epoch: 5866/8000  Traning Loss: 90.57954120635986  Train_Reconstruction: 87.05904483795166  Train_KL: 3.520496129989624  Validation Loss : 90.52395248413086 Val_Reconstruction : 87.0411491394043 Val_KL : 3.482801079750061\n","Epoch: 5867/8000  Traning Loss: 90.9774227142334  Train_Reconstruction: 87.44911766052246  Train_KL: 3.5283050537109375  Validation Loss : 90.954345703125 Val_Reconstruction : 87.46738815307617 Val_KL : 3.4869563579559326\n","Epoch: 5868/8000  Traning Loss: 90.60145378112793  Train_Reconstruction: 87.07985210418701  Train_KL: 3.5216016471385956  Validation Loss : 90.7177734375 Val_Reconstruction : 87.23757553100586 Val_KL : 3.4802006483078003\n","Epoch: 5869/8000  Traning Loss: 90.53002262115479  Train_Reconstruction: 87.0071849822998  Train_KL: 3.522837519645691  Validation Loss : 90.47208404541016 Val_Reconstruction : 86.9983139038086 Val_KL : 3.4737690687179565\n","Epoch: 5870/8000  Traning Loss: 90.87552547454834  Train_Reconstruction: 87.36040687561035  Train_KL: 3.515117794275284  Validation Loss : 91.1613540649414 Val_Reconstruction : 87.69023513793945 Val_KL : 3.471117377281189\n","Epoch: 5871/8000  Traning Loss: 90.80123138427734  Train_Reconstruction: 87.29247379302979  Train_KL: 3.5087555646896362  Validation Loss : 90.71818161010742 Val_Reconstruction : 87.24804306030273 Val_KL : 3.470136880874634\n","Epoch: 5872/8000  Traning Loss: 90.44200992584229  Train_Reconstruction: 86.91937732696533  Train_KL: 3.5226332545280457  Validation Loss : 90.3404312133789 Val_Reconstruction : 86.85320663452148 Val_KL : 3.4872244596481323\n","Epoch: 5873/8000  Traning Loss: 90.54442310333252  Train_Reconstruction: 87.02730941772461  Train_KL: 3.5171149373054504  Validation Loss : 91.34420776367188 Val_Reconstruction : 87.86568069458008 Val_KL : 3.4785256385803223\n","Epoch: 5874/8000  Traning Loss: 90.6668586730957  Train_Reconstruction: 87.14648628234863  Train_KL: 3.520372658967972  Validation Loss : 90.70507049560547 Val_Reconstruction : 87.22347259521484 Val_KL : 3.48159921169281\n","Epoch: 5875/8000  Traning Loss: 90.4012508392334  Train_Reconstruction: 86.88799571990967  Train_KL: 3.5132568180561066  Validation Loss : 90.42646026611328 Val_Reconstruction : 86.9603042602539 Val_KL : 3.4661569595336914\n","Epoch: 5876/8000  Traning Loss: 90.44216632843018  Train_Reconstruction: 86.93120288848877  Train_KL: 3.5109647512435913  Validation Loss : 90.47441101074219 Val_Reconstruction : 86.99831771850586 Val_KL : 3.476091504096985\n","Epoch: 5877/8000  Traning Loss: 90.50241947174072  Train_Reconstruction: 86.98324298858643  Train_KL: 3.519176661968231  Validation Loss : 90.44002151489258 Val_Reconstruction : 86.95907592773438 Val_KL : 3.4809430837631226\n","Epoch: 5878/8000  Traning Loss: 90.50872325897217  Train_Reconstruction: 86.98733043670654  Train_KL: 3.521394729614258  Validation Loss : 90.43888473510742 Val_Reconstruction : 86.9566535949707 Val_KL : 3.4822323322296143\n","Epoch: 5879/8000  Traning Loss: 90.72861671447754  Train_Reconstruction: 87.20636558532715  Train_KL: 3.522252321243286  Validation Loss : 91.01786041259766 Val_Reconstruction : 87.53598403930664 Val_KL : 3.4818763732910156\n","Epoch: 5880/8000  Traning Loss: 91.75733757019043  Train_Reconstruction: 88.23307609558105  Train_KL: 3.5242629945278168  Validation Loss : 91.97026443481445 Val_Reconstruction : 88.4954719543457 Val_KL : 3.4747939109802246\n","Epoch: 5881/8000  Traning Loss: 92.31790828704834  Train_Reconstruction: 88.80293846130371  Train_KL: 3.514970153570175  Validation Loss : 92.2137336730957 Val_Reconstruction : 88.74192428588867 Val_KL : 3.4718077182769775\n","Epoch: 5882/8000  Traning Loss: 92.59342670440674  Train_Reconstruction: 89.07983303070068  Train_KL: 3.5135924220085144  Validation Loss : 92.08811569213867 Val_Reconstruction : 88.61318969726562 Val_KL : 3.474926710128784\n","Epoch: 5883/8000  Traning Loss: 92.30010604858398  Train_Reconstruction: 88.77994155883789  Train_KL: 3.520163416862488  Validation Loss : 92.18548202514648 Val_Reconstruction : 88.70438003540039 Val_KL : 3.4811043739318848\n","Epoch: 5884/8000  Traning Loss: 91.69133377075195  Train_Reconstruction: 88.16386222839355  Train_KL: 3.527471572160721  Validation Loss : 91.11283111572266 Val_Reconstruction : 87.62596893310547 Val_KL : 3.4868630170822144\n","Epoch: 5885/8000  Traning Loss: 91.23370552062988  Train_Reconstruction: 87.71085739135742  Train_KL: 3.522847205400467  Validation Loss : 91.26285552978516 Val_Reconstruction : 87.78818130493164 Val_KL : 3.4746731519699097\n","Epoch: 5886/8000  Traning Loss: 91.21097183227539  Train_Reconstruction: 87.69441890716553  Train_KL: 3.5165539979934692  Validation Loss : 91.2255630493164 Val_Reconstruction : 87.74403762817383 Val_KL : 3.4815229177474976\n","Epoch: 5887/8000  Traning Loss: 90.9808874130249  Train_Reconstruction: 87.4544906616211  Train_KL: 3.5263961255550385  Validation Loss : 91.07588195800781 Val_Reconstruction : 87.59149932861328 Val_KL : 3.4843826293945312\n","Epoch: 5888/8000  Traning Loss: 90.68145561218262  Train_Reconstruction: 87.1670150756836  Train_KL: 3.5144386887550354  Validation Loss : 90.57928085327148 Val_Reconstruction : 87.10321426391602 Val_KL : 3.476065158843994\n","Epoch: 5889/8000  Traning Loss: 90.54497146606445  Train_Reconstruction: 87.0158920288086  Train_KL: 3.5290797352790833  Validation Loss : 90.51061248779297 Val_Reconstruction : 87.01383972167969 Val_KL : 3.4967697858810425\n","Epoch: 5890/8000  Traning Loss: 90.35834789276123  Train_Reconstruction: 86.82494258880615  Train_KL: 3.5334052741527557  Validation Loss : 90.42085266113281 Val_Reconstruction : 86.92913055419922 Val_KL : 3.4917190074920654\n","Epoch: 5891/8000  Traning Loss: 90.31567478179932  Train_Reconstruction: 86.79501533508301  Train_KL: 3.5206591486930847  Validation Loss : 90.55131530761719 Val_Reconstruction : 87.07316589355469 Val_KL : 3.478148579597473\n","Epoch: 5892/8000  Traning Loss: 90.31432914733887  Train_Reconstruction: 86.80243301391602  Train_KL: 3.5118969082832336  Validation Loss : 90.30141830444336 Val_Reconstruction : 86.83043670654297 Val_KL : 3.4709807634353638\n","Epoch: 5893/8000  Traning Loss: 90.24652671813965  Train_Reconstruction: 86.71663856506348  Train_KL: 3.529887467622757  Validation Loss : 90.30058670043945 Val_Reconstruction : 86.80448532104492 Val_KL : 3.4961003065109253\n","Epoch: 5894/8000  Traning Loss: 90.43586921691895  Train_Reconstruction: 86.89935398101807  Train_KL: 3.5365161895751953  Validation Loss : 90.57013702392578 Val_Reconstruction : 87.07672119140625 Val_KL : 3.493414878845215\n","Epoch: 5895/8000  Traning Loss: 90.73389625549316  Train_Reconstruction: 87.20981884002686  Train_KL: 3.524078071117401  Validation Loss : 90.66422271728516 Val_Reconstruction : 87.19002532958984 Val_KL : 3.4741969108581543\n","Epoch: 5896/8000  Traning Loss: 90.26082229614258  Train_Reconstruction: 86.74882125854492  Train_KL: 3.512000799179077  Validation Loss : 90.30130004882812 Val_Reconstruction : 86.82823181152344 Val_KL : 3.473070502281189\n","Epoch: 5897/8000  Traning Loss: 90.31995391845703  Train_Reconstruction: 86.80276393890381  Train_KL: 3.517190605401993  Validation Loss : 90.36747741699219 Val_Reconstruction : 86.8803596496582 Val_KL : 3.4871166944503784\n","Epoch: 5898/8000  Traning Loss: 90.3392276763916  Train_Reconstruction: 86.80174160003662  Train_KL: 3.537485808134079  Validation Loss : 90.31733703613281 Val_Reconstruction : 86.82057189941406 Val_KL : 3.4967631101608276\n","Epoch: 5899/8000  Traning Loss: 90.50789833068848  Train_Reconstruction: 86.98130416870117  Train_KL: 3.5265944600105286  Validation Loss : 90.72688293457031 Val_Reconstruction : 87.24869918823242 Val_KL : 3.4781829118728638\n","Epoch: 5900/8000  Traning Loss: 90.53099155426025  Train_Reconstruction: 87.00723361968994  Train_KL: 3.523758500814438  Validation Loss : 90.61692810058594 Val_Reconstruction : 87.1293716430664 Val_KL : 3.4875577688217163\n","Epoch: 5901/8000  Traning Loss: 90.90252780914307  Train_Reconstruction: 87.38070487976074  Train_KL: 3.521822839975357  Validation Loss : 90.88634490966797 Val_Reconstruction : 87.39813232421875 Val_KL : 3.488213539123535\n","Epoch: 5902/8000  Traning Loss: 90.58831977844238  Train_Reconstruction: 87.06881618499756  Train_KL: 3.5195033848285675  Validation Loss : 90.47771072387695 Val_Reconstruction : 87.00032043457031 Val_KL : 3.4773887395858765\n","Epoch: 5903/8000  Traning Loss: 90.62976837158203  Train_Reconstruction: 87.11404323577881  Train_KL: 3.5157247185707092  Validation Loss : 90.56759262084961 Val_Reconstruction : 87.08806991577148 Val_KL : 3.479525327682495\n","Epoch: 5904/8000  Traning Loss: 90.84732151031494  Train_Reconstruction: 87.32037544250488  Train_KL: 3.526945948600769  Validation Loss : 91.06811141967773 Val_Reconstruction : 87.57372665405273 Val_KL : 3.4943844079971313\n","Epoch: 5905/8000  Traning Loss: 90.9512357711792  Train_Reconstruction: 87.43185806274414  Train_KL: 3.5193783044815063  Validation Loss : 90.83782196044922 Val_Reconstruction : 87.36442184448242 Val_KL : 3.473400115966797\n","Epoch: 5906/8000  Traning Loss: 90.75796794891357  Train_Reconstruction: 87.25222110748291  Train_KL: 3.5057471692562103  Validation Loss : 90.84959030151367 Val_Reconstruction : 87.38018035888672 Val_KL : 3.469411849975586\n","Epoch: 5907/8000  Traning Loss: 90.60282802581787  Train_Reconstruction: 87.08125114440918  Train_KL: 3.521576911211014  Validation Loss : 90.66949462890625 Val_Reconstruction : 87.18440246582031 Val_KL : 3.4850900173187256\n","Epoch: 5908/8000  Traning Loss: 90.51983547210693  Train_Reconstruction: 86.99302959442139  Train_KL: 3.526807427406311  Validation Loss : 90.4854621887207 Val_Reconstruction : 87.00102615356445 Val_KL : 3.484434723854065\n","Epoch: 5909/8000  Traning Loss: 90.50058174133301  Train_Reconstruction: 86.9820785522461  Train_KL: 3.518502026796341  Validation Loss : 90.60647583007812 Val_Reconstruction : 87.12846755981445 Val_KL : 3.4780101776123047\n","Epoch: 5910/8000  Traning Loss: 90.95596885681152  Train_Reconstruction: 87.43963241577148  Train_KL: 3.5163367688655853  Validation Loss : 91.2060546875 Val_Reconstruction : 87.73067855834961 Val_KL : 3.4753752946853638\n","Epoch: 5911/8000  Traning Loss: 90.99464797973633  Train_Reconstruction: 87.48061847686768  Train_KL: 3.514028251171112  Validation Loss : 91.09017562866211 Val_Reconstruction : 87.61061096191406 Val_KL : 3.4795641899108887\n","Epoch: 5912/8000  Traning Loss: 90.63746166229248  Train_Reconstruction: 87.11964321136475  Train_KL: 3.517819792032242  Validation Loss : 90.35702896118164 Val_Reconstruction : 86.8796157836914 Val_KL : 3.4774110317230225\n","Epoch: 5913/8000  Traning Loss: 90.62005996704102  Train_Reconstruction: 87.10002994537354  Train_KL: 3.520030230283737  Validation Loss : 90.69624328613281 Val_Reconstruction : 87.20821380615234 Val_KL : 3.488028049468994\n","Epoch: 5914/8000  Traning Loss: 90.60123348236084  Train_Reconstruction: 87.0770034790039  Train_KL: 3.524230659008026  Validation Loss : 90.75131607055664 Val_Reconstruction : 87.26679229736328 Val_KL : 3.484522819519043\n","Epoch: 5915/8000  Traning Loss: 90.53732109069824  Train_Reconstruction: 87.00972843170166  Train_KL: 3.527591794729233  Validation Loss : 90.52409362792969 Val_Reconstruction : 87.03936386108398 Val_KL : 3.48473060131073\n","Epoch: 5916/8000  Traning Loss: 90.34089279174805  Train_Reconstruction: 86.8206262588501  Train_KL: 3.5202667713165283  Validation Loss : 90.2869758605957 Val_Reconstruction : 86.8092041015625 Val_KL : 3.477774143218994\n","Epoch: 5917/8000  Traning Loss: 90.78514671325684  Train_Reconstruction: 87.26921844482422  Train_KL: 3.515927344560623  Validation Loss : 90.8411750793457 Val_Reconstruction : 87.36258316040039 Val_KL : 3.478590965270996\n","Epoch: 5918/8000  Traning Loss: 90.86229610443115  Train_Reconstruction: 87.3337631225586  Train_KL: 3.5285322964191437  Validation Loss : 90.7769889831543 Val_Reconstruction : 87.2849349975586 Val_KL : 3.492053747177124\n","Epoch: 5919/8000  Traning Loss: 90.87014675140381  Train_Reconstruction: 87.34340476989746  Train_KL: 3.5267431437969208  Validation Loss : 90.8965072631836 Val_Reconstruction : 87.41411972045898 Val_KL : 3.4823849201202393\n","Epoch: 5920/8000  Traning Loss: 90.62240219116211  Train_Reconstruction: 87.10516738891602  Train_KL: 3.5172344744205475  Validation Loss : 90.43544387817383 Val_Reconstruction : 86.95751953125 Val_KL : 3.4779231548309326\n","Epoch: 5921/8000  Traning Loss: 90.51797389984131  Train_Reconstruction: 87.00179862976074  Train_KL: 3.516174703836441  Validation Loss : 90.69299697875977 Val_Reconstruction : 87.21735000610352 Val_KL : 3.4756460189819336\n","Epoch: 5922/8000  Traning Loss: 90.6662244796753  Train_Reconstruction: 87.14747142791748  Train_KL: 3.5187542736530304  Validation Loss : 90.70000457763672 Val_Reconstruction : 87.21593475341797 Val_KL : 3.4840701818466187\n","Epoch: 5923/8000  Traning Loss: 90.89297771453857  Train_Reconstruction: 87.37212944030762  Train_KL: 3.520847409963608  Validation Loss : 91.06763458251953 Val_Reconstruction : 87.59449005126953 Val_KL : 3.473144054412842\n","Epoch: 5924/8000  Traning Loss: 90.77382564544678  Train_Reconstruction: 87.26327610015869  Train_KL: 3.5105494558811188  Validation Loss : 91.10114669799805 Val_Reconstruction : 87.63226699829102 Val_KL : 3.4688820838928223\n","Epoch: 5925/8000  Traning Loss: 91.03260326385498  Train_Reconstruction: 87.51693725585938  Train_KL: 3.5156646966934204  Validation Loss : 90.7525863647461 Val_Reconstruction : 87.27257919311523 Val_KL : 3.480003833770752\n","Epoch: 5926/8000  Traning Loss: 90.60865497589111  Train_Reconstruction: 87.08166122436523  Train_KL: 3.526994049549103  Validation Loss : 90.49193954467773 Val_Reconstruction : 87.00664520263672 Val_KL : 3.485294222831726\n","Epoch: 5927/8000  Traning Loss: 90.47682666778564  Train_Reconstruction: 86.95372676849365  Train_KL: 3.5230995416641235  Validation Loss : 90.49515151977539 Val_Reconstruction : 87.00900650024414 Val_KL : 3.486146926879883\n","Epoch: 5928/8000  Traning Loss: 91.01655769348145  Train_Reconstruction: 87.48481941223145  Train_KL: 3.5317376852035522  Validation Loss : 91.09906768798828 Val_Reconstruction : 87.6053237915039 Val_KL : 3.4937461614608765\n","Epoch: 5929/8000  Traning Loss: 90.9392614364624  Train_Reconstruction: 87.42207908630371  Train_KL: 3.517183005809784  Validation Loss : 90.95330810546875 Val_Reconstruction : 87.48838806152344 Val_KL : 3.464922308921814\n","Epoch: 5930/8000  Traning Loss: 91.0033369064331  Train_Reconstruction: 87.49935054779053  Train_KL: 3.503986656665802  Validation Loss : 91.14909744262695 Val_Reconstruction : 87.67987823486328 Val_KL : 3.469217896461487\n","Epoch: 5931/8000  Traning Loss: 91.35476589202881  Train_Reconstruction: 87.83402061462402  Train_KL: 3.5207455158233643  Validation Loss : 91.3122444152832 Val_Reconstruction : 87.82438278198242 Val_KL : 3.48785936832428\n","Epoch: 5932/8000  Traning Loss: 90.9260892868042  Train_Reconstruction: 87.40366458892822  Train_KL: 3.5224239826202393  Validation Loss : 90.93149185180664 Val_Reconstruction : 87.45760726928711 Val_KL : 3.473883628845215\n","Epoch: 5933/8000  Traning Loss: 90.58919906616211  Train_Reconstruction: 87.07324314117432  Train_KL: 3.515956163406372  Validation Loss : 90.63061141967773 Val_Reconstruction : 87.15649795532227 Val_KL : 3.474114418029785\n","Epoch: 5934/8000  Traning Loss: 90.55797290802002  Train_Reconstruction: 87.04021453857422  Train_KL: 3.5177572667598724  Validation Loss : 90.63138198852539 Val_Reconstruction : 87.15312957763672 Val_KL : 3.4782538414001465\n","Epoch: 5935/8000  Traning Loss: 90.40171241760254  Train_Reconstruction: 86.88541507720947  Train_KL: 3.5162963569164276  Validation Loss : 90.7084732055664 Val_Reconstruction : 87.23268127441406 Val_KL : 3.4757914543151855\n","Epoch: 5936/8000  Traning Loss: 90.85898399353027  Train_Reconstruction: 87.33632564544678  Train_KL: 3.522659420967102  Validation Loss : 91.00324630737305 Val_Reconstruction : 87.5206298828125 Val_KL : 3.482614278793335\n","Epoch: 5937/8000  Traning Loss: 90.7546157836914  Train_Reconstruction: 87.22841835021973  Train_KL: 3.5261987447738647  Validation Loss : 91.0039176940918 Val_Reconstruction : 87.52201843261719 Val_KL : 3.4818999767303467\n","Epoch: 5938/8000  Traning Loss: 90.63222599029541  Train_Reconstruction: 87.10179328918457  Train_KL: 3.530432850122452  Validation Loss : 90.64267349243164 Val_Reconstruction : 87.16030883789062 Val_KL : 3.4823639392852783\n","Epoch: 5939/8000  Traning Loss: 90.65281200408936  Train_Reconstruction: 87.12856101989746  Train_KL: 3.5242503881454468  Validation Loss : 90.96893692016602 Val_Reconstruction : 87.4822769165039 Val_KL : 3.4866602420806885\n","Epoch: 5940/8000  Traning Loss: 90.69006633758545  Train_Reconstruction: 87.16944122314453  Train_KL: 3.5206264555454254  Validation Loss : 90.33403778076172 Val_Reconstruction : 86.86627960205078 Val_KL : 3.46776020526886\n","Epoch: 5941/8000  Traning Loss: 90.5629072189331  Train_Reconstruction: 87.05242824554443  Train_KL: 3.510478585958481  Validation Loss : 90.67189025878906 Val_Reconstruction : 87.19211196899414 Val_KL : 3.479776978492737\n","Epoch: 5942/8000  Traning Loss: 90.663498878479  Train_Reconstruction: 87.1353235244751  Train_KL: 3.5281749069690704  Validation Loss : 90.78233337402344 Val_Reconstruction : 87.29261779785156 Val_KL : 3.4897156953811646\n","Epoch: 5943/8000  Traning Loss: 91.04828834533691  Train_Reconstruction: 87.51901817321777  Train_KL: 3.5292716026306152  Validation Loss : 91.42361068725586 Val_Reconstruction : 87.9342155456543 Val_KL : 3.489393472671509\n","Epoch: 5944/8000  Traning Loss: 90.92334747314453  Train_Reconstruction: 87.39928150177002  Train_KL: 3.5240644812583923  Validation Loss : 90.6422004699707 Val_Reconstruction : 87.16411590576172 Val_KL : 3.4780865907669067\n","Epoch: 5945/8000  Traning Loss: 90.44733905792236  Train_Reconstruction: 86.92679595947266  Train_KL: 3.5205438137054443  Validation Loss : 90.24960708618164 Val_Reconstruction : 86.76990509033203 Val_KL : 3.479702115058899\n","Epoch: 5946/8000  Traning Loss: 90.52058410644531  Train_Reconstruction: 86.99687957763672  Train_KL: 3.5237049758434296  Validation Loss : 90.82867431640625 Val_Reconstruction : 87.35358428955078 Val_KL : 3.4750876426696777\n","Epoch: 5947/8000  Traning Loss: 90.62446403503418  Train_Reconstruction: 87.10853958129883  Train_KL: 3.5159241259098053  Validation Loss : 90.46795654296875 Val_Reconstruction : 87.0022087097168 Val_KL : 3.4657492637634277\n","Epoch: 5948/8000  Traning Loss: 90.60138607025146  Train_Reconstruction: 87.08746528625488  Train_KL: 3.5139212906360626  Validation Loss : 90.93975830078125 Val_Reconstruction : 87.45512008666992 Val_KL : 3.484636902809143\n","Epoch: 5949/8000  Traning Loss: 90.68448066711426  Train_Reconstruction: 87.15715217590332  Train_KL: 3.527329057455063  Validation Loss : 90.52662658691406 Val_Reconstruction : 87.04009628295898 Val_KL : 3.4865306615829468\n","Epoch: 5950/8000  Traning Loss: 90.68163585662842  Train_Reconstruction: 87.16106605529785  Train_KL: 3.5205701291561127  Validation Loss : 91.15211868286133 Val_Reconstruction : 87.67309951782227 Val_KL : 3.4790176153182983\n","Epoch: 5951/8000  Traning Loss: 91.16480159759521  Train_Reconstruction: 87.64782333374023  Train_KL: 3.5169783234596252  Validation Loss : 91.04061889648438 Val_Reconstruction : 87.5635871887207 Val_KL : 3.4770301580429077\n","Epoch: 5952/8000  Traning Loss: 90.68081569671631  Train_Reconstruction: 87.16349983215332  Train_KL: 3.5173161327838898  Validation Loss : 90.5902099609375 Val_Reconstruction : 87.10894012451172 Val_KL : 3.4812731742858887\n","Epoch: 5953/8000  Traning Loss: 90.58971500396729  Train_Reconstruction: 87.06184768676758  Train_KL: 3.5278661847114563  Validation Loss : 90.63973617553711 Val_Reconstruction : 87.15028762817383 Val_KL : 3.489448070526123\n","Epoch: 5954/8000  Traning Loss: 90.8458137512207  Train_Reconstruction: 87.3291540145874  Train_KL: 3.5166606307029724  Validation Loss : 91.00936126708984 Val_Reconstruction : 87.54302215576172 Val_KL : 3.4663368463516235\n","Epoch: 5955/8000  Traning Loss: 90.95137691497803  Train_Reconstruction: 87.44408130645752  Train_KL: 3.507296562194824  Validation Loss : 91.15828704833984 Val_Reconstruction : 87.69054412841797 Val_KL : 3.4677445888519287\n","Epoch: 5956/8000  Traning Loss: 91.14609432220459  Train_Reconstruction: 87.62718963623047  Train_KL: 3.518904685974121  Validation Loss : 91.19397354125977 Val_Reconstruction : 87.70357894897461 Val_KL : 3.4903942346572876\n","Epoch: 5957/8000  Traning Loss: 90.97111415863037  Train_Reconstruction: 87.44831466674805  Train_KL: 3.522799998521805  Validation Loss : 90.94255828857422 Val_Reconstruction : 87.46152877807617 Val_KL : 3.481027603149414\n","Epoch: 5958/8000  Traning Loss: 90.40124797821045  Train_Reconstruction: 86.88736248016357  Train_KL: 3.5138854384422302  Validation Loss : 90.48287963867188 Val_Reconstruction : 87.0107650756836 Val_KL : 3.472113251686096\n","Epoch: 5959/8000  Traning Loss: 90.22790336608887  Train_Reconstruction: 86.70264625549316  Train_KL: 3.5252563059329987  Validation Loss : 90.29534149169922 Val_Reconstruction : 86.81562423706055 Val_KL : 3.4797184467315674\n","Epoch: 5960/8000  Traning Loss: 90.38701725006104  Train_Reconstruction: 86.87516593933105  Train_KL: 3.511851191520691  Validation Loss : 90.57884216308594 Val_Reconstruction : 87.11237716674805 Val_KL : 3.466467499732971\n","Epoch: 5961/8000  Traning Loss: 90.68067646026611  Train_Reconstruction: 87.16068172454834  Train_KL: 3.519993096590042  Validation Loss : 90.86749267578125 Val_Reconstruction : 87.37189483642578 Val_KL : 3.495598554611206\n","Epoch: 5962/8000  Traning Loss: 90.55702018737793  Train_Reconstruction: 87.01497840881348  Train_KL: 3.5420420169830322  Validation Loss : 90.70389175415039 Val_Reconstruction : 87.20450210571289 Val_KL : 3.4993871450424194\n","Epoch: 5963/8000  Traning Loss: 90.15469455718994  Train_Reconstruction: 86.62395572662354  Train_KL: 3.530739963054657  Validation Loss : 90.40484619140625 Val_Reconstruction : 86.92096328735352 Val_KL : 3.4838836193084717\n","Epoch: 5964/8000  Traning Loss: 90.37925720214844  Train_Reconstruction: 86.85005187988281  Train_KL: 3.529205799102783  Validation Loss : 90.59593963623047 Val_Reconstruction : 87.10824584960938 Val_KL : 3.4876959323883057\n","Epoch: 5965/8000  Traning Loss: 90.69696998596191  Train_Reconstruction: 87.17230892181396  Train_KL: 3.5246616899967194  Validation Loss : 90.47747802734375 Val_Reconstruction : 87.00403594970703 Val_KL : 3.473440170288086\n","Epoch: 5966/8000  Traning Loss: 91.02675533294678  Train_Reconstruction: 87.51529216766357  Train_KL: 3.511463075876236  Validation Loss : 91.51425170898438 Val_Reconstruction : 88.03900146484375 Val_KL : 3.4752508401870728\n","Epoch: 5967/8000  Traning Loss: 91.14054298400879  Train_Reconstruction: 87.62774753570557  Train_KL: 3.5127949714660645  Validation Loss : 91.40549850463867 Val_Reconstruction : 87.92995071411133 Val_KL : 3.47554874420166\n","Epoch: 5968/8000  Traning Loss: 91.07101821899414  Train_Reconstruction: 87.54973316192627  Train_KL: 3.5212868452072144  Validation Loss : 91.05749893188477 Val_Reconstruction : 87.57106399536133 Val_KL : 3.4864317178726196\n","Epoch: 5969/8000  Traning Loss: 90.86585426330566  Train_Reconstruction: 87.33581733703613  Train_KL: 3.5300380289554596  Validation Loss : 90.69753646850586 Val_Reconstruction : 87.21127319335938 Val_KL : 3.4862619638442993\n","Epoch: 5970/8000  Traning Loss: 90.47966861724854  Train_Reconstruction: 86.95280742645264  Train_KL: 3.5268616676330566  Validation Loss : 90.53684616088867 Val_Reconstruction : 87.06079864501953 Val_KL : 3.476048707962036\n","Epoch: 5971/8000  Traning Loss: 90.45884227752686  Train_Reconstruction: 86.93951225280762  Train_KL: 3.519329160451889  Validation Loss : 90.3694953918457 Val_Reconstruction : 86.89922332763672 Val_KL : 3.4702717065811157\n","Epoch: 5972/8000  Traning Loss: 90.60598468780518  Train_Reconstruction: 87.0834903717041  Train_KL: 3.5224938690662384  Validation Loss : 90.90187072753906 Val_Reconstruction : 87.43062210083008 Val_KL : 3.4712499380111694\n","Epoch: 5973/8000  Traning Loss: 90.68282985687256  Train_Reconstruction: 87.15663146972656  Train_KL: 3.526198148727417  Validation Loss : 91.15388870239258 Val_Reconstruction : 87.6721420288086 Val_KL : 3.4817501306533813\n","Epoch: 5974/8000  Traning Loss: 90.41326522827148  Train_Reconstruction: 86.89207458496094  Train_KL: 3.52119117975235  Validation Loss : 90.42471313476562 Val_Reconstruction : 86.94768142700195 Val_KL : 3.477033853530884\n","Epoch: 5975/8000  Traning Loss: 90.20740604400635  Train_Reconstruction: 86.68519687652588  Train_KL: 3.522208273410797  Validation Loss : 90.39771270751953 Val_Reconstruction : 86.90763473510742 Val_KL : 3.4900777339935303\n","Epoch: 5976/8000  Traning Loss: 90.45869827270508  Train_Reconstruction: 86.93738079071045  Train_KL: 3.521316736936569  Validation Loss : 90.57089233398438 Val_Reconstruction : 87.08662414550781 Val_KL : 3.4842671155929565\n","Epoch: 5977/8000  Traning Loss: 90.98269939422607  Train_Reconstruction: 87.45564270019531  Train_KL: 3.527055323123932  Validation Loss : 91.43952178955078 Val_Reconstruction : 87.95102310180664 Val_KL : 3.48849880695343\n","Epoch: 5978/8000  Traning Loss: 91.20395183563232  Train_Reconstruction: 87.67658615112305  Train_KL: 3.527366280555725  Validation Loss : 91.13912200927734 Val_Reconstruction : 87.65578079223633 Val_KL : 3.4833414554595947\n","Epoch: 5979/8000  Traning Loss: 90.63192653656006  Train_Reconstruction: 87.11243534088135  Train_KL: 3.5194903314113617  Validation Loss : 90.56599044799805 Val_Reconstruction : 87.09234619140625 Val_KL : 3.473641276359558\n","Epoch: 5980/8000  Traning Loss: 90.56218719482422  Train_Reconstruction: 87.0412769317627  Train_KL: 3.520910620689392  Validation Loss : 91.3908805847168 Val_Reconstruction : 87.9121208190918 Val_KL : 3.4787591695785522\n","Epoch: 5981/8000  Traning Loss: 91.80743885040283  Train_Reconstruction: 88.27894115447998  Train_KL: 3.528497576713562  Validation Loss : 92.86625289916992 Val_Reconstruction : 89.38275146484375 Val_KL : 3.4834994077682495\n","Epoch: 5982/8000  Traning Loss: 92.38010501861572  Train_Reconstruction: 88.86408615112305  Train_KL: 3.5160179436206818  Validation Loss : 92.15776062011719 Val_Reconstruction : 88.68818283081055 Val_KL : 3.469577670097351\n","Epoch: 5983/8000  Traning Loss: 91.86806869506836  Train_Reconstruction: 88.3563346862793  Train_KL: 3.511732816696167  Validation Loss : 91.48929977416992 Val_Reconstruction : 88.01226425170898 Val_KL : 3.4770331382751465\n","Epoch: 5984/8000  Traning Loss: 91.16502094268799  Train_Reconstruction: 87.6369457244873  Train_KL: 3.5280748307704926  Validation Loss : 91.09777069091797 Val_Reconstruction : 87.60485458374023 Val_KL : 3.4929161071777344\n","Epoch: 5985/8000  Traning Loss: 91.20258903503418  Train_Reconstruction: 87.67334842681885  Train_KL: 3.5292404294013977  Validation Loss : 91.27673721313477 Val_Reconstruction : 87.7956314086914 Val_KL : 3.481106162071228\n","Epoch: 5986/8000  Traning Loss: 91.62694644927979  Train_Reconstruction: 88.10772705078125  Train_KL: 3.519218385219574  Validation Loss : 91.43453216552734 Val_Reconstruction : 87.95216751098633 Val_KL : 3.4823646545410156\n","Epoch: 5987/8000  Traning Loss: 90.91396236419678  Train_Reconstruction: 87.39189434051514  Train_KL: 3.522068053483963  Validation Loss : 91.50813293457031 Val_Reconstruction : 88.02973175048828 Val_KL : 3.478400230407715\n","Epoch: 5988/8000  Traning Loss: 91.26148128509521  Train_Reconstruction: 87.73655414581299  Train_KL: 3.5249262154102325  Validation Loss : 91.2861328125 Val_Reconstruction : 87.79739761352539 Val_KL : 3.4887341260910034\n","Epoch: 5989/8000  Traning Loss: 91.23187732696533  Train_Reconstruction: 87.70403385162354  Train_KL: 3.5278426706790924  Validation Loss : 91.05596923828125 Val_Reconstruction : 87.57449722290039 Val_KL : 3.481472611427307\n","Epoch: 5990/8000  Traning Loss: 91.0375337600708  Train_Reconstruction: 87.51398277282715  Train_KL: 3.523551344871521  Validation Loss : 91.05804061889648 Val_Reconstruction : 87.56918334960938 Val_KL : 3.4888566732406616\n","Epoch: 5991/8000  Traning Loss: 90.36060905456543  Train_Reconstruction: 86.82665920257568  Train_KL: 3.5339505076408386  Validation Loss : 90.38270568847656 Val_Reconstruction : 86.88747787475586 Val_KL : 3.4952280521392822\n","Epoch: 5992/8000  Traning Loss: 90.38930130004883  Train_Reconstruction: 86.85538959503174  Train_KL: 3.5339117646217346  Validation Loss : 90.57137298583984 Val_Reconstruction : 87.08345413208008 Val_KL : 3.487920045852661\n","Epoch: 5993/8000  Traning Loss: 90.6843729019165  Train_Reconstruction: 87.1615800857544  Train_KL: 3.522793173789978  Validation Loss : 90.99553680419922 Val_Reconstruction : 87.52042770385742 Val_KL : 3.475110173225403\n","Epoch: 5994/8000  Traning Loss: 90.6874008178711  Train_Reconstruction: 87.1644697189331  Train_KL: 3.5229307115077972  Validation Loss : 90.55168914794922 Val_Reconstruction : 87.0633544921875 Val_KL : 3.488334536552429\n","Epoch: 5995/8000  Traning Loss: 90.67294979095459  Train_Reconstruction: 87.14382266998291  Train_KL: 3.529127776622772  Validation Loss : 90.99864959716797 Val_Reconstruction : 87.510986328125 Val_KL : 3.487663507461548\n","Epoch: 5996/8000  Traning Loss: 90.76901912689209  Train_Reconstruction: 87.25170516967773  Train_KL: 3.5173133611679077  Validation Loss : 90.75818252563477 Val_Reconstruction : 87.28512954711914 Val_KL : 3.4730526208877563\n","Epoch: 5997/8000  Traning Loss: 90.73208999633789  Train_Reconstruction: 87.21607494354248  Train_KL: 3.5160156190395355  Validation Loss : 90.93952178955078 Val_Reconstruction : 87.46087646484375 Val_KL : 3.4786456823349\n","Epoch: 5998/8000  Traning Loss: 90.84240818023682  Train_Reconstruction: 87.31166172027588  Train_KL: 3.5307476222515106  Validation Loss : 90.7314682006836 Val_Reconstruction : 87.24047470092773 Val_KL : 3.4909937381744385\n","Epoch: 5999/8000  Traning Loss: 90.36910915374756  Train_Reconstruction: 86.8425989151001  Train_KL: 3.526509404182434  Validation Loss : 90.55868530273438 Val_Reconstruction : 87.07416915893555 Val_KL : 3.4845181703567505\n","Epoch: 6000/8000  Traning Loss: 90.65733432769775  Train_Reconstruction: 87.13197040557861  Train_KL: 3.525364935398102  Validation Loss : 90.725341796875 Val_Reconstruction : 87.24187088012695 Val_KL : 3.483473062515259\n","Epoch: 6001/8000  Traning Loss: 91.00989151000977  Train_Reconstruction: 87.49219608306885  Train_KL: 3.5176956355571747  Validation Loss : 92.35366439819336 Val_Reconstruction : 88.8799057006836 Val_KL : 3.4737584590911865\n","Epoch: 6002/8000  Traning Loss: 90.96065044403076  Train_Reconstruction: 87.45040607452393  Train_KL: 3.5102448761463165  Validation Loss : 90.74930953979492 Val_Reconstruction : 87.28646850585938 Val_KL : 3.462843179702759\n","Epoch: 6003/8000  Traning Loss: 90.73194599151611  Train_Reconstruction: 87.2173204421997  Train_KL: 3.514626204967499  Validation Loss : 90.74137878417969 Val_Reconstruction : 87.25505065917969 Val_KL : 3.4863253831863403\n","Epoch: 6004/8000  Traning Loss: 90.66049480438232  Train_Reconstruction: 87.13515377044678  Train_KL: 3.525341361761093  Validation Loss : 90.90422058105469 Val_Reconstruction : 87.42376708984375 Val_KL : 3.4804545640945435\n","Epoch: 6005/8000  Traning Loss: 90.84282207489014  Train_Reconstruction: 87.32494640350342  Train_KL: 3.5178747475147247  Validation Loss : 90.92709350585938 Val_Reconstruction : 87.45528793334961 Val_KL : 3.471804976463318\n","Epoch: 6006/8000  Traning Loss: 90.99858951568604  Train_Reconstruction: 87.47797107696533  Train_KL: 3.5206199288368225  Validation Loss : 91.2541732788086 Val_Reconstruction : 87.77115249633789 Val_KL : 3.4830214977264404\n","Epoch: 6007/8000  Traning Loss: 91.05187511444092  Train_Reconstruction: 87.52383041381836  Train_KL: 3.528043955564499  Validation Loss : 90.65221786499023 Val_Reconstruction : 87.15652847290039 Val_KL : 3.495689630508423\n","Epoch: 6008/8000  Traning Loss: 90.47701930999756  Train_Reconstruction: 86.95210266113281  Train_KL: 3.52491757273674  Validation Loss : 90.665283203125 Val_Reconstruction : 87.19001388549805 Val_KL : 3.475271701812744\n","Epoch: 6009/8000  Traning Loss: 90.53547859191895  Train_Reconstruction: 87.01785659790039  Train_KL: 3.5176219940185547  Validation Loss : 90.65753173828125 Val_Reconstruction : 87.18021392822266 Val_KL : 3.477318286895752\n","Epoch: 6010/8000  Traning Loss: 90.51311492919922  Train_Reconstruction: 86.98393058776855  Train_KL: 3.529184252023697  Validation Loss : 90.40889739990234 Val_Reconstruction : 86.92167282104492 Val_KL : 3.4872255325317383\n","Epoch: 6011/8000  Traning Loss: 90.54834842681885  Train_Reconstruction: 87.02304935455322  Train_KL: 3.5252992510795593  Validation Loss : 90.61005783081055 Val_Reconstruction : 87.12876510620117 Val_KL : 3.4812910556793213\n","Epoch: 6012/8000  Traning Loss: 90.63874244689941  Train_Reconstruction: 87.12413311004639  Train_KL: 3.5146082937717438  Validation Loss : 90.43257141113281 Val_Reconstruction : 86.95649337768555 Val_KL : 3.4760786294937134\n","Epoch: 6013/8000  Traning Loss: 90.47966957092285  Train_Reconstruction: 86.96098041534424  Train_KL: 3.5186910927295685  Validation Loss : 90.26905059814453 Val_Reconstruction : 86.79139709472656 Val_KL : 3.4776562452316284\n","Epoch: 6014/8000  Traning Loss: 90.40373516082764  Train_Reconstruction: 86.88761520385742  Train_KL: 3.5161194503307343  Validation Loss : 90.61098861694336 Val_Reconstruction : 87.14391326904297 Val_KL : 3.4670764207839966\n","Epoch: 6015/8000  Traning Loss: 90.60031604766846  Train_Reconstruction: 87.08174896240234  Train_KL: 3.5185673236846924  Validation Loss : 90.87329864501953 Val_Reconstruction : 87.39102172851562 Val_KL : 3.4822741746902466\n","Epoch: 6016/8000  Traning Loss: 91.03771781921387  Train_Reconstruction: 87.51286602020264  Train_KL: 3.5248528718948364  Validation Loss : 91.28602981567383 Val_Reconstruction : 87.79683303833008 Val_KL : 3.4891982078552246\n","Epoch: 6017/8000  Traning Loss: 90.79742527008057  Train_Reconstruction: 87.27033615112305  Train_KL: 3.5270904898643494  Validation Loss : 90.76531600952148 Val_Reconstruction : 87.28477478027344 Val_KL : 3.480541229248047\n","Epoch: 6018/8000  Traning Loss: 90.64815711975098  Train_Reconstruction: 87.11834239959717  Train_KL: 3.5298141837120056  Validation Loss : 90.68720626831055 Val_Reconstruction : 87.2017593383789 Val_KL : 3.4854493141174316\n","Epoch: 6019/8000  Traning Loss: 90.46845436096191  Train_Reconstruction: 86.9462890625  Train_KL: 3.5221657156944275  Validation Loss : 90.40199279785156 Val_Reconstruction : 86.92216873168945 Val_KL : 3.479823350906372\n","Epoch: 6020/8000  Traning Loss: 90.28510570526123  Train_Reconstruction: 86.76422214508057  Train_KL: 3.520882725715637  Validation Loss : 90.17678451538086 Val_Reconstruction : 86.69902038574219 Val_KL : 3.477764844894409\n","Epoch: 6021/8000  Traning Loss: 90.3746690750122  Train_Reconstruction: 86.84869384765625  Train_KL: 3.5259750187397003  Validation Loss : 90.42018127441406 Val_Reconstruction : 86.93389511108398 Val_KL : 3.4862855672836304\n","Epoch: 6022/8000  Traning Loss: 90.3894853591919  Train_Reconstruction: 86.86176490783691  Train_KL: 3.5277196168899536  Validation Loss : 90.62167739868164 Val_Reconstruction : 87.13640975952148 Val_KL : 3.485268712043762\n","Epoch: 6023/8000  Traning Loss: 90.37668704986572  Train_Reconstruction: 86.8534164428711  Train_KL: 3.5232717990875244  Validation Loss : 90.38295364379883 Val_Reconstruction : 86.90765762329102 Val_KL : 3.4752964973449707\n","Epoch: 6024/8000  Traning Loss: 90.70416069030762  Train_Reconstruction: 87.18301486968994  Train_KL: 3.5211464762687683  Validation Loss : 90.95314407348633 Val_Reconstruction : 87.46988677978516 Val_KL : 3.4832557439804077\n","Epoch: 6025/8000  Traning Loss: 90.66041469573975  Train_Reconstruction: 87.13552761077881  Train_KL: 3.5248879194259644  Validation Loss : 90.57049942016602 Val_Reconstruction : 87.09020614624023 Val_KL : 3.4802931547164917\n","Epoch: 6026/8000  Traning Loss: 90.31404685974121  Train_Reconstruction: 86.78879833221436  Train_KL: 3.5252485275268555  Validation Loss : 90.41040420532227 Val_Reconstruction : 86.93194198608398 Val_KL : 3.4784626960754395\n","Epoch: 6027/8000  Traning Loss: 90.44464492797852  Train_Reconstruction: 86.92325592041016  Train_KL: 3.5213896334171295  Validation Loss : 90.07401275634766 Val_Reconstruction : 86.5982666015625 Val_KL : 3.475743532180786\n","Epoch: 6028/8000  Traning Loss: 90.36166191101074  Train_Reconstruction: 86.83939170837402  Train_KL: 3.522269695997238  Validation Loss : 90.38071823120117 Val_Reconstruction : 86.89469146728516 Val_KL : 3.4860243797302246\n","Epoch: 6029/8000  Traning Loss: 90.44385147094727  Train_Reconstruction: 86.92242813110352  Train_KL: 3.5214220583438873  Validation Loss : 90.57208633422852 Val_Reconstruction : 87.08954238891602 Val_KL : 3.4825457334518433\n","Epoch: 6030/8000  Traning Loss: 90.50010395050049  Train_Reconstruction: 86.97227478027344  Train_KL: 3.5278292894363403  Validation Loss : 90.70273971557617 Val_Reconstruction : 87.22272872924805 Val_KL : 3.480010747909546\n","Epoch: 6031/8000  Traning Loss: 90.69272422790527  Train_Reconstruction: 87.17290019989014  Train_KL: 3.5198251008987427  Validation Loss : 90.99791717529297 Val_Reconstruction : 87.52921295166016 Val_KL : 3.4687036275863647\n","Epoch: 6032/8000  Traning Loss: 90.68647289276123  Train_Reconstruction: 87.16730499267578  Train_KL: 3.5191684663295746  Validation Loss : 90.6478271484375 Val_Reconstruction : 87.16755294799805 Val_KL : 3.4802730083465576\n","Epoch: 6033/8000  Traning Loss: 90.41851997375488  Train_Reconstruction: 86.88918113708496  Train_KL: 3.529337614774704  Validation Loss : 90.31029510498047 Val_Reconstruction : 86.82667541503906 Val_KL : 3.483622670173645\n","Epoch: 6034/8000  Traning Loss: 90.35587787628174  Train_Reconstruction: 86.83344554901123  Train_KL: 3.5224313139915466  Validation Loss : 90.7320671081543 Val_Reconstruction : 87.24947738647461 Val_KL : 3.4825897216796875\n","Epoch: 6035/8000  Traning Loss: 90.77481079101562  Train_Reconstruction: 87.24983978271484  Train_KL: 3.5249707400798798  Validation Loss : 90.71205139160156 Val_Reconstruction : 87.22411346435547 Val_KL : 3.4879390001296997\n","Epoch: 6036/8000  Traning Loss: 90.60140895843506  Train_Reconstruction: 87.07343864440918  Train_KL: 3.5279698371887207  Validation Loss : 90.60611724853516 Val_Reconstruction : 87.10848617553711 Val_KL : 3.4976320266723633\n","Epoch: 6037/8000  Traning Loss: 90.38061046600342  Train_Reconstruction: 86.8502779006958  Train_KL: 3.530333459377289  Validation Loss : 90.5840072631836 Val_Reconstruction : 87.09500122070312 Val_KL : 3.489005208015442\n","Epoch: 6038/8000  Traning Loss: 90.66332626342773  Train_Reconstruction: 87.13407516479492  Train_KL: 3.529250979423523  Validation Loss : 90.79671478271484 Val_Reconstruction : 87.31187057495117 Val_KL : 3.484841823577881\n","Epoch: 6039/8000  Traning Loss: 90.56659030914307  Train_Reconstruction: 87.04759502410889  Train_KL: 3.5189947485923767  Validation Loss : 90.97290802001953 Val_Reconstruction : 87.50219345092773 Val_KL : 3.470715045928955\n","Epoch: 6040/8000  Traning Loss: 90.57379150390625  Train_Reconstruction: 87.06185817718506  Train_KL: 3.5119341611862183  Validation Loss : 90.75865936279297 Val_Reconstruction : 87.28375625610352 Val_KL : 3.4749021530151367\n","Epoch: 6041/8000  Traning Loss: 90.44359302520752  Train_Reconstruction: 86.91596031188965  Train_KL: 3.5276330709457397  Validation Loss : 90.43625259399414 Val_Reconstruction : 86.95135116577148 Val_KL : 3.484904170036316\n","Epoch: 6042/8000  Traning Loss: 90.52257919311523  Train_Reconstruction: 87.00390243530273  Train_KL: 3.518676996231079  Validation Loss : 90.36150360107422 Val_Reconstruction : 86.88899993896484 Val_KL : 3.4725033044815063\n","Epoch: 6043/8000  Traning Loss: 90.5123643875122  Train_Reconstruction: 86.99537181854248  Train_KL: 3.5169927179813385  Validation Loss : 91.02839279174805 Val_Reconstruction : 87.55400466918945 Val_KL : 3.4743881225585938\n","Epoch: 6044/8000  Traning Loss: 90.9580945968628  Train_Reconstruction: 87.43047046661377  Train_KL: 3.527623951435089  Validation Loss : 90.81580352783203 Val_Reconstruction : 87.32440567016602 Val_KL : 3.491400122642517\n","Epoch: 6045/8000  Traning Loss: 90.89067935943604  Train_Reconstruction: 87.35519123077393  Train_KL: 3.5354886054992676  Validation Loss : 91.01339721679688 Val_Reconstruction : 87.52428817749023 Val_KL : 3.489110231399536\n","Epoch: 6046/8000  Traning Loss: 90.80033493041992  Train_Reconstruction: 87.27274990081787  Train_KL: 3.527585744857788  Validation Loss : 90.86046981811523 Val_Reconstruction : 87.37824630737305 Val_KL : 3.4822232723236084\n","Epoch: 6047/8000  Traning Loss: 90.74181461334229  Train_Reconstruction: 87.22230625152588  Train_KL: 3.519508183002472  Validation Loss : 91.21354675292969 Val_Reconstruction : 87.73799514770508 Val_KL : 3.475552558898926\n","Epoch: 6048/8000  Traning Loss: 90.7441987991333  Train_Reconstruction: 87.23449897766113  Train_KL: 3.509699136018753  Validation Loss : 90.74878311157227 Val_Reconstruction : 87.27781295776367 Val_KL : 3.470968008041382\n","Epoch: 6049/8000  Traning Loss: 90.56117820739746  Train_Reconstruction: 87.04456043243408  Train_KL: 3.516616106033325  Validation Loss : 91.01002883911133 Val_Reconstruction : 87.53847122192383 Val_KL : 3.4715561866760254\n","Epoch: 6050/8000  Traning Loss: 90.48027229309082  Train_Reconstruction: 86.96112728118896  Train_KL: 3.519145429134369  Validation Loss : 90.5429916381836 Val_Reconstruction : 87.05830764770508 Val_KL : 3.484685778617859\n","Epoch: 6051/8000  Traning Loss: 90.34234523773193  Train_Reconstruction: 86.81106472015381  Train_KL: 3.5312798619270325  Validation Loss : 90.51778793334961 Val_Reconstruction : 87.03155136108398 Val_KL : 3.486237049102783\n","Epoch: 6052/8000  Traning Loss: 90.52234840393066  Train_Reconstruction: 86.9956464767456  Train_KL: 3.526700258255005  Validation Loss : 90.58495330810547 Val_Reconstruction : 87.1041488647461 Val_KL : 3.4808048009872437\n","Epoch: 6053/8000  Traning Loss: 90.57522964477539  Train_Reconstruction: 87.04954051971436  Train_KL: 3.5256891548633575  Validation Loss : 90.6202621459961 Val_Reconstruction : 87.12375259399414 Val_KL : 3.4965099096298218\n","Epoch: 6054/8000  Traning Loss: 90.91622924804688  Train_Reconstruction: 87.38094234466553  Train_KL: 3.535286009311676  Validation Loss : 91.10724639892578 Val_Reconstruction : 87.61821365356445 Val_KL : 3.4890307188034058\n","Epoch: 6055/8000  Traning Loss: 90.47623443603516  Train_Reconstruction: 86.96073818206787  Train_KL: 3.5154969394207  Validation Loss : 90.45599746704102 Val_Reconstruction : 86.99040603637695 Val_KL : 3.465591788291931\n","Epoch: 6056/8000  Traning Loss: 90.35605716705322  Train_Reconstruction: 86.84515190124512  Train_KL: 3.5109035670757294  Validation Loss : 90.3956184387207 Val_Reconstruction : 86.92148971557617 Val_KL : 3.4741311073303223\n","Epoch: 6057/8000  Traning Loss: 90.4069128036499  Train_Reconstruction: 86.87884330749512  Train_KL: 3.528069794178009  Validation Loss : 90.51786804199219 Val_Reconstruction : 87.03542709350586 Val_KL : 3.4824399948120117\n","Epoch: 6058/8000  Traning Loss: 90.80480003356934  Train_Reconstruction: 87.28699111938477  Train_KL: 3.5178073942661285  Validation Loss : 91.50854873657227 Val_Reconstruction : 88.03736114501953 Val_KL : 3.471188187599182\n","Epoch: 6059/8000  Traning Loss: 91.01514339447021  Train_Reconstruction: 87.50097751617432  Train_KL: 3.5141648650169373  Validation Loss : 90.96175384521484 Val_Reconstruction : 87.48876571655273 Val_KL : 3.472987413406372\n","Epoch: 6060/8000  Traning Loss: 90.53641128540039  Train_Reconstruction: 87.01623916625977  Train_KL: 3.5201728641986847  Validation Loss : 90.34476470947266 Val_Reconstruction : 86.87043380737305 Val_KL : 3.474329710006714\n","Epoch: 6061/8000  Traning Loss: 90.44218349456787  Train_Reconstruction: 86.91499328613281  Train_KL: 3.5271892845630646  Validation Loss : 90.75545501708984 Val_Reconstruction : 87.27532196044922 Val_KL : 3.4801340103149414\n","Epoch: 6062/8000  Traning Loss: 90.39087104797363  Train_Reconstruction: 86.87098789215088  Train_KL: 3.519882708787918  Validation Loss : 90.55233383178711 Val_Reconstruction : 87.08065032958984 Val_KL : 3.4716806411743164\n","Epoch: 6063/8000  Traning Loss: 90.24139213562012  Train_Reconstruction: 86.72784423828125  Train_KL: 3.5135465562343597  Validation Loss : 90.18668365478516 Val_Reconstruction : 86.71257019042969 Val_KL : 3.474114179611206\n","Epoch: 6064/8000  Traning Loss: 90.30961227416992  Train_Reconstruction: 86.7899923324585  Train_KL: 3.519619196653366  Validation Loss : 90.44501876831055 Val_Reconstruction : 86.96749877929688 Val_KL : 3.477518677711487\n","Epoch: 6065/8000  Traning Loss: 90.60579109191895  Train_Reconstruction: 87.08540630340576  Train_KL: 3.5203847885131836  Validation Loss : 90.67922592163086 Val_Reconstruction : 87.20271301269531 Val_KL : 3.4765138626098633\n","Epoch: 6066/8000  Traning Loss: 90.31345081329346  Train_Reconstruction: 86.79696083068848  Train_KL: 3.516489416360855  Validation Loss : 90.37236404418945 Val_Reconstruction : 86.89713287353516 Val_KL : 3.4752299785614014\n","Epoch: 6067/8000  Traning Loss: 90.25585556030273  Train_Reconstruction: 86.73782062530518  Train_KL: 3.518035888671875  Validation Loss : 90.31146621704102 Val_Reconstruction : 86.83384704589844 Val_KL : 3.4776183366775513\n","Epoch: 6068/8000  Traning Loss: 90.41401100158691  Train_Reconstruction: 86.89594268798828  Train_KL: 3.518069565296173  Validation Loss : 90.2947769165039 Val_Reconstruction : 86.81946563720703 Val_KL : 3.4753113985061646\n","Epoch: 6069/8000  Traning Loss: 90.38573932647705  Train_Reconstruction: 86.85986423492432  Train_KL: 3.5258761048316956  Validation Loss : 90.47595596313477 Val_Reconstruction : 86.98588562011719 Val_KL : 3.4900704622268677\n","Epoch: 6070/8000  Traning Loss: 90.60365295410156  Train_Reconstruction: 87.07220363616943  Train_KL: 3.531449556350708  Validation Loss : 90.76798248291016 Val_Reconstruction : 87.29359436035156 Val_KL : 3.474388360977173\n","Epoch: 6071/8000  Traning Loss: 90.72694301605225  Train_Reconstruction: 87.21522045135498  Train_KL: 3.5117222666740417  Validation Loss : 90.59537887573242 Val_Reconstruction : 87.13229751586914 Val_KL : 3.463083267211914\n","Epoch: 6072/8000  Traning Loss: 90.6066541671753  Train_Reconstruction: 87.0855541229248  Train_KL: 3.5210998356342316  Validation Loss : 90.35591125488281 Val_Reconstruction : 86.86691284179688 Val_KL : 3.4889973402023315\n","Epoch: 6073/8000  Traning Loss: 90.60639667510986  Train_Reconstruction: 87.06134510040283  Train_KL: 3.545051544904709  Validation Loss : 90.58221435546875 Val_Reconstruction : 87.07634735107422 Val_KL : 3.505866765975952\n","Epoch: 6074/8000  Traning Loss: 90.60973834991455  Train_Reconstruction: 87.0730676651001  Train_KL: 3.5366697907447815  Validation Loss : 90.37850189208984 Val_Reconstruction : 86.89139556884766 Val_KL : 3.487106204032898\n","Epoch: 6075/8000  Traning Loss: 90.48589038848877  Train_Reconstruction: 86.96400260925293  Train_KL: 3.5218884646892548  Validation Loss : 90.56112289428711 Val_Reconstruction : 87.08024597167969 Val_KL : 3.4808783531188965\n","Epoch: 6076/8000  Traning Loss: 90.40633583068848  Train_Reconstruction: 86.89069747924805  Train_KL: 3.5156373977661133  Validation Loss : 90.37865829467773 Val_Reconstruction : 86.90656280517578 Val_KL : 3.472097158432007\n","Epoch: 6077/8000  Traning Loss: 90.2138614654541  Train_Reconstruction: 86.69393539428711  Train_KL: 3.519926816225052  Validation Loss : 90.59271621704102 Val_Reconstruction : 87.10773849487305 Val_KL : 3.484976649284363\n","Epoch: 6078/8000  Traning Loss: 90.3573350906372  Train_Reconstruction: 86.82762813568115  Train_KL: 3.529707044363022  Validation Loss : 90.30887603759766 Val_Reconstruction : 86.81597900390625 Val_KL : 3.4928945302963257\n","Epoch: 6079/8000  Traning Loss: 90.32261371612549  Train_Reconstruction: 86.78870487213135  Train_KL: 3.5339087545871735  Validation Loss : 90.19698333740234 Val_Reconstruction : 86.71196365356445 Val_KL : 3.485017776489258\n","Epoch: 6080/8000  Traning Loss: 90.35344505310059  Train_Reconstruction: 86.8325834274292  Train_KL: 3.520860940217972  Validation Loss : 90.40383529663086 Val_Reconstruction : 86.92082977294922 Val_KL : 3.483007073402405\n","Epoch: 6081/8000  Traning Loss: 90.62225532531738  Train_Reconstruction: 87.0984001159668  Train_KL: 3.523856282234192  Validation Loss : 90.85381698608398 Val_Reconstruction : 87.36796188354492 Val_KL : 3.4858551025390625\n","Epoch: 6082/8000  Traning Loss: 90.68794918060303  Train_Reconstruction: 87.16959571838379  Train_KL: 3.518353193998337  Validation Loss : 90.9296875 Val_Reconstruction : 87.45630264282227 Val_KL : 3.4733821153640747\n","Epoch: 6083/8000  Traning Loss: 90.69887447357178  Train_Reconstruction: 87.1814136505127  Train_KL: 3.5174614787101746  Validation Loss : 90.93366622924805 Val_Reconstruction : 87.44747543334961 Val_KL : 3.486189603805542\n","Epoch: 6084/8000  Traning Loss: 90.42899513244629  Train_Reconstruction: 86.89886379241943  Train_KL: 3.530131995677948  Validation Loss : 90.48208999633789 Val_Reconstruction : 86.99317932128906 Val_KL : 3.488912582397461\n","Epoch: 6085/8000  Traning Loss: 90.25396060943604  Train_Reconstruction: 86.7313175201416  Train_KL: 3.522643119096756  Validation Loss : 90.48760604858398 Val_Reconstruction : 87.01554870605469 Val_KL : 3.4720582962036133\n","Epoch: 6086/8000  Traning Loss: 90.40363216400146  Train_Reconstruction: 86.88540267944336  Train_KL: 3.5182293951511383  Validation Loss : 90.49755859375 Val_Reconstruction : 87.02098846435547 Val_KL : 3.476570248603821\n","Epoch: 6087/8000  Traning Loss: 90.60503959655762  Train_Reconstruction: 87.08409976959229  Train_KL: 3.5209408700466156  Validation Loss : 90.54646682739258 Val_Reconstruction : 87.06271743774414 Val_KL : 3.4837498664855957\n","Epoch: 6088/8000  Traning Loss: 90.88346672058105  Train_Reconstruction: 87.3536319732666  Train_KL: 3.5298349857330322  Validation Loss : 90.85984420776367 Val_Reconstruction : 87.37237930297852 Val_KL : 3.487465500831604\n","Epoch: 6089/8000  Traning Loss: 90.75548553466797  Train_Reconstruction: 87.22807884216309  Train_KL: 3.5274062752723694  Validation Loss : 90.82147598266602 Val_Reconstruction : 87.33916854858398 Val_KL : 3.4823077917099\n","Epoch: 6090/8000  Traning Loss: 90.42600345611572  Train_Reconstruction: 86.90233325958252  Train_KL: 3.5236696302890778  Validation Loss : 90.71133804321289 Val_Reconstruction : 87.22754669189453 Val_KL : 3.483789563179016\n","Epoch: 6091/8000  Traning Loss: 90.61625385284424  Train_Reconstruction: 87.09798431396484  Train_KL: 3.518269419670105  Validation Loss : 90.3836784362793 Val_Reconstruction : 86.90746688842773 Val_KL : 3.4762126207351685\n","Epoch: 6092/8000  Traning Loss: 90.663893699646  Train_Reconstruction: 87.13139533996582  Train_KL: 3.5324977934360504  Validation Loss : 91.16922760009766 Val_Reconstruction : 87.66755294799805 Val_KL : 3.501671552658081\n","Epoch: 6093/8000  Traning Loss: 91.27302837371826  Train_Reconstruction: 87.73625183105469  Train_KL: 3.536776453256607  Validation Loss : 91.64082336425781 Val_Reconstruction : 88.15055465698242 Val_KL : 3.4902689456939697\n","Epoch: 6094/8000  Traning Loss: 91.0194730758667  Train_Reconstruction: 87.4966459274292  Train_KL: 3.5228272080421448  Validation Loss : 91.2079086303711 Val_Reconstruction : 87.72886276245117 Val_KL : 3.4790470600128174\n","Epoch: 6095/8000  Traning Loss: 90.87364101409912  Train_Reconstruction: 87.35148906707764  Train_KL: 3.522153228521347  Validation Loss : 91.80221939086914 Val_Reconstruction : 88.3150634765625 Val_KL : 3.487155795097351\n","Epoch: 6096/8000  Traning Loss: 91.17817974090576  Train_Reconstruction: 87.64414310455322  Train_KL: 3.5340375304222107  Validation Loss : 91.50214004516602 Val_Reconstruction : 88.01240921020508 Val_KL : 3.4897282123565674\n","Epoch: 6097/8000  Traning Loss: 91.14074993133545  Train_Reconstruction: 87.62646198272705  Train_KL: 3.514288157224655  Validation Loss : 91.35322189331055 Val_Reconstruction : 87.896484375 Val_KL : 3.4567376375198364\n","Epoch: 6098/8000  Traning Loss: 90.81591320037842  Train_Reconstruction: 87.3098840713501  Train_KL: 3.5060303807258606  Validation Loss : 90.37535858154297 Val_Reconstruction : 86.88982009887695 Val_KL : 3.4855382442474365\n","Epoch: 6099/8000  Traning Loss: 90.41068840026855  Train_Reconstruction: 86.88290405273438  Train_KL: 3.5277844071388245  Validation Loss : 90.39833450317383 Val_Reconstruction : 86.91477966308594 Val_KL : 3.4835546016693115\n","Epoch: 6100/8000  Traning Loss: 90.4843397140503  Train_Reconstruction: 86.96500301361084  Train_KL: 3.5193375945091248  Validation Loss : 90.6584587097168 Val_Reconstruction : 87.1798095703125 Val_KL : 3.478650450706482\n","Epoch: 6101/8000  Traning Loss: 90.64086055755615  Train_Reconstruction: 87.10896110534668  Train_KL: 3.531899333000183  Validation Loss : 90.75178146362305 Val_Reconstruction : 87.26289749145508 Val_KL : 3.488884449005127\n","Epoch: 6102/8000  Traning Loss: 90.70332050323486  Train_Reconstruction: 87.17373275756836  Train_KL: 3.5295886397361755  Validation Loss : 90.67010498046875 Val_Reconstruction : 87.18954086303711 Val_KL : 3.4805644750595093\n","Epoch: 6103/8000  Traning Loss: 90.50284576416016  Train_Reconstruction: 86.98004722595215  Train_KL: 3.522798001766205  Validation Loss : 90.45409393310547 Val_Reconstruction : 86.9679946899414 Val_KL : 3.486096978187561\n","Epoch: 6104/8000  Traning Loss: 90.69272136688232  Train_Reconstruction: 87.16383647918701  Train_KL: 3.5288840532302856  Validation Loss : 90.99868392944336 Val_Reconstruction : 87.5091323852539 Val_KL : 3.4895503520965576\n","Epoch: 6105/8000  Traning Loss: 91.33154010772705  Train_Reconstruction: 87.8031873703003  Train_KL: 3.5283538699150085  Validation Loss : 91.38492202758789 Val_Reconstruction : 87.90293502807617 Val_KL : 3.4819871187210083\n","Epoch: 6106/8000  Traning Loss: 91.84078311920166  Train_Reconstruction: 88.32109546661377  Train_KL: 3.5196882784366608  Validation Loss : 91.73550033569336 Val_Reconstruction : 88.262939453125 Val_KL : 3.4725582599639893\n","Epoch: 6107/8000  Traning Loss: 91.2604923248291  Train_Reconstruction: 87.74229621887207  Train_KL: 3.5181964337825775  Validation Loss : 91.10834121704102 Val_Reconstruction : 87.62854385375977 Val_KL : 3.479796528816223\n","Epoch: 6108/8000  Traning Loss: 90.953200340271  Train_Reconstruction: 87.43262672424316  Train_KL: 3.520573675632477  Validation Loss : 90.65987396240234 Val_Reconstruction : 87.18232727050781 Val_KL : 3.477545738220215\n","Epoch: 6109/8000  Traning Loss: 90.78725337982178  Train_Reconstruction: 87.2718152999878  Train_KL: 3.5154378712177277  Validation Loss : 90.71513748168945 Val_Reconstruction : 87.24742889404297 Val_KL : 3.4677094221115112\n","Epoch: 6110/8000  Traning Loss: 90.6045389175415  Train_Reconstruction: 87.09521484375  Train_KL: 3.509324014186859  Validation Loss : 90.36409759521484 Val_Reconstruction : 86.8908805847168 Val_KL : 3.4732152223587036\n","Epoch: 6111/8000  Traning Loss: 90.86792469024658  Train_Reconstruction: 87.3390941619873  Train_KL: 3.5288319885730743  Validation Loss : 91.06267166137695 Val_Reconstruction : 87.57081985473633 Val_KL : 3.4918512105941772\n","Epoch: 6112/8000  Traning Loss: 91.25270080566406  Train_Reconstruction: 87.72317409515381  Train_KL: 3.5295265913009644  Validation Loss : 91.36056137084961 Val_Reconstruction : 87.87660598754883 Val_KL : 3.48395574092865\n","Epoch: 6113/8000  Traning Loss: 90.91417026519775  Train_Reconstruction: 87.39604091644287  Train_KL: 3.5181296467781067  Validation Loss : 91.02144241333008 Val_Reconstruction : 87.54045104980469 Val_KL : 3.4809911251068115\n","Epoch: 6114/8000  Traning Loss: 90.7497787475586  Train_Reconstruction: 87.2219705581665  Train_KL: 3.527807980775833  Validation Loss : 90.65584182739258 Val_Reconstruction : 87.1608772277832 Val_KL : 3.4949634075164795\n","Epoch: 6115/8000  Traning Loss: 90.45963859558105  Train_Reconstruction: 86.9299201965332  Train_KL: 3.5297186374664307  Validation Loss : 90.93710708618164 Val_Reconstruction : 87.45512008666992 Val_KL : 3.4819871187210083\n","Epoch: 6116/8000  Traning Loss: 90.56165409088135  Train_Reconstruction: 87.04209327697754  Train_KL: 3.51956045627594  Validation Loss : 90.67497634887695 Val_Reconstruction : 87.20521545410156 Val_KL : 3.469758629798889\n","Epoch: 6117/8000  Traning Loss: 90.65039253234863  Train_Reconstruction: 87.14093017578125  Train_KL: 3.509461909532547  Validation Loss : 90.31331253051758 Val_Reconstruction : 86.843994140625 Val_KL : 3.4693208932876587\n","Epoch: 6118/8000  Traning Loss: 90.39481163024902  Train_Reconstruction: 86.86862373352051  Train_KL: 3.5261864960193634  Validation Loss : 90.21390533447266 Val_Reconstruction : 86.7182731628418 Val_KL : 3.4956326484680176\n","Epoch: 6119/8000  Traning Loss: 90.18097305297852  Train_Reconstruction: 86.6476411819458  Train_KL: 3.5333317518234253  Validation Loss : 90.46975326538086 Val_Reconstruction : 86.9879379272461 Val_KL : 3.4818140268325806\n","Epoch: 6120/8000  Traning Loss: 90.48423957824707  Train_Reconstruction: 86.97308444976807  Train_KL: 3.511154592037201  Validation Loss : 90.75704956054688 Val_Reconstruction : 87.28798294067383 Val_KL : 3.469067335128784\n","Epoch: 6121/8000  Traning Loss: 90.61918258666992  Train_Reconstruction: 87.1099214553833  Train_KL: 3.5092623233795166  Validation Loss : 90.48070526123047 Val_Reconstruction : 87.00484085083008 Val_KL : 3.4758670330047607\n","Epoch: 6122/8000  Traning Loss: 90.66439723968506  Train_Reconstruction: 87.14062690734863  Train_KL: 3.523769497871399  Validation Loss : 90.64899826049805 Val_Reconstruction : 87.16497421264648 Val_KL : 3.484026312828064\n","Epoch: 6123/8000  Traning Loss: 90.74579620361328  Train_Reconstruction: 87.2222490310669  Train_KL: 3.5235475897789  Validation Loss : 90.79721069335938 Val_Reconstruction : 87.3129653930664 Val_KL : 3.4842482805252075\n","Epoch: 6124/8000  Traning Loss: 91.04561996459961  Train_Reconstruction: 87.52718734741211  Train_KL: 3.5184325873851776  Validation Loss : 91.45710754394531 Val_Reconstruction : 87.97835159301758 Val_KL : 3.47875714302063\n","Epoch: 6125/8000  Traning Loss: 90.84222507476807  Train_Reconstruction: 87.31814193725586  Train_KL: 3.524083375930786  Validation Loss : 90.99150848388672 Val_Reconstruction : 87.5071792602539 Val_KL : 3.484325885772705\n","Epoch: 6126/8000  Traning Loss: 90.47651386260986  Train_Reconstruction: 86.9536657333374  Train_KL: 3.522848218679428  Validation Loss : 90.45354461669922 Val_Reconstruction : 86.98100280761719 Val_KL : 3.4725394248962402\n","Epoch: 6127/8000  Traning Loss: 90.1433515548706  Train_Reconstruction: 86.62610054016113  Train_KL: 3.517250418663025  Validation Loss : 90.18019485473633 Val_Reconstruction : 86.69549179077148 Val_KL : 3.4847015142440796\n","Epoch: 6128/8000  Traning Loss: 90.07783889770508  Train_Reconstruction: 86.55026721954346  Train_KL: 3.5275731086730957  Validation Loss : 90.25500869750977 Val_Reconstruction : 86.77263641357422 Val_KL : 3.482369303703308\n","Epoch: 6129/8000  Traning Loss: 89.92131042480469  Train_Reconstruction: 86.40280055999756  Train_KL: 3.518510490655899  Validation Loss : 89.89337539672852 Val_Reconstruction : 86.43042755126953 Val_KL : 3.4629499912261963\n","Epoch: 6130/8000  Traning Loss: 90.15402126312256  Train_Reconstruction: 86.6474380493164  Train_KL: 3.506583660840988  Validation Loss : 90.28700256347656 Val_Reconstruction : 86.82206726074219 Val_KL : 3.4649332761764526\n","Epoch: 6131/8000  Traning Loss: 90.0585880279541  Train_Reconstruction: 86.5387659072876  Train_KL: 3.519820213317871  Validation Loss : 90.26374435424805 Val_Reconstruction : 86.78293228149414 Val_KL : 3.4808132648468018\n","Epoch: 6132/8000  Traning Loss: 90.24316024780273  Train_Reconstruction: 86.72236728668213  Train_KL: 3.5207934081554413  Validation Loss : 90.49386978149414 Val_Reconstruction : 87.01431655883789 Val_KL : 3.4795533418655396\n","Epoch: 6133/8000  Traning Loss: 90.45460510253906  Train_Reconstruction: 86.9305534362793  Train_KL: 3.524053156375885  Validation Loss : 90.5453109741211 Val_Reconstruction : 87.05873107910156 Val_KL : 3.4865787029266357\n","Epoch: 6134/8000  Traning Loss: 90.38320350646973  Train_Reconstruction: 86.8516731262207  Train_KL: 3.5315299928188324  Validation Loss : 90.37789916992188 Val_Reconstruction : 86.89195251464844 Val_KL : 3.4859474897384644\n","Epoch: 6135/8000  Traning Loss: 90.18894481658936  Train_Reconstruction: 86.6638240814209  Train_KL: 3.5251214802265167  Validation Loss : 90.49363708496094 Val_Reconstruction : 87.01583480834961 Val_KL : 3.4778040647506714\n","Epoch: 6136/8000  Traning Loss: 90.33653354644775  Train_Reconstruction: 86.8188886642456  Train_KL: 3.51764640212059  Validation Loss : 90.66997528076172 Val_Reconstruction : 87.18943405151367 Val_KL : 3.480539917945862\n","Epoch: 6137/8000  Traning Loss: 90.20424938201904  Train_Reconstruction: 86.68093967437744  Train_KL: 3.523310035467148  Validation Loss : 90.55456924438477 Val_Reconstruction : 87.07329177856445 Val_KL : 3.4812769889831543\n","Epoch: 6138/8000  Traning Loss: 90.40762138366699  Train_Reconstruction: 86.89224147796631  Train_KL: 3.515381634235382  Validation Loss : 90.41337585449219 Val_Reconstruction : 86.9366683959961 Val_KL : 3.476710319519043\n","Epoch: 6139/8000  Traning Loss: 90.47577953338623  Train_Reconstruction: 86.95931911468506  Train_KL: 3.5164594650268555  Validation Loss : 90.4506721496582 Val_Reconstruction : 86.97483825683594 Val_KL : 3.475832939147949\n","Epoch: 6140/8000  Traning Loss: 90.69547367095947  Train_Reconstruction: 87.17337417602539  Train_KL: 3.5221003890037537  Validation Loss : 91.3790512084961 Val_Reconstruction : 87.89519119262695 Val_KL : 3.4838606119155884\n","Epoch: 6141/8000  Traning Loss: 90.60203838348389  Train_Reconstruction: 87.08184051513672  Train_KL: 3.5201966762542725  Validation Loss : 90.90530014038086 Val_Reconstruction : 87.43217849731445 Val_KL : 3.4731236696243286\n","Epoch: 6142/8000  Traning Loss: 90.47524833679199  Train_Reconstruction: 86.96391582489014  Train_KL: 3.511332005262375  Validation Loss : 90.45048904418945 Val_Reconstruction : 86.97818756103516 Val_KL : 3.4723026752471924\n","Epoch: 6143/8000  Traning Loss: 90.40420055389404  Train_Reconstruction: 86.88267993927002  Train_KL: 3.521521717309952  Validation Loss : 90.50239944458008 Val_Reconstruction : 87.019287109375 Val_KL : 3.4831104278564453\n","Epoch: 6144/8000  Traning Loss: 90.19802188873291  Train_Reconstruction: 86.66211318969727  Train_KL: 3.535909742116928  Validation Loss : 90.25350189208984 Val_Reconstruction : 86.75817489624023 Val_KL : 3.4953256845474243\n","Epoch: 6145/8000  Traning Loss: 90.32157802581787  Train_Reconstruction: 86.79523944854736  Train_KL: 3.526339590549469  Validation Loss : 90.49626541137695 Val_Reconstruction : 87.0133056640625 Val_KL : 3.4829591512680054\n","Epoch: 6146/8000  Traning Loss: 90.49445533752441  Train_Reconstruction: 86.97602558135986  Train_KL: 3.5184288918972015  Validation Loss : 90.65470504760742 Val_Reconstruction : 87.16986465454102 Val_KL : 3.4848417043685913\n","Epoch: 6147/8000  Traning Loss: 90.50897216796875  Train_Reconstruction: 86.98917388916016  Train_KL: 3.5197996497154236  Validation Loss : 90.31402587890625 Val_Reconstruction : 86.8466567993164 Val_KL : 3.4673688411712646\n","Epoch: 6148/8000  Traning Loss: 90.15662002563477  Train_Reconstruction: 86.64002799987793  Train_KL: 3.516591399908066  Validation Loss : 90.31140899658203 Val_Reconstruction : 86.83248901367188 Val_KL : 3.4789175987243652\n","Epoch: 6149/8000  Traning Loss: 90.3080883026123  Train_Reconstruction: 86.78324031829834  Train_KL: 3.5248485803604126  Validation Loss : 90.43023681640625 Val_Reconstruction : 86.94573211669922 Val_KL : 3.4845051765441895\n","Epoch: 6150/8000  Traning Loss: 90.77733516693115  Train_Reconstruction: 87.24553108215332  Train_KL: 3.531804144382477  Validation Loss : 91.20378494262695 Val_Reconstruction : 87.709228515625 Val_KL : 3.4945569038391113\n","Epoch: 6151/8000  Traning Loss: 90.74233627319336  Train_Reconstruction: 87.2143325805664  Train_KL: 3.5280033946037292  Validation Loss : 91.18229675292969 Val_Reconstruction : 87.69539642333984 Val_KL : 3.486900568008423\n","Epoch: 6152/8000  Traning Loss: 90.74313545227051  Train_Reconstruction: 87.2187910079956  Train_KL: 3.524344503879547  Validation Loss : 90.5880241394043 Val_Reconstruction : 87.09793090820312 Val_KL : 3.4900938272476196\n","Epoch: 6153/8000  Traning Loss: 90.75081443786621  Train_Reconstruction: 87.22171401977539  Train_KL: 3.5291018187999725  Validation Loss : 91.34487533569336 Val_Reconstruction : 87.86494827270508 Val_KL : 3.4799253940582275\n","Epoch: 6154/8000  Traning Loss: 91.17445182800293  Train_Reconstruction: 87.65432834625244  Train_KL: 3.5201222598552704  Validation Loss : 91.58080673217773 Val_Reconstruction : 88.09675216674805 Val_KL : 3.484055519104004\n","Epoch: 6155/8000  Traning Loss: 91.05481910705566  Train_Reconstruction: 87.52848052978516  Train_KL: 3.526338815689087  Validation Loss : 91.02468872070312 Val_Reconstruction : 87.5356330871582 Val_KL : 3.489055633544922\n","Epoch: 6156/8000  Traning Loss: 90.67831134796143  Train_Reconstruction: 87.15134906768799  Train_KL: 3.52696231007576  Validation Loss : 90.8714828491211 Val_Reconstruction : 87.38533782958984 Val_KL : 3.486145257949829\n","Epoch: 6157/8000  Traning Loss: 90.51662158966064  Train_Reconstruction: 86.99113655090332  Train_KL: 3.5254836678504944  Validation Loss : 90.56827545166016 Val_Reconstruction : 87.08597564697266 Val_KL : 3.4823018312454224\n","Epoch: 6158/8000  Traning Loss: 90.86542701721191  Train_Reconstruction: 87.34939002990723  Train_KL: 3.516035795211792  Validation Loss : 90.74958419799805 Val_Reconstruction : 87.27394485473633 Val_KL : 3.4756420850753784\n","Epoch: 6159/8000  Traning Loss: 90.52260398864746  Train_Reconstruction: 86.99851608276367  Train_KL: 3.52409029006958  Validation Loss : 90.41367721557617 Val_Reconstruction : 86.92204666137695 Val_KL : 3.4916300773620605\n","Epoch: 6160/8000  Traning Loss: 90.41895198822021  Train_Reconstruction: 86.8824405670166  Train_KL: 3.5365107655525208  Validation Loss : 90.52999114990234 Val_Reconstruction : 87.03136825561523 Val_KL : 3.498622417449951\n","Epoch: 6161/8000  Traning Loss: 90.28587818145752  Train_Reconstruction: 86.75122928619385  Train_KL: 3.5346480906009674  Validation Loss : 90.44446563720703 Val_Reconstruction : 86.95129013061523 Val_KL : 3.493174195289612\n","Epoch: 6162/8000  Traning Loss: 90.47139930725098  Train_Reconstruction: 86.94634342193604  Train_KL: 3.525055170059204  Validation Loss : 90.60314178466797 Val_Reconstruction : 87.1193618774414 Val_KL : 3.483778238296509\n","Epoch: 6163/8000  Traning Loss: 90.64041328430176  Train_Reconstruction: 87.11635780334473  Train_KL: 3.524055600166321  Validation Loss : 90.59469223022461 Val_Reconstruction : 87.1091537475586 Val_KL : 3.485537528991699\n","Epoch: 6164/8000  Traning Loss: 90.56721878051758  Train_Reconstruction: 87.03785800933838  Train_KL: 3.5293608605861664  Validation Loss : 90.52201080322266 Val_Reconstruction : 87.03371810913086 Val_KL : 3.48829448223114\n","Epoch: 6165/8000  Traning Loss: 90.50051021575928  Train_Reconstruction: 86.97286701202393  Train_KL: 3.527643859386444  Validation Loss : 90.21581649780273 Val_Reconstruction : 86.7292251586914 Val_KL : 3.48659086227417\n","Epoch: 6166/8000  Traning Loss: 90.59833717346191  Train_Reconstruction: 87.07574081420898  Train_KL: 3.5225968956947327  Validation Loss : 90.92808151245117 Val_Reconstruction : 87.44780731201172 Val_KL : 3.4802753925323486\n","Epoch: 6167/8000  Traning Loss: 90.54124546051025  Train_Reconstruction: 87.02024555206299  Train_KL: 3.5210010707378387  Validation Loss : 90.29048538208008 Val_Reconstruction : 86.81237030029297 Val_KL : 3.478114128112793\n","Epoch: 6168/8000  Traning Loss: 90.5733871459961  Train_Reconstruction: 87.05863761901855  Train_KL: 3.514748841524124  Validation Loss : 90.6499137878418 Val_Reconstruction : 87.17327499389648 Val_KL : 3.4766374826431274\n","Epoch: 6169/8000  Traning Loss: 90.63626480102539  Train_Reconstruction: 87.12033176422119  Train_KL: 3.5159323811531067  Validation Loss : 90.67038345336914 Val_Reconstruction : 87.19694900512695 Val_KL : 3.4734323024749756\n","Epoch: 6170/8000  Traning Loss: 90.67923736572266  Train_Reconstruction: 87.1594123840332  Train_KL: 3.519825965166092  Validation Loss : 91.11954879760742 Val_Reconstruction : 87.64508819580078 Val_KL : 3.474458932876587\n","Epoch: 6171/8000  Traning Loss: 90.56947135925293  Train_Reconstruction: 87.04338073730469  Train_KL: 3.526090621948242  Validation Loss : 90.44347763061523 Val_Reconstruction : 86.94911193847656 Val_KL : 3.494364857673645\n","Epoch: 6172/8000  Traning Loss: 90.25541687011719  Train_Reconstruction: 86.71927070617676  Train_KL: 3.5361463129520416  Validation Loss : 90.27576065063477 Val_Reconstruction : 86.79032897949219 Val_KL : 3.485431671142578\n","Epoch: 6173/8000  Traning Loss: 90.30056571960449  Train_Reconstruction: 86.77599716186523  Train_KL: 3.5245681703090668  Validation Loss : 90.50967025756836 Val_Reconstruction : 87.02794647216797 Val_KL : 3.4817256927490234\n","Epoch: 6174/8000  Traning Loss: 90.34628105163574  Train_Reconstruction: 86.82266139984131  Train_KL: 3.52361860871315  Validation Loss : 90.30661392211914 Val_Reconstruction : 86.81962203979492 Val_KL : 3.4869916439056396\n","Epoch: 6175/8000  Traning Loss: 90.32606887817383  Train_Reconstruction: 86.7959213256836  Train_KL: 3.5301478803157806  Validation Loss : 90.4023551940918 Val_Reconstruction : 86.90626525878906 Val_KL : 3.4960910081863403\n","Epoch: 6176/8000  Traning Loss: 90.34361839294434  Train_Reconstruction: 86.81295680999756  Train_KL: 3.530662089586258  Validation Loss : 90.39287948608398 Val_Reconstruction : 86.91458892822266 Val_KL : 3.478290319442749\n","Epoch: 6177/8000  Traning Loss: 90.37289237976074  Train_Reconstruction: 86.84766864776611  Train_KL: 3.52522411942482  Validation Loss : 90.80154037475586 Val_Reconstruction : 87.32037734985352 Val_KL : 3.481162667274475\n","Epoch: 6178/8000  Traning Loss: 90.80281448364258  Train_Reconstruction: 87.27661800384521  Train_KL: 3.526196628808975  Validation Loss : 90.8802604675293 Val_Reconstruction : 87.39433670043945 Val_KL : 3.485923409461975\n","Epoch: 6179/8000  Traning Loss: 90.58457851409912  Train_Reconstruction: 87.05459785461426  Train_KL: 3.5299799740314484  Validation Loss : 90.30685424804688 Val_Reconstruction : 86.8257942199707 Val_KL : 3.4810597896575928\n","Epoch: 6180/8000  Traning Loss: 90.50815200805664  Train_Reconstruction: 86.98982048034668  Train_KL: 3.518331855535507  Validation Loss : 90.55950546264648 Val_Reconstruction : 87.07821655273438 Val_KL : 3.481290340423584\n","Epoch: 6181/8000  Traning Loss: 90.47157764434814  Train_Reconstruction: 86.93874168395996  Train_KL: 3.5328361988067627  Validation Loss : 90.36575698852539 Val_Reconstruction : 86.86523818969727 Val_KL : 3.500520944595337\n","Epoch: 6182/8000  Traning Loss: 90.36976718902588  Train_Reconstruction: 86.8399715423584  Train_KL: 3.5297954976558685  Validation Loss : 90.56356811523438 Val_Reconstruction : 87.08525085449219 Val_KL : 3.4783148765563965\n","Epoch: 6183/8000  Traning Loss: 90.44777774810791  Train_Reconstruction: 86.93151473999023  Train_KL: 3.516261875629425  Validation Loss : 90.54588317871094 Val_Reconstruction : 87.07064819335938 Val_KL : 3.475235342979431\n","Epoch: 6184/8000  Traning Loss: 90.5452938079834  Train_Reconstruction: 87.01752185821533  Train_KL: 3.527771681547165  Validation Loss : 90.75732421875 Val_Reconstruction : 87.26803207397461 Val_KL : 3.489294171333313\n","Epoch: 6185/8000  Traning Loss: 90.49903202056885  Train_Reconstruction: 86.96697425842285  Train_KL: 3.53205743432045  Validation Loss : 90.97517395019531 Val_Reconstruction : 87.4816665649414 Val_KL : 3.493507504463196\n","Epoch: 6186/8000  Traning Loss: 91.01167011260986  Train_Reconstruction: 87.47980880737305  Train_KL: 3.5318613052368164  Validation Loss : 91.63775634765625 Val_Reconstruction : 88.15670394897461 Val_KL : 3.481054425239563\n","Epoch: 6187/8000  Traning Loss: 91.5080099105835  Train_Reconstruction: 87.9885778427124  Train_KL: 3.5194323658943176  Validation Loss : 92.22642517089844 Val_Reconstruction : 88.74748992919922 Val_KL : 3.4789336919784546\n","Epoch: 6188/8000  Traning Loss: 91.56103134155273  Train_Reconstruction: 88.03361797332764  Train_KL: 3.527413159608841  Validation Loss : 91.82613372802734 Val_Reconstruction : 88.34067153930664 Val_KL : 3.4854625463485718\n","Epoch: 6189/8000  Traning Loss: 91.33396339416504  Train_Reconstruction: 87.8061408996582  Train_KL: 3.5278237760066986  Validation Loss : 91.39573669433594 Val_Reconstruction : 87.91266250610352 Val_KL : 3.483073592185974\n","Epoch: 6190/8000  Traning Loss: 90.7643871307373  Train_Reconstruction: 87.2473030090332  Train_KL: 3.5170844197273254  Validation Loss : 90.70455551147461 Val_Reconstruction : 87.2383804321289 Val_KL : 3.4661749601364136\n","Epoch: 6191/8000  Traning Loss: 90.52833271026611  Train_Reconstruction: 87.01919078826904  Train_KL: 3.5091409385204315  Validation Loss : 90.79078674316406 Val_Reconstruction : 87.31620025634766 Val_KL : 3.474586009979248\n","Epoch: 6192/8000  Traning Loss: 90.46479225158691  Train_Reconstruction: 86.94346618652344  Train_KL: 3.521327167749405  Validation Loss : 90.75304412841797 Val_Reconstruction : 87.28050231933594 Val_KL : 3.472541570663452\n","Epoch: 6193/8000  Traning Loss: 90.60831546783447  Train_Reconstruction: 87.0892276763916  Train_KL: 3.5190887451171875  Validation Loss : 90.47102737426758 Val_Reconstruction : 86.99093246459961 Val_KL : 3.4800968170166016\n","Epoch: 6194/8000  Traning Loss: 90.60177040100098  Train_Reconstruction: 87.07927989959717  Train_KL: 3.5224891006946564  Validation Loss : 90.64614486694336 Val_Reconstruction : 87.17741775512695 Val_KL : 3.468727231025696\n","Epoch: 6195/8000  Traning Loss: 90.6464729309082  Train_Reconstruction: 87.1342716217041  Train_KL: 3.512200266122818  Validation Loss : 90.71961975097656 Val_Reconstruction : 87.24669647216797 Val_KL : 3.4729238748550415\n","Epoch: 6196/8000  Traning Loss: 90.71006107330322  Train_Reconstruction: 87.17947483062744  Train_KL: 3.530586451292038  Validation Loss : 90.94970703125 Val_Reconstruction : 87.44623565673828 Val_KL : 3.503471851348877\n","Epoch: 6197/8000  Traning Loss: 90.7182092666626  Train_Reconstruction: 87.18051528930664  Train_KL: 3.537693530321121  Validation Loss : 90.8912467956543 Val_Reconstruction : 87.39864730834961 Val_KL : 3.4926018714904785\n","Epoch: 6198/8000  Traning Loss: 90.4922285079956  Train_Reconstruction: 86.9681510925293  Train_KL: 3.5240772366523743  Validation Loss : 90.65934371948242 Val_Reconstruction : 87.18098068237305 Val_KL : 3.4783660173416138\n","Epoch: 6199/8000  Traning Loss: 90.58988857269287  Train_Reconstruction: 87.07717418670654  Train_KL: 3.5127156376838684  Validation Loss : 90.84745025634766 Val_Reconstruction : 87.3778076171875 Val_KL : 3.4696414470672607\n","Epoch: 6200/8000  Traning Loss: 90.5225658416748  Train_Reconstruction: 87.0068473815918  Train_KL: 3.5157198011875153  Validation Loss : 90.86686706542969 Val_Reconstruction : 87.39996337890625 Val_KL : 3.4669042825698853\n","Epoch: 6201/8000  Traning Loss: 90.62581634521484  Train_Reconstruction: 87.10510158538818  Train_KL: 3.5207139551639557  Validation Loss : 90.59005355834961 Val_Reconstruction : 87.11433792114258 Val_KL : 3.4757139682769775\n","Epoch: 6202/8000  Traning Loss: 90.65990543365479  Train_Reconstruction: 87.13871574401855  Train_KL: 3.521189719438553  Validation Loss : 90.74117660522461 Val_Reconstruction : 87.26423263549805 Val_KL : 3.4769418239593506\n","Epoch: 6203/8000  Traning Loss: 90.44796180725098  Train_Reconstruction: 86.92662143707275  Train_KL: 3.5213392972946167  Validation Loss : 90.5894775390625 Val_Reconstruction : 87.11384201049805 Val_KL : 3.475639224052429\n","Epoch: 6204/8000  Traning Loss: 90.51984786987305  Train_Reconstruction: 86.99577236175537  Train_KL: 3.5240767002105713  Validation Loss : 90.99773406982422 Val_Reconstruction : 87.51877212524414 Val_KL : 3.4789650440216064\n","Epoch: 6205/8000  Traning Loss: 91.06604099273682  Train_Reconstruction: 87.5514268875122  Train_KL: 3.514613837003708  Validation Loss : 91.02510833740234 Val_Reconstruction : 87.55443572998047 Val_KL : 3.4706753492355347\n","Epoch: 6206/8000  Traning Loss: 90.91978931427002  Train_Reconstruction: 87.41133975982666  Train_KL: 3.50844943523407  Validation Loss : 91.0274543762207 Val_Reconstruction : 87.55931854248047 Val_KL : 3.4681366682052612\n","Epoch: 6207/8000  Traning Loss: 91.29748439788818  Train_Reconstruction: 87.77270793914795  Train_KL: 3.5247771441936493  Validation Loss : 91.76134490966797 Val_Reconstruction : 88.26800537109375 Val_KL : 3.493340015411377\n","Epoch: 6208/8000  Traning Loss: 90.99637985229492  Train_Reconstruction: 87.46932029724121  Train_KL: 3.527059704065323  Validation Loss : 90.5092887878418 Val_Reconstruction : 87.02787017822266 Val_KL : 3.48142147064209\n","Epoch: 6209/8000  Traning Loss: 90.46895694732666  Train_Reconstruction: 86.95157146453857  Train_KL: 3.517386019229889  Validation Loss : 90.76864242553711 Val_Reconstruction : 87.30224990844727 Val_KL : 3.4663922786712646\n","Epoch: 6210/8000  Traning Loss: 90.9182186126709  Train_Reconstruction: 87.40464878082275  Train_KL: 3.513570636510849  Validation Loss : 90.85855484008789 Val_Reconstruction : 87.38293838500977 Val_KL : 3.475616693496704\n","Epoch: 6211/8000  Traning Loss: 90.84237957000732  Train_Reconstruction: 87.32566356658936  Train_KL: 3.5167170763015747  Validation Loss : 90.78786849975586 Val_Reconstruction : 87.3158950805664 Val_KL : 3.4719738960266113\n","Epoch: 6212/8000  Traning Loss: 90.51140689849854  Train_Reconstruction: 86.9988431930542  Train_KL: 3.5125642120838165  Validation Loss : 90.52913665771484 Val_Reconstruction : 87.06204223632812 Val_KL : 3.4670945405960083\n","Epoch: 6213/8000  Traning Loss: 90.38958168029785  Train_Reconstruction: 86.86756801605225  Train_KL: 3.522015333175659  Validation Loss : 90.35844802856445 Val_Reconstruction : 86.86753463745117 Val_KL : 3.490914463996887\n","Epoch: 6214/8000  Traning Loss: 90.29841136932373  Train_Reconstruction: 86.76366424560547  Train_KL: 3.5347472429275513  Validation Loss : 90.3578109741211 Val_Reconstruction : 86.86825942993164 Val_KL : 3.489549994468689\n","Epoch: 6215/8000  Traning Loss: 90.47156429290771  Train_Reconstruction: 86.94640064239502  Train_KL: 3.5251637399196625  Validation Loss : 90.64967346191406 Val_Reconstruction : 87.17601776123047 Val_KL : 3.473652720451355\n","Epoch: 6216/8000  Traning Loss: 90.49217128753662  Train_Reconstruction: 86.96695804595947  Train_KL: 3.5252133309841156  Validation Loss : 90.7391471862793 Val_Reconstruction : 87.24995803833008 Val_KL : 3.4891879558563232\n","Epoch: 6217/8000  Traning Loss: 90.52469062805176  Train_Reconstruction: 86.99761581420898  Train_KL: 3.5270738899707794  Validation Loss : 90.62415313720703 Val_Reconstruction : 87.15252685546875 Val_KL : 3.4716274738311768\n","Epoch: 6218/8000  Traning Loss: 90.44673252105713  Train_Reconstruction: 86.93593406677246  Train_KL: 3.510797142982483  Validation Loss : 90.88618469238281 Val_Reconstruction : 87.41416931152344 Val_KL : 3.472015380859375\n","Epoch: 6219/8000  Traning Loss: 90.40002346038818  Train_Reconstruction: 86.88047885894775  Train_KL: 3.519544869661331  Validation Loss : 90.39850616455078 Val_Reconstruction : 86.90219116210938 Val_KL : 3.496316075325012\n","Epoch: 6220/8000  Traning Loss: 90.42976951599121  Train_Reconstruction: 86.89023399353027  Train_KL: 3.5395356118679047  Validation Loss : 90.27710342407227 Val_Reconstruction : 86.78102493286133 Val_KL : 3.4960813522338867\n","Epoch: 6221/8000  Traning Loss: 90.2535982131958  Train_Reconstruction: 86.72313594818115  Train_KL: 3.5304621756076813  Validation Loss : 90.63668060302734 Val_Reconstruction : 87.15167617797852 Val_KL : 3.4850059747695923\n","Epoch: 6222/8000  Traning Loss: 90.559157371521  Train_Reconstruction: 87.0357494354248  Train_KL: 3.5234083235263824  Validation Loss : 90.48583984375 Val_Reconstruction : 87.00884628295898 Val_KL : 3.4769928455352783\n","Epoch: 6223/8000  Traning Loss: 90.63340759277344  Train_Reconstruction: 87.10969543457031  Train_KL: 3.523710787296295  Validation Loss : 90.75476837158203 Val_Reconstruction : 87.267822265625 Val_KL : 3.486944317817688\n","Epoch: 6224/8000  Traning Loss: 90.49677085876465  Train_Reconstruction: 86.96409702301025  Train_KL: 3.53267440199852  Validation Loss : 90.39883422851562 Val_Reconstruction : 86.91474533081055 Val_KL : 3.4840891361236572\n","Epoch: 6225/8000  Traning Loss: 90.3131799697876  Train_Reconstruction: 86.79120540618896  Train_KL: 3.521974503993988  Validation Loss : 90.26930618286133 Val_Reconstruction : 86.79463195800781 Val_KL : 3.4746737480163574\n","Epoch: 6226/8000  Traning Loss: 90.3474349975586  Train_Reconstruction: 86.82807540893555  Train_KL: 3.5193592309951782  Validation Loss : 90.1885757446289 Val_Reconstruction : 86.71061325073242 Val_KL : 3.4779645204544067\n","Epoch: 6227/8000  Traning Loss: 90.42462730407715  Train_Reconstruction: 86.90051555633545  Train_KL: 3.5241120159626007  Validation Loss : 90.43020248413086 Val_Reconstruction : 86.94620132446289 Val_KL : 3.483999013900757\n","Epoch: 6228/8000  Traning Loss: 90.21084117889404  Train_Reconstruction: 86.68063449859619  Train_KL: 3.5302065312862396  Validation Loss : 90.13097381591797 Val_Reconstruction : 86.64583587646484 Val_KL : 3.4851382970809937\n","Epoch: 6229/8000  Traning Loss: 90.30283451080322  Train_Reconstruction: 86.77858448028564  Train_KL: 3.524250239133835  Validation Loss : 90.49258422851562 Val_Reconstruction : 87.01128768920898 Val_KL : 3.481296181678772\n","Epoch: 6230/8000  Traning Loss: 90.34992694854736  Train_Reconstruction: 86.83571147918701  Train_KL: 3.5142166316509247  Validation Loss : 90.66295623779297 Val_Reconstruction : 87.19192123413086 Val_KL : 3.471035361289978\n","Epoch: 6231/8000  Traning Loss: 90.4949369430542  Train_Reconstruction: 86.9814920425415  Train_KL: 3.5134426653385162  Validation Loss : 90.7300796508789 Val_Reconstruction : 87.24896240234375 Val_KL : 3.4811171293258667\n","Epoch: 6232/8000  Traning Loss: 90.42425537109375  Train_Reconstruction: 86.9015531539917  Train_KL: 3.522702693939209  Validation Loss : 90.53812408447266 Val_Reconstruction : 87.05421447753906 Val_KL : 3.483912229537964\n","Epoch: 6233/8000  Traning Loss: 90.41385173797607  Train_Reconstruction: 86.8935604095459  Train_KL: 3.520290642976761  Validation Loss : 90.5391616821289 Val_Reconstruction : 87.06348037719727 Val_KL : 3.475679636001587\n","Epoch: 6234/8000  Traning Loss: 90.73732566833496  Train_Reconstruction: 87.21741104125977  Train_KL: 3.5199142694473267  Validation Loss : 91.34675979614258 Val_Reconstruction : 87.8634147644043 Val_KL : 3.4833463430404663\n","Epoch: 6235/8000  Traning Loss: 91.12135314941406  Train_Reconstruction: 87.59463310241699  Train_KL: 3.526720404624939  Validation Loss : 90.6368293762207 Val_Reconstruction : 87.15268325805664 Val_KL : 3.4841439723968506\n","Epoch: 6236/8000  Traning Loss: 91.35505199432373  Train_Reconstruction: 87.83664321899414  Train_KL: 3.5184082984924316  Validation Loss : 91.15202713012695 Val_Reconstruction : 87.67354583740234 Val_KL : 3.4784821271896362\n","Epoch: 6237/8000  Traning Loss: 90.75242710113525  Train_Reconstruction: 87.23058605194092  Train_KL: 3.5218397974967957  Validation Loss : 90.38001251220703 Val_Reconstruction : 86.89816665649414 Val_KL : 3.4818469285964966\n","Epoch: 6238/8000  Traning Loss: 90.51819038391113  Train_Reconstruction: 86.99581909179688  Train_KL: 3.5223715901374817  Validation Loss : 90.32192611694336 Val_Reconstruction : 86.8349609375 Val_KL : 3.486963987350464\n","Epoch: 6239/8000  Traning Loss: 90.48349094390869  Train_Reconstruction: 86.96691226959229  Train_KL: 3.5165791511535645  Validation Loss : 90.29567337036133 Val_Reconstruction : 86.82038497924805 Val_KL : 3.475290298461914\n","Epoch: 6240/8000  Traning Loss: 90.5109224319458  Train_Reconstruction: 86.99413967132568  Train_KL: 3.516781806945801  Validation Loss : 91.00481033325195 Val_Reconstruction : 87.52301406860352 Val_KL : 3.4817949533462524\n","Epoch: 6241/8000  Traning Loss: 90.95269298553467  Train_Reconstruction: 87.41969108581543  Train_KL: 3.5330012142658234  Validation Loss : 90.90215301513672 Val_Reconstruction : 87.41509246826172 Val_KL : 3.487057328224182\n","Epoch: 6242/8000  Traning Loss: 90.98221492767334  Train_Reconstruction: 87.46122741699219  Train_KL: 3.520987778902054  Validation Loss : 91.5128059387207 Val_Reconstruction : 88.03883743286133 Val_KL : 3.473970413208008\n","Epoch: 6243/8000  Traning Loss: 90.43695259094238  Train_Reconstruction: 86.91809463500977  Train_KL: 3.5188591182231903  Validation Loss : 90.1294059753418 Val_Reconstruction : 86.65006637573242 Val_KL : 3.479338049888611\n","Epoch: 6244/8000  Traning Loss: 90.24160766601562  Train_Reconstruction: 86.7169542312622  Train_KL: 3.5246531665325165  Validation Loss : 90.55757522583008 Val_Reconstruction : 87.06802368164062 Val_KL : 3.4895488023757935\n","Epoch: 6245/8000  Traning Loss: 90.1265811920166  Train_Reconstruction: 86.5962963104248  Train_KL: 3.5302842259407043  Validation Loss : 90.34933471679688 Val_Reconstruction : 86.86273193359375 Val_KL : 3.4866031408309937\n","Epoch: 6246/8000  Traning Loss: 90.54308986663818  Train_Reconstruction: 87.02745914459229  Train_KL: 3.5156313478946686  Validation Loss : 90.85760498046875 Val_Reconstruction : 87.3952522277832 Val_KL : 3.462350845336914\n","Epoch: 6247/8000  Traning Loss: 91.12162208557129  Train_Reconstruction: 87.61047267913818  Train_KL: 3.5111515522003174  Validation Loss : 90.85899353027344 Val_Reconstruction : 87.38460922241211 Val_KL : 3.4743826389312744\n","Epoch: 6248/8000  Traning Loss: 91.08799076080322  Train_Reconstruction: 87.55567741394043  Train_KL: 3.532313257455826  Validation Loss : 91.09056854248047 Val_Reconstruction : 87.5952377319336 Val_KL : 3.495330572128296\n","Epoch: 6249/8000  Traning Loss: 90.93987846374512  Train_Reconstruction: 87.41265678405762  Train_KL: 3.5272223353385925  Validation Loss : 90.8263053894043 Val_Reconstruction : 87.35852432250977 Val_KL : 3.4677798748016357\n","Epoch: 6250/8000  Traning Loss: 90.49239921569824  Train_Reconstruction: 86.98530960083008  Train_KL: 3.507089287042618  Validation Loss : 90.53731155395508 Val_Reconstruction : 87.07326889038086 Val_KL : 3.464041590690613\n","Epoch: 6251/8000  Traning Loss: 90.38746452331543  Train_Reconstruction: 86.8717679977417  Train_KL: 3.5156968235969543  Validation Loss : 90.28757095336914 Val_Reconstruction : 86.81024169921875 Val_KL : 3.477330803871155\n","Epoch: 6252/8000  Traning Loss: 90.37724590301514  Train_Reconstruction: 86.85297298431396  Train_KL: 3.524271696805954  Validation Loss : 90.30115509033203 Val_Reconstruction : 86.81071472167969 Val_KL : 3.490440249443054\n","Epoch: 6253/8000  Traning Loss: 90.20322227478027  Train_Reconstruction: 86.67893600463867  Train_KL: 3.52428662776947  Validation Loss : 90.18777465820312 Val_Reconstruction : 86.70526123046875 Val_KL : 3.4825128316879272\n","Epoch: 6254/8000  Traning Loss: 90.3071460723877  Train_Reconstruction: 86.78428363800049  Train_KL: 3.5228614807128906  Validation Loss : 90.33562469482422 Val_Reconstruction : 86.85249710083008 Val_KL : 3.483128309249878\n","Epoch: 6255/8000  Traning Loss: 90.48825931549072  Train_Reconstruction: 86.95984935760498  Train_KL: 3.5284103453159332  Validation Loss : 90.54875564575195 Val_Reconstruction : 87.05653762817383 Val_KL : 3.4922187328338623\n","Epoch: 6256/8000  Traning Loss: 90.80616188049316  Train_Reconstruction: 87.27447414398193  Train_KL: 3.531687378883362  Validation Loss : 90.84820175170898 Val_Reconstruction : 87.35428237915039 Val_KL : 3.4939205646514893\n","Epoch: 6257/8000  Traning Loss: 90.43046951293945  Train_Reconstruction: 86.90793323516846  Train_KL: 3.52253657579422  Validation Loss : 90.56537628173828 Val_Reconstruction : 87.0860824584961 Val_KL : 3.4792953729629517\n","Epoch: 6258/8000  Traning Loss: 90.66893005371094  Train_Reconstruction: 87.15019989013672  Train_KL: 3.518730014562607  Validation Loss : 90.75736618041992 Val_Reconstruction : 87.28277587890625 Val_KL : 3.4745919704437256\n","Epoch: 6259/8000  Traning Loss: 90.77097225189209  Train_Reconstruction: 87.24812126159668  Train_KL: 3.522851288318634  Validation Loss : 90.54993438720703 Val_Reconstruction : 87.06514739990234 Val_KL : 3.484789729118347\n","Epoch: 6260/8000  Traning Loss: 90.4128065109253  Train_Reconstruction: 86.88976669311523  Train_KL: 3.523041248321533  Validation Loss : 90.484130859375 Val_Reconstruction : 87.0128173828125 Val_KL : 3.471312165260315\n","Epoch: 6261/8000  Traning Loss: 90.51866340637207  Train_Reconstruction: 86.99863624572754  Train_KL: 3.520027756690979  Validation Loss : 90.6770133972168 Val_Reconstruction : 87.19535446166992 Val_KL : 3.4816588163375854\n","Epoch: 6262/8000  Traning Loss: 90.61574077606201  Train_Reconstruction: 87.09565734863281  Train_KL: 3.520083338022232  Validation Loss : 90.91663360595703 Val_Reconstruction : 87.43574905395508 Val_KL : 3.4808831214904785\n","Epoch: 6263/8000  Traning Loss: 90.71382236480713  Train_Reconstruction: 87.19299030303955  Train_KL: 3.5208316147327423  Validation Loss : 90.86648559570312 Val_Reconstruction : 87.3883171081543 Val_KL : 3.478165864944458\n","Epoch: 6264/8000  Traning Loss: 90.6393575668335  Train_Reconstruction: 87.11569213867188  Train_KL: 3.523665130138397  Validation Loss : 91.14599227905273 Val_Reconstruction : 87.66025924682617 Val_KL : 3.485732913017273\n","Epoch: 6265/8000  Traning Loss: 90.85263347625732  Train_Reconstruction: 87.32502269744873  Train_KL: 3.5276106894016266  Validation Loss : 90.83240509033203 Val_Reconstruction : 87.34876251220703 Val_KL : 3.4836424589157104\n","Epoch: 6266/8000  Traning Loss: 90.27943992614746  Train_Reconstruction: 86.75986576080322  Train_KL: 3.519574999809265  Validation Loss : 90.31832504272461 Val_Reconstruction : 86.84405899047852 Val_KL : 3.4742671251296997\n","Epoch: 6267/8000  Traning Loss: 90.20895099639893  Train_Reconstruction: 86.69502067565918  Train_KL: 3.513931691646576  Validation Loss : 90.35717391967773 Val_Reconstruction : 86.87841415405273 Val_KL : 3.478760242462158\n","Epoch: 6268/8000  Traning Loss: 90.26391220092773  Train_Reconstruction: 86.73358154296875  Train_KL: 3.5303297638893127  Validation Loss : 90.35374069213867 Val_Reconstruction : 86.86433410644531 Val_KL : 3.489407777786255\n","Epoch: 6269/8000  Traning Loss: 90.22552108764648  Train_Reconstruction: 86.70656490325928  Train_KL: 3.5189565122127533  Validation Loss : 90.17009735107422 Val_Reconstruction : 86.7060432434082 Val_KL : 3.4640527963638306\n","Epoch: 6270/8000  Traning Loss: 90.14220714569092  Train_Reconstruction: 86.63166332244873  Train_KL: 3.510545015335083  Validation Loss : 90.24261856079102 Val_Reconstruction : 86.77017211914062 Val_KL : 3.4724440574645996\n","Epoch: 6271/8000  Traning Loss: 90.19802761077881  Train_Reconstruction: 86.680100440979  Train_KL: 3.517926961183548  Validation Loss : 90.22383499145508 Val_Reconstruction : 86.74295806884766 Val_KL : 3.480877161026001\n","Epoch: 6272/8000  Traning Loss: 90.18464374542236  Train_Reconstruction: 86.65966510772705  Train_KL: 3.524979680776596  Validation Loss : 90.32398986816406 Val_Reconstruction : 86.84666061401367 Val_KL : 3.4773309230804443\n","Epoch: 6273/8000  Traning Loss: 90.31511974334717  Train_Reconstruction: 86.7996597290039  Train_KL: 3.515459328889847  Validation Loss : 90.54657363891602 Val_Reconstruction : 87.06921768188477 Val_KL : 3.4773542881011963\n","Epoch: 6274/8000  Traning Loss: 90.44318962097168  Train_Reconstruction: 86.92636966705322  Train_KL: 3.5168203115463257  Validation Loss : 90.40096664428711 Val_Reconstruction : 86.91526412963867 Val_KL : 3.485704183578491\n","Epoch: 6275/8000  Traning Loss: 90.42129039764404  Train_Reconstruction: 86.88864707946777  Train_KL: 3.5326437056064606  Validation Loss : 90.47898864746094 Val_Reconstruction : 86.98563003540039 Val_KL : 3.4933598041534424\n","Epoch: 6276/8000  Traning Loss: 90.23327922821045  Train_Reconstruction: 86.70263957977295  Train_KL: 3.5306396186351776  Validation Loss : 90.21712875366211 Val_Reconstruction : 86.73120880126953 Val_KL : 3.4859230518341064\n","Epoch: 6277/8000  Traning Loss: 90.31862163543701  Train_Reconstruction: 86.79515743255615  Train_KL: 3.523464620113373  Validation Loss : 90.80627059936523 Val_Reconstruction : 87.32693862915039 Val_KL : 3.4793336391448975\n","Epoch: 6278/8000  Traning Loss: 90.92571640014648  Train_Reconstruction: 87.40691375732422  Train_KL: 3.5188036262989044  Validation Loss : 91.38334274291992 Val_Reconstruction : 87.90371704101562 Val_KL : 3.479625105857849\n","Epoch: 6279/8000  Traning Loss: 91.1580286026001  Train_Reconstruction: 87.63430404663086  Train_KL: 3.523723930120468  Validation Loss : 90.94247055053711 Val_Reconstruction : 87.45858764648438 Val_KL : 3.483883261680603\n","Epoch: 6280/8000  Traning Loss: 90.84554386138916  Train_Reconstruction: 87.32747173309326  Train_KL: 3.518071800470352  Validation Loss : 91.02854919433594 Val_Reconstruction : 87.55368041992188 Val_KL : 3.474868059158325\n","Epoch: 6281/8000  Traning Loss: 90.94628238677979  Train_Reconstruction: 87.42864990234375  Train_KL: 3.5176320374011993  Validation Loss : 91.53074264526367 Val_Reconstruction : 88.04938125610352 Val_KL : 3.4813631772994995\n","Epoch: 6282/8000  Traning Loss: 91.5145902633667  Train_Reconstruction: 87.98947429656982  Train_KL: 3.525116950273514  Validation Loss : 91.8961410522461 Val_Reconstruction : 88.41551208496094 Val_KL : 3.48063063621521\n","Epoch: 6283/8000  Traning Loss: 91.28139877319336  Train_Reconstruction: 87.76014614105225  Train_KL: 3.5212532579898834  Validation Loss : 91.61966705322266 Val_Reconstruction : 88.1438980102539 Val_KL : 3.475766897201538\n","Epoch: 6284/8000  Traning Loss: 91.0818042755127  Train_Reconstruction: 87.56005477905273  Train_KL: 3.5217494070529938  Validation Loss : 90.82278060913086 Val_Reconstruction : 87.34850311279297 Val_KL : 3.4742772579193115\n","Epoch: 6285/8000  Traning Loss: 90.74939918518066  Train_Reconstruction: 87.23295593261719  Train_KL: 3.516442745923996  Validation Loss : 90.98529815673828 Val_Reconstruction : 87.51699447631836 Val_KL : 3.4683057069778442\n","Epoch: 6286/8000  Traning Loss: 90.64680004119873  Train_Reconstruction: 87.1274185180664  Train_KL: 3.5193825364112854  Validation Loss : 90.58182525634766 Val_Reconstruction : 87.10551834106445 Val_KL : 3.476308584213257\n","Epoch: 6287/8000  Traning Loss: 90.24510097503662  Train_Reconstruction: 86.72510719299316  Train_KL: 3.519994467496872  Validation Loss : 90.31133270263672 Val_Reconstruction : 86.83430862426758 Val_KL : 3.4770249128341675\n","Epoch: 6288/8000  Traning Loss: 90.11039066314697  Train_Reconstruction: 86.58802509307861  Train_KL: 3.5223658680915833  Validation Loss : 90.03322982788086 Val_Reconstruction : 86.55683898925781 Val_KL : 3.4763906002044678\n","Epoch: 6289/8000  Traning Loss: 90.33460807800293  Train_Reconstruction: 86.81789588928223  Train_KL: 3.516713112592697  Validation Loss : 90.4751091003418 Val_Reconstruction : 87.00149154663086 Val_KL : 3.4736177921295166\n","Epoch: 6290/8000  Traning Loss: 90.26273441314697  Train_Reconstruction: 86.73984050750732  Train_KL: 3.522894024848938  Validation Loss : 90.21181869506836 Val_Reconstruction : 86.72658920288086 Val_KL : 3.4852304458618164\n","Epoch: 6291/8000  Traning Loss: 90.3007755279541  Train_Reconstruction: 86.77330780029297  Train_KL: 3.527466803789139  Validation Loss : 90.40642547607422 Val_Reconstruction : 86.92133331298828 Val_KL : 3.4850921630859375\n","Epoch: 6292/8000  Traning Loss: 90.8359489440918  Train_Reconstruction: 87.31678581237793  Train_KL: 3.5191642940044403  Validation Loss : 91.22285842895508 Val_Reconstruction : 87.74922561645508 Val_KL : 3.4736355543136597\n","Epoch: 6293/8000  Traning Loss: 90.63950729370117  Train_Reconstruction: 87.12276363372803  Train_KL: 3.5167421400547028  Validation Loss : 90.55079650878906 Val_Reconstruction : 87.07223510742188 Val_KL : 3.478558301925659\n","Epoch: 6294/8000  Traning Loss: 90.21842384338379  Train_Reconstruction: 86.69698333740234  Train_KL: 3.521441102027893  Validation Loss : 90.1063117980957 Val_Reconstruction : 86.62665939331055 Val_KL : 3.4796531200408936\n","Epoch: 6295/8000  Traning Loss: 89.9856185913086  Train_Reconstruction: 86.46632385253906  Train_KL: 3.5192951261997223  Validation Loss : 90.04547500610352 Val_Reconstruction : 86.56916427612305 Val_KL : 3.4763118028640747\n","Epoch: 6296/8000  Traning Loss: 90.21545219421387  Train_Reconstruction: 86.6951904296875  Train_KL: 3.520261734724045  Validation Loss : 90.4044418334961 Val_Reconstruction : 86.93225479125977 Val_KL : 3.4721862077713013\n","Epoch: 6297/8000  Traning Loss: 90.4111442565918  Train_Reconstruction: 86.89259147644043  Train_KL: 3.5185518860816956  Validation Loss : 90.51318359375 Val_Reconstruction : 87.03591918945312 Val_KL : 3.4772638082504272\n","Epoch: 6298/8000  Traning Loss: 90.51104068756104  Train_Reconstruction: 86.99022769927979  Train_KL: 3.520814597606659  Validation Loss : 90.6238784790039 Val_Reconstruction : 87.14494705200195 Val_KL : 3.4789291620254517\n","Epoch: 6299/8000  Traning Loss: 90.8570146560669  Train_Reconstruction: 87.32305335998535  Train_KL: 3.533960670232773  Validation Loss : 91.04539489746094 Val_Reconstruction : 87.5475959777832 Val_KL : 3.4977978467941284\n","Epoch: 6300/8000  Traning Loss: 91.39410400390625  Train_Reconstruction: 87.86932277679443  Train_KL: 3.5247803032398224  Validation Loss : 91.6966667175293 Val_Reconstruction : 88.22983169555664 Val_KL : 3.4668359756469727\n","Epoch: 6301/8000  Traning Loss: 91.23031806945801  Train_Reconstruction: 87.71376037597656  Train_KL: 3.516557961702347  Validation Loss : 91.53892517089844 Val_Reconstruction : 88.06113052368164 Val_KL : 3.4777936935424805\n","Epoch: 6302/8000  Traning Loss: 90.72839450836182  Train_Reconstruction: 87.2013635635376  Train_KL: 3.5270312130451202  Validation Loss : 90.97711181640625 Val_Reconstruction : 87.48906326293945 Val_KL : 3.488049626350403\n","Epoch: 6303/8000  Traning Loss: 90.58836269378662  Train_Reconstruction: 87.06294536590576  Train_KL: 3.525417983531952  Validation Loss : 90.5291976928711 Val_Reconstruction : 87.04381942749023 Val_KL : 3.4853774309158325\n","Epoch: 6304/8000  Traning Loss: 90.5044116973877  Train_Reconstruction: 86.983717918396  Train_KL: 3.52069428563118  Validation Loss : 90.70326232910156 Val_Reconstruction : 87.22873306274414 Val_KL : 3.4745287895202637\n","Epoch: 6305/8000  Traning Loss: 90.36212539672852  Train_Reconstruction: 86.84893321990967  Train_KL: 3.513192504644394  Validation Loss : 90.38689041137695 Val_Reconstruction : 86.92222595214844 Val_KL : 3.4646658897399902\n","Epoch: 6306/8000  Traning Loss: 90.08241271972656  Train_Reconstruction: 86.56535911560059  Train_KL: 3.51705339550972  Validation Loss : 90.06689071655273 Val_Reconstruction : 86.59336471557617 Val_KL : 3.4735231399536133\n","Epoch: 6307/8000  Traning Loss: 90.11652374267578  Train_Reconstruction: 86.58465671539307  Train_KL: 3.5318661630153656  Validation Loss : 90.50606918334961 Val_Reconstruction : 87.01962280273438 Val_KL : 3.4864484071731567\n","Epoch: 6308/8000  Traning Loss: 90.30003833770752  Train_Reconstruction: 86.77918148040771  Train_KL: 3.520857036113739  Validation Loss : 90.7540512084961 Val_Reconstruction : 87.29210662841797 Val_KL : 3.4619420766830444\n","Epoch: 6309/8000  Traning Loss: 90.82488632202148  Train_Reconstruction: 87.31097316741943  Train_KL: 3.5139117538928986  Validation Loss : 90.95984268188477 Val_Reconstruction : 87.48532104492188 Val_KL : 3.47451913356781\n","Epoch: 6310/8000  Traning Loss: 90.8949384689331  Train_Reconstruction: 87.37119674682617  Train_KL: 3.523741662502289  Validation Loss : 90.70824432373047 Val_Reconstruction : 87.2300910949707 Val_KL : 3.4781525135040283\n","Epoch: 6311/8000  Traning Loss: 90.78478145599365  Train_Reconstruction: 87.27129173278809  Train_KL: 3.5134887099266052  Validation Loss : 90.6114616394043 Val_Reconstruction : 87.14786529541016 Val_KL : 3.4635947942733765\n","Epoch: 6312/8000  Traning Loss: 90.33434581756592  Train_Reconstruction: 86.8247938156128  Train_KL: 3.509554237127304  Validation Loss : 90.45072555541992 Val_Reconstruction : 86.9801254272461 Val_KL : 3.4706000089645386\n","Epoch: 6313/8000  Traning Loss: 90.27694416046143  Train_Reconstruction: 86.75762939453125  Train_KL: 3.519314616918564  Validation Loss : 90.30067825317383 Val_Reconstruction : 86.82559585571289 Val_KL : 3.475082039833069\n","Epoch: 6314/8000  Traning Loss: 90.18893146514893  Train_Reconstruction: 86.67265129089355  Train_KL: 3.5162784457206726  Validation Loss : 90.11101150512695 Val_Reconstruction : 86.64310073852539 Val_KL : 3.467913031578064\n","Epoch: 6315/8000  Traning Loss: 90.10530853271484  Train_Reconstruction: 86.58790874481201  Train_KL: 3.5173997282981873  Validation Loss : 90.1573371887207 Val_Reconstruction : 86.68136215209961 Val_KL : 3.4759719371795654\n","Epoch: 6316/8000  Traning Loss: 89.94171237945557  Train_Reconstruction: 86.4181776046753  Train_KL: 3.523534417152405  Validation Loss : 90.08341979980469 Val_Reconstruction : 86.60208129882812 Val_KL : 3.481341004371643\n","Epoch: 6317/8000  Traning Loss: 90.18360710144043  Train_Reconstruction: 86.6601152420044  Train_KL: 3.5234926640987396  Validation Loss : 90.24782180786133 Val_Reconstruction : 86.77146911621094 Val_KL : 3.47635555267334\n","Epoch: 6318/8000  Traning Loss: 90.15107917785645  Train_Reconstruction: 86.63238716125488  Train_KL: 3.5186903178691864  Validation Loss : 90.09473037719727 Val_Reconstruction : 86.6247329711914 Val_KL : 3.4699959754943848\n","Epoch: 6319/8000  Traning Loss: 89.9658374786377  Train_Reconstruction: 86.44671630859375  Train_KL: 3.5191206336021423  Validation Loss : 89.97929763793945 Val_Reconstruction : 86.49942779541016 Val_KL : 3.479867100715637\n","Epoch: 6320/8000  Traning Loss: 90.09047317504883  Train_Reconstruction: 86.56182098388672  Train_KL: 3.5286532938480377  Validation Loss : 90.28341674804688 Val_Reconstruction : 86.80284118652344 Val_KL : 3.480573892593384\n","Epoch: 6321/8000  Traning Loss: 90.46884536743164  Train_Reconstruction: 86.94926834106445  Train_KL: 3.5195778012275696  Validation Loss : 90.64460754394531 Val_Reconstruction : 87.16766738891602 Val_KL : 3.4769396781921387\n","Epoch: 6322/8000  Traning Loss: 90.52840518951416  Train_Reconstruction: 87.00784015655518  Train_KL: 3.520564168691635  Validation Loss : 90.7912483215332 Val_Reconstruction : 87.3162612915039 Val_KL : 3.4749871492385864\n","Epoch: 6323/8000  Traning Loss: 90.46172618865967  Train_Reconstruction: 86.93956565856934  Train_KL: 3.5221610367298126  Validation Loss : 90.69263458251953 Val_Reconstruction : 87.21660614013672 Val_KL : 3.476029396057129\n","Epoch: 6324/8000  Traning Loss: 90.54785537719727  Train_Reconstruction: 87.02829360961914  Train_KL: 3.5195620357990265  Validation Loss : 90.66814041137695 Val_Reconstruction : 87.19385147094727 Val_KL : 3.4742867946624756\n","Epoch: 6325/8000  Traning Loss: 90.95232486724854  Train_Reconstruction: 87.4271469116211  Train_KL: 3.525176525115967  Validation Loss : 91.09374237060547 Val_Reconstruction : 87.60881042480469 Val_KL : 3.484935164451599\n","Epoch: 6326/8000  Traning Loss: 91.00762176513672  Train_Reconstruction: 87.48801040649414  Train_KL: 3.519610643386841  Validation Loss : 91.31268692016602 Val_Reconstruction : 87.83386611938477 Val_KL : 3.4788182973861694\n","Epoch: 6327/8000  Traning Loss: 90.6329574584961  Train_Reconstruction: 87.12149238586426  Train_KL: 3.511464536190033  Validation Loss : 90.72411346435547 Val_Reconstruction : 87.25234985351562 Val_KL : 3.4717636108398438\n","Epoch: 6328/8000  Traning Loss: 90.40669631958008  Train_Reconstruction: 86.88332176208496  Train_KL: 3.523374557495117  Validation Loss : 89.88909912109375 Val_Reconstruction : 86.40423965454102 Val_KL : 3.484861731529236\n","Epoch: 6329/8000  Traning Loss: 90.23007297515869  Train_Reconstruction: 86.70774269104004  Train_KL: 3.5223320722579956  Validation Loss : 90.66772842407227 Val_Reconstruction : 87.19322967529297 Val_KL : 3.474497079849243\n","Epoch: 6330/8000  Traning Loss: 90.54423236846924  Train_Reconstruction: 87.02260112762451  Train_KL: 3.521631747484207  Validation Loss : 90.42776870727539 Val_Reconstruction : 86.94540786743164 Val_KL : 3.482361078262329\n","Epoch: 6331/8000  Traning Loss: 90.16759872436523  Train_Reconstruction: 86.64070415496826  Train_KL: 3.5268945693969727  Validation Loss : 90.09931182861328 Val_Reconstruction : 86.62484359741211 Val_KL : 3.4744685888290405\n","Epoch: 6332/8000  Traning Loss: 89.96520233154297  Train_Reconstruction: 86.44755172729492  Train_KL: 3.5176509618759155  Validation Loss : 90.07098770141602 Val_Reconstruction : 86.59309387207031 Val_KL : 3.477895975112915\n","Epoch: 6333/8000  Traning Loss: 90.26750946044922  Train_Reconstruction: 86.74445724487305  Train_KL: 3.523051381111145  Validation Loss : 90.25100326538086 Val_Reconstruction : 86.78253936767578 Val_KL : 3.468466281890869\n","Epoch: 6334/8000  Traning Loss: 90.1770486831665  Train_Reconstruction: 86.65796375274658  Train_KL: 3.5190848112106323  Validation Loss : 90.32407760620117 Val_Reconstruction : 86.84464645385742 Val_KL : 3.479429841041565\n","Epoch: 6335/8000  Traning Loss: 90.16210651397705  Train_Reconstruction: 86.63175582885742  Train_KL: 3.5303509533405304  Validation Loss : 90.39188766479492 Val_Reconstruction : 86.89910125732422 Val_KL : 3.492788553237915\n","Epoch: 6336/8000  Traning Loss: 90.2057056427002  Train_Reconstruction: 86.6692705154419  Train_KL: 3.536434978246689  Validation Loss : 90.39838790893555 Val_Reconstruction : 86.90414428710938 Val_KL : 3.4942415952682495\n","Epoch: 6337/8000  Traning Loss: 90.22681045532227  Train_Reconstruction: 86.70784282684326  Train_KL: 3.51896795630455  Validation Loss : 90.47624206542969 Val_Reconstruction : 87.00949096679688 Val_KL : 3.4667489528656006\n","Epoch: 6338/8000  Traning Loss: 90.45195293426514  Train_Reconstruction: 86.94238662719727  Train_KL: 3.5095658600330353  Validation Loss : 90.38432693481445 Val_Reconstruction : 86.91925430297852 Val_KL : 3.465070962905884\n","Epoch: 6339/8000  Traning Loss: 90.26798248291016  Train_Reconstruction: 86.74895858764648  Train_KL: 3.519024670124054  Validation Loss : 90.2890510559082 Val_Reconstruction : 86.80781936645508 Val_KL : 3.4812322854995728\n","Epoch: 6340/8000  Traning Loss: 90.10884189605713  Train_Reconstruction: 86.58642482757568  Train_KL: 3.5224160850048065  Validation Loss : 90.16235733032227 Val_Reconstruction : 86.6900520324707 Val_KL : 3.4723085165023804\n","Epoch: 6341/8000  Traning Loss: 90.26817989349365  Train_Reconstruction: 86.75158309936523  Train_KL: 3.5165962874889374  Validation Loss : 90.43942642211914 Val_Reconstruction : 86.9665756225586 Val_KL : 3.472848892211914\n","Epoch: 6342/8000  Traning Loss: 90.40748977661133  Train_Reconstruction: 86.87805080413818  Train_KL: 3.529438853263855  Validation Loss : 90.62784957885742 Val_Reconstruction : 87.13285446166992 Val_KL : 3.4949947595596313\n","Epoch: 6343/8000  Traning Loss: 90.3881311416626  Train_Reconstruction: 86.8581771850586  Train_KL: 3.529954195022583  Validation Loss : 90.25054168701172 Val_Reconstruction : 86.76873397827148 Val_KL : 3.4818071126937866\n","Epoch: 6344/8000  Traning Loss: 90.29566955566406  Train_Reconstruction: 86.77127838134766  Train_KL: 3.524390608072281  Validation Loss : 90.63067245483398 Val_Reconstruction : 87.15084457397461 Val_KL : 3.4798284769058228\n","Epoch: 6345/8000  Traning Loss: 90.46238136291504  Train_Reconstruction: 86.94003391265869  Train_KL: 3.522347331047058  Validation Loss : 90.807861328125 Val_Reconstruction : 87.32700729370117 Val_KL : 3.4808541536331177\n","Epoch: 6346/8000  Traning Loss: 90.65155220031738  Train_Reconstruction: 87.13312911987305  Train_KL: 3.5184237957000732  Validation Loss : 91.13796615600586 Val_Reconstruction : 87.66097640991211 Val_KL : 3.47699236869812\n","Epoch: 6347/8000  Traning Loss: 90.87164974212646  Train_Reconstruction: 87.35074710845947  Train_KL: 3.5209012627601624  Validation Loss : 91.22117614746094 Val_Reconstruction : 87.75136947631836 Val_KL : 3.4698071479797363\n","Epoch: 6348/8000  Traning Loss: 90.85612678527832  Train_Reconstruction: 87.33820247650146  Train_KL: 3.517924726009369  Validation Loss : 91.15932083129883 Val_Reconstruction : 87.69050216674805 Val_KL : 3.46881902217865\n","Epoch: 6349/8000  Traning Loss: 90.72187423706055  Train_Reconstruction: 87.2101001739502  Train_KL: 3.5117750465869904  Validation Loss : 90.50542068481445 Val_Reconstruction : 87.04017639160156 Val_KL : 3.4652442932128906\n","Epoch: 6350/8000  Traning Loss: 90.46944808959961  Train_Reconstruction: 86.95545482635498  Train_KL: 3.5139931738376617  Validation Loss : 90.35429000854492 Val_Reconstruction : 86.88062286376953 Val_KL : 3.473666191101074\n","Epoch: 6351/8000  Traning Loss: 90.20735454559326  Train_Reconstruction: 86.68590545654297  Train_KL: 3.5214481949806213  Validation Loss : 90.45927810668945 Val_Reconstruction : 86.97659301757812 Val_KL : 3.4826849699020386\n","Epoch: 6352/8000  Traning Loss: 90.38782501220703  Train_Reconstruction: 86.86764144897461  Train_KL: 3.5201827585697174  Validation Loss : 90.46335220336914 Val_Reconstruction : 86.99512481689453 Val_KL : 3.468228340148926\n","Epoch: 6353/8000  Traning Loss: 90.52060508728027  Train_Reconstruction: 87.00357913970947  Train_KL: 3.5170255601406097  Validation Loss : 91.07548904418945 Val_Reconstruction : 87.5981330871582 Val_KL : 3.477354884147644\n","Epoch: 6354/8000  Traning Loss: 90.83422565460205  Train_Reconstruction: 87.31117630004883  Train_KL: 3.5230483412742615  Validation Loss : 91.21894836425781 Val_Reconstruction : 87.73310470581055 Val_KL : 3.4858431816101074\n","Epoch: 6355/8000  Traning Loss: 91.10858631134033  Train_Reconstruction: 87.57992935180664  Train_KL: 3.5286571085453033  Validation Loss : 91.89701843261719 Val_Reconstruction : 88.40769577026367 Val_KL : 3.4893219470977783\n","Epoch: 6356/8000  Traning Loss: 91.53514766693115  Train_Reconstruction: 88.0092544555664  Train_KL: 3.5258933305740356  Validation Loss : 91.42473983764648 Val_Reconstruction : 87.94699096679688 Val_KL : 3.477749824523926\n","Epoch: 6357/8000  Traning Loss: 91.34080028533936  Train_Reconstruction: 87.82710266113281  Train_KL: 3.5136983692646027  Validation Loss : 91.0472183227539 Val_Reconstruction : 87.58076858520508 Val_KL : 3.466449022293091\n","Epoch: 6358/8000  Traning Loss: 90.41798877716064  Train_Reconstruction: 86.91142272949219  Train_KL: 3.506565809249878  Validation Loss : 90.0970687866211 Val_Reconstruction : 86.6383171081543 Val_KL : 3.458748936653137\n","Epoch: 6359/8000  Traning Loss: 90.00018119812012  Train_Reconstruction: 86.47770118713379  Train_KL: 3.5224815011024475  Validation Loss : 90.03337478637695 Val_Reconstruction : 86.54659271240234 Val_KL : 3.486782193183899\n","Epoch: 6360/8000  Traning Loss: 90.08820819854736  Train_Reconstruction: 86.55322170257568  Train_KL: 3.5349858701229095  Validation Loss : 90.31647109985352 Val_Reconstruction : 86.82914352416992 Val_KL : 3.4873262643814087\n","Epoch: 6361/8000  Traning Loss: 90.48248863220215  Train_Reconstruction: 86.9638671875  Train_KL: 3.518622249364853  Validation Loss : 90.98117065429688 Val_Reconstruction : 87.51211929321289 Val_KL : 3.4690507650375366\n","Epoch: 6362/8000  Traning Loss: 90.73353481292725  Train_Reconstruction: 87.22310161590576  Train_KL: 3.5104338824748993  Validation Loss : 91.02236557006836 Val_Reconstruction : 87.55362701416016 Val_KL : 3.4687384366989136\n","Epoch: 6363/8000  Traning Loss: 91.02273845672607  Train_Reconstruction: 87.50698947906494  Train_KL: 3.5157491266727448  Validation Loss : 91.43211364746094 Val_Reconstruction : 87.96001434326172 Val_KL : 3.472099781036377\n","Epoch: 6364/8000  Traning Loss: 90.98265838623047  Train_Reconstruction: 87.46888732910156  Train_KL: 3.513770341873169  Validation Loss : 90.7597541809082 Val_Reconstruction : 87.29135513305664 Val_KL : 3.4683997631073\n","Epoch: 6365/8000  Traning Loss: 91.15437507629395  Train_Reconstruction: 87.64077568054199  Train_KL: 3.513597398996353  Validation Loss : 90.94227600097656 Val_Reconstruction : 87.46456527709961 Val_KL : 3.4777098894119263\n","Epoch: 6366/8000  Traning Loss: 90.55124187469482  Train_Reconstruction: 87.02770328521729  Train_KL: 3.5235390663146973  Validation Loss : 90.21503448486328 Val_Reconstruction : 86.73497772216797 Val_KL : 3.4800573587417603\n","Epoch: 6367/8000  Traning Loss: 90.1381893157959  Train_Reconstruction: 86.61607646942139  Train_KL: 3.5221120417118073  Validation Loss : 90.39836502075195 Val_Reconstruction : 86.9165153503418 Val_KL : 3.481849193572998\n","Epoch: 6368/8000  Traning Loss: 90.23853206634521  Train_Reconstruction: 86.72024536132812  Train_KL: 3.5182867646217346  Validation Loss : 90.3275032043457 Val_Reconstruction : 86.86045837402344 Val_KL : 3.4670462608337402\n","Epoch: 6369/8000  Traning Loss: 90.02590465545654  Train_Reconstruction: 86.51237773895264  Train_KL: 3.51352596282959  Validation Loss : 90.08776092529297 Val_Reconstruction : 86.61847686767578 Val_KL : 3.4692822694778442\n","Epoch: 6370/8000  Traning Loss: 90.41263961791992  Train_Reconstruction: 86.8923864364624  Train_KL: 3.520253360271454  Validation Loss : 90.83892440795898 Val_Reconstruction : 87.35995101928711 Val_KL : 3.4789745807647705\n","Epoch: 6371/8000  Traning Loss: 90.47297954559326  Train_Reconstruction: 86.94872188568115  Train_KL: 3.524258404970169  Validation Loss : 90.42009353637695 Val_Reconstruction : 86.93931579589844 Val_KL : 3.4807779788970947\n","Epoch: 6372/8000  Traning Loss: 90.34634113311768  Train_Reconstruction: 86.82943153381348  Train_KL: 3.5169104635715485  Validation Loss : 90.13804626464844 Val_Reconstruction : 86.66683959960938 Val_KL : 3.4712082147598267\n","Epoch: 6373/8000  Traning Loss: 90.29777240753174  Train_Reconstruction: 86.7803897857666  Train_KL: 3.517381429672241  Validation Loss : 90.23630142211914 Val_Reconstruction : 86.75780487060547 Val_KL : 3.4784996509552\n","Epoch: 6374/8000  Traning Loss: 90.18479537963867  Train_Reconstruction: 86.66093635559082  Train_KL: 3.5238603949546814  Validation Loss : 90.10094833374023 Val_Reconstruction : 86.61481857299805 Val_KL : 3.4861302375793457\n","Epoch: 6375/8000  Traning Loss: 90.10831451416016  Train_Reconstruction: 86.58712577819824  Train_KL: 3.5211868286132812  Validation Loss : 90.14980697631836 Val_Reconstruction : 86.67380142211914 Val_KL : 3.4760066270828247\n","Epoch: 6376/8000  Traning Loss: 90.47171115875244  Train_Reconstruction: 86.94464588165283  Train_KL: 3.5270651280879974  Validation Loss : 90.5827522277832 Val_Reconstruction : 87.09005737304688 Val_KL : 3.49269437789917\n","Epoch: 6377/8000  Traning Loss: 90.40609550476074  Train_Reconstruction: 86.88343811035156  Train_KL: 3.5226579308509827  Validation Loss : 90.5384521484375 Val_Reconstruction : 87.06579971313477 Val_KL : 3.4726520776748657\n","Epoch: 6378/8000  Traning Loss: 90.42640113830566  Train_Reconstruction: 86.90968704223633  Train_KL: 3.516713112592697  Validation Loss : 90.49189758300781 Val_Reconstruction : 87.01572036743164 Val_KL : 3.4761773347854614\n","Epoch: 6379/8000  Traning Loss: 90.6155366897583  Train_Reconstruction: 87.09733772277832  Train_KL: 3.518199324607849  Validation Loss : 90.49537658691406 Val_Reconstruction : 87.02411651611328 Val_KL : 3.471261501312256\n","Epoch: 6380/8000  Traning Loss: 90.79469299316406  Train_Reconstruction: 87.27493572235107  Train_KL: 3.5197572708129883  Validation Loss : 90.68724060058594 Val_Reconstruction : 87.2046012878418 Val_KL : 3.482641577720642\n","Epoch: 6381/8000  Traning Loss: 90.37818908691406  Train_Reconstruction: 86.8564624786377  Train_KL: 3.5217285454273224  Validation Loss : 90.33568572998047 Val_Reconstruction : 86.85696411132812 Val_KL : 3.478721857070923\n","Epoch: 6382/8000  Traning Loss: 90.32502555847168  Train_Reconstruction: 86.80740928649902  Train_KL: 3.517616182565689  Validation Loss : 90.56220626831055 Val_Reconstruction : 87.07898712158203 Val_KL : 3.4832193851470947\n","Epoch: 6383/8000  Traning Loss: 90.37626934051514  Train_Reconstruction: 86.86133766174316  Train_KL: 3.51493176817894  Validation Loss : 90.28073501586914 Val_Reconstruction : 86.81355285644531 Val_KL : 3.467181444168091\n","Epoch: 6384/8000  Traning Loss: 90.7142972946167  Train_Reconstruction: 87.20004463195801  Train_KL: 3.51425102353096  Validation Loss : 91.09209060668945 Val_Reconstruction : 87.61031341552734 Val_KL : 3.481776475906372\n","Epoch: 6385/8000  Traning Loss: 90.83669376373291  Train_Reconstruction: 87.31467533111572  Train_KL: 3.522018611431122  Validation Loss : 90.95792007446289 Val_Reconstruction : 87.47910690307617 Val_KL : 3.4788116216659546\n","Epoch: 6386/8000  Traning Loss: 90.3820390701294  Train_Reconstruction: 86.86779689788818  Train_KL: 3.5142414569854736  Validation Loss : 90.63378524780273 Val_Reconstruction : 87.16453552246094 Val_KL : 3.4692494869232178\n","Epoch: 6387/8000  Traning Loss: 90.29341983795166  Train_Reconstruction: 86.77782535552979  Train_KL: 3.515594393014908  Validation Loss : 90.49834060668945 Val_Reconstruction : 87.02011108398438 Val_KL : 3.4782298803329468\n","Epoch: 6388/8000  Traning Loss: 90.23158931732178  Train_Reconstruction: 86.70796012878418  Train_KL: 3.52362984418869  Validation Loss : 90.19298934936523 Val_Reconstruction : 86.71211242675781 Val_KL : 3.4808748960494995\n","Epoch: 6389/8000  Traning Loss: 90.20746517181396  Train_Reconstruction: 86.69492530822754  Train_KL: 3.5125401616096497  Validation Loss : 90.22764205932617 Val_Reconstruction : 86.75248336791992 Val_KL : 3.475159168243408\n","Epoch: 6390/8000  Traning Loss: 90.8533353805542  Train_Reconstruction: 87.32778739929199  Train_KL: 3.5255470871925354  Validation Loss : 91.86405563354492 Val_Reconstruction : 88.38425827026367 Val_KL : 3.4797991514205933\n","Epoch: 6391/8000  Traning Loss: 91.59550285339355  Train_Reconstruction: 88.0831880569458  Train_KL: 3.5123139023780823  Validation Loss : 91.57250213623047 Val_Reconstruction : 88.10809326171875 Val_KL : 3.464410901069641\n","Epoch: 6392/8000  Traning Loss: 90.68850326538086  Train_Reconstruction: 87.18447208404541  Train_KL: 3.504030853509903  Validation Loss : 90.5290412902832 Val_Reconstruction : 87.07102584838867 Val_KL : 3.4580143690109253\n","Epoch: 6393/8000  Traning Loss: 90.31575679779053  Train_Reconstruction: 86.80232334136963  Train_KL: 3.513434588909149  Validation Loss : 90.28216552734375 Val_Reconstruction : 86.8109016418457 Val_KL : 3.4712668657302856\n","Epoch: 6394/8000  Traning Loss: 90.61237907409668  Train_Reconstruction: 87.08907413482666  Train_KL: 3.523304373025894  Validation Loss : 90.5058708190918 Val_Reconstruction : 87.03135681152344 Val_KL : 3.4745137691497803\n","Epoch: 6395/8000  Traning Loss: 90.45778560638428  Train_Reconstruction: 86.93124103546143  Train_KL: 3.5265437960624695  Validation Loss : 90.50965881347656 Val_Reconstruction : 87.02128601074219 Val_KL : 3.4883713722229004\n","Epoch: 6396/8000  Traning Loss: 90.53798866271973  Train_Reconstruction: 87.01343536376953  Train_KL: 3.5245523154735565  Validation Loss : 90.42653274536133 Val_Reconstruction : 86.9536018371582 Val_KL : 3.4729301929473877\n","Epoch: 6397/8000  Traning Loss: 90.38826274871826  Train_Reconstruction: 86.86781597137451  Train_KL: 3.5204465985298157  Validation Loss : 90.39871597290039 Val_Reconstruction : 86.91633987426758 Val_KL : 3.4823755025863647\n","Epoch: 6398/8000  Traning Loss: 90.1771469116211  Train_Reconstruction: 86.65646839141846  Train_KL: 3.5206789076328278  Validation Loss : 90.24512100219727 Val_Reconstruction : 86.76698684692383 Val_KL : 3.478135585784912\n","Epoch: 6399/8000  Traning Loss: 90.36343288421631  Train_Reconstruction: 86.85180759429932  Train_KL: 3.511626213788986  Validation Loss : 90.39494323730469 Val_Reconstruction : 86.92696380615234 Val_KL : 3.467978596687317\n","Epoch: 6400/8000  Traning Loss: 90.10611343383789  Train_Reconstruction: 86.59358501434326  Train_KL: 3.5125285387039185  Validation Loss : 90.21917724609375 Val_Reconstruction : 86.74469757080078 Val_KL : 3.474479079246521\n","Epoch: 6401/8000  Traning Loss: 90.68553352355957  Train_Reconstruction: 87.15966701507568  Train_KL: 3.525868445634842  Validation Loss : 91.46515274047852 Val_Reconstruction : 87.97500991821289 Val_KL : 3.49014413356781\n","Epoch: 6402/8000  Traning Loss: 90.81982707977295  Train_Reconstruction: 87.29345703125  Train_KL: 3.5263711810112  Validation Loss : 90.80170822143555 Val_Reconstruction : 87.33252334594727 Val_KL : 3.4691842794418335\n","Epoch: 6403/8000  Traning Loss: 90.31741142272949  Train_Reconstruction: 86.79844665527344  Train_KL: 3.5189646184444427  Validation Loss : 90.63248825073242 Val_Reconstruction : 87.14853286743164 Val_KL : 3.483954668045044\n","Epoch: 6404/8000  Traning Loss: 90.24318790435791  Train_Reconstruction: 86.71622276306152  Train_KL: 3.5269649028778076  Validation Loss : 90.31169128417969 Val_Reconstruction : 86.83111953735352 Val_KL : 3.4805699586868286\n","Epoch: 6405/8000  Traning Loss: 90.11989498138428  Train_Reconstruction: 86.60388374328613  Train_KL: 3.5160099267959595  Validation Loss : 90.16801834106445 Val_Reconstruction : 86.69356155395508 Val_KL : 3.4744569063186646\n","Epoch: 6406/8000  Traning Loss: 90.11002540588379  Train_Reconstruction: 86.59244441986084  Train_KL: 3.517580419778824  Validation Loss : 90.11881637573242 Val_Reconstruction : 86.63917541503906 Val_KL : 3.4796382188796997\n","Epoch: 6407/8000  Traning Loss: 89.94126605987549  Train_Reconstruction: 86.41255760192871  Train_KL: 3.5287099182605743  Validation Loss : 90.02080535888672 Val_Reconstruction : 86.53194427490234 Val_KL : 3.4888617992401123\n","Epoch: 6408/8000  Traning Loss: 90.09716606140137  Train_Reconstruction: 86.56625175476074  Train_KL: 3.5309157967567444  Validation Loss : 90.2194938659668 Val_Reconstruction : 86.73649597167969 Val_KL : 3.482996344566345\n","Epoch: 6409/8000  Traning Loss: 90.02364540100098  Train_Reconstruction: 86.50134086608887  Train_KL: 3.522304028272629  Validation Loss : 90.27067184448242 Val_Reconstruction : 86.7885627746582 Val_KL : 3.482108473777771\n","Epoch: 6410/8000  Traning Loss: 89.96959209442139  Train_Reconstruction: 86.45289707183838  Train_KL: 3.516696274280548  Validation Loss : 90.20967102050781 Val_Reconstruction : 86.74345397949219 Val_KL : 3.466215133666992\n","Epoch: 6411/8000  Traning Loss: 90.18701934814453  Train_Reconstruction: 86.67405033111572  Train_KL: 3.5129684805870056  Validation Loss : 90.38360214233398 Val_Reconstruction : 86.90652084350586 Val_KL : 3.4770809412002563\n","Epoch: 6412/8000  Traning Loss: 90.35279941558838  Train_Reconstruction: 86.81853866577148  Train_KL: 3.5342601239681244  Validation Loss : 90.35489654541016 Val_Reconstruction : 86.85992813110352 Val_KL : 3.4949684143066406\n","Epoch: 6413/8000  Traning Loss: 90.32970523834229  Train_Reconstruction: 86.80064964294434  Train_KL: 3.529054820537567  Validation Loss : 90.36150741577148 Val_Reconstruction : 86.8844985961914 Val_KL : 3.4770069122314453\n","Epoch: 6414/8000  Traning Loss: 90.35937023162842  Train_Reconstruction: 86.8412446975708  Train_KL: 3.5181248784065247  Validation Loss : 90.77385330200195 Val_Reconstruction : 87.29549407958984 Val_KL : 3.478359818458557\n","Epoch: 6415/8000  Traning Loss: 90.1223955154419  Train_Reconstruction: 86.59461402893066  Train_KL: 3.527781456708908  Validation Loss : 90.37247848510742 Val_Reconstruction : 86.88220977783203 Val_KL : 3.490268588066101\n","Epoch: 6416/8000  Traning Loss: 90.1456069946289  Train_Reconstruction: 86.6138334274292  Train_KL: 3.5317749977111816  Validation Loss : 90.2420539855957 Val_Reconstruction : 86.74983596801758 Val_KL : 3.492216944694519\n","Epoch: 6417/8000  Traning Loss: 89.91161632537842  Train_Reconstruction: 86.38000297546387  Train_KL: 3.531612902879715  Validation Loss : 90.13569259643555 Val_Reconstruction : 86.65436935424805 Val_KL : 3.4813235998153687\n","Epoch: 6418/8000  Traning Loss: 90.22646522521973  Train_Reconstruction: 86.69735622406006  Train_KL: 3.529109090566635  Validation Loss : 90.60790252685547 Val_Reconstruction : 87.12293243408203 Val_KL : 3.484971523284912\n","Epoch: 6419/8000  Traning Loss: 90.32224655151367  Train_Reconstruction: 86.79728889465332  Train_KL: 3.5249579548835754  Validation Loss : 90.07869720458984 Val_Reconstruction : 86.59708786010742 Val_KL : 3.4816081523895264\n","Epoch: 6420/8000  Traning Loss: 90.06005001068115  Train_Reconstruction: 86.54107856750488  Train_KL: 3.518971085548401  Validation Loss : 90.1861457824707 Val_Reconstruction : 86.71430206298828 Val_KL : 3.4718436002731323\n","Epoch: 6421/8000  Traning Loss: 90.4475679397583  Train_Reconstruction: 86.93030834197998  Train_KL: 3.517259657382965  Validation Loss : 90.5285873413086 Val_Reconstruction : 87.0451889038086 Val_KL : 3.483397364616394\n","Epoch: 6422/8000  Traning Loss: 90.92632675170898  Train_Reconstruction: 87.40239524841309  Train_KL: 3.5239309072494507  Validation Loss : 91.11247634887695 Val_Reconstruction : 87.63419342041016 Val_KL : 3.478281021118164\n","Epoch: 6423/8000  Traning Loss: 90.70609378814697  Train_Reconstruction: 87.18401527404785  Train_KL: 3.5220789313316345  Validation Loss : 90.51465225219727 Val_Reconstruction : 87.04179382324219 Val_KL : 3.4728574752807617\n","Epoch: 6424/8000  Traning Loss: 90.18121528625488  Train_Reconstruction: 86.65940475463867  Train_KL: 3.5218100249767303  Validation Loss : 90.02382278442383 Val_Reconstruction : 86.53640365600586 Val_KL : 3.4874186515808105\n","Epoch: 6425/8000  Traning Loss: 90.01758003234863  Train_Reconstruction: 86.48179817199707  Train_KL: 3.5357816219329834  Validation Loss : 89.99384307861328 Val_Reconstruction : 86.50237655639648 Val_KL : 3.491468071937561\n","Epoch: 6426/8000  Traning Loss: 90.20725440979004  Train_Reconstruction: 86.68115043640137  Train_KL: 3.526103228330612  Validation Loss : 90.31313705444336 Val_Reconstruction : 86.83721160888672 Val_KL : 3.4759247303009033\n","Epoch: 6427/8000  Traning Loss: 90.29352283477783  Train_Reconstruction: 86.78398609161377  Train_KL: 3.5095365047454834  Validation Loss : 90.55918502807617 Val_Reconstruction : 87.08793640136719 Val_KL : 3.4712518453598022\n","Epoch: 6428/8000  Traning Loss: 90.213942527771  Train_Reconstruction: 86.69545269012451  Train_KL: 3.5184905529022217  Validation Loss : 90.58431243896484 Val_Reconstruction : 87.0918083190918 Val_KL : 3.4925018548965454\n","Epoch: 6429/8000  Traning Loss: 90.49263763427734  Train_Reconstruction: 86.95603942871094  Train_KL: 3.536598116159439  Validation Loss : 90.45105361938477 Val_Reconstruction : 86.9584732055664 Val_KL : 3.4925814867019653\n","Epoch: 6430/8000  Traning Loss: 90.36591911315918  Train_Reconstruction: 86.84071350097656  Train_KL: 3.525207608938217  Validation Loss : 90.2528305053711 Val_Reconstruction : 86.77507400512695 Val_KL : 3.477756381034851\n","Epoch: 6431/8000  Traning Loss: 90.39561557769775  Train_Reconstruction: 86.88115978240967  Train_KL: 3.514454483985901  Validation Loss : 90.57000732421875 Val_Reconstruction : 87.0972785949707 Val_KL : 3.4727314710617065\n","Epoch: 6432/8000  Traning Loss: 90.41690158843994  Train_Reconstruction: 86.89957237243652  Train_KL: 3.517330378293991  Validation Loss : 90.56149673461914 Val_Reconstruction : 87.08871078491211 Val_KL : 3.4727866649627686\n","Epoch: 6433/8000  Traning Loss: 90.86305046081543  Train_Reconstruction: 87.34838581085205  Train_KL: 3.5146647095680237  Validation Loss : 91.64904403686523 Val_Reconstruction : 88.17715835571289 Val_KL : 3.471887230873108\n","Epoch: 6434/8000  Traning Loss: 91.44128894805908  Train_Reconstruction: 87.91991519927979  Train_KL: 3.521373987197876  Validation Loss : 91.19415664672852 Val_Reconstruction : 87.71021270751953 Val_KL : 3.483944058418274\n","Epoch: 6435/8000  Traning Loss: 91.25796031951904  Train_Reconstruction: 87.73352146148682  Train_KL: 3.524437963962555  Validation Loss : 91.21514511108398 Val_Reconstruction : 87.73987197875977 Val_KL : 3.4752734899520874\n","Epoch: 6436/8000  Traning Loss: 91.00157642364502  Train_Reconstruction: 87.48324680328369  Train_KL: 3.5183301866054535  Validation Loss : 90.74430465698242 Val_Reconstruction : 87.26308059692383 Val_KL : 3.481224536895752\n","Epoch: 6437/8000  Traning Loss: 90.5313024520874  Train_Reconstruction: 87.01702785491943  Train_KL: 3.514273226261139  Validation Loss : 90.47289276123047 Val_Reconstruction : 87.00607299804688 Val_KL : 3.4668190479278564\n","Epoch: 6438/8000  Traning Loss: 90.14148139953613  Train_Reconstruction: 86.62522220611572  Train_KL: 3.51625856757164  Validation Loss : 90.44235610961914 Val_Reconstruction : 86.96515274047852 Val_KL : 3.477201223373413\n","Epoch: 6439/8000  Traning Loss: 90.14128017425537  Train_Reconstruction: 86.61614227294922  Train_KL: 3.5251382887363434  Validation Loss : 90.20427322387695 Val_Reconstruction : 86.72802352905273 Val_KL : 3.4762498140335083\n","Epoch: 6440/8000  Traning Loss: 90.1016321182251  Train_Reconstruction: 86.58921813964844  Train_KL: 3.5124131441116333  Validation Loss : 90.17400741577148 Val_Reconstruction : 86.71289825439453 Val_KL : 3.4611096382141113\n","Epoch: 6441/8000  Traning Loss: 89.92998886108398  Train_Reconstruction: 86.4057674407959  Train_KL: 3.5242217779159546  Validation Loss : 90.29771041870117 Val_Reconstruction : 86.80634689331055 Val_KL : 3.491362690925598\n","Epoch: 6442/8000  Traning Loss: 89.95799160003662  Train_Reconstruction: 86.4214735031128  Train_KL: 3.5365180373191833  Validation Loss : 90.17817687988281 Val_Reconstruction : 86.69020080566406 Val_KL : 3.4879748821258545\n","Epoch: 6443/8000  Traning Loss: 90.1208028793335  Train_Reconstruction: 86.59785461425781  Train_KL: 3.5229478776454926  Validation Loss : 90.19715118408203 Val_Reconstruction : 86.72118759155273 Val_KL : 3.4759644269943237\n","Epoch: 6444/8000  Traning Loss: 90.08876419067383  Train_Reconstruction: 86.56915760040283  Train_KL: 3.5196063816547394  Validation Loss : 90.0510368347168 Val_Reconstruction : 86.56392669677734 Val_KL : 3.48711097240448\n","Epoch: 6445/8000  Traning Loss: 90.12626457214355  Train_Reconstruction: 86.59328746795654  Train_KL: 3.5329773128032684  Validation Loss : 90.32674407958984 Val_Reconstruction : 86.83130264282227 Val_KL : 3.495439052581787\n","Epoch: 6446/8000  Traning Loss: 89.98773860931396  Train_Reconstruction: 86.46149444580078  Train_KL: 3.52624449133873  Validation Loss : 89.98623275756836 Val_Reconstruction : 86.51047897338867 Val_KL : 3.475754141807556\n","Epoch: 6447/8000  Traning Loss: 90.04569149017334  Train_Reconstruction: 86.52859783172607  Train_KL: 3.517093777656555  Validation Loss : 89.86849594116211 Val_Reconstruction : 86.39322662353516 Val_KL : 3.4752697944641113\n","Epoch: 6448/8000  Traning Loss: 89.76887798309326  Train_Reconstruction: 86.24319553375244  Train_KL: 3.5256813764572144  Validation Loss : 90.00752258300781 Val_Reconstruction : 86.52828216552734 Val_KL : 3.479238986968994\n","Epoch: 6449/8000  Traning Loss: 90.18250846862793  Train_Reconstruction: 86.65643501281738  Train_KL: 3.526074081659317  Validation Loss : 90.41544342041016 Val_Reconstruction : 86.93125534057617 Val_KL : 3.484186887741089\n","Epoch: 6450/8000  Traning Loss: 90.259765625  Train_Reconstruction: 86.74223327636719  Train_KL: 3.5175333619117737  Validation Loss : 90.5042953491211 Val_Reconstruction : 87.03414154052734 Val_KL : 3.4701534509658813\n","Epoch: 6451/8000  Traning Loss: 90.3316650390625  Train_Reconstruction: 86.81281566619873  Train_KL: 3.518850475549698  Validation Loss : 90.11673355102539 Val_Reconstruction : 86.63508987426758 Val_KL : 3.481644868850708\n","Epoch: 6452/8000  Traning Loss: 90.02188491821289  Train_Reconstruction: 86.49487590789795  Train_KL: 3.5270082354545593  Validation Loss : 89.84625244140625 Val_Reconstruction : 86.36516571044922 Val_KL : 3.48108971118927\n","Epoch: 6453/8000  Traning Loss: 90.1146593093872  Train_Reconstruction: 86.58703517913818  Train_KL: 3.5276255309581757  Validation Loss : 90.11320114135742 Val_Reconstruction : 86.62729263305664 Val_KL : 3.4859061241149902\n","Epoch: 6454/8000  Traning Loss: 90.14858436584473  Train_Reconstruction: 86.61809921264648  Train_KL: 3.5304853320121765  Validation Loss : 90.22223663330078 Val_Reconstruction : 86.7380599975586 Val_KL : 3.48417866230011\n","Epoch: 6455/8000  Traning Loss: 89.91731834411621  Train_Reconstruction: 86.3995008468628  Train_KL: 3.5178184807300568  Validation Loss : 89.88308715820312 Val_Reconstruction : 86.41116333007812 Val_KL : 3.471923351287842\n","Epoch: 6456/8000  Traning Loss: 89.76821804046631  Train_Reconstruction: 86.2448959350586  Train_KL: 3.523322194814682  Validation Loss : 89.90585327148438 Val_Reconstruction : 86.4149055480957 Val_KL : 3.490946888923645\n","Epoch: 6457/8000  Traning Loss: 89.83251857757568  Train_Reconstruction: 86.30010414123535  Train_KL: 3.532414138317108  Validation Loss : 89.9430160522461 Val_Reconstruction : 86.45856475830078 Val_KL : 3.484451174736023\n","Epoch: 6458/8000  Traning Loss: 89.85402011871338  Train_Reconstruction: 86.33285236358643  Train_KL: 3.521168529987335  Validation Loss : 90.17922973632812 Val_Reconstruction : 86.7014274597168 Val_KL : 3.4778021574020386\n","Epoch: 6459/8000  Traning Loss: 89.85276508331299  Train_Reconstruction: 86.32915782928467  Train_KL: 3.5236065685749054  Validation Loss : 90.11361312866211 Val_Reconstruction : 86.63050842285156 Val_KL : 3.483104705810547\n","Epoch: 6460/8000  Traning Loss: 90.31492233276367  Train_Reconstruction: 86.78500461578369  Train_KL: 3.529918521642685  Validation Loss : 90.39022064208984 Val_Reconstruction : 86.90232467651367 Val_KL : 3.4878945350646973\n","Epoch: 6461/8000  Traning Loss: 90.58479022979736  Train_Reconstruction: 87.0597677230835  Train_KL: 3.5250226855278015  Validation Loss : 90.4021987915039 Val_Reconstruction : 86.91997909545898 Val_KL : 3.4822216033935547\n","Epoch: 6462/8000  Traning Loss: 90.52451515197754  Train_Reconstruction: 87.00401973724365  Train_KL: 3.5204948484897614  Validation Loss : 90.68659591674805 Val_Reconstruction : 87.20504760742188 Val_KL : 3.481545090675354\n","Epoch: 6463/8000  Traning Loss: 90.75444412231445  Train_Reconstruction: 87.22463703155518  Train_KL: 3.529806673526764  Validation Loss : 90.99688720703125 Val_Reconstruction : 87.52009582519531 Val_KL : 3.4767924547195435\n","Epoch: 6464/8000  Traning Loss: 90.57532501220703  Train_Reconstruction: 87.06113624572754  Train_KL: 3.5141883492469788  Validation Loss : 90.53913116455078 Val_Reconstruction : 87.07197952270508 Val_KL : 3.4671539068222046\n","Epoch: 6465/8000  Traning Loss: 90.19088459014893  Train_Reconstruction: 86.67266464233398  Train_KL: 3.5182207822799683  Validation Loss : 90.15022277832031 Val_Reconstruction : 86.67058944702148 Val_KL : 3.479632258415222\n","Epoch: 6466/8000  Traning Loss: 90.1395149230957  Train_Reconstruction: 86.60797214508057  Train_KL: 3.5315439701080322  Validation Loss : 90.25228118896484 Val_Reconstruction : 86.76110076904297 Val_KL : 3.4911820888519287\n","Epoch: 6467/8000  Traning Loss: 90.00269317626953  Train_Reconstruction: 86.47609519958496  Train_KL: 3.5265983045101166  Validation Loss : 90.1334228515625 Val_Reconstruction : 86.66040802001953 Val_KL : 3.4730172157287598\n","Epoch: 6468/8000  Traning Loss: 89.97584056854248  Train_Reconstruction: 86.45982933044434  Train_KL: 3.516011744737625  Validation Loss : 90.25704574584961 Val_Reconstruction : 86.78127670288086 Val_KL : 3.4757702350616455\n","Epoch: 6469/8000  Traning Loss: 90.31782341003418  Train_Reconstruction: 86.78909206390381  Train_KL: 3.5287300646305084  Validation Loss : 90.8249397277832 Val_Reconstruction : 87.33967971801758 Val_KL : 3.4852631092071533\n","Epoch: 6470/8000  Traning Loss: 90.5378065109253  Train_Reconstruction: 87.00685405731201  Train_KL: 3.530951887369156  Validation Loss : 90.98615264892578 Val_Reconstruction : 87.4976692199707 Val_KL : 3.4884824752807617\n","Epoch: 6471/8000  Traning Loss: 90.71815395355225  Train_Reconstruction: 87.19004249572754  Train_KL: 3.5281122028827667  Validation Loss : 90.77398300170898 Val_Reconstruction : 87.29075622558594 Val_KL : 3.4832276105880737\n","Epoch: 6472/8000  Traning Loss: 90.8907995223999  Train_Reconstruction: 87.3623104095459  Train_KL: 3.528487980365753  Validation Loss : 91.08136749267578 Val_Reconstruction : 87.59759521484375 Val_KL : 3.483771800994873\n","Epoch: 6473/8000  Traning Loss: 90.44496822357178  Train_Reconstruction: 86.92014122009277  Train_KL: 3.5248261988162994  Validation Loss : 90.47590255737305 Val_Reconstruction : 86.99586868286133 Val_KL : 3.4800362586975098\n","Epoch: 6474/8000  Traning Loss: 90.09436511993408  Train_Reconstruction: 86.57312393188477  Train_KL: 3.521241694688797  Validation Loss : 90.23719787597656 Val_Reconstruction : 86.76526641845703 Val_KL : 3.4719334840774536\n","Epoch: 6475/8000  Traning Loss: 89.97258949279785  Train_Reconstruction: 86.460618019104  Train_KL: 3.511971741914749  Validation Loss : 89.8927001953125 Val_Reconstruction : 86.42837142944336 Val_KL : 3.4643267393112183\n","Epoch: 6476/8000  Traning Loss: 90.13225936889648  Train_Reconstruction: 86.6243486404419  Train_KL: 3.5079114735126495  Validation Loss : 90.17782592773438 Val_Reconstruction : 86.70516586303711 Val_KL : 3.472659111022949\n","Epoch: 6477/8000  Traning Loss: 90.06331157684326  Train_Reconstruction: 86.53900623321533  Train_KL: 3.5243053436279297  Validation Loss : 89.93720626831055 Val_Reconstruction : 86.45523452758789 Val_KL : 3.4819729328155518\n","Epoch: 6478/8000  Traning Loss: 90.0076265335083  Train_Reconstruction: 86.48059749603271  Train_KL: 3.5270282328128815  Validation Loss : 90.11598205566406 Val_Reconstruction : 86.62770462036133 Val_KL : 3.48827862739563\n","Epoch: 6479/8000  Traning Loss: 89.98600673675537  Train_Reconstruction: 86.4554214477539  Train_KL: 3.5305849611759186  Validation Loss : 90.2313117980957 Val_Reconstruction : 86.74666213989258 Val_KL : 3.484652280807495\n","Epoch: 6480/8000  Traning Loss: 90.50505256652832  Train_Reconstruction: 86.97603797912598  Train_KL: 3.529015749692917  Validation Loss : 90.57862091064453 Val_Reconstruction : 87.09062194824219 Val_KL : 3.4879987239837646\n","Epoch: 6481/8000  Traning Loss: 90.74292469024658  Train_Reconstruction: 87.22315311431885  Train_KL: 3.5197715163230896  Validation Loss : 91.21332168579102 Val_Reconstruction : 87.73531341552734 Val_KL : 3.478005528450012\n","Epoch: 6482/8000  Traning Loss: 90.56233024597168  Train_Reconstruction: 87.04290390014648  Train_KL: 3.519426107406616  Validation Loss : 90.2812728881836 Val_Reconstruction : 86.80120468139648 Val_KL : 3.480065107345581\n","Epoch: 6483/8000  Traning Loss: 89.98552131652832  Train_Reconstruction: 86.459397315979  Train_KL: 3.526123285293579  Validation Loss : 90.01446914672852 Val_Reconstruction : 86.52827072143555 Val_KL : 3.4861985445022583\n","Epoch: 6484/8000  Traning Loss: 89.93556308746338  Train_Reconstruction: 86.41124534606934  Train_KL: 3.5243173837661743  Validation Loss : 90.2775764465332 Val_Reconstruction : 86.80110931396484 Val_KL : 3.4764702320098877\n","Epoch: 6485/8000  Traning Loss: 90.16076564788818  Train_Reconstruction: 86.6498613357544  Train_KL: 3.510904371738434  Validation Loss : 90.32197952270508 Val_Reconstruction : 86.85278701782227 Val_KL : 3.4691909551620483\n","Epoch: 6486/8000  Traning Loss: 90.70861721038818  Train_Reconstruction: 87.19668674468994  Train_KL: 3.511929541826248  Validation Loss : 90.68115234375 Val_Reconstruction : 87.20641326904297 Val_KL : 3.474740147590637\n","Epoch: 6487/8000  Traning Loss: 90.59040355682373  Train_Reconstruction: 87.06899642944336  Train_KL: 3.521406352519989  Validation Loss : 90.93707656860352 Val_Reconstruction : 87.45575714111328 Val_KL : 3.481320381164551\n","Epoch: 6488/8000  Traning Loss: 90.94039535522461  Train_Reconstruction: 87.4142713546753  Train_KL: 3.526123881340027  Validation Loss : 91.18271255493164 Val_Reconstruction : 87.7029800415039 Val_KL : 3.479731798171997\n","Epoch: 6489/8000  Traning Loss: 90.92259979248047  Train_Reconstruction: 87.40474796295166  Train_KL: 3.51785084605217  Validation Loss : 90.89410018920898 Val_Reconstruction : 87.42156982421875 Val_KL : 3.4725311994552612\n","Epoch: 6490/8000  Traning Loss: 90.21486282348633  Train_Reconstruction: 86.69141960144043  Train_KL: 3.523442506790161  Validation Loss : 90.24850463867188 Val_Reconstruction : 86.77937698364258 Val_KL : 3.4691269397735596\n","Epoch: 6491/8000  Traning Loss: 89.99052333831787  Train_Reconstruction: 86.46406364440918  Train_KL: 3.5264596045017242  Validation Loss : 90.16951370239258 Val_Reconstruction : 86.68942642211914 Val_KL : 3.4800868034362793\n","Epoch: 6492/8000  Traning Loss: 90.11003875732422  Train_Reconstruction: 86.59264755249023  Train_KL: 3.517391711473465  Validation Loss : 90.27090454101562 Val_Reconstruction : 86.80739212036133 Val_KL : 3.4635140895843506\n","Epoch: 6493/8000  Traning Loss: 90.14311504364014  Train_Reconstruction: 86.6251220703125  Train_KL: 3.5179946422576904  Validation Loss : 90.1447868347168 Val_Reconstruction : 86.66565322875977 Val_KL : 3.479133129119873\n","Epoch: 6494/8000  Traning Loss: 90.27273464202881  Train_Reconstruction: 86.75298881530762  Train_KL: 3.519745171070099  Validation Loss : 90.38662338256836 Val_Reconstruction : 86.91320037841797 Val_KL : 3.473422408103943\n","Epoch: 6495/8000  Traning Loss: 90.25670909881592  Train_Reconstruction: 86.73877620697021  Train_KL: 3.5179336965084076  Validation Loss : 90.91981506347656 Val_Reconstruction : 87.44252014160156 Val_KL : 3.477293848991394\n","Epoch: 6496/8000  Traning Loss: 90.68260860443115  Train_Reconstruction: 87.16143894195557  Train_KL: 3.5211717188358307  Validation Loss : 90.41874313354492 Val_Reconstruction : 86.93415451049805 Val_KL : 3.484589099884033\n","Epoch: 6497/8000  Traning Loss: 90.43864917755127  Train_Reconstruction: 86.90984058380127  Train_KL: 3.5288087725639343  Validation Loss : 90.60688018798828 Val_Reconstruction : 87.12017822265625 Val_KL : 3.4867045879364014\n","Epoch: 6498/8000  Traning Loss: 89.97180080413818  Train_Reconstruction: 86.4381742477417  Train_KL: 3.53362774848938  Validation Loss : 90.24202346801758 Val_Reconstruction : 86.74784088134766 Val_KL : 3.4941797256469727\n","Epoch: 6499/8000  Traning Loss: 90.26332950592041  Train_Reconstruction: 86.7327356338501  Train_KL: 3.5305938124656677  Validation Loss : 90.38116073608398 Val_Reconstruction : 86.90383529663086 Val_KL : 3.4773237705230713\n","Epoch: 6500/8000  Traning Loss: 90.27283763885498  Train_Reconstruction: 86.75472354888916  Train_KL: 3.5181133449077606  Validation Loss : 90.32587432861328 Val_Reconstruction : 86.84706497192383 Val_KL : 3.4788107872009277\n","Epoch: 6501/8000  Traning Loss: 89.828537940979  Train_Reconstruction: 86.30876541137695  Train_KL: 3.51977202296257  Validation Loss : 89.88459777832031 Val_Reconstruction : 86.41754913330078 Val_KL : 3.467049479484558\n","Epoch: 6502/8000  Traning Loss: 89.91695880889893  Train_Reconstruction: 86.40369319915771  Train_KL: 3.513265609741211  Validation Loss : 90.2739143371582 Val_Reconstruction : 86.80781555175781 Val_KL : 3.4660972356796265\n","Epoch: 6503/8000  Traning Loss: 89.96389389038086  Train_Reconstruction: 86.44536113739014  Train_KL: 3.51853284239769  Validation Loss : 89.95986938476562 Val_Reconstruction : 86.4827651977539 Val_KL : 3.4771006107330322\n","Epoch: 6504/8000  Traning Loss: 90.03095436096191  Train_Reconstruction: 86.49957180023193  Train_KL: 3.531382828950882  Validation Loss : 90.28069305419922 Val_Reconstruction : 86.79417037963867 Val_KL : 3.4865232706069946\n","Epoch: 6505/8000  Traning Loss: 90.266037940979  Train_Reconstruction: 86.74087142944336  Train_KL: 3.5251654386520386  Validation Loss : 90.66736221313477 Val_Reconstruction : 87.19035720825195 Val_KL : 3.4770021438598633\n","Epoch: 6506/8000  Traning Loss: 90.25410652160645  Train_Reconstruction: 86.73512935638428  Train_KL: 3.5189760625362396  Validation Loss : 90.36971282958984 Val_Reconstruction : 86.89176177978516 Val_KL : 3.477953553199768\n","Epoch: 6507/8000  Traning Loss: 90.57085609436035  Train_Reconstruction: 87.04393768310547  Train_KL: 3.5269189178943634  Validation Loss : 91.04010772705078 Val_Reconstruction : 87.55464172363281 Val_KL : 3.4854689836502075\n","Epoch: 6508/8000  Traning Loss: 90.44095611572266  Train_Reconstruction: 86.91578102111816  Train_KL: 3.5251752734184265  Validation Loss : 90.58837890625 Val_Reconstruction : 87.11818313598633 Val_KL : 3.470197916030884\n","Epoch: 6509/8000  Traning Loss: 90.11903953552246  Train_Reconstruction: 86.60351753234863  Train_KL: 3.5155234038829803  Validation Loss : 90.08753204345703 Val_Reconstruction : 86.611083984375 Val_KL : 3.476448893547058\n","Epoch: 6510/8000  Traning Loss: 89.88721370697021  Train_Reconstruction: 86.36229705810547  Train_KL: 3.524916708469391  Validation Loss : 89.98250579833984 Val_Reconstruction : 86.4907455444336 Val_KL : 3.4917609691619873\n","Epoch: 6511/8000  Traning Loss: 90.10917091369629  Train_Reconstruction: 86.57870674133301  Train_KL: 3.5304640233516693  Validation Loss : 90.94232940673828 Val_Reconstruction : 87.4523811340332 Val_KL : 3.4899498224258423\n","Epoch: 6512/8000  Traning Loss: 90.61303806304932  Train_Reconstruction: 87.09336948394775  Train_KL: 3.5196692049503326  Validation Loss : 90.91189575195312 Val_Reconstruction : 87.4375 Val_KL : 3.4743969440460205\n","Epoch: 6513/8000  Traning Loss: 90.90800952911377  Train_Reconstruction: 87.3844575881958  Train_KL: 3.5235513150691986  Validation Loss : 91.15507125854492 Val_Reconstruction : 87.67089462280273 Val_KL : 3.484178066253662\n","Epoch: 6514/8000  Traning Loss: 90.4132776260376  Train_Reconstruction: 86.88195133209229  Train_KL: 3.531326413154602  Validation Loss : 90.22871017456055 Val_Reconstruction : 86.74490737915039 Val_KL : 3.4838041067123413\n","Epoch: 6515/8000  Traning Loss: 89.86158275604248  Train_Reconstruction: 86.33282375335693  Train_KL: 3.5287591218948364  Validation Loss : 90.11729431152344 Val_Reconstruction : 86.64746856689453 Val_KL : 3.469827175140381\n","Epoch: 6516/8000  Traning Loss: 89.88184356689453  Train_Reconstruction: 86.37002754211426  Train_KL: 3.5118155777454376  Validation Loss : 90.08416366577148 Val_Reconstruction : 86.61955642700195 Val_KL : 3.4646075963974\n","Epoch: 6517/8000  Traning Loss: 90.09274864196777  Train_Reconstruction: 86.56423664093018  Train_KL: 3.52851203083992  Validation Loss : 90.58921813964844 Val_Reconstruction : 87.09334182739258 Val_KL : 3.4958763122558594\n","Epoch: 6518/8000  Traning Loss: 90.23948383331299  Train_Reconstruction: 86.71734237670898  Train_KL: 3.522140473127365  Validation Loss : 90.56726455688477 Val_Reconstruction : 87.098388671875 Val_KL : 3.4688750505447388\n","Epoch: 6519/8000  Traning Loss: 90.78628826141357  Train_Reconstruction: 87.2698860168457  Train_KL: 3.5164011120796204  Validation Loss : 90.55868148803711 Val_Reconstruction : 87.08015060424805 Val_KL : 3.4785321950912476\n","Epoch: 6520/8000  Traning Loss: 90.27671146392822  Train_Reconstruction: 86.75381755828857  Train_KL: 3.5228932201862335  Validation Loss : 90.17551040649414 Val_Reconstruction : 86.69758987426758 Val_KL : 3.4779212474823\n","Epoch: 6521/8000  Traning Loss: 90.32964324951172  Train_Reconstruction: 86.8077621459961  Train_KL: 3.521881490945816  Validation Loss : 90.71952819824219 Val_Reconstruction : 87.23898315429688 Val_KL : 3.480544090270996\n","Epoch: 6522/8000  Traning Loss: 90.28752708435059  Train_Reconstruction: 86.75611686706543  Train_KL: 3.5314094722270966  Validation Loss : 90.34219360351562 Val_Reconstruction : 86.85089492797852 Val_KL : 3.491300106048584\n","Epoch: 6523/8000  Traning Loss: 90.41458702087402  Train_Reconstruction: 86.88465881347656  Train_KL: 3.5299278795719147  Validation Loss : 90.294921875 Val_Reconstruction : 86.81608581542969 Val_KL : 3.478836178779602\n","Epoch: 6524/8000  Traning Loss: 90.25236225128174  Train_Reconstruction: 86.7240343093872  Train_KL: 3.5283285081386566  Validation Loss : 90.08181762695312 Val_Reconstruction : 86.60585021972656 Val_KL : 3.4759702682495117\n","Epoch: 6525/8000  Traning Loss: 89.89653301239014  Train_Reconstruction: 86.3732967376709  Train_KL: 3.5232368111610413  Validation Loss : 90.27921676635742 Val_Reconstruction : 86.79960632324219 Val_KL : 3.479608654975891\n","Epoch: 6526/8000  Traning Loss: 90.15311527252197  Train_Reconstruction: 86.62753295898438  Train_KL: 3.525581955909729  Validation Loss : 90.0877799987793 Val_Reconstruction : 86.6114387512207 Val_KL : 3.4763436317443848\n","Epoch: 6527/8000  Traning Loss: 90.1778793334961  Train_Reconstruction: 86.6599931716919  Train_KL: 3.517886608839035  Validation Loss : 90.29813003540039 Val_Reconstruction : 86.82297134399414 Val_KL : 3.4751594066619873\n","Epoch: 6528/8000  Traning Loss: 90.13711643218994  Train_Reconstruction: 86.61330699920654  Train_KL: 3.5238099098205566  Validation Loss : 90.33635711669922 Val_Reconstruction : 86.85388565063477 Val_KL : 3.482473134994507\n","Epoch: 6529/8000  Traning Loss: 90.16214084625244  Train_Reconstruction: 86.63464069366455  Train_KL: 3.5274999737739563  Validation Loss : 90.12051773071289 Val_Reconstruction : 86.63646697998047 Val_KL : 3.484050750732422\n","Epoch: 6530/8000  Traning Loss: 90.22594261169434  Train_Reconstruction: 86.70627498626709  Train_KL: 3.519666463136673  Validation Loss : 90.59075927734375 Val_Reconstruction : 87.11797332763672 Val_KL : 3.472785472869873\n","Epoch: 6531/8000  Traning Loss: 90.28454971313477  Train_Reconstruction: 86.76826190948486  Train_KL: 3.516286700963974  Validation Loss : 90.43350601196289 Val_Reconstruction : 86.95619201660156 Val_KL : 3.4773120880126953\n","Epoch: 6532/8000  Traning Loss: 90.24606227874756  Train_Reconstruction: 86.72571754455566  Train_KL: 3.5203448235988617  Validation Loss : 90.3213882446289 Val_Reconstruction : 86.83837890625 Val_KL : 3.4830087423324585\n","Epoch: 6533/8000  Traning Loss: 90.21497821807861  Train_Reconstruction: 86.68754005432129  Train_KL: 3.527438849210739  Validation Loss : 90.55810546875 Val_Reconstruction : 87.07498931884766 Val_KL : 3.4831151962280273\n","Epoch: 6534/8000  Traning Loss: 90.25662899017334  Train_Reconstruction: 86.73367023468018  Train_KL: 3.5229585468769073  Validation Loss : 90.1754150390625 Val_Reconstruction : 86.6886978149414 Val_KL : 3.486714720726013\n","Epoch: 6535/8000  Traning Loss: 90.02200508117676  Train_Reconstruction: 86.4871187210083  Train_KL: 3.534887045621872  Validation Loss : 90.01968002319336 Val_Reconstruction : 86.53326034545898 Val_KL : 3.486419916152954\n","Epoch: 6536/8000  Traning Loss: 90.25090503692627  Train_Reconstruction: 86.72991847991943  Train_KL: 3.520986884832382  Validation Loss : 90.52851486206055 Val_Reconstruction : 87.05783462524414 Val_KL : 3.4706826210021973\n","Epoch: 6537/8000  Traning Loss: 90.7016658782959  Train_Reconstruction: 87.18775272369385  Train_KL: 3.513913780450821  Validation Loss : 90.78866577148438 Val_Reconstruction : 87.31709671020508 Val_KL : 3.471571207046509\n","Epoch: 6538/8000  Traning Loss: 90.22314548492432  Train_Reconstruction: 86.7049789428711  Train_KL: 3.518166273832321  Validation Loss : 90.1115837097168 Val_Reconstruction : 86.63783645629883 Val_KL : 3.473749279975891\n","Epoch: 6539/8000  Traning Loss: 89.84650707244873  Train_Reconstruction: 86.32694435119629  Train_KL: 3.51956307888031  Validation Loss : 90.23833847045898 Val_Reconstruction : 86.7601318359375 Val_KL : 3.4782074689865112\n","Epoch: 6540/8000  Traning Loss: 89.91212368011475  Train_Reconstruction: 86.38638114929199  Train_KL: 3.5257428884506226  Validation Loss : 90.35993194580078 Val_Reconstruction : 86.87115478515625 Val_KL : 3.4887771606445312\n","Epoch: 6541/8000  Traning Loss: 90.40599155426025  Train_Reconstruction: 86.88066959381104  Train_KL: 3.5253220796585083  Validation Loss : 91.06452941894531 Val_Reconstruction : 87.5859603881836 Val_KL : 3.4785666465759277\n","Epoch: 6542/8000  Traning Loss: 90.72528457641602  Train_Reconstruction: 87.20605564117432  Train_KL: 3.519228309392929  Validation Loss : 91.14621353149414 Val_Reconstruction : 87.66748809814453 Val_KL : 3.47872531414032\n","Epoch: 6543/8000  Traning Loss: 90.45698833465576  Train_Reconstruction: 86.93396472930908  Train_KL: 3.523023396730423  Validation Loss : 90.51234817504883 Val_Reconstruction : 87.02830123901367 Val_KL : 3.4840492010116577\n","Epoch: 6544/8000  Traning Loss: 90.1168041229248  Train_Reconstruction: 86.5913314819336  Train_KL: 3.525474339723587  Validation Loss : 90.03223037719727 Val_Reconstruction : 86.5479507446289 Val_KL : 3.484280824661255\n","Epoch: 6545/8000  Traning Loss: 90.06186866760254  Train_Reconstruction: 86.53696250915527  Train_KL: 3.5249061286449432  Validation Loss : 90.16269302368164 Val_Reconstruction : 86.68288040161133 Val_KL : 3.479813814163208\n","Epoch: 6546/8000  Traning Loss: 89.93263053894043  Train_Reconstruction: 86.40766620635986  Train_KL: 3.5249651074409485  Validation Loss : 89.99309158325195 Val_Reconstruction : 86.5088882446289 Val_KL : 3.484204411506653\n","Epoch: 6547/8000  Traning Loss: 90.08946704864502  Train_Reconstruction: 86.55637550354004  Train_KL: 3.5330918431282043  Validation Loss : 90.42061614990234 Val_Reconstruction : 86.93317031860352 Val_KL : 3.487445831298828\n","Epoch: 6548/8000  Traning Loss: 90.14221954345703  Train_Reconstruction: 86.61542415618896  Train_KL: 3.5267949402332306  Validation Loss : 90.19204711914062 Val_Reconstruction : 86.71719741821289 Val_KL : 3.4748517274856567\n","Epoch: 6549/8000  Traning Loss: 90.3623218536377  Train_Reconstruction: 86.84305477142334  Train_KL: 3.519269049167633  Validation Loss : 90.7660026550293 Val_Reconstruction : 87.28662490844727 Val_KL : 3.4793776273727417\n","Epoch: 6550/8000  Traning Loss: 90.65714454650879  Train_Reconstruction: 87.12796592712402  Train_KL: 3.5291799306869507  Validation Loss : 91.0294418334961 Val_Reconstruction : 87.54605865478516 Val_KL : 3.483383893966675\n","Epoch: 6551/8000  Traning Loss: 90.66746711730957  Train_Reconstruction: 87.15839767456055  Train_KL: 3.5090692937374115  Validation Loss : 90.74457550048828 Val_Reconstruction : 87.2785530090332 Val_KL : 3.4660210609436035\n","Epoch: 6552/8000  Traning Loss: 90.41035556793213  Train_Reconstruction: 86.89632225036621  Train_KL: 3.514033228158951  Validation Loss : 90.58748626708984 Val_Reconstruction : 87.10441970825195 Val_KL : 3.4830697774887085\n","Epoch: 6553/8000  Traning Loss: 90.17850494384766  Train_Reconstruction: 86.65531063079834  Train_KL: 3.5231958627700806  Validation Loss : 90.14927673339844 Val_Reconstruction : 86.66605758666992 Val_KL : 3.4832208156585693\n","Epoch: 6554/8000  Traning Loss: 89.88149166107178  Train_Reconstruction: 86.36178779602051  Train_KL: 3.5197044014930725  Validation Loss : 89.9715805053711 Val_Reconstruction : 86.49275970458984 Val_KL : 3.4788224697113037\n","Epoch: 6555/8000  Traning Loss: 90.2574815750122  Train_Reconstruction: 86.73522758483887  Train_KL: 3.522254765033722  Validation Loss : 90.22221755981445 Val_Reconstruction : 86.73034286499023 Val_KL : 3.4918774366378784\n","Epoch: 6556/8000  Traning Loss: 90.28360557556152  Train_Reconstruction: 86.75568675994873  Train_KL: 3.5279174745082855  Validation Loss : 90.41159439086914 Val_Reconstruction : 86.9259147644043 Val_KL : 3.485678553581238\n","Epoch: 6557/8000  Traning Loss: 90.60982704162598  Train_Reconstruction: 87.090012550354  Train_KL: 3.5198142528533936  Validation Loss : 90.84073257446289 Val_Reconstruction : 87.37541198730469 Val_KL : 3.4653185606002808\n","Epoch: 6558/8000  Traning Loss: 90.49543762207031  Train_Reconstruction: 86.9874963760376  Train_KL: 3.5079410672187805  Validation Loss : 90.52610397338867 Val_Reconstruction : 87.05634689331055 Val_KL : 3.4697537422180176\n","Epoch: 6559/8000  Traning Loss: 90.10670185089111  Train_Reconstruction: 86.58914756774902  Train_KL: 3.517552524805069  Validation Loss : 90.2922134399414 Val_Reconstruction : 86.82120513916016 Val_KL : 3.4710079431533813\n","Epoch: 6560/8000  Traning Loss: 90.07382678985596  Train_Reconstruction: 86.55562114715576  Train_KL: 3.5182067453861237  Validation Loss : 90.11544036865234 Val_Reconstruction : 86.63820266723633 Val_KL : 3.477240562438965\n","Epoch: 6561/8000  Traning Loss: 90.21434307098389  Train_Reconstruction: 86.68939876556396  Train_KL: 3.524943560361862  Validation Loss : 90.35124969482422 Val_Reconstruction : 86.86471557617188 Val_KL : 3.4865317344665527\n","Epoch: 6562/8000  Traning Loss: 90.05474758148193  Train_Reconstruction: 86.52794170379639  Train_KL: 3.5268067717552185  Validation Loss : 90.26575088500977 Val_Reconstruction : 86.78921508789062 Val_KL : 3.4765363931655884\n","Epoch: 6563/8000  Traning Loss: 90.20830154418945  Train_Reconstruction: 86.68603324890137  Train_KL: 3.5222687125205994  Validation Loss : 89.96035385131836 Val_Reconstruction : 86.47584915161133 Val_KL : 3.4845043420791626\n","Epoch: 6564/8000  Traning Loss: 90.50952911376953  Train_Reconstruction: 86.98116779327393  Train_KL: 3.5283614695072174  Validation Loss : 90.602783203125 Val_Reconstruction : 87.11742401123047 Val_KL : 3.4853585958480835\n","Epoch: 6565/8000  Traning Loss: 90.5100326538086  Train_Reconstruction: 86.99585628509521  Train_KL: 3.5141761302948  Validation Loss : 90.66713333129883 Val_Reconstruction : 87.20402908325195 Val_KL : 3.4631054401397705\n","Epoch: 6566/8000  Traning Loss: 90.53894233703613  Train_Reconstruction: 87.02200412750244  Train_KL: 3.516937792301178  Validation Loss : 90.63787841796875 Val_Reconstruction : 87.15666198730469 Val_KL : 3.4812188148498535\n","Epoch: 6567/8000  Traning Loss: 90.34119701385498  Train_Reconstruction: 86.81083297729492  Train_KL: 3.5303636491298676  Validation Loss : 90.2374153137207 Val_Reconstruction : 86.75071334838867 Val_KL : 3.48669970035553\n","Epoch: 6568/8000  Traning Loss: 90.31802368164062  Train_Reconstruction: 86.79344463348389  Train_KL: 3.5245785415172577  Validation Loss : 90.60399627685547 Val_Reconstruction : 87.12506484985352 Val_KL : 3.4789302349090576\n","Epoch: 6569/8000  Traning Loss: 90.43066310882568  Train_Reconstruction: 86.90565204620361  Train_KL: 3.5250115394592285  Validation Loss : 90.76493453979492 Val_Reconstruction : 87.27853393554688 Val_KL : 3.486401677131653\n","Epoch: 6570/8000  Traning Loss: 90.26281642913818  Train_Reconstruction: 86.74230098724365  Train_KL: 3.520516127347946  Validation Loss : 90.50155639648438 Val_Reconstruction : 87.02425384521484 Val_KL : 3.4773038625717163\n","Epoch: 6571/8000  Traning Loss: 90.57676219940186  Train_Reconstruction: 87.05650234222412  Train_KL: 3.5202595591545105  Validation Loss : 90.38106155395508 Val_Reconstruction : 86.9083023071289 Val_KL : 3.4727611541748047\n","Epoch: 6572/8000  Traning Loss: 90.08496761322021  Train_Reconstruction: 86.56041145324707  Train_KL: 3.5245552361011505  Validation Loss : 90.10199737548828 Val_Reconstruction : 86.62080001831055 Val_KL : 3.4812002182006836\n","Epoch: 6573/8000  Traning Loss: 90.00236415863037  Train_Reconstruction: 86.48499202728271  Train_KL: 3.5173723101615906  Validation Loss : 90.4077377319336 Val_Reconstruction : 86.93252182006836 Val_KL : 3.475215792655945\n","Epoch: 6574/8000  Traning Loss: 90.15872287750244  Train_Reconstruction: 86.6334810256958  Train_KL: 3.5252423882484436  Validation Loss : 90.66439056396484 Val_Reconstruction : 87.17779922485352 Val_KL : 3.486591339111328\n","Epoch: 6575/8000  Traning Loss: 90.28680610656738  Train_Reconstruction: 86.7675371170044  Train_KL: 3.5192691385746  Validation Loss : 90.2568473815918 Val_Reconstruction : 86.77623748779297 Val_KL : 3.480609178543091\n","Epoch: 6576/8000  Traning Loss: 90.05847072601318  Train_Reconstruction: 86.52902126312256  Train_KL: 3.5294484198093414  Validation Loss : 90.27251434326172 Val_Reconstruction : 86.78409576416016 Val_KL : 3.488418936729431\n","Epoch: 6577/8000  Traning Loss: 90.23544406890869  Train_Reconstruction: 86.70722675323486  Train_KL: 3.5282175540924072  Validation Loss : 90.77020645141602 Val_Reconstruction : 87.2862434387207 Val_KL : 3.483965039253235\n","Epoch: 6578/8000  Traning Loss: 91.09025478363037  Train_Reconstruction: 87.56167888641357  Train_KL: 3.5285757780075073  Validation Loss : 91.1834487915039 Val_Reconstruction : 87.70210647583008 Val_KL : 3.4813398122787476\n","Epoch: 6579/8000  Traning Loss: 90.62955951690674  Train_Reconstruction: 87.1068000793457  Train_KL: 3.522758662700653  Validation Loss : 90.42814636230469 Val_Reconstruction : 86.95328140258789 Val_KL : 3.474865198135376\n","Epoch: 6580/8000  Traning Loss: 90.25467014312744  Train_Reconstruction: 86.74327945709229  Train_KL: 3.5113924145698547  Validation Loss : 90.11619567871094 Val_Reconstruction : 86.64779281616211 Val_KL : 3.468400478363037\n","Epoch: 6581/8000  Traning Loss: 89.96514892578125  Train_Reconstruction: 86.44456481933594  Train_KL: 3.520583748817444  Validation Loss : 90.63644409179688 Val_Reconstruction : 87.14194869995117 Val_KL : 3.494495153427124\n","Epoch: 6582/8000  Traning Loss: 90.2052173614502  Train_Reconstruction: 86.66302394866943  Train_KL: 3.542192429304123  Validation Loss : 90.4933090209961 Val_Reconstruction : 86.99438095092773 Val_KL : 3.498928427696228\n","Epoch: 6583/8000  Traning Loss: 90.30567359924316  Train_Reconstruction: 86.77893447875977  Train_KL: 3.5267381072044373  Validation Loss : 90.24309921264648 Val_Reconstruction : 86.76359939575195 Val_KL : 3.4795002937316895\n","Epoch: 6584/8000  Traning Loss: 90.0716724395752  Train_Reconstruction: 86.55362319946289  Train_KL: 3.5180492103099823  Validation Loss : 89.95647430419922 Val_Reconstruction : 86.4848518371582 Val_KL : 3.4716230630874634\n","Epoch: 6585/8000  Traning Loss: 90.05421161651611  Train_Reconstruction: 86.53686809539795  Train_KL: 3.5173456370830536  Validation Loss : 90.68229675292969 Val_Reconstruction : 87.2042121887207 Val_KL : 3.478082776069641\n","Epoch: 6586/8000  Traning Loss: 90.33869934082031  Train_Reconstruction: 86.8205394744873  Train_KL: 3.518159180879593  Validation Loss : 90.9338150024414 Val_Reconstruction : 87.46104049682617 Val_KL : 3.4727765321731567\n","Epoch: 6587/8000  Traning Loss: 90.37180423736572  Train_Reconstruction: 86.85719394683838  Train_KL: 3.5146107375621796  Validation Loss : 90.35910415649414 Val_Reconstruction : 86.8869514465332 Val_KL : 3.472153425216675\n","Epoch: 6588/8000  Traning Loss: 89.92285442352295  Train_Reconstruction: 86.40367603302002  Train_KL: 3.5191795229911804  Validation Loss : 89.8111572265625 Val_Reconstruction : 86.32538604736328 Val_KL : 3.485772490501404\n","Epoch: 6589/8000  Traning Loss: 89.77040386199951  Train_Reconstruction: 86.24029159545898  Train_KL: 3.5301121175289154  Validation Loss : 89.74688339233398 Val_Reconstruction : 86.26145935058594 Val_KL : 3.4854249954223633\n","Epoch: 6590/8000  Traning Loss: 90.04493427276611  Train_Reconstruction: 86.52773952484131  Train_KL: 3.517194151878357  Validation Loss : 90.54366302490234 Val_Reconstruction : 87.07316589355469 Val_KL : 3.470497250556946\n","Epoch: 6591/8000  Traning Loss: 90.05301570892334  Train_Reconstruction: 86.53420734405518  Train_KL: 3.5188078582286835  Validation Loss : 90.28202819824219 Val_Reconstruction : 86.7975845336914 Val_KL : 3.484441876411438\n","Epoch: 6592/8000  Traning Loss: 90.6059513092041  Train_Reconstruction: 87.07617378234863  Train_KL: 3.529776632785797  Validation Loss : 91.2288932800293 Val_Reconstruction : 87.74252700805664 Val_KL : 3.486366629600525\n","Epoch: 6593/8000  Traning Loss: 92.32522773742676  Train_Reconstruction: 88.8116512298584  Train_KL: 3.5135768949985504  Validation Loss : 92.75364303588867 Val_Reconstruction : 89.29315567016602 Val_KL : 3.460488438606262\n","Epoch: 6594/8000  Traning Loss: 92.083740234375  Train_Reconstruction: 88.57155323028564  Train_KL: 3.512185037136078  Validation Loss : 91.34599685668945 Val_Reconstruction : 87.86477661132812 Val_KL : 3.4812201261520386\n","Epoch: 6595/8000  Traning Loss: 90.88350009918213  Train_Reconstruction: 87.35296440124512  Train_KL: 3.530535727739334  Validation Loss : 90.81711959838867 Val_Reconstruction : 87.32584762573242 Val_KL : 3.491270422935486\n","Epoch: 6596/8000  Traning Loss: 90.69167518615723  Train_Reconstruction: 87.16422462463379  Train_KL: 3.527451992034912  Validation Loss : 90.95541000366211 Val_Reconstruction : 87.47563934326172 Val_KL : 3.479767918586731\n","Epoch: 6597/8000  Traning Loss: 90.53174591064453  Train_Reconstruction: 87.01122856140137  Train_KL: 3.5205167829990387  Validation Loss : 90.24288177490234 Val_Reconstruction : 86.76207733154297 Val_KL : 3.480803370475769\n","Epoch: 6598/8000  Traning Loss: 90.14327335357666  Train_Reconstruction: 86.63045024871826  Train_KL: 3.512822985649109  Validation Loss : 90.45929718017578 Val_Reconstruction : 86.9918098449707 Val_KL : 3.4674869775772095\n","Epoch: 6599/8000  Traning Loss: 90.3110876083374  Train_Reconstruction: 86.79477214813232  Train_KL: 3.5163146555423737  Validation Loss : 90.71234893798828 Val_Reconstruction : 87.23726654052734 Val_KL : 3.4750818014144897\n","Epoch: 6600/8000  Traning Loss: 90.02938747406006  Train_Reconstruction: 86.51312065124512  Train_KL: 3.5162675082683563  Validation Loss : 90.28403854370117 Val_Reconstruction : 86.81440734863281 Val_KL : 3.469631791114807\n","Epoch: 6601/8000  Traning Loss: 89.92770957946777  Train_Reconstruction: 86.39983081817627  Train_KL: 3.5278797447681427  Validation Loss : 90.12385559082031 Val_Reconstruction : 86.62218475341797 Val_KL : 3.501668334007263\n","Epoch: 6602/8000  Traning Loss: 89.68672180175781  Train_Reconstruction: 86.1458911895752  Train_KL: 3.5408310294151306  Validation Loss : 89.86555480957031 Val_Reconstruction : 86.37247467041016 Val_KL : 3.493080973625183\n","Epoch: 6603/8000  Traning Loss: 90.03035545349121  Train_Reconstruction: 86.50466632843018  Train_KL: 3.5256890654563904  Validation Loss : 90.32629013061523 Val_Reconstruction : 86.85551834106445 Val_KL : 3.4707703590393066\n","Epoch: 6604/8000  Traning Loss: 90.26379203796387  Train_Reconstruction: 86.73773574829102  Train_KL: 3.5260570645332336  Validation Loss : 90.22983932495117 Val_Reconstruction : 86.73943710327148 Val_KL : 3.490399122238159\n","Epoch: 6605/8000  Traning Loss: 90.14811515808105  Train_Reconstruction: 86.6257152557373  Train_KL: 3.5223990380764008  Validation Loss : 90.13953018188477 Val_Reconstruction : 86.6687126159668 Val_KL : 3.4708187580108643\n","Epoch: 6606/8000  Traning Loss: 90.03392028808594  Train_Reconstruction: 86.51910400390625  Train_KL: 3.514816105365753  Validation Loss : 90.51505279541016 Val_Reconstruction : 87.03664016723633 Val_KL : 3.4784101247787476\n","Epoch: 6607/8000  Traning Loss: 90.32502460479736  Train_Reconstruction: 86.80030822753906  Train_KL: 3.524716854095459  Validation Loss : 90.52276992797852 Val_Reconstruction : 87.03891372680664 Val_KL : 3.4838547706604004\n","Epoch: 6608/8000  Traning Loss: 90.28427791595459  Train_Reconstruction: 86.76671886444092  Train_KL: 3.5175588130950928  Validation Loss : 90.66647720336914 Val_Reconstruction : 87.19804000854492 Val_KL : 3.4684360027313232\n","Epoch: 6609/8000  Traning Loss: 90.48507308959961  Train_Reconstruction: 86.9747667312622  Train_KL: 3.510306179523468  Validation Loss : 90.45867156982422 Val_Reconstruction : 86.99128341674805 Val_KL : 3.46738600730896\n","Epoch: 6610/8000  Traning Loss: 90.22747325897217  Train_Reconstruction: 86.69802951812744  Train_KL: 3.5294431149959564  Validation Loss : 90.0602035522461 Val_Reconstruction : 86.56173324584961 Val_KL : 3.4984683990478516\n","Epoch: 6611/8000  Traning Loss: 90.08688735961914  Train_Reconstruction: 86.55385684967041  Train_KL: 3.533030152320862  Validation Loss : 90.27609634399414 Val_Reconstruction : 86.7988395690918 Val_KL : 3.477258086204529\n","Epoch: 6612/8000  Traning Loss: 90.47305679321289  Train_Reconstruction: 86.95850658416748  Train_KL: 3.5145505368709564  Validation Loss : 90.47262191772461 Val_Reconstruction : 86.99850845336914 Val_KL : 3.4741148948669434\n","Epoch: 6613/8000  Traning Loss: 90.53231143951416  Train_Reconstruction: 87.02079200744629  Train_KL: 3.511520653963089  Validation Loss : 90.75955200195312 Val_Reconstruction : 87.28948211669922 Val_KL : 3.47006893157959\n","Epoch: 6614/8000  Traning Loss: 90.40699768066406  Train_Reconstruction: 86.89112377166748  Train_KL: 3.5158737301826477  Validation Loss : 90.7633171081543 Val_Reconstruction : 87.28736877441406 Val_KL : 3.475947618484497\n","Epoch: 6615/8000  Traning Loss: 90.37891483306885  Train_Reconstruction: 86.8550968170166  Train_KL: 3.5238175988197327  Validation Loss : 90.35551071166992 Val_Reconstruction : 86.87716674804688 Val_KL : 3.47834575176239\n","Epoch: 6616/8000  Traning Loss: 90.06851005554199  Train_Reconstruction: 86.54817295074463  Train_KL: 3.520337551832199  Validation Loss : 90.43827438354492 Val_Reconstruction : 86.95661163330078 Val_KL : 3.4816616773605347\n","Epoch: 6617/8000  Traning Loss: 90.04731845855713  Train_Reconstruction: 86.52241897583008  Train_KL: 3.5248981416225433  Validation Loss : 90.41953659057617 Val_Reconstruction : 86.93393325805664 Val_KL : 3.485605835914612\n","Epoch: 6618/8000  Traning Loss: 89.78139781951904  Train_Reconstruction: 86.24368476867676  Train_KL: 3.537712901830673  Validation Loss : 90.03373718261719 Val_Reconstruction : 86.53425216674805 Val_KL : 3.499487519264221\n","Epoch: 6619/8000  Traning Loss: 89.87365627288818  Train_Reconstruction: 86.34857845306396  Train_KL: 3.5250770151615143  Validation Loss : 90.29192733764648 Val_Reconstruction : 86.82319259643555 Val_KL : 3.4687323570251465\n","Epoch: 6620/8000  Traning Loss: 89.87404537200928  Train_Reconstruction: 86.3679084777832  Train_KL: 3.506138324737549  Validation Loss : 90.0349349975586 Val_Reconstruction : 86.56588745117188 Val_KL : 3.469046115875244\n","Epoch: 6621/8000  Traning Loss: 90.61687660217285  Train_Reconstruction: 87.09253311157227  Train_KL: 3.524342507123947  Validation Loss : 90.96273040771484 Val_Reconstruction : 87.47655868530273 Val_KL : 3.4861689805984497\n","Epoch: 6622/8000  Traning Loss: 90.86139488220215  Train_Reconstruction: 87.33875751495361  Train_KL: 3.5226370692253113  Validation Loss : 91.18064498901367 Val_Reconstruction : 87.70311737060547 Val_KL : 3.4775279760360718\n","Epoch: 6623/8000  Traning Loss: 90.2330379486084  Train_Reconstruction: 86.71326923370361  Train_KL: 3.5197681188583374  Validation Loss : 89.90150451660156 Val_Reconstruction : 86.42535781860352 Val_KL : 3.4761465787887573\n","Epoch: 6624/8000  Traning Loss: 89.94615650177002  Train_Reconstruction: 86.42018604278564  Train_KL: 3.5259703397750854  Validation Loss : 90.12245178222656 Val_Reconstruction : 86.64179229736328 Val_KL : 3.4806597232818604\n","Epoch: 6625/8000  Traning Loss: 90.05464553833008  Train_Reconstruction: 86.53132724761963  Train_KL: 3.5233157575130463  Validation Loss : 90.12975692749023 Val_Reconstruction : 86.64962005615234 Val_KL : 3.480139374732971\n","Epoch: 6626/8000  Traning Loss: 90.14841365814209  Train_Reconstruction: 86.62193775177002  Train_KL: 3.526474356651306  Validation Loss : 90.21611022949219 Val_Reconstruction : 86.73491668701172 Val_KL : 3.481192111968994\n","Epoch: 6627/8000  Traning Loss: 90.5134744644165  Train_Reconstruction: 86.9948320388794  Train_KL: 3.518642097711563  Validation Loss : 90.72346115112305 Val_Reconstruction : 87.2483024597168 Val_KL : 3.4751596450805664\n","Epoch: 6628/8000  Traning Loss: 90.2325210571289  Train_Reconstruction: 86.71969032287598  Train_KL: 3.5128296613693237  Validation Loss : 90.22964477539062 Val_Reconstruction : 86.75688552856445 Val_KL : 3.4727590084075928\n","Epoch: 6629/8000  Traning Loss: 90.246337890625  Train_Reconstruction: 86.72405433654785  Train_KL: 3.522282510995865  Validation Loss : 90.50123596191406 Val_Reconstruction : 87.01380157470703 Val_KL : 3.487431526184082\n","Epoch: 6630/8000  Traning Loss: 90.3361988067627  Train_Reconstruction: 86.8092393875122  Train_KL: 3.5269599854946136  Validation Loss : 90.25956344604492 Val_Reconstruction : 86.77255630493164 Val_KL : 3.4870060682296753\n","Epoch: 6631/8000  Traning Loss: 90.04101753234863  Train_Reconstruction: 86.52759170532227  Train_KL: 3.513424426317215  Validation Loss : 90.1212158203125 Val_Reconstruction : 86.65390396118164 Val_KL : 3.46731173992157\n","Epoch: 6632/8000  Traning Loss: 90.2791395187378  Train_Reconstruction: 86.76225852966309  Train_KL: 3.5168801844120026  Validation Loss : 90.43610000610352 Val_Reconstruction : 86.96002197265625 Val_KL : 3.4760786294937134\n","Epoch: 6633/8000  Traning Loss: 90.15680980682373  Train_Reconstruction: 86.63248538970947  Train_KL: 3.5243238508701324  Validation Loss : 90.03409576416016 Val_Reconstruction : 86.5478744506836 Val_KL : 3.486222267150879\n","Epoch: 6634/8000  Traning Loss: 89.93632698059082  Train_Reconstruction: 86.41339015960693  Train_KL: 3.5229368805885315  Validation Loss : 90.08304595947266 Val_Reconstruction : 86.60890197753906 Val_KL : 3.4741469621658325\n","Epoch: 6635/8000  Traning Loss: 89.88401985168457  Train_Reconstruction: 86.37625408172607  Train_KL: 3.507765531539917  Validation Loss : 89.86050415039062 Val_Reconstruction : 86.38906478881836 Val_KL : 3.471438407897949\n","Epoch: 6636/8000  Traning Loss: 89.97702693939209  Train_Reconstruction: 86.45370197296143  Train_KL: 3.5233261585235596  Validation Loss : 90.30361557006836 Val_Reconstruction : 86.81707000732422 Val_KL : 3.486544132232666\n","Epoch: 6637/8000  Traning Loss: 90.03309345245361  Train_Reconstruction: 86.50313091278076  Train_KL: 3.52996364235878  Validation Loss : 90.35710144042969 Val_Reconstruction : 86.86910247802734 Val_KL : 3.4879965782165527\n","Epoch: 6638/8000  Traning Loss: 90.33180904388428  Train_Reconstruction: 86.81022453308105  Train_KL: 3.5215844810009003  Validation Loss : 90.40428924560547 Val_Reconstruction : 86.93421936035156 Val_KL : 3.470072388648987\n","Epoch: 6639/8000  Traning Loss: 90.27925777435303  Train_Reconstruction: 86.77029514312744  Train_KL: 3.508962571620941  Validation Loss : 90.16703033447266 Val_Reconstruction : 86.70383834838867 Val_KL : 3.463193655014038\n","Epoch: 6640/8000  Traning Loss: 90.6244707107544  Train_Reconstruction: 87.09754085540771  Train_KL: 3.5269300043582916  Validation Loss : 90.99311828613281 Val_Reconstruction : 87.49774551391602 Val_KL : 3.4953724145889282\n","Epoch: 6641/8000  Traning Loss: 90.23363971710205  Train_Reconstruction: 86.70264911651611  Train_KL: 3.5309910476207733  Validation Loss : 90.21208190917969 Val_Reconstruction : 86.72960662841797 Val_KL : 3.482478141784668\n","Epoch: 6642/8000  Traning Loss: 90.0306453704834  Train_Reconstruction: 86.51780414581299  Train_KL: 3.5128414630889893  Validation Loss : 89.96361923217773 Val_Reconstruction : 86.49585342407227 Val_KL : 3.467766761779785\n","Epoch: 6643/8000  Traning Loss: 90.18602085113525  Train_Reconstruction: 86.66729068756104  Train_KL: 3.518730968236923  Validation Loss : 90.3415298461914 Val_Reconstruction : 86.85770797729492 Val_KL : 3.48382306098938\n","Epoch: 6644/8000  Traning Loss: 90.03478813171387  Train_Reconstruction: 86.50213050842285  Train_KL: 3.5326564013957977  Validation Loss : 90.36732482910156 Val_Reconstruction : 86.87897491455078 Val_KL : 3.488350987434387\n","Epoch: 6645/8000  Traning Loss: 90.26256370544434  Train_Reconstruction: 86.74112606048584  Train_KL: 3.5214360654354095  Validation Loss : 90.58388137817383 Val_Reconstruction : 87.11188125610352 Val_KL : 3.471999764442444\n","Epoch: 6646/8000  Traning Loss: 90.21595859527588  Train_Reconstruction: 86.70647430419922  Train_KL: 3.509484648704529  Validation Loss : 90.49093246459961 Val_Reconstruction : 87.02518081665039 Val_KL : 3.465749144554138\n","Epoch: 6647/8000  Traning Loss: 90.10260009765625  Train_Reconstruction: 86.58064651489258  Train_KL: 3.52195280790329  Validation Loss : 90.44438552856445 Val_Reconstruction : 86.95917510986328 Val_KL : 3.485211133956909\n","Epoch: 6648/8000  Traning Loss: 90.09958553314209  Train_Reconstruction: 86.56818008422852  Train_KL: 3.531406819820404  Validation Loss : 90.10684967041016 Val_Reconstruction : 86.61929321289062 Val_KL : 3.4875547885894775\n","Epoch: 6649/8000  Traning Loss: 89.87871265411377  Train_Reconstruction: 86.35121059417725  Train_KL: 3.52750164270401  Validation Loss : 89.84882736206055 Val_Reconstruction : 86.36633682250977 Val_KL : 3.4824886322021484\n","Epoch: 6650/8000  Traning Loss: 89.79556655883789  Train_Reconstruction: 86.26627445220947  Train_KL: 3.529292732477188  Validation Loss : 89.8879280090332 Val_Reconstruction : 86.403076171875 Val_KL : 3.4848554134368896\n","Epoch: 6651/8000  Traning Loss: 89.73416805267334  Train_Reconstruction: 86.20652961730957  Train_KL: 3.5276388227939606  Validation Loss : 89.96690368652344 Val_Reconstruction : 86.49282455444336 Val_KL : 3.474077343940735\n","Epoch: 6652/8000  Traning Loss: 89.90575981140137  Train_Reconstruction: 86.39077663421631  Train_KL: 3.5149843394756317  Validation Loss : 90.04122924804688 Val_Reconstruction : 86.57106399536133 Val_KL : 3.470165252685547\n","Epoch: 6653/8000  Traning Loss: 90.18344402313232  Train_Reconstruction: 86.6637716293335  Train_KL: 3.5196720361709595  Validation Loss : 90.4397087097168 Val_Reconstruction : 86.95489883422852 Val_KL : 3.4848098754882812\n","Epoch: 6654/8000  Traning Loss: 90.56031513214111  Train_Reconstruction: 87.03288650512695  Train_KL: 3.527429223060608  Validation Loss : 90.50690078735352 Val_Reconstruction : 87.01658248901367 Val_KL : 3.490319013595581\n","Epoch: 6655/8000  Traning Loss: 90.53213500976562  Train_Reconstruction: 87.0118579864502  Train_KL: 3.520276755094528  Validation Loss : 90.18385314941406 Val_Reconstruction : 86.71180725097656 Val_KL : 3.472046375274658\n","Epoch: 6656/8000  Traning Loss: 90.06110095977783  Train_Reconstruction: 86.53757190704346  Train_KL: 3.5235291719436646  Validation Loss : 89.85491943359375 Val_Reconstruction : 86.37022018432617 Val_KL : 3.48469877243042\n","Epoch: 6657/8000  Traning Loss: 89.85603332519531  Train_Reconstruction: 86.33171081542969  Train_KL: 3.5243226885795593  Validation Loss : 89.85057067871094 Val_Reconstruction : 86.3729019165039 Val_KL : 3.4776687622070312\n","Epoch: 6658/8000  Traning Loss: 89.83412075042725  Train_Reconstruction: 86.32550430297852  Train_KL: 3.5086170434951782  Validation Loss : 89.9444465637207 Val_Reconstruction : 86.48263168334961 Val_KL : 3.4618141651153564\n","Epoch: 6659/8000  Traning Loss: 89.93578243255615  Train_Reconstruction: 86.42453479766846  Train_KL: 3.5112482607364655  Validation Loss : 90.05502700805664 Val_Reconstruction : 86.5847282409668 Val_KL : 3.470298171043396\n","Epoch: 6660/8000  Traning Loss: 90.01555061340332  Train_Reconstruction: 86.48866844177246  Train_KL: 3.5268821120262146  Validation Loss : 90.09895706176758 Val_Reconstruction : 86.6117057800293 Val_KL : 3.4872511625289917\n","Epoch: 6661/8000  Traning Loss: 89.9378719329834  Train_Reconstruction: 86.41009330749512  Train_KL: 3.5277795493602753  Validation Loss : 89.91986083984375 Val_Reconstruction : 86.43396759033203 Val_KL : 3.4858932495117188\n","Epoch: 6662/8000  Traning Loss: 90.07823085784912  Train_Reconstruction: 86.55548191070557  Train_KL: 3.5227489471435547  Validation Loss : 90.31411361694336 Val_Reconstruction : 86.84247589111328 Val_KL : 3.4716371297836304\n","Epoch: 6663/8000  Traning Loss: 90.17217254638672  Train_Reconstruction: 86.65830135345459  Train_KL: 3.513870447874069  Validation Loss : 90.65130996704102 Val_Reconstruction : 87.17756271362305 Val_KL : 3.4737441539764404\n","Epoch: 6664/8000  Traning Loss: 90.10258960723877  Train_Reconstruction: 86.57819557189941  Train_KL: 3.524393618106842  Validation Loss : 90.61981964111328 Val_Reconstruction : 87.13264846801758 Val_KL : 3.4871710538864136\n","Epoch: 6665/8000  Traning Loss: 90.33990955352783  Train_Reconstruction: 86.81385040283203  Train_KL: 3.526057720184326  Validation Loss : 90.60297393798828 Val_Reconstruction : 87.12080001831055 Val_KL : 3.4821730852127075\n","Epoch: 6666/8000  Traning Loss: 90.1316556930542  Train_Reconstruction: 86.61025810241699  Train_KL: 3.521398276090622  Validation Loss : 90.40752792358398 Val_Reconstruction : 86.9314079284668 Val_KL : 3.476121187210083\n","Epoch: 6667/8000  Traning Loss: 89.95527172088623  Train_Reconstruction: 86.43746757507324  Train_KL: 3.517804205417633  Validation Loss : 90.13808059692383 Val_Reconstruction : 86.66055679321289 Val_KL : 3.477526307106018\n","Epoch: 6668/8000  Traning Loss: 90.16885948181152  Train_Reconstruction: 86.64799404144287  Train_KL: 3.520866483449936  Validation Loss : 90.34160995483398 Val_Reconstruction : 86.86096954345703 Val_KL : 3.4806398153305054\n","Epoch: 6669/8000  Traning Loss: 90.09294986724854  Train_Reconstruction: 86.56252002716064  Train_KL: 3.5304311513900757  Validation Loss : 90.76712036132812 Val_Reconstruction : 87.2692642211914 Val_KL : 3.4978575706481934\n","Epoch: 6670/8000  Traning Loss: 89.93301582336426  Train_Reconstruction: 86.40622043609619  Train_KL: 3.526795357465744  Validation Loss : 89.89400863647461 Val_Reconstruction : 86.41304016113281 Val_KL : 3.480969190597534\n","Epoch: 6671/8000  Traning Loss: 90.04649066925049  Train_Reconstruction: 86.53153800964355  Train_KL: 3.5149518847465515  Validation Loss : 90.38116455078125 Val_Reconstruction : 86.91011810302734 Val_KL : 3.471047520637512\n","Epoch: 6672/8000  Traning Loss: 90.35364627838135  Train_Reconstruction: 86.83728122711182  Train_KL: 3.516363948583603  Validation Loss : 90.81243515014648 Val_Reconstruction : 87.32534790039062 Val_KL : 3.487086772918701\n","Epoch: 6673/8000  Traning Loss: 90.3563060760498  Train_Reconstruction: 86.82233428955078  Train_KL: 3.533972144126892  Validation Loss : 90.46566390991211 Val_Reconstruction : 86.96890640258789 Val_KL : 3.4967581033706665\n","Epoch: 6674/8000  Traning Loss: 89.9843397140503  Train_Reconstruction: 86.45547199249268  Train_KL: 3.5288687646389008  Validation Loss : 90.1633529663086 Val_Reconstruction : 86.67845916748047 Val_KL : 3.484895944595337\n","Epoch: 6675/8000  Traning Loss: 90.00977802276611  Train_Reconstruction: 86.48721694946289  Train_KL: 3.5225613117218018  Validation Loss : 90.09786224365234 Val_Reconstruction : 86.6198844909668 Val_KL : 3.477978229522705\n","Epoch: 6676/8000  Traning Loss: 90.12603569030762  Train_Reconstruction: 86.60895729064941  Train_KL: 3.5170785784721375  Validation Loss : 90.27652740478516 Val_Reconstruction : 86.80021286010742 Val_KL : 3.476318120956421\n","Epoch: 6677/8000  Traning Loss: 90.62602519989014  Train_Reconstruction: 87.10686683654785  Train_KL: 3.5191591680049896  Validation Loss : 91.13742065429688 Val_Reconstruction : 87.65606689453125 Val_KL : 3.481355905532837\n","Epoch: 6678/8000  Traning Loss: 90.54657077789307  Train_Reconstruction: 87.02637958526611  Train_KL: 3.5201906263828278  Validation Loss : 90.56077575683594 Val_Reconstruction : 87.08467864990234 Val_KL : 3.476097822189331\n","Epoch: 6679/8000  Traning Loss: 90.52103996276855  Train_Reconstruction: 87.00073528289795  Train_KL: 3.520304560661316  Validation Loss : 90.53461074829102 Val_Reconstruction : 87.04450988769531 Val_KL : 3.490103602409363\n","Epoch: 6680/8000  Traning Loss: 90.07737636566162  Train_Reconstruction: 86.54590606689453  Train_KL: 3.5314687192440033  Validation Loss : 89.84453582763672 Val_Reconstruction : 86.35739135742188 Val_KL : 3.4871463775634766\n","Epoch: 6681/8000  Traning Loss: 89.99816036224365  Train_Reconstruction: 86.47855854034424  Train_KL: 3.5196023285388947  Validation Loss : 90.27251434326172 Val_Reconstruction : 86.79573822021484 Val_KL : 3.476779580116272\n","Epoch: 6682/8000  Traning Loss: 90.15553951263428  Train_Reconstruction: 86.63547897338867  Train_KL: 3.520060956478119  Validation Loss : 90.34325790405273 Val_Reconstruction : 86.87005996704102 Val_KL : 3.4731996059417725\n","Epoch: 6683/8000  Traning Loss: 90.30347728729248  Train_Reconstruction: 86.78750801086426  Train_KL: 3.5159696340560913  Validation Loss : 90.34106063842773 Val_Reconstruction : 86.86323165893555 Val_KL : 3.4778276681900024\n","Epoch: 6684/8000  Traning Loss: 90.16847705841064  Train_Reconstruction: 86.6380262374878  Train_KL: 3.5304495990276337  Validation Loss : 90.25557708740234 Val_Reconstruction : 86.76416397094727 Val_KL : 3.491415023803711\n","Epoch: 6685/8000  Traning Loss: 90.55489253997803  Train_Reconstruction: 87.02902603149414  Train_KL: 3.5258662700653076  Validation Loss : 90.74882507324219 Val_Reconstruction : 87.26974487304688 Val_KL : 3.4790825843811035\n","Epoch: 6686/8000  Traning Loss: 90.4062147140503  Train_Reconstruction: 86.8805980682373  Train_KL: 3.5256169140338898  Validation Loss : 90.68955612182617 Val_Reconstruction : 87.21086120605469 Val_KL : 3.4786940813064575\n","Epoch: 6687/8000  Traning Loss: 90.19990634918213  Train_Reconstruction: 86.68075180053711  Train_KL: 3.519152820110321  Validation Loss : 90.37943267822266 Val_Reconstruction : 86.90412521362305 Val_KL : 3.4753079414367676\n","Epoch: 6688/8000  Traning Loss: 90.09967994689941  Train_Reconstruction: 86.5753870010376  Train_KL: 3.5242932438850403  Validation Loss : 90.2932357788086 Val_Reconstruction : 86.8134536743164 Val_KL : 3.4797812700271606\n","Epoch: 6689/8000  Traning Loss: 89.81128787994385  Train_Reconstruction: 86.29780578613281  Train_KL: 3.513480544090271  Validation Loss : 90.00028991699219 Val_Reconstruction : 86.5286750793457 Val_KL : 3.4716131687164307\n","Epoch: 6690/8000  Traning Loss: 90.0339765548706  Train_Reconstruction: 86.50931739807129  Train_KL: 3.5246606171131134  Validation Loss : 90.19574356079102 Val_Reconstruction : 86.7035140991211 Val_KL : 3.4922317266464233\n","Epoch: 6691/8000  Traning Loss: 89.96666812896729  Train_Reconstruction: 86.44601058959961  Train_KL: 3.5206583738327026  Validation Loss : 90.18697357177734 Val_Reconstruction : 86.70957565307617 Val_KL : 3.4773969650268555\n","Epoch: 6692/8000  Traning Loss: 89.96187973022461  Train_Reconstruction: 86.44170951843262  Train_KL: 3.5201688706874847  Validation Loss : 90.11388778686523 Val_Reconstruction : 86.63437271118164 Val_KL : 3.479515552520752\n","Epoch: 6693/8000  Traning Loss: 89.76967239379883  Train_Reconstruction: 86.24217700958252  Train_KL: 3.527495890855789  Validation Loss : 89.92259979248047 Val_Reconstruction : 86.43176651000977 Val_KL : 3.4908347129821777\n","Epoch: 6694/8000  Traning Loss: 89.87582397460938  Train_Reconstruction: 86.33910083770752  Train_KL: 3.536722958087921  Validation Loss : 90.29367446899414 Val_Reconstruction : 86.79779434204102 Val_KL : 3.495882272720337\n","Epoch: 6695/8000  Traning Loss: 90.08106803894043  Train_Reconstruction: 86.55431938171387  Train_KL: 3.526748538017273  Validation Loss : 90.09353637695312 Val_Reconstruction : 86.61748886108398 Val_KL : 3.476048231124878\n","Epoch: 6696/8000  Traning Loss: 89.7911548614502  Train_Reconstruction: 86.27803325653076  Train_KL: 3.5131219625473022  Validation Loss : 90.15460586547852 Val_Reconstruction : 86.68376541137695 Val_KL : 3.470841646194458\n","Epoch: 6697/8000  Traning Loss: 89.97450828552246  Train_Reconstruction: 86.45780849456787  Train_KL: 3.516698718070984  Validation Loss : 90.18286514282227 Val_Reconstruction : 86.70942687988281 Val_KL : 3.473438262939453\n","Epoch: 6698/8000  Traning Loss: 90.0363359451294  Train_Reconstruction: 86.52160263061523  Train_KL: 3.5147346556186676  Validation Loss : 90.03565979003906 Val_Reconstruction : 86.56273651123047 Val_KL : 3.4729251861572266\n","Epoch: 6699/8000  Traning Loss: 90.24110221862793  Train_Reconstruction: 86.71284484863281  Train_KL: 3.5282585322856903  Validation Loss : 90.36486053466797 Val_Reconstruction : 86.87670135498047 Val_KL : 3.488157272338867\n","Epoch: 6700/8000  Traning Loss: 90.01373386383057  Train_Reconstruction: 86.49137783050537  Train_KL: 3.5223557353019714  Validation Loss : 90.22110366821289 Val_Reconstruction : 86.74980163574219 Val_KL : 3.471301794052124\n","Epoch: 6701/8000  Traning Loss: 90.0322904586792  Train_Reconstruction: 86.52341175079346  Train_KL: 3.508879244327545  Validation Loss : 90.26694107055664 Val_Reconstruction : 86.80592346191406 Val_KL : 3.4610172510147095\n","Epoch: 6702/8000  Traning Loss: 89.8588809967041  Train_Reconstruction: 86.33674621582031  Train_KL: 3.5221349000930786  Validation Loss : 90.06049346923828 Val_Reconstruction : 86.57406616210938 Val_KL : 3.4864258766174316\n","Epoch: 6703/8000  Traning Loss: 89.6845474243164  Train_Reconstruction: 86.15981864929199  Train_KL: 3.52472785115242  Validation Loss : 89.75387573242188 Val_Reconstruction : 86.28427124023438 Val_KL : 3.469606041908264\n","Epoch: 6704/8000  Traning Loss: 89.9300479888916  Train_Reconstruction: 86.41368865966797  Train_KL: 3.516359865665436  Validation Loss : 90.20666885375977 Val_Reconstruction : 86.72954559326172 Val_KL : 3.4771249294281006\n","Epoch: 6705/8000  Traning Loss: 89.75930118560791  Train_Reconstruction: 86.23278999328613  Train_KL: 3.526510924100876  Validation Loss : 89.89242553710938 Val_Reconstruction : 86.39898681640625 Val_KL : 3.4934396743774414\n","Epoch: 6706/8000  Traning Loss: 90.19423484802246  Train_Reconstruction: 86.66323852539062  Train_KL: 3.5309965014457703  Validation Loss : 90.56145858764648 Val_Reconstruction : 87.08252716064453 Val_KL : 3.4789313077926636\n","Epoch: 6707/8000  Traning Loss: 90.69527053833008  Train_Reconstruction: 87.182204246521  Train_KL: 3.5130646526813507  Validation Loss : 90.9383430480957 Val_Reconstruction : 87.4749755859375 Val_KL : 3.4633692502975464\n","Epoch: 6708/8000  Traning Loss: 90.9301233291626  Train_Reconstruction: 87.41526126861572  Train_KL: 3.5148628056049347  Validation Loss : 91.05829620361328 Val_Reconstruction : 87.57169723510742 Val_KL : 3.486599564552307\n","Epoch: 6709/8000  Traning Loss: 90.62994289398193  Train_Reconstruction: 87.09834003448486  Train_KL: 3.531602591276169  Validation Loss : 90.832763671875 Val_Reconstruction : 87.3448257446289 Val_KL : 3.4879382848739624\n","Epoch: 6710/8000  Traning Loss: 90.41291046142578  Train_Reconstruction: 86.88855457305908  Train_KL: 3.5243556201457977  Validation Loss : 90.32783126831055 Val_Reconstruction : 86.84909439086914 Val_KL : 3.4787362813949585\n","Epoch: 6711/8000  Traning Loss: 90.28437232971191  Train_Reconstruction: 86.76499843597412  Train_KL: 3.5193739235401154  Validation Loss : 90.31709289550781 Val_Reconstruction : 86.83760070800781 Val_KL : 3.479490280151367\n","Epoch: 6712/8000  Traning Loss: 90.04121494293213  Train_Reconstruction: 86.52083969116211  Train_KL: 3.52037513256073  Validation Loss : 90.49169921875 Val_Reconstruction : 87.00864791870117 Val_KL : 3.4830507040023804\n","Epoch: 6713/8000  Traning Loss: 89.89783000946045  Train_Reconstruction: 86.37444305419922  Train_KL: 3.5233870446681976  Validation Loss : 90.23527526855469 Val_Reconstruction : 86.75475311279297 Val_KL : 3.4805201292037964\n","Epoch: 6714/8000  Traning Loss: 89.92235279083252  Train_Reconstruction: 86.40438652038574  Train_KL: 3.5179653465747833  Validation Loss : 90.19012069702148 Val_Reconstruction : 86.71089553833008 Val_KL : 3.479224443435669\n","Epoch: 6715/8000  Traning Loss: 90.00128364562988  Train_Reconstruction: 86.47647094726562  Train_KL: 3.52481272816658  Validation Loss : 90.054931640625 Val_Reconstruction : 86.5615005493164 Val_KL : 3.4934288263320923\n","Epoch: 6716/8000  Traning Loss: 89.98825740814209  Train_Reconstruction: 86.46312999725342  Train_KL: 3.5251273810863495  Validation Loss : 90.35138320922852 Val_Reconstruction : 86.87002944946289 Val_KL : 3.4813514947891235\n","Epoch: 6717/8000  Traning Loss: 90.03492546081543  Train_Reconstruction: 86.5154800415039  Train_KL: 3.519446164369583  Validation Loss : 90.10403442382812 Val_Reconstruction : 86.62964248657227 Val_KL : 3.4743930101394653\n","Epoch: 6718/8000  Traning Loss: 90.56070137023926  Train_Reconstruction: 87.04280090332031  Train_KL: 3.5179018080234528  Validation Loss : 91.18071746826172 Val_Reconstruction : 87.7015266418457 Val_KL : 3.4791924953460693\n","Epoch: 6719/8000  Traning Loss: 90.86502265930176  Train_Reconstruction: 87.34468078613281  Train_KL: 3.520342379808426  Validation Loss : 90.55781936645508 Val_Reconstruction : 87.08186340332031 Val_KL : 3.4759562015533447\n","Epoch: 6720/8000  Traning Loss: 90.39944839477539  Train_Reconstruction: 86.8773422241211  Train_KL: 3.5221071541309357  Validation Loss : 90.77574157714844 Val_Reconstruction : 87.29448699951172 Val_KL : 3.4812557697296143\n","Epoch: 6721/8000  Traning Loss: 90.27838706970215  Train_Reconstruction: 86.75683498382568  Train_KL: 3.5215513706207275  Validation Loss : 90.57479858398438 Val_Reconstruction : 87.0911636352539 Val_KL : 3.483635902404785\n","Epoch: 6722/8000  Traning Loss: 90.46990203857422  Train_Reconstruction: 86.94124126434326  Train_KL: 3.5286606550216675  Validation Loss : 90.41949462890625 Val_Reconstruction : 86.9385986328125 Val_KL : 3.48089861869812\n","Epoch: 6723/8000  Traning Loss: 90.42727565765381  Train_Reconstruction: 86.90791606903076  Train_KL: 3.51936012506485  Validation Loss : 90.26733016967773 Val_Reconstruction : 86.79058456420898 Val_KL : 3.4767441749572754\n","Epoch: 6724/8000  Traning Loss: 90.17999267578125  Train_Reconstruction: 86.66417598724365  Train_KL: 3.51581671833992  Validation Loss : 90.25925064086914 Val_Reconstruction : 86.7876091003418 Val_KL : 3.4716397523880005\n","Epoch: 6725/8000  Traning Loss: 90.25227642059326  Train_Reconstruction: 86.73106861114502  Train_KL: 3.5212085843086243  Validation Loss : 90.40154647827148 Val_Reconstruction : 86.9231948852539 Val_KL : 3.478354334831238\n","Epoch: 6726/8000  Traning Loss: 90.16016387939453  Train_Reconstruction: 86.63066864013672  Train_KL: 3.52949395775795  Validation Loss : 90.38869094848633 Val_Reconstruction : 86.90076446533203 Val_KL : 3.487925887107849\n","Epoch: 6727/8000  Traning Loss: 90.41892910003662  Train_Reconstruction: 86.89607429504395  Train_KL: 3.5228536427021027  Validation Loss : 91.23226547241211 Val_Reconstruction : 87.76732635498047 Val_KL : 3.4649380445480347\n","Epoch: 6728/8000  Traning Loss: 90.83436489105225  Train_Reconstruction: 87.31453323364258  Train_KL: 3.5198329091072083  Validation Loss : 90.86535263061523 Val_Reconstruction : 87.3826789855957 Val_KL : 3.4826743602752686\n","Epoch: 6729/8000  Traning Loss: 90.7142972946167  Train_Reconstruction: 87.18635272979736  Train_KL: 3.5279441475868225  Validation Loss : 91.14684677124023 Val_Reconstruction : 87.66512680053711 Val_KL : 3.4817193746566772\n","Epoch: 6730/8000  Traning Loss: 91.01038646697998  Train_Reconstruction: 87.49732208251953  Train_KL: 3.5130656361579895  Validation Loss : 90.69286346435547 Val_Reconstruction : 87.2200813293457 Val_KL : 3.472783088684082\n","Epoch: 6731/8000  Traning Loss: 90.12094974517822  Train_Reconstruction: 86.60060501098633  Train_KL: 3.520344167947769  Validation Loss : 90.1654052734375 Val_Reconstruction : 86.68763732910156 Val_KL : 3.4777653217315674\n","Epoch: 6732/8000  Traning Loss: 89.97968864440918  Train_Reconstruction: 86.45282173156738  Train_KL: 3.526867061853409  Validation Loss : 90.34181594848633 Val_Reconstruction : 86.85733413696289 Val_KL : 3.4844781160354614\n","Epoch: 6733/8000  Traning Loss: 90.11376762390137  Train_Reconstruction: 86.59480476379395  Train_KL: 3.5189629197120667  Validation Loss : 90.24255752563477 Val_Reconstruction : 86.77543640136719 Val_KL : 3.4671207666397095\n","Epoch: 6734/8000  Traning Loss: 89.98356914520264  Train_Reconstruction: 86.47488594055176  Train_KL: 3.508683919906616  Validation Loss : 90.28885650634766 Val_Reconstruction : 86.8128776550293 Val_KL : 3.4759814739227295\n","Epoch: 6735/8000  Traning Loss: 90.02391624450684  Train_Reconstruction: 86.492995262146  Train_KL: 3.530921518802643  Validation Loss : 90.21384811401367 Val_Reconstruction : 86.72388458251953 Val_KL : 3.4899662733078003\n","Epoch: 6736/8000  Traning Loss: 90.3452033996582  Train_Reconstruction: 86.8274564743042  Train_KL: 3.517747223377228  Validation Loss : 90.0941162109375 Val_Reconstruction : 86.62219619750977 Val_KL : 3.471919298171997\n","Epoch: 6737/8000  Traning Loss: 90.18751811981201  Train_Reconstruction: 86.67786026000977  Train_KL: 3.5096578299999237  Validation Loss : 89.89724731445312 Val_Reconstruction : 86.43051147460938 Val_KL : 3.46673583984375\n","Epoch: 6738/8000  Traning Loss: 89.99957752227783  Train_Reconstruction: 86.48817157745361  Train_KL: 3.5114056169986725  Validation Loss : 90.03554153442383 Val_Reconstruction : 86.56918716430664 Val_KL : 3.4663538932800293\n","Epoch: 6739/8000  Traning Loss: 90.2476692199707  Train_Reconstruction: 86.72800540924072  Train_KL: 3.5196632742881775  Validation Loss : 90.39838409423828 Val_Reconstruction : 86.92022323608398 Val_KL : 3.4781606197357178\n","Epoch: 6740/8000  Traning Loss: 90.0868558883667  Train_Reconstruction: 86.5696268081665  Train_KL: 3.517228990793228  Validation Loss : 90.05657958984375 Val_Reconstruction : 86.57898712158203 Val_KL : 3.4775887727737427\n","Epoch: 6741/8000  Traning Loss: 89.82266616821289  Train_Reconstruction: 86.29461002349854  Train_KL: 3.5280556678771973  Validation Loss : 89.83218383789062 Val_Reconstruction : 86.34000778198242 Val_KL : 3.4921751022338867\n","Epoch: 6742/8000  Traning Loss: 89.78901290893555  Train_Reconstruction: 86.26154708862305  Train_KL: 3.5274652242660522  Validation Loss : 89.73875045776367 Val_Reconstruction : 86.25728988647461 Val_KL : 3.481459856033325\n","Epoch: 6743/8000  Traning Loss: 89.719313621521  Train_Reconstruction: 86.20395374298096  Train_KL: 3.515360116958618  Validation Loss : 89.86947631835938 Val_Reconstruction : 86.39913177490234 Val_KL : 3.470345377922058\n","Epoch: 6744/8000  Traning Loss: 89.9127197265625  Train_Reconstruction: 86.39087295532227  Train_KL: 3.5218470692634583  Validation Loss : 90.04267883300781 Val_Reconstruction : 86.55477142333984 Val_KL : 3.487907648086548\n","Epoch: 6745/8000  Traning Loss: 89.82246017456055  Train_Reconstruction: 86.28792190551758  Train_KL: 3.534538149833679  Validation Loss : 90.09802627563477 Val_Reconstruction : 86.61202621459961 Val_KL : 3.486000180244446\n","Epoch: 6746/8000  Traning Loss: 89.8737907409668  Train_Reconstruction: 86.34614562988281  Train_KL: 3.527645617723465  Validation Loss : 89.9476432800293 Val_Reconstruction : 86.4669418334961 Val_KL : 3.480700731277466\n","Epoch: 6747/8000  Traning Loss: 90.02880954742432  Train_Reconstruction: 86.49960517883301  Train_KL: 3.5292035043239594  Validation Loss : 90.25983810424805 Val_Reconstruction : 86.77750015258789 Val_KL : 3.482338547706604\n","Epoch: 6748/8000  Traning Loss: 90.00972557067871  Train_Reconstruction: 86.48527145385742  Train_KL: 3.5244543254375458  Validation Loss : 90.11259078979492 Val_Reconstruction : 86.63790512084961 Val_KL : 3.4746859073638916\n","Epoch: 6749/8000  Traning Loss: 90.39225101470947  Train_Reconstruction: 86.87414836883545  Train_KL: 3.518102318048477  Validation Loss : 90.38113021850586 Val_Reconstruction : 86.90668487548828 Val_KL : 3.474443554878235\n","Epoch: 6750/8000  Traning Loss: 90.73352432250977  Train_Reconstruction: 87.2080078125  Train_KL: 3.5255172848701477  Validation Loss : 91.33669662475586 Val_Reconstruction : 87.85720825195312 Val_KL : 3.479488968849182\n","Epoch: 6751/8000  Traning Loss: 90.42272472381592  Train_Reconstruction: 86.89922904968262  Train_KL: 3.523495852947235  Validation Loss : 90.560791015625 Val_Reconstruction : 87.0855827331543 Val_KL : 3.4752100706100464\n","Epoch: 6752/8000  Traning Loss: 90.02346897125244  Train_Reconstruction: 86.49950218200684  Train_KL: 3.523966997861862  Validation Loss : 90.25654220581055 Val_Reconstruction : 86.77872467041016 Val_KL : 3.4778189659118652\n","Epoch: 6753/8000  Traning Loss: 89.71203327178955  Train_Reconstruction: 86.19448661804199  Train_KL: 3.517545521259308  Validation Loss : 89.84748458862305 Val_Reconstruction : 86.37141799926758 Val_KL : 3.4760661125183105\n","Epoch: 6754/8000  Traning Loss: 90.05379295349121  Train_Reconstruction: 86.53082752227783  Train_KL: 3.5229658484458923  Validation Loss : 90.73078155517578 Val_Reconstruction : 87.2463493347168 Val_KL : 3.484432339668274\n","Epoch: 6755/8000  Traning Loss: 90.89143180847168  Train_Reconstruction: 87.37376880645752  Train_KL: 3.5176622569561005  Validation Loss : 91.17251968383789 Val_Reconstruction : 87.7043342590332 Val_KL : 3.4681819677352905\n","Epoch: 6756/8000  Traning Loss: 90.9425048828125  Train_Reconstruction: 87.42945098876953  Train_KL: 3.5130535662174225  Validation Loss : 90.3145751953125 Val_Reconstruction : 86.83831024169922 Val_KL : 3.4762619733810425\n","Epoch: 6757/8000  Traning Loss: 90.85709190368652  Train_Reconstruction: 87.33427810668945  Train_KL: 3.5228134095668793  Validation Loss : 90.9559440612793 Val_Reconstruction : 87.47862243652344 Val_KL : 3.477322578430176\n","Epoch: 6758/8000  Traning Loss: 91.12420463562012  Train_Reconstruction: 87.61032009124756  Train_KL: 3.5138848423957825  Validation Loss : 90.83114624023438 Val_Reconstruction : 87.36212158203125 Val_KL : 3.469026565551758\n","Epoch: 6759/8000  Traning Loss: 90.5080213546753  Train_Reconstruction: 86.98992443084717  Train_KL: 3.5180965065956116  Validation Loss : 90.52547073364258 Val_Reconstruction : 87.04911422729492 Val_KL : 3.4763567447662354\n","Epoch: 6760/8000  Traning Loss: 90.24517822265625  Train_Reconstruction: 86.72430801391602  Train_KL: 3.520869880914688  Validation Loss : 90.2010612487793 Val_Reconstruction : 86.71835708618164 Val_KL : 3.482706904411316\n","Epoch: 6761/8000  Traning Loss: 90.13978385925293  Train_Reconstruction: 86.61350154876709  Train_KL: 3.5262832045555115  Validation Loss : 90.01775360107422 Val_Reconstruction : 86.53564453125 Val_KL : 3.4821102619171143\n","Epoch: 6762/8000  Traning Loss: 89.99809074401855  Train_Reconstruction: 86.47467041015625  Train_KL: 3.5234201550483704  Validation Loss : 90.16251373291016 Val_Reconstruction : 86.68315124511719 Val_KL : 3.4793612957000732\n","Epoch: 6763/8000  Traning Loss: 90.00268173217773  Train_Reconstruction: 86.47024250030518  Train_KL: 3.532439261674881  Validation Loss : 90.07281494140625 Val_Reconstruction : 86.58619689941406 Val_KL : 3.4866156578063965\n","Epoch: 6764/8000  Traning Loss: 89.65547847747803  Train_Reconstruction: 86.12464332580566  Train_KL: 3.53083473443985  Validation Loss : 89.86661911010742 Val_Reconstruction : 86.38240051269531 Val_KL : 3.4842207431793213\n","Epoch: 6765/8000  Traning Loss: 89.80013465881348  Train_Reconstruction: 86.27045249938965  Train_KL: 3.529681622982025  Validation Loss : 90.22161483764648 Val_Reconstruction : 86.73851776123047 Val_KL : 3.4831000566482544\n","Epoch: 6766/8000  Traning Loss: 90.12013339996338  Train_Reconstruction: 86.59142875671387  Train_KL: 3.528704971075058  Validation Loss : 90.41645812988281 Val_Reconstruction : 86.92242431640625 Val_KL : 3.4940344095230103\n","Epoch: 6767/8000  Traning Loss: 90.45926570892334  Train_Reconstruction: 86.92149353027344  Train_KL: 3.5377739667892456  Validation Loss : 91.03950881958008 Val_Reconstruction : 87.54974746704102 Val_KL : 3.4897594451904297\n","Epoch: 6768/8000  Traning Loss: 90.21935081481934  Train_Reconstruction: 86.69643306732178  Train_KL: 3.5229179561138153  Validation Loss : 90.24552154541016 Val_Reconstruction : 86.77033996582031 Val_KL : 3.4751802682876587\n","Epoch: 6769/8000  Traning Loss: 89.9311933517456  Train_Reconstruction: 86.41746711730957  Train_KL: 3.5137267410755157  Validation Loss : 90.09243774414062 Val_Reconstruction : 86.6279067993164 Val_KL : 3.4645321369171143\n","Epoch: 6770/8000  Traning Loss: 90.10513877868652  Train_Reconstruction: 86.58656692504883  Train_KL: 3.5185728669166565  Validation Loss : 90.47235107421875 Val_Reconstruction : 86.98943328857422 Val_KL : 3.48292076587677\n","Epoch: 6771/8000  Traning Loss: 90.40484619140625  Train_Reconstruction: 86.88440322875977  Train_KL: 3.520443379878998  Validation Loss : 90.92950820922852 Val_Reconstruction : 87.45096206665039 Val_KL : 3.4785438776016235\n","Epoch: 6772/8000  Traning Loss: 91.07571220397949  Train_Reconstruction: 87.55974388122559  Train_KL: 3.5159679651260376  Validation Loss : 91.76930236816406 Val_Reconstruction : 88.29743194580078 Val_KL : 3.471871256828308\n","Epoch: 6773/8000  Traning Loss: 91.47490692138672  Train_Reconstruction: 87.95609092712402  Train_KL: 3.5188154876232147  Validation Loss : 90.92978286743164 Val_Reconstruction : 87.4576530456543 Val_KL : 3.472130298614502\n","Epoch: 6774/8000  Traning Loss: 90.20637607574463  Train_Reconstruction: 86.68302822113037  Train_KL: 3.5233477652072906  Validation Loss : 90.65052795410156 Val_Reconstruction : 87.16826629638672 Val_KL : 3.4822614192962646\n","Epoch: 6775/8000  Traning Loss: 90.0853853225708  Train_Reconstruction: 86.55066204071045  Train_KL: 3.5347239077091217  Validation Loss : 90.3235855102539 Val_Reconstruction : 86.83114242553711 Val_KL : 3.4924451112747192\n","Epoch: 6776/8000  Traning Loss: 90.34205722808838  Train_Reconstruction: 86.81825923919678  Train_KL: 3.523798108100891  Validation Loss : 90.57349014282227 Val_Reconstruction : 87.10647583007812 Val_KL : 3.467012643814087\n","Epoch: 6777/8000  Traning Loss: 90.35420894622803  Train_Reconstruction: 86.84169673919678  Train_KL: 3.5125135481357574  Validation Loss : 90.36703872680664 Val_Reconstruction : 86.89607238769531 Val_KL : 3.4709692001342773\n","Epoch: 6778/8000  Traning Loss: 89.86350440979004  Train_Reconstruction: 86.34524345397949  Train_KL: 3.518262505531311  Validation Loss : 90.02755355834961 Val_Reconstruction : 86.55438995361328 Val_KL : 3.4731630086898804\n","Epoch: 6779/8000  Traning Loss: 89.95626926422119  Train_Reconstruction: 86.43298625946045  Train_KL: 3.523283004760742  Validation Loss : 90.39297866821289 Val_Reconstruction : 86.91282272338867 Val_KL : 3.4801570177078247\n","Epoch: 6780/8000  Traning Loss: 90.40797901153564  Train_Reconstruction: 86.88554286956787  Train_KL: 3.5224360525608063  Validation Loss : 90.42036819458008 Val_Reconstruction : 86.9439811706543 Val_KL : 3.476385474205017\n","Epoch: 6781/8000  Traning Loss: 90.33388233184814  Train_Reconstruction: 86.80776309967041  Train_KL: 3.5261192619800568  Validation Loss : 90.56975173950195 Val_Reconstruction : 87.09552001953125 Val_KL : 3.4742337465286255\n","Epoch: 6782/8000  Traning Loss: 90.2459945678711  Train_Reconstruction: 86.72183799743652  Train_KL: 3.5241564214229584  Validation Loss : 90.51958847045898 Val_Reconstruction : 87.04340744018555 Val_KL : 3.4761804342269897\n","Epoch: 6783/8000  Traning Loss: 90.4866247177124  Train_Reconstruction: 86.96931457519531  Train_KL: 3.5173101127147675  Validation Loss : 90.28620910644531 Val_Reconstruction : 86.81783294677734 Val_KL : 3.4683761596679688\n","Epoch: 6784/8000  Traning Loss: 90.16198253631592  Train_Reconstruction: 86.63726425170898  Train_KL: 3.5247199833393097  Validation Loss : 90.21644973754883 Val_Reconstruction : 86.7305793762207 Val_KL : 3.4858717918395996\n","Epoch: 6785/8000  Traning Loss: 89.9615478515625  Train_Reconstruction: 86.43389320373535  Train_KL: 3.5276545584201813  Validation Loss : 90.10811996459961 Val_Reconstruction : 86.62726211547852 Val_KL : 3.4808592796325684\n","Epoch: 6786/8000  Traning Loss: 90.06097602844238  Train_Reconstruction: 86.5350570678711  Train_KL: 3.52591872215271  Validation Loss : 90.4736099243164 Val_Reconstruction : 86.98910140991211 Val_KL : 3.4845091104507446\n","Epoch: 6787/8000  Traning Loss: 90.3297929763794  Train_Reconstruction: 86.80162048339844  Train_KL: 3.5281718969345093  Validation Loss : 90.58194351196289 Val_Reconstruction : 87.10359954833984 Val_KL : 3.4783453941345215\n","Epoch: 6788/8000  Traning Loss: 90.0991621017456  Train_Reconstruction: 86.57733154296875  Train_KL: 3.52182999253273  Validation Loss : 90.30010604858398 Val_Reconstruction : 86.82852172851562 Val_KL : 3.471585750579834\n","Epoch: 6789/8000  Traning Loss: 90.12921905517578  Train_Reconstruction: 86.6012773513794  Train_KL: 3.5279425382614136  Validation Loss : 90.39673233032227 Val_Reconstruction : 86.91532516479492 Val_KL : 3.4814077615737915\n","Epoch: 6790/8000  Traning Loss: 90.39081192016602  Train_Reconstruction: 86.86063289642334  Train_KL: 3.530179023742676  Validation Loss : 90.26945114135742 Val_Reconstruction : 86.78731536865234 Val_KL : 3.4821362495422363\n","Epoch: 6791/8000  Traning Loss: 89.9544038772583  Train_Reconstruction: 86.4310131072998  Train_KL: 3.523390084505081  Validation Loss : 90.12040328979492 Val_Reconstruction : 86.64776229858398 Val_KL : 3.4726407527923584\n","Epoch: 6792/8000  Traning Loss: 90.03196430206299  Train_Reconstruction: 86.50840282440186  Train_KL: 3.5235608518123627  Validation Loss : 90.56403350830078 Val_Reconstruction : 87.08012390136719 Val_KL : 3.4839115142822266\n","Epoch: 6793/8000  Traning Loss: 90.5074110031128  Train_Reconstruction: 86.98261451721191  Train_KL: 3.5247975885868073  Validation Loss : 91.09045028686523 Val_Reconstruction : 87.61039733886719 Val_KL : 3.480053424835205\n","Epoch: 6794/8000  Traning Loss: 90.58590126037598  Train_Reconstruction: 87.07500743865967  Train_KL: 3.510894477367401  Validation Loss : 90.40789031982422 Val_Reconstruction : 86.93914031982422 Val_KL : 3.468749523162842\n","Epoch: 6795/8000  Traning Loss: 90.07392978668213  Train_Reconstruction: 86.55514907836914  Train_KL: 3.5187798738479614  Validation Loss : 90.46756744384766 Val_Reconstruction : 86.98736190795898 Val_KL : 3.4802056550979614\n","Epoch: 6796/8000  Traning Loss: 90.8444766998291  Train_Reconstruction: 87.3206558227539  Train_KL: 3.523818761110306  Validation Loss : 91.06687927246094 Val_Reconstruction : 87.58592224121094 Val_KL : 3.4809576272964478\n","Epoch: 6797/8000  Traning Loss: 91.19374465942383  Train_Reconstruction: 87.66623306274414  Train_KL: 3.5275111198425293  Validation Loss : 91.28391647338867 Val_Reconstruction : 87.79508972167969 Val_KL : 3.488826036453247\n","Epoch: 6798/8000  Traning Loss: 90.41967582702637  Train_Reconstruction: 86.89528751373291  Train_KL: 3.5243886411190033  Validation Loss : 90.35331726074219 Val_Reconstruction : 86.87703704833984 Val_KL : 3.4762812852859497\n","Epoch: 6799/8000  Traning Loss: 90.56652927398682  Train_Reconstruction: 87.05531597137451  Train_KL: 3.511213779449463  Validation Loss : 91.15701675415039 Val_Reconstruction : 87.68923950195312 Val_KL : 3.467776298522949\n","Epoch: 6800/8000  Traning Loss: 91.83706092834473  Train_Reconstruction: 88.31546115875244  Train_KL: 3.521599441766739  Validation Loss : 91.83380126953125 Val_Reconstruction : 88.35164260864258 Val_KL : 3.4821581840515137\n","Epoch: 6801/8000  Traning Loss: 90.71519660949707  Train_Reconstruction: 87.19517230987549  Train_KL: 3.5200245678424835  Validation Loss : 90.23514556884766 Val_Reconstruction : 86.76260757446289 Val_KL : 3.4725364446640015\n","Epoch: 6802/8000  Traning Loss: 90.21683502197266  Train_Reconstruction: 86.703369140625  Train_KL: 3.513465464115143  Validation Loss : 90.37856674194336 Val_Reconstruction : 86.90153503417969 Val_KL : 3.477034568786621\n","Epoch: 6803/8000  Traning Loss: 89.80294513702393  Train_Reconstruction: 86.28347778320312  Train_KL: 3.51946684718132  Validation Loss : 90.05093383789062 Val_Reconstruction : 86.57570266723633 Val_KL : 3.4752310514450073\n","Epoch: 6804/8000  Traning Loss: 89.75006103515625  Train_Reconstruction: 86.23642826080322  Train_KL: 3.5136318802833557  Validation Loss : 89.79710006713867 Val_Reconstruction : 86.32929229736328 Val_KL : 3.4678101539611816\n","Epoch: 6805/8000  Traning Loss: 89.73440837860107  Train_Reconstruction: 86.21037006378174  Train_KL: 3.524038404226303  Validation Loss : 89.87831115722656 Val_Reconstruction : 86.39225006103516 Val_KL : 3.486060619354248\n","Epoch: 6806/8000  Traning Loss: 89.69110870361328  Train_Reconstruction: 86.16489791870117  Train_KL: 3.5262089371681213  Validation Loss : 89.94173049926758 Val_Reconstruction : 86.45691299438477 Val_KL : 3.4848181009292603\n","Epoch: 6807/8000  Traning Loss: 89.83652019500732  Train_Reconstruction: 86.30904293060303  Train_KL: 3.527477353811264  Validation Loss : 89.97749328613281 Val_Reconstruction : 86.50323867797852 Val_KL : 3.4742560386657715\n","Epoch: 6808/8000  Traning Loss: 90.37270545959473  Train_Reconstruction: 86.85194110870361  Train_KL: 3.520763874053955  Validation Loss : 90.65146255493164 Val_Reconstruction : 87.16869354248047 Val_KL : 3.482770323753357\n","Epoch: 6809/8000  Traning Loss: 90.4328031539917  Train_Reconstruction: 86.9016342163086  Train_KL: 3.5311699509620667  Validation Loss : 90.24824142456055 Val_Reconstruction : 86.7662467956543 Val_KL : 3.4819923639297485\n","Epoch: 6810/8000  Traning Loss: 90.11557865142822  Train_Reconstruction: 86.58724975585938  Train_KL: 3.5283272862434387  Validation Loss : 90.41776275634766 Val_Reconstruction : 86.93019485473633 Val_KL : 3.487567663192749\n","Epoch: 6811/8000  Traning Loss: 90.0151309967041  Train_Reconstruction: 86.4770393371582  Train_KL: 3.5380914211273193  Validation Loss : 90.18250274658203 Val_Reconstruction : 86.68791198730469 Val_KL : 3.4945908784866333\n","Epoch: 6812/8000  Traning Loss: 89.97057151794434  Train_Reconstruction: 86.43326950073242  Train_KL: 3.5373015105724335  Validation Loss : 89.93013763427734 Val_Reconstruction : 86.44539260864258 Val_KL : 3.484747886657715\n","Epoch: 6813/8000  Traning Loss: 89.89709281921387  Train_Reconstruction: 86.38250160217285  Train_KL: 3.5145909786224365  Validation Loss : 90.11886978149414 Val_Reconstruction : 86.6488037109375 Val_KL : 3.4700679779052734\n","Epoch: 6814/8000  Traning Loss: 89.93441009521484  Train_Reconstruction: 86.41492557525635  Train_KL: 3.5194851458072662  Validation Loss : 90.01360702514648 Val_Reconstruction : 86.53951263427734 Val_KL : 3.4740909337997437\n","Epoch: 6815/8000  Traning Loss: 89.8916130065918  Train_Reconstruction: 86.37749290466309  Train_KL: 3.51411896944046  Validation Loss : 90.25525665283203 Val_Reconstruction : 86.78681564331055 Val_KL : 3.468443274497986\n","Epoch: 6816/8000  Traning Loss: 90.03110122680664  Train_Reconstruction: 86.51315402984619  Train_KL: 3.5179482102394104  Validation Loss : 90.09312438964844 Val_Reconstruction : 86.61447525024414 Val_KL : 3.478652000427246\n","Epoch: 6817/8000  Traning Loss: 90.34268569946289  Train_Reconstruction: 86.81716346740723  Train_KL: 3.5255221128463745  Validation Loss : 90.47539901733398 Val_Reconstruction : 86.99369812011719 Val_KL : 3.481703281402588\n","Epoch: 6818/8000  Traning Loss: 90.46023273468018  Train_Reconstruction: 86.93470668792725  Train_KL: 3.5255257189273834  Validation Loss : 90.10949325561523 Val_Reconstruction : 86.62730407714844 Val_KL : 3.4821903705596924\n","Epoch: 6819/8000  Traning Loss: 90.34554958343506  Train_Reconstruction: 86.81994247436523  Train_KL: 3.525607079267502  Validation Loss : 90.45517349243164 Val_Reconstruction : 86.97558975219727 Val_KL : 3.4795833826065063\n","Epoch: 6820/8000  Traning Loss: 90.02769756317139  Train_Reconstruction: 86.5068769454956  Train_KL: 3.5208202600479126  Validation Loss : 90.37921524047852 Val_Reconstruction : 86.90447616577148 Val_KL : 3.4747380018234253\n","Epoch: 6821/8000  Traning Loss: 90.06874084472656  Train_Reconstruction: 86.54745483398438  Train_KL: 3.5212848782539368  Validation Loss : 89.80185317993164 Val_Reconstruction : 86.3275146484375 Val_KL : 3.4743393659591675\n","Epoch: 6822/8000  Traning Loss: 89.63601207733154  Train_Reconstruction: 86.11078357696533  Train_KL: 3.525229036808014  Validation Loss : 89.86802291870117 Val_Reconstruction : 86.3827896118164 Val_KL : 3.48523211479187\n","Epoch: 6823/8000  Traning Loss: 89.83322525024414  Train_Reconstruction: 86.30078029632568  Train_KL: 3.532443732023239  Validation Loss : 89.8429069519043 Val_Reconstruction : 86.36072540283203 Val_KL : 3.482182025909424\n","Epoch: 6824/8000  Traning Loss: 89.82913780212402  Train_Reconstruction: 86.31249904632568  Train_KL: 3.5166389644145966  Validation Loss : 90.14843368530273 Val_Reconstruction : 86.68017196655273 Val_KL : 3.4682631492614746\n","Epoch: 6825/8000  Traning Loss: 90.22997570037842  Train_Reconstruction: 86.7149305343628  Train_KL: 3.515044867992401  Validation Loss : 90.8554916381836 Val_Reconstruction : 87.37715530395508 Val_KL : 3.4783382415771484\n","Epoch: 6826/8000  Traning Loss: 90.47169971466064  Train_Reconstruction: 86.94971752166748  Train_KL: 3.521981358528137  Validation Loss : 91.0550422668457 Val_Reconstruction : 87.5741081237793 Val_KL : 3.480935573577881\n","Epoch: 6827/8000  Traning Loss: 90.24415874481201  Train_Reconstruction: 86.72464942932129  Train_KL: 3.519509732723236  Validation Loss : 90.08961486816406 Val_Reconstruction : 86.6168327331543 Val_KL : 3.472780704498291\n","Epoch: 6828/8000  Traning Loss: 89.94621562957764  Train_Reconstruction: 86.42653942108154  Train_KL: 3.5196757912635803  Validation Loss : 90.50360870361328 Val_Reconstruction : 87.02720260620117 Val_KL : 3.476407527923584\n","Epoch: 6829/8000  Traning Loss: 90.09288501739502  Train_Reconstruction: 86.56693172454834  Train_KL: 3.5259531438350677  Validation Loss : 90.63934707641602 Val_Reconstruction : 87.16133117675781 Val_KL : 3.4780176877975464\n","Epoch: 6830/8000  Traning Loss: 90.17967891693115  Train_Reconstruction: 86.65668392181396  Train_KL: 3.522995412349701  Validation Loss : 90.11162567138672 Val_Reconstruction : 86.63471603393555 Val_KL : 3.476909875869751\n","Epoch: 6831/8000  Traning Loss: 89.90068435668945  Train_Reconstruction: 86.37522506713867  Train_KL: 3.5254596769809723  Validation Loss : 90.07409286499023 Val_Reconstruction : 86.59452056884766 Val_KL : 3.4795702695846558\n","Epoch: 6832/8000  Traning Loss: 89.98330116271973  Train_Reconstruction: 86.46068954467773  Train_KL: 3.5226123929023743  Validation Loss : 90.0493049621582 Val_Reconstruction : 86.56728744506836 Val_KL : 3.482018232345581\n","Epoch: 6833/8000  Traning Loss: 89.60284900665283  Train_Reconstruction: 86.06916236877441  Train_KL: 3.5336865186691284  Validation Loss : 89.79203033447266 Val_Reconstruction : 86.30635452270508 Val_KL : 3.4856761693954468\n","Epoch: 6834/8000  Traning Loss: 89.88140106201172  Train_Reconstruction: 86.35539245605469  Train_KL: 3.526008725166321  Validation Loss : 90.45742797851562 Val_Reconstruction : 86.97736740112305 Val_KL : 3.480063796043396\n","Epoch: 6835/8000  Traning Loss: 90.02005863189697  Train_Reconstruction: 86.49208164215088  Train_KL: 3.527977079153061  Validation Loss : 90.12378692626953 Val_Reconstruction : 86.63689804077148 Val_KL : 3.486889123916626\n","Epoch: 6836/8000  Traning Loss: 89.9213342666626  Train_Reconstruction: 86.38991641998291  Train_KL: 3.5314178466796875  Validation Loss : 90.07096099853516 Val_Reconstruction : 86.58795928955078 Val_KL : 3.48300039768219\n","Epoch: 6837/8000  Traning Loss: 89.84041404724121  Train_Reconstruction: 86.31492900848389  Train_KL: 3.5254865884780884  Validation Loss : 89.94210815429688 Val_Reconstruction : 86.4628677368164 Val_KL : 3.4792436361312866\n","Epoch: 6838/8000  Traning Loss: 89.88190364837646  Train_Reconstruction: 86.35387516021729  Train_KL: 3.5280272364616394  Validation Loss : 89.98916244506836 Val_Reconstruction : 86.51008605957031 Val_KL : 3.4790793657302856\n","Epoch: 6839/8000  Traning Loss: 90.31524562835693  Train_Reconstruction: 86.7948751449585  Train_KL: 3.5203704833984375  Validation Loss : 90.47335433959961 Val_Reconstruction : 87.00433731079102 Val_KL : 3.469018340110779\n","Epoch: 6840/8000  Traning Loss: 90.61773204803467  Train_Reconstruction: 87.09703254699707  Train_KL: 3.520700514316559  Validation Loss : 91.31204223632812 Val_Reconstruction : 87.82719421386719 Val_KL : 3.4848482608795166\n","Epoch: 6841/8000  Traning Loss: 90.70561504364014  Train_Reconstruction: 87.18704032897949  Train_KL: 3.5185744166374207  Validation Loss : 90.42940139770508 Val_Reconstruction : 86.95464706420898 Val_KL : 3.4747519493103027\n","Epoch: 6842/8000  Traning Loss: 90.1771011352539  Train_Reconstruction: 86.65594387054443  Train_KL: 3.521156072616577  Validation Loss : 90.09819030761719 Val_Reconstruction : 86.62444686889648 Val_KL : 3.4737457036972046\n","Epoch: 6843/8000  Traning Loss: 90.16080474853516  Train_Reconstruction: 86.63418960571289  Train_KL: 3.5266160368919373  Validation Loss : 90.12832260131836 Val_Reconstruction : 86.6435775756836 Val_KL : 3.4847445487976074\n","Epoch: 6844/8000  Traning Loss: 89.992919921875  Train_Reconstruction: 86.46926975250244  Train_KL: 3.523649513721466  Validation Loss : 90.19720840454102 Val_Reconstruction : 86.72124481201172 Val_KL : 3.4759628772735596\n","Epoch: 6845/8000  Traning Loss: 89.7508544921875  Train_Reconstruction: 86.2237901687622  Train_KL: 3.5270632803440094  Validation Loss : 89.89502334594727 Val_Reconstruction : 86.40855026245117 Val_KL : 3.486473560333252\n","Epoch: 6846/8000  Traning Loss: 89.84133052825928  Train_Reconstruction: 86.3060827255249  Train_KL: 3.53524911403656  Validation Loss : 90.17295455932617 Val_Reconstruction : 86.68104553222656 Val_KL : 3.491907835006714\n","Epoch: 6847/8000  Traning Loss: 89.97731113433838  Train_Reconstruction: 86.4450159072876  Train_KL: 3.53229421377182  Validation Loss : 90.22407150268555 Val_Reconstruction : 86.73933792114258 Val_KL : 3.4847313165664673\n","Epoch: 6848/8000  Traning Loss: 89.90848541259766  Train_Reconstruction: 86.39034366607666  Train_KL: 3.518141508102417  Validation Loss : 90.05669021606445 Val_Reconstruction : 86.58728408813477 Val_KL : 3.469406247138977\n","Epoch: 6849/8000  Traning Loss: 89.91164302825928  Train_Reconstruction: 86.39148998260498  Train_KL: 3.5201527774333954  Validation Loss : 90.49381256103516 Val_Reconstruction : 87.00331115722656 Val_KL : 3.4905006885528564\n","Epoch: 6850/8000  Traning Loss: 90.07142543792725  Train_Reconstruction: 86.53529930114746  Train_KL: 3.5361256301403046  Validation Loss : 90.16187286376953 Val_Reconstruction : 86.67412948608398 Val_KL : 3.487742781639099\n","Epoch: 6851/8000  Traning Loss: 89.57280921936035  Train_Reconstruction: 86.05286979675293  Train_KL: 3.519940674304962  Validation Loss : 89.8465576171875 Val_Reconstruction : 86.3785285949707 Val_KL : 3.468029022216797\n","Epoch: 6852/8000  Traning Loss: 90.04204654693604  Train_Reconstruction: 86.52793025970459  Train_KL: 3.514116555452347  Validation Loss : 90.3532943725586 Val_Reconstruction : 86.88002395629883 Val_KL : 3.47327196598053\n","Epoch: 6853/8000  Traning Loss: 90.12030410766602  Train_Reconstruction: 86.60196876525879  Train_KL: 3.518336534500122  Validation Loss : 90.22753143310547 Val_Reconstruction : 86.75237655639648 Val_KL : 3.4751569032669067\n","Epoch: 6854/8000  Traning Loss: 90.02666568756104  Train_Reconstruction: 86.50421333312988  Train_KL: 3.5224535763263702  Validation Loss : 90.18646621704102 Val_Reconstruction : 86.71107482910156 Val_KL : 3.475388765335083\n","Epoch: 6855/8000  Traning Loss: 89.95366859436035  Train_Reconstruction: 86.42877960205078  Train_KL: 3.5248900949954987  Validation Loss : 90.07691955566406 Val_Reconstruction : 86.58815002441406 Val_KL : 3.4887694120407104\n","Epoch: 6856/8000  Traning Loss: 89.82628726959229  Train_Reconstruction: 86.29008197784424  Train_KL: 3.5362062752246857  Validation Loss : 89.94591522216797 Val_Reconstruction : 86.454833984375 Val_KL : 3.491081714630127\n","Epoch: 6857/8000  Traning Loss: 89.8277940750122  Train_Reconstruction: 86.3038444519043  Train_KL: 3.523950219154358  Validation Loss : 89.72867584228516 Val_Reconstruction : 86.26304244995117 Val_KL : 3.4656312465667725\n","Epoch: 6858/8000  Traning Loss: 90.17533493041992  Train_Reconstruction: 86.66201400756836  Train_KL: 3.5133203268051147  Validation Loss : 90.96678161621094 Val_Reconstruction : 87.48077011108398 Val_KL : 3.486011505126953\n","Epoch: 6859/8000  Traning Loss: 90.23509883880615  Train_Reconstruction: 86.70743656158447  Train_KL: 3.527662843465805  Validation Loss : 90.19378662109375 Val_Reconstruction : 86.69833374023438 Val_KL : 3.4954551458358765\n","Epoch: 6860/8000  Traning Loss: 89.75461101531982  Train_Reconstruction: 86.22523307800293  Train_KL: 3.5293784737586975  Validation Loss : 89.73800277709961 Val_Reconstruction : 86.25774765014648 Val_KL : 3.480257034301758\n","Epoch: 6861/8000  Traning Loss: 89.73679637908936  Train_Reconstruction: 86.21944427490234  Train_KL: 3.5173527002334595  Validation Loss : 89.87590026855469 Val_Reconstruction : 86.4036979675293 Val_KL : 3.4722033739089966\n","Epoch: 6862/8000  Traning Loss: 89.9317741394043  Train_Reconstruction: 86.41842365264893  Train_KL: 3.5133508145809174  Validation Loss : 90.09724426269531 Val_Reconstruction : 86.62696075439453 Val_KL : 3.470281720161438\n","Epoch: 6863/8000  Traning Loss: 89.90876293182373  Train_Reconstruction: 86.3822832107544  Train_KL: 3.5264801681041718  Validation Loss : 90.22113418579102 Val_Reconstruction : 86.74038696289062 Val_KL : 3.4807486534118652\n","Epoch: 6864/8000  Traning Loss: 89.89247226715088  Train_Reconstruction: 86.37144088745117  Train_KL: 3.5210308730602264  Validation Loss : 90.36920928955078 Val_Reconstruction : 86.89194107055664 Val_KL : 3.4772692918777466\n","Epoch: 6865/8000  Traning Loss: 90.11489295959473  Train_Reconstruction: 86.58765983581543  Train_KL: 3.527233511209488  Validation Loss : 90.70371627807617 Val_Reconstruction : 87.21324920654297 Val_KL : 3.4904680252075195\n","Epoch: 6866/8000  Traning Loss: 89.95109462738037  Train_Reconstruction: 86.42813301086426  Train_KL: 3.522961050271988  Validation Loss : 90.20166778564453 Val_Reconstruction : 86.7307357788086 Val_KL : 3.470932126045227\n","Epoch: 6867/8000  Traning Loss: 89.83101558685303  Train_Reconstruction: 86.31204509735107  Train_KL: 3.5189707577228546  Validation Loss : 90.07592010498047 Val_Reconstruction : 86.60213470458984 Val_KL : 3.473788022994995\n","Epoch: 6868/8000  Traning Loss: 89.77253723144531  Train_Reconstruction: 86.2537784576416  Train_KL: 3.518759101629257  Validation Loss : 90.08836364746094 Val_Reconstruction : 86.61105728149414 Val_KL : 3.4773037433624268\n","Epoch: 6869/8000  Traning Loss: 89.86263465881348  Train_Reconstruction: 86.3373031616211  Train_KL: 3.5253297984600067  Validation Loss : 90.82662582397461 Val_Reconstruction : 87.33696746826172 Val_KL : 3.4896618127822876\n","Epoch: 6870/8000  Traning Loss: 90.02071952819824  Train_Reconstruction: 86.49150562286377  Train_KL: 3.529213309288025  Validation Loss : 90.39024353027344 Val_Reconstruction : 86.9139404296875 Val_KL : 3.4763063192367554\n","Epoch: 6871/8000  Traning Loss: 90.02021789550781  Train_Reconstruction: 86.49246883392334  Train_KL: 3.527747720479965  Validation Loss : 90.19560623168945 Val_Reconstruction : 86.70732116699219 Val_KL : 3.4882869720458984\n","Epoch: 6872/8000  Traning Loss: 89.97629737854004  Train_Reconstruction: 86.44700336456299  Train_KL: 3.529294103384018  Validation Loss : 90.09280014038086 Val_Reconstruction : 86.61319351196289 Val_KL : 3.479604959487915\n","Epoch: 6873/8000  Traning Loss: 89.77133369445801  Train_Reconstruction: 86.25339317321777  Train_KL: 3.51793971657753  Validation Loss : 90.11366271972656 Val_Reconstruction : 86.6418228149414 Val_KL : 3.471838116645813\n","Epoch: 6874/8000  Traning Loss: 89.93548488616943  Train_Reconstruction: 86.41397666931152  Train_KL: 3.521508604288101  Validation Loss : 90.1121826171875 Val_Reconstruction : 86.63040924072266 Val_KL : 3.4817718267440796\n","Epoch: 6875/8000  Traning Loss: 89.98957252502441  Train_Reconstruction: 86.4567003250122  Train_KL: 3.5328727066516876  Validation Loss : 90.45006942749023 Val_Reconstruction : 86.96183776855469 Val_KL : 3.488231658935547\n","Epoch: 6876/8000  Traning Loss: 90.57274341583252  Train_Reconstruction: 87.0442066192627  Train_KL: 3.5285351872444153  Validation Loss : 90.71037673950195 Val_Reconstruction : 87.22616958618164 Val_KL : 3.4842108488082886\n","Epoch: 6877/8000  Traning Loss: 90.20100402832031  Train_Reconstruction: 86.67468547821045  Train_KL: 3.5263181626796722  Validation Loss : 90.13446426391602 Val_Reconstruction : 86.65494537353516 Val_KL : 3.4795186519622803\n","Epoch: 6878/8000  Traning Loss: 89.90080261230469  Train_Reconstruction: 86.37748527526855  Train_KL: 3.523317188024521  Validation Loss : 90.18833541870117 Val_Reconstruction : 86.70902252197266 Val_KL : 3.4793134927749634\n","Epoch: 6879/8000  Traning Loss: 89.97432327270508  Train_Reconstruction: 86.4510555267334  Train_KL: 3.5232668220996857  Validation Loss : 90.0693244934082 Val_Reconstruction : 86.58946228027344 Val_KL : 3.479863405227661\n","Epoch: 6880/8000  Traning Loss: 90.05534648895264  Train_Reconstruction: 86.53872585296631  Train_KL: 3.5166198909282684  Validation Loss : 90.32038879394531 Val_Reconstruction : 86.84981536865234 Val_KL : 3.470574736595154\n","Epoch: 6881/8000  Traning Loss: 90.05136203765869  Train_Reconstruction: 86.53847122192383  Train_KL: 3.512890547513962  Validation Loss : 90.3874626159668 Val_Reconstruction : 86.91573715209961 Val_KL : 3.471726655960083\n","Epoch: 6882/8000  Traning Loss: 90.07944202423096  Train_Reconstruction: 86.55678176879883  Train_KL: 3.522660493850708  Validation Loss : 90.00334167480469 Val_Reconstruction : 86.51909255981445 Val_KL : 3.4842491149902344\n","Epoch: 6883/8000  Traning Loss: 89.90361499786377  Train_Reconstruction: 86.3800401687622  Train_KL: 3.5235753059387207  Validation Loss : 89.9603385925293 Val_Reconstruction : 86.48237991333008 Val_KL : 3.477961301803589\n","Epoch: 6884/8000  Traning Loss: 89.71902847290039  Train_Reconstruction: 86.19834041595459  Train_KL: 3.5206867456436157  Validation Loss : 90.0859603881836 Val_Reconstruction : 86.60974884033203 Val_KL : 3.476208448410034\n","Epoch: 6885/8000  Traning Loss: 90.01429176330566  Train_Reconstruction: 86.488938331604  Train_KL: 3.5253542959690094  Validation Loss : 90.53583908081055 Val_Reconstruction : 87.04565048217773 Val_KL : 3.490188717842102\n","Epoch: 6886/8000  Traning Loss: 89.98297023773193  Train_Reconstruction: 86.46011066436768  Train_KL: 3.522858649492264  Validation Loss : 89.86308670043945 Val_Reconstruction : 86.38641738891602 Val_KL : 3.476670742034912\n","Epoch: 6887/8000  Traning Loss: 89.77836799621582  Train_Reconstruction: 86.26594638824463  Train_KL: 3.512421727180481  Validation Loss : 90.01192855834961 Val_Reconstruction : 86.53973388671875 Val_KL : 3.4721925258636475\n","Epoch: 6888/8000  Traning Loss: 90.62924480438232  Train_Reconstruction: 87.11127853393555  Train_KL: 3.5179666578769684  Validation Loss : 90.96490097045898 Val_Reconstruction : 87.48613357543945 Val_KL : 3.478767991065979\n","Epoch: 6889/8000  Traning Loss: 90.83945083618164  Train_Reconstruction: 87.31657695770264  Train_KL: 3.5228740870952606  Validation Loss : 90.50008773803711 Val_Reconstruction : 87.0221939086914 Val_KL : 3.477892756462097\n","Epoch: 6890/8000  Traning Loss: 90.5736665725708  Train_Reconstruction: 87.04583263397217  Train_KL: 3.5278332233428955  Validation Loss : 90.52242279052734 Val_Reconstruction : 87.03787231445312 Val_KL : 3.484551429748535\n","Epoch: 6891/8000  Traning Loss: 90.20145416259766  Train_Reconstruction: 86.67500400543213  Train_KL: 3.526450514793396  Validation Loss : 89.86361694335938 Val_Reconstruction : 86.37730026245117 Val_KL : 3.486315965652466\n","Epoch: 6892/8000  Traning Loss: 89.77977561950684  Train_Reconstruction: 86.25400733947754  Train_KL: 3.5257685780525208  Validation Loss : 89.83069229125977 Val_Reconstruction : 86.35303497314453 Val_KL : 3.4776580333709717\n","Epoch: 6893/8000  Traning Loss: 89.73436546325684  Train_Reconstruction: 86.21135997772217  Train_KL: 3.52300563454628  Validation Loss : 90.28493118286133 Val_Reconstruction : 86.79835510253906 Val_KL : 3.4865756034851074\n","Epoch: 6894/8000  Traning Loss: 89.98833465576172  Train_Reconstruction: 86.45624732971191  Train_KL: 3.532088041305542  Validation Loss : 90.29862213134766 Val_Reconstruction : 86.81642150878906 Val_KL : 3.4822022914886475\n","Epoch: 6895/8000  Traning Loss: 90.02061557769775  Train_Reconstruction: 86.50010299682617  Train_KL: 3.5205134451389313  Validation Loss : 90.4494857788086 Val_Reconstruction : 86.97274398803711 Val_KL : 3.4767439365386963\n","Epoch: 6896/8000  Traning Loss: 90.23593139648438  Train_Reconstruction: 86.71657943725586  Train_KL: 3.5193525850772858  Validation Loss : 90.14180755615234 Val_Reconstruction : 86.6661262512207 Val_KL : 3.475679636001587\n","Epoch: 6897/8000  Traning Loss: 89.91722202301025  Train_Reconstruction: 86.39240837097168  Train_KL: 3.524814009666443  Validation Loss : 89.92589569091797 Val_Reconstruction : 86.44146728515625 Val_KL : 3.4844284057617188\n","Epoch: 6898/8000  Traning Loss: 89.83429622650146  Train_Reconstruction: 86.30795097351074  Train_KL: 3.526346802711487  Validation Loss : 89.99162292480469 Val_Reconstruction : 86.51862335205078 Val_KL : 3.472999930381775\n","Epoch: 6899/8000  Traning Loss: 89.83918952941895  Train_Reconstruction: 86.32412338256836  Train_KL: 3.5150657892227173  Validation Loss : 90.13243865966797 Val_Reconstruction : 86.66471481323242 Val_KL : 3.4677246809005737\n","Epoch: 6900/8000  Traning Loss: 90.51625347137451  Train_Reconstruction: 86.98554229736328  Train_KL: 3.5307100117206573  Validation Loss : 90.24555206298828 Val_Reconstruction : 86.75630950927734 Val_KL : 3.489241361618042\n","Epoch: 6901/8000  Traning Loss: 90.24071216583252  Train_Reconstruction: 86.7169828414917  Train_KL: 3.5237284302711487  Validation Loss : 90.24607849121094 Val_Reconstruction : 86.76384735107422 Val_KL : 3.482234001159668\n","Epoch: 6902/8000  Traning Loss: 90.05548000335693  Train_Reconstruction: 86.53612899780273  Train_KL: 3.5193507969379425  Validation Loss : 90.02958297729492 Val_Reconstruction : 86.55679321289062 Val_KL : 3.4727895259857178\n","Epoch: 6903/8000  Traning Loss: 89.79010486602783  Train_Reconstruction: 86.27679634094238  Train_KL: 3.513309359550476  Validation Loss : 89.99342727661133 Val_Reconstruction : 86.51577758789062 Val_KL : 3.4776498079299927\n","Epoch: 6904/8000  Traning Loss: 89.73679542541504  Train_Reconstruction: 86.20608711242676  Train_KL: 3.530707359313965  Validation Loss : 89.7564697265625 Val_Reconstruction : 86.26355743408203 Val_KL : 3.4929134845733643\n","Epoch: 6905/8000  Traning Loss: 89.78515815734863  Train_Reconstruction: 86.25222587585449  Train_KL: 3.532932788133621  Validation Loss : 90.71450805664062 Val_Reconstruction : 87.2318229675293 Val_KL : 3.482685685157776\n","Epoch: 6906/8000  Traning Loss: 89.91271114349365  Train_Reconstruction: 86.39140605926514  Train_KL: 3.5213046073913574  Validation Loss : 89.889404296875 Val_Reconstruction : 86.4183120727539 Val_KL : 3.4710938930511475\n","Epoch: 6907/8000  Traning Loss: 89.76216793060303  Train_Reconstruction: 86.2494535446167  Train_KL: 3.5127144157886505  Validation Loss : 90.0149917602539 Val_Reconstruction : 86.54841613769531 Val_KL : 3.466575026512146\n","Epoch: 6908/8000  Traning Loss: 90.01541996002197  Train_Reconstruction: 86.49608612060547  Train_KL: 3.5193347334861755  Validation Loss : 90.16198348999023 Val_Reconstruction : 86.67989349365234 Val_KL : 3.4820910692214966\n","Epoch: 6909/8000  Traning Loss: 90.08378028869629  Train_Reconstruction: 86.55235862731934  Train_KL: 3.5314210653305054  Validation Loss : 90.50960540771484 Val_Reconstruction : 87.00955581665039 Val_KL : 3.500050902366638\n","Epoch: 6910/8000  Traning Loss: 90.05978679656982  Train_Reconstruction: 86.53116512298584  Train_KL: 3.528620421886444  Validation Loss : 90.26623916625977 Val_Reconstruction : 86.79063034057617 Val_KL : 3.475610852241516\n","Epoch: 6911/8000  Traning Loss: 91.14881134033203  Train_Reconstruction: 87.62809371948242  Train_KL: 3.5207170248031616  Validation Loss : 91.7055778503418 Val_Reconstruction : 88.22413635253906 Val_KL : 3.4814393520355225\n","Epoch: 6912/8000  Traning Loss: 91.49358367919922  Train_Reconstruction: 87.96273708343506  Train_KL: 3.5308472216129303  Validation Loss : 91.60201644897461 Val_Reconstruction : 88.1147346496582 Val_KL : 3.487282633781433\n","Epoch: 6913/8000  Traning Loss: 90.91473865509033  Train_Reconstruction: 87.39483451843262  Train_KL: 3.5199039578437805  Validation Loss : 90.35877990722656 Val_Reconstruction : 86.88561630249023 Val_KL : 3.473163604736328\n","Epoch: 6914/8000  Traning Loss: 90.10700702667236  Train_Reconstruction: 86.59914970397949  Train_KL: 3.507856100797653  Validation Loss : 90.19878387451172 Val_Reconstruction : 86.73361206054688 Val_KL : 3.46517276763916\n","Epoch: 6915/8000  Traning Loss: 89.73271083831787  Train_Reconstruction: 86.21527671813965  Train_KL: 3.517434984445572  Validation Loss : 89.83920288085938 Val_Reconstruction : 86.36467361450195 Val_KL : 3.4745270013809204\n","Epoch: 6916/8000  Traning Loss: 89.93853950500488  Train_Reconstruction: 86.41244125366211  Train_KL: 3.5260981917381287  Validation Loss : 90.20515060424805 Val_Reconstruction : 86.7238540649414 Val_KL : 3.4812986850738525\n","Epoch: 6917/8000  Traning Loss: 90.0737476348877  Train_Reconstruction: 86.54490089416504  Train_KL: 3.5288460850715637  Validation Loss : 90.39058303833008 Val_Reconstruction : 86.90162658691406 Val_KL : 3.488957643508911\n","Epoch: 6918/8000  Traning Loss: 90.25022506713867  Train_Reconstruction: 86.72070789337158  Train_KL: 3.5295172333717346  Validation Loss : 90.55813598632812 Val_Reconstruction : 87.08329772949219 Val_KL : 3.4748406410217285\n","Epoch: 6919/8000  Traning Loss: 90.21265697479248  Train_Reconstruction: 86.69988536834717  Train_KL: 3.512771725654602  Validation Loss : 90.28067398071289 Val_Reconstruction : 86.80577850341797 Val_KL : 3.474894881248474\n","Epoch: 6920/8000  Traning Loss: 90.1278657913208  Train_Reconstruction: 86.59839344024658  Train_KL: 3.5294717848300934  Validation Loss : 90.42020034790039 Val_Reconstruction : 86.93572235107422 Val_KL : 3.4844768047332764\n","Epoch: 6921/8000  Traning Loss: 90.17366123199463  Train_Reconstruction: 86.64780807495117  Train_KL: 3.5258533656597137  Validation Loss : 90.51177978515625 Val_Reconstruction : 87.02763366699219 Val_KL : 3.4841469526290894\n","Epoch: 6922/8000  Traning Loss: 90.7882604598999  Train_Reconstruction: 87.26285934448242  Train_KL: 3.5253994464874268  Validation Loss : 91.39155960083008 Val_Reconstruction : 87.91274642944336 Val_KL : 3.4788122177124023\n","Epoch: 6923/8000  Traning Loss: 91.38580226898193  Train_Reconstruction: 87.85789585113525  Train_KL: 3.5279062688350677  Validation Loss : 91.4539680480957 Val_Reconstruction : 87.97504425048828 Val_KL : 3.4789230823516846\n","Epoch: 6924/8000  Traning Loss: 91.1092882156372  Train_Reconstruction: 87.5890245437622  Train_KL: 3.520263969898224  Validation Loss : 91.00345611572266 Val_Reconstruction : 87.53303909301758 Val_KL : 3.470415234565735\n","Epoch: 6925/8000  Traning Loss: 90.89497756958008  Train_Reconstruction: 87.37312030792236  Train_KL: 3.5218565762043  Validation Loss : 91.18496322631836 Val_Reconstruction : 87.70276260375977 Val_KL : 3.4821977615356445\n","Epoch: 6926/8000  Traning Loss: 91.1851634979248  Train_Reconstruction: 87.66181659698486  Train_KL: 3.523347347974777  Validation Loss : 91.0552864074707 Val_Reconstruction : 87.58367538452148 Val_KL : 3.47161066532135\n","Epoch: 6927/8000  Traning Loss: 90.48890590667725  Train_Reconstruction: 86.97220706939697  Train_KL: 3.516698718070984  Validation Loss : 90.57090377807617 Val_Reconstruction : 87.09633255004883 Val_KL : 3.474574089050293\n","Epoch: 6928/8000  Traning Loss: 90.43876934051514  Train_Reconstruction: 86.92694664001465  Train_KL: 3.5118234157562256  Validation Loss : 91.37629318237305 Val_Reconstruction : 87.91263580322266 Val_KL : 3.4636590480804443\n","Epoch: 6929/8000  Traning Loss: 90.75762939453125  Train_Reconstruction: 87.2472791671753  Train_KL: 3.510349929332733  Validation Loss : 90.78660583496094 Val_Reconstruction : 87.3101577758789 Val_KL : 3.4764469861984253\n","Epoch: 6930/8000  Traning Loss: 90.04309749603271  Train_Reconstruction: 86.52178478240967  Train_KL: 3.5213135182857513  Validation Loss : 89.91721725463867 Val_Reconstruction : 86.43615341186523 Val_KL : 3.481062173843384\n","Epoch: 6931/8000  Traning Loss: 89.80806541442871  Train_Reconstruction: 86.28498935699463  Train_KL: 3.523075044155121  Validation Loss : 89.91321563720703 Val_Reconstruction : 86.43384552001953 Val_KL : 3.479368567466736\n","Epoch: 6932/8000  Traning Loss: 90.03396606445312  Train_Reconstruction: 86.51887321472168  Train_KL: 3.5150947272777557  Validation Loss : 90.447998046875 Val_Reconstruction : 86.97149658203125 Val_KL : 3.47650408744812\n","Epoch: 6933/8000  Traning Loss: 90.40392303466797  Train_Reconstruction: 86.8856782913208  Train_KL: 3.518245369195938  Validation Loss : 90.46996307373047 Val_Reconstruction : 86.99563598632812 Val_KL : 3.4743272066116333\n","Epoch: 6934/8000  Traning Loss: 90.78067874908447  Train_Reconstruction: 87.26118278503418  Train_KL: 3.519496351480484  Validation Loss : 90.8934326171875 Val_Reconstruction : 87.41859436035156 Val_KL : 3.474835753440857\n","Epoch: 6935/8000  Traning Loss: 90.52274513244629  Train_Reconstruction: 87.00859451293945  Train_KL: 3.514149159193039  Validation Loss : 90.53923034667969 Val_Reconstruction : 87.0709342956543 Val_KL : 3.468293070793152\n","Epoch: 6936/8000  Traning Loss: 90.30080127716064  Train_Reconstruction: 86.78541851043701  Train_KL: 3.5153823792934418  Validation Loss : 90.2895393371582 Val_Reconstruction : 86.80874252319336 Val_KL : 3.4807974100112915\n","Epoch: 6937/8000  Traning Loss: 90.0269718170166  Train_Reconstruction: 86.49403762817383  Train_KL: 3.532934069633484  Validation Loss : 90.1528549194336 Val_Reconstruction : 86.67217636108398 Val_KL : 3.48067843914032\n","Epoch: 6938/8000  Traning Loss: 89.76281642913818  Train_Reconstruction: 86.24410057067871  Train_KL: 3.518717586994171  Validation Loss : 89.73341369628906 Val_Reconstruction : 86.2588996887207 Val_KL : 3.474515438079834\n","Epoch: 6939/8000  Traning Loss: 89.64140224456787  Train_Reconstruction: 86.12378597259521  Train_KL: 3.5176165401935577  Validation Loss : 90.10250854492188 Val_Reconstruction : 86.6323127746582 Val_KL : 3.4701977968215942\n","Epoch: 6940/8000  Traning Loss: 89.93628692626953  Train_Reconstruction: 86.41883087158203  Train_KL: 3.5174576342105865  Validation Loss : 90.58306884765625 Val_Reconstruction : 87.09904479980469 Val_KL : 3.4840247631073\n","Epoch: 6941/8000  Traning Loss: 90.23140716552734  Train_Reconstruction: 86.70688056945801  Train_KL: 3.524527281522751  Validation Loss : 90.51042556762695 Val_Reconstruction : 87.02618408203125 Val_KL : 3.484242796897888\n","Epoch: 6942/8000  Traning Loss: 90.43256282806396  Train_Reconstruction: 86.90876483917236  Train_KL: 3.5237991213798523  Validation Loss : 90.56279754638672 Val_Reconstruction : 87.08246231079102 Val_KL : 3.4803342819213867\n","Epoch: 6943/8000  Traning Loss: 90.84148025512695  Train_Reconstruction: 87.31981086730957  Train_KL: 3.5216696858406067  Validation Loss : 90.81919479370117 Val_Reconstruction : 87.34772491455078 Val_KL : 3.4714717864990234\n","Epoch: 6944/8000  Traning Loss: 90.77519416809082  Train_Reconstruction: 87.24819946289062  Train_KL: 3.526995152235031  Validation Loss : 91.11157608032227 Val_Reconstruction : 87.62081527709961 Val_KL : 3.490760922431946\n","Epoch: 6945/8000  Traning Loss: 90.04912853240967  Train_Reconstruction: 86.51390266418457  Train_KL: 3.535226672887802  Validation Loss : 90.34850311279297 Val_Reconstruction : 86.86247634887695 Val_KL : 3.486027956008911\n","Epoch: 6946/8000  Traning Loss: 89.90279579162598  Train_Reconstruction: 86.38046360015869  Train_KL: 3.5223312973976135  Validation Loss : 90.16011047363281 Val_Reconstruction : 86.68048477172852 Val_KL : 3.4796249866485596\n","Epoch: 6947/8000  Traning Loss: 90.13438701629639  Train_Reconstruction: 86.61247730255127  Train_KL: 3.521909326314926  Validation Loss : 90.32552337646484 Val_Reconstruction : 86.84369277954102 Val_KL : 3.4818296432495117\n","Epoch: 6948/8000  Traning Loss: 90.32219886779785  Train_Reconstruction: 86.79999828338623  Train_KL: 3.5222013890743256  Validation Loss : 90.09426498413086 Val_Reconstruction : 86.60610961914062 Val_KL : 3.4881564378738403\n","Epoch: 6949/8000  Traning Loss: 90.39451789855957  Train_Reconstruction: 86.86464881896973  Train_KL: 3.529869109392166  Validation Loss : 91.21345901489258 Val_Reconstruction : 87.72576904296875 Val_KL : 3.4876898527145386\n","Epoch: 6950/8000  Traning Loss: 90.44247531890869  Train_Reconstruction: 86.92187976837158  Train_KL: 3.5205961167812347  Validation Loss : 90.41304397583008 Val_Reconstruction : 86.93748092651367 Val_KL : 3.475562572479248\n","Epoch: 6951/8000  Traning Loss: 90.61450958251953  Train_Reconstruction: 87.1065092086792  Train_KL: 3.5080001056194305  Validation Loss : 91.01225662231445 Val_Reconstruction : 87.5418472290039 Val_KL : 3.470408797264099\n","Epoch: 6952/8000  Traning Loss: 91.13112449645996  Train_Reconstruction: 87.6139965057373  Train_KL: 3.5171284675598145  Validation Loss : 91.3347396850586 Val_Reconstruction : 87.85211181640625 Val_KL : 3.4826273918151855\n","Epoch: 6953/8000  Traning Loss: 91.50107669830322  Train_Reconstruction: 87.97639465332031  Train_KL: 3.5246827602386475  Validation Loss : 91.25575637817383 Val_Reconstruction : 87.77496337890625 Val_KL : 3.4807910919189453\n","Epoch: 6954/8000  Traning Loss: 91.00410270690918  Train_Reconstruction: 87.48266124725342  Train_KL: 3.5214411914348602  Validation Loss : 90.65997314453125 Val_Reconstruction : 87.17372512817383 Val_KL : 3.4862496852874756\n","Epoch: 6955/8000  Traning Loss: 90.84002113342285  Train_Reconstruction: 87.31643581390381  Train_KL: 3.5235859155654907  Validation Loss : 90.47477340698242 Val_Reconstruction : 86.99871826171875 Val_KL : 3.476055145263672\n","Epoch: 6956/8000  Traning Loss: 89.96661376953125  Train_Reconstruction: 86.4512243270874  Train_KL: 3.515389084815979  Validation Loss : 89.99264907836914 Val_Reconstruction : 86.51995468139648 Val_KL : 3.47269344329834\n","Epoch: 6957/8000  Traning Loss: 90.23112487792969  Train_Reconstruction: 86.71872425079346  Train_KL: 3.512399047613144  Validation Loss : 90.42703628540039 Val_Reconstruction : 86.95146179199219 Val_KL : 3.4755743741989136\n","Epoch: 6958/8000  Traning Loss: 90.5829381942749  Train_Reconstruction: 87.05360221862793  Train_KL: 3.529334992170334  Validation Loss : 90.79330825805664 Val_Reconstruction : 87.30855560302734 Val_KL : 3.4847538471221924\n","Epoch: 6959/8000  Traning Loss: 90.52666568756104  Train_Reconstruction: 87.0045280456543  Train_KL: 3.522137552499771  Validation Loss : 90.86480331420898 Val_Reconstruction : 87.39066314697266 Val_KL : 3.4741395711898804\n","Epoch: 6960/8000  Traning Loss: 90.45569038391113  Train_Reconstruction: 86.93528461456299  Train_KL: 3.520406097173691  Validation Loss : 90.63412094116211 Val_Reconstruction : 87.15397644042969 Val_KL : 3.4801440238952637\n","Epoch: 6961/8000  Traning Loss: 90.08230304718018  Train_Reconstruction: 86.55827617645264  Train_KL: 3.5240273475646973  Validation Loss : 90.37456512451172 Val_Reconstruction : 86.89459228515625 Val_KL : 3.4799723625183105\n","Epoch: 6962/8000  Traning Loss: 90.24964714050293  Train_Reconstruction: 86.73023509979248  Train_KL: 3.5194112062454224  Validation Loss : 90.88429260253906 Val_Reconstruction : 87.4062271118164 Val_KL : 3.47806453704834\n","Epoch: 6963/8000  Traning Loss: 90.4429931640625  Train_Reconstruction: 86.92785167694092  Train_KL: 3.515141576528549  Validation Loss : 90.58332443237305 Val_Reconstruction : 87.10863494873047 Val_KL : 3.4746897220611572\n","Epoch: 6964/8000  Traning Loss: 90.08440685272217  Train_Reconstruction: 86.56432247161865  Train_KL: 3.520083099603653  Validation Loss : 90.23220443725586 Val_Reconstruction : 86.75995254516602 Val_KL : 3.4722495079040527\n","Epoch: 6965/8000  Traning Loss: 90.11682415008545  Train_Reconstruction: 86.60242462158203  Train_KL: 3.514400154352188  Validation Loss : 90.60306549072266 Val_Reconstruction : 87.13066864013672 Val_KL : 3.4723979234695435\n","Epoch: 6966/8000  Traning Loss: 90.13383102416992  Train_Reconstruction: 86.60786819458008  Train_KL: 3.5259625613689423  Validation Loss : 90.912109375 Val_Reconstruction : 87.42413711547852 Val_KL : 3.4879724979400635\n","Epoch: 6967/8000  Traning Loss: 90.53574180603027  Train_Reconstruction: 87.00958824157715  Train_KL: 3.5261541306972504  Validation Loss : 90.76653671264648 Val_Reconstruction : 87.28853607177734 Val_KL : 3.478001594543457\n","Epoch: 6968/8000  Traning Loss: 90.8481912612915  Train_Reconstruction: 87.33413600921631  Train_KL: 3.5140537917613983  Validation Loss : 91.26412582397461 Val_Reconstruction : 87.79388427734375 Val_KL : 3.4702439308166504\n","Epoch: 6969/8000  Traning Loss: 89.86777305603027  Train_Reconstruction: 86.34728240966797  Train_KL: 3.520491898059845  Validation Loss : 89.86898040771484 Val_Reconstruction : 86.38556289672852 Val_KL : 3.4834169149398804\n","Epoch: 6970/8000  Traning Loss: 89.73816776275635  Train_Reconstruction: 86.21519184112549  Train_KL: 3.5229767560958862  Validation Loss : 89.7431869506836 Val_Reconstruction : 86.26274108886719 Val_KL : 3.480448603630066\n","Epoch: 6971/8000  Traning Loss: 89.68730354309082  Train_Reconstruction: 86.16430759429932  Train_KL: 3.5229947566986084  Validation Loss : 89.80630493164062 Val_Reconstruction : 86.3182373046875 Val_KL : 3.4880692958831787\n","Epoch: 6972/8000  Traning Loss: 89.7309045791626  Train_Reconstruction: 86.21199321746826  Train_KL: 3.518911689519882  Validation Loss : 89.87256240844727 Val_Reconstruction : 86.4007453918457 Val_KL : 3.471816658973694\n","Epoch: 6973/8000  Traning Loss: 89.62666893005371  Train_Reconstruction: 86.10706806182861  Train_KL: 3.5196013748645782  Validation Loss : 89.7820930480957 Val_Reconstruction : 86.2999038696289 Val_KL : 3.482187509536743\n","Epoch: 6974/8000  Traning Loss: 89.69266891479492  Train_Reconstruction: 86.17064094543457  Train_KL: 3.522028774023056  Validation Loss : 89.95736312866211 Val_Reconstruction : 86.48680114746094 Val_KL : 3.4705638885498047\n","Epoch: 6975/8000  Traning Loss: 89.86682033538818  Train_Reconstruction: 86.34966659545898  Train_KL: 3.5171526968479156  Validation Loss : 90.15333938598633 Val_Reconstruction : 86.67310333251953 Val_KL : 3.480238199234009\n","Epoch: 6976/8000  Traning Loss: 90.15754890441895  Train_Reconstruction: 86.6299295425415  Train_KL: 3.5276206135749817  Validation Loss : 90.5518569946289 Val_Reconstruction : 87.07324981689453 Val_KL : 3.478609561920166\n","Epoch: 6977/8000  Traning Loss: 90.68636417388916  Train_Reconstruction: 87.16225910186768  Train_KL: 3.524104744195938  Validation Loss : 91.3336410522461 Val_Reconstruction : 87.85731887817383 Val_KL : 3.476325273513794\n","Epoch: 6978/8000  Traning Loss: 91.01060485839844  Train_Reconstruction: 87.49420738220215  Train_KL: 3.516396850347519  Validation Loss : 90.86316299438477 Val_Reconstruction : 87.39553833007812 Val_KL : 3.4676260948181152\n","Epoch: 6979/8000  Traning Loss: 90.12795352935791  Train_Reconstruction: 86.60200595855713  Train_KL: 3.5259472131729126  Validation Loss : 90.23096466064453 Val_Reconstruction : 86.74770736694336 Val_KL : 3.483256459236145\n","Epoch: 6980/8000  Traning Loss: 89.62459564208984  Train_Reconstruction: 86.09174823760986  Train_KL: 3.532847046852112  Validation Loss : 89.66534805297852 Val_Reconstruction : 86.18175888061523 Val_KL : 3.4835867881774902\n","Epoch: 6981/8000  Traning Loss: 89.46220207214355  Train_Reconstruction: 85.94440174102783  Train_KL: 3.517800509929657  Validation Loss : 89.8017807006836 Val_Reconstruction : 86.33430862426758 Val_KL : 3.467469573020935\n","Epoch: 6982/8000  Traning Loss: 89.62765884399414  Train_Reconstruction: 86.10504245758057  Train_KL: 3.5226146280765533  Validation Loss : 90.08032989501953 Val_Reconstruction : 86.59884643554688 Val_KL : 3.4814847707748413\n","Epoch: 6983/8000  Traning Loss: 89.73380756378174  Train_Reconstruction: 86.20269584655762  Train_KL: 3.5311127603054047  Validation Loss : 89.74281692504883 Val_Reconstruction : 86.25725936889648 Val_KL : 3.4855599403381348\n","Epoch: 6984/8000  Traning Loss: 89.63822555541992  Train_Reconstruction: 86.11432933807373  Train_KL: 3.5238964557647705  Validation Loss : 89.77265930175781 Val_Reconstruction : 86.29370880126953 Val_KL : 3.478949189186096\n","Epoch: 6985/8000  Traning Loss: 89.6836519241333  Train_Reconstruction: 86.16417694091797  Train_KL: 3.5194742679595947  Validation Loss : 90.06111145019531 Val_Reconstruction : 86.58941650390625 Val_KL : 3.471693754196167\n","Epoch: 6986/8000  Traning Loss: 89.82637977600098  Train_Reconstruction: 86.30752754211426  Train_KL: 3.518851488828659  Validation Loss : 90.04206085205078 Val_Reconstruction : 86.56624221801758 Val_KL : 3.475817561149597\n","Epoch: 6987/8000  Traning Loss: 90.23403358459473  Train_Reconstruction: 86.71172523498535  Train_KL: 3.5223080217838287  Validation Loss : 90.4398422241211 Val_Reconstruction : 86.9623031616211 Val_KL : 3.4775389432907104\n","Epoch: 6988/8000  Traning Loss: 90.31495761871338  Train_Reconstruction: 86.79724216461182  Train_KL: 3.517714947462082  Validation Loss : 90.45008087158203 Val_Reconstruction : 86.9873275756836 Val_KL : 3.4627543687820435\n","Epoch: 6989/8000  Traning Loss: 90.88111019134521  Train_Reconstruction: 87.3696346282959  Train_KL: 3.5114759504795074  Validation Loss : 91.64448928833008 Val_Reconstruction : 88.17988967895508 Val_KL : 3.4645994901657104\n","Epoch: 6990/8000  Traning Loss: 90.52504920959473  Train_Reconstruction: 87.009934425354  Train_KL: 3.5151149332523346  Validation Loss : 90.2548942565918 Val_Reconstruction : 86.78037643432617 Val_KL : 3.4745194911956787\n","Epoch: 6991/8000  Traning Loss: 89.91214942932129  Train_Reconstruction: 86.39302730560303  Train_KL: 3.5191234052181244  Validation Loss : 90.28067016601562 Val_Reconstruction : 86.81170272827148 Val_KL : 3.4689688682556152\n","Epoch: 6992/8000  Traning Loss: 90.15870094299316  Train_Reconstruction: 86.64144229888916  Train_KL: 3.517258256673813  Validation Loss : 90.31006622314453 Val_Reconstruction : 86.8327751159668 Val_KL : 3.4772937297821045\n","Epoch: 6993/8000  Traning Loss: 89.75254440307617  Train_Reconstruction: 86.22349834442139  Train_KL: 3.529047042131424  Validation Loss : 89.9580078125 Val_Reconstruction : 86.47406387329102 Val_KL : 3.483946919441223\n","Epoch: 6994/8000  Traning Loss: 89.62447738647461  Train_Reconstruction: 86.09810447692871  Train_KL: 3.5263726711273193  Validation Loss : 89.68995666503906 Val_Reconstruction : 86.2171630859375 Val_KL : 3.4727920293807983\n","Epoch: 6995/8000  Traning Loss: 89.71802234649658  Train_Reconstruction: 86.20243263244629  Train_KL: 3.5155902206897736  Validation Loss : 90.63914108276367 Val_Reconstruction : 87.1705207824707 Val_KL : 3.4686200618743896\n","Epoch: 6996/8000  Traning Loss: 90.2558364868164  Train_Reconstruction: 86.73862266540527  Train_KL: 3.517215669155121  Validation Loss : 90.85903930664062 Val_Reconstruction : 87.38282012939453 Val_KL : 3.476217746734619\n","Epoch: 6997/8000  Traning Loss: 90.35665035247803  Train_Reconstruction: 86.83781242370605  Train_KL: 3.518838405609131  Validation Loss : 90.04862594604492 Val_Reconstruction : 86.5767707824707 Val_KL : 3.4718576669692993\n","Epoch: 6998/8000  Traning Loss: 90.01903820037842  Train_Reconstruction: 86.50666332244873  Train_KL: 3.5123746395111084  Validation Loss : 90.78961181640625 Val_Reconstruction : 87.3247184753418 Val_KL : 3.4648948907852173\n","Epoch: 6999/8000  Traning Loss: 89.99542236328125  Train_Reconstruction: 86.47937965393066  Train_KL: 3.516041934490204  Validation Loss : 89.9461555480957 Val_Reconstruction : 86.46968078613281 Val_KL : 3.476475715637207\n","Epoch: 7000/8000  Traning Loss: 89.77892303466797  Train_Reconstruction: 86.25123500823975  Train_KL: 3.5276864171028137  Validation Loss : 90.0393180847168 Val_Reconstruction : 86.55816268920898 Val_KL : 3.4811559915542603\n","Epoch: 7001/8000  Traning Loss: 90.07252597808838  Train_Reconstruction: 86.54375457763672  Train_KL: 3.5287705659866333  Validation Loss : 90.12491607666016 Val_Reconstruction : 86.64432525634766 Val_KL : 3.480589747428894\n","Epoch: 7002/8000  Traning Loss: 90.4175443649292  Train_Reconstruction: 86.89739799499512  Train_KL: 3.5201447010040283  Validation Loss : 90.64877700805664 Val_Reconstruction : 87.17684936523438 Val_KL : 3.4719290733337402\n","Epoch: 7003/8000  Traning Loss: 90.34850311279297  Train_Reconstruction: 86.83138656616211  Train_KL: 3.517115890979767  Validation Loss : 90.7339096069336 Val_Reconstruction : 87.25873947143555 Val_KL : 3.4751713275909424\n","EarlyStopping counter: 1 out of 100\n","Epoch: 7004/8000  Traning Loss: 91.08437538146973  Train_Reconstruction: 87.56392288208008  Train_KL: 3.520453006029129  Validation Loss : 91.49517059326172 Val_Reconstruction : 88.02072143554688 Val_KL : 3.4744486808776855\n","EarlyStopping counter: 2 out of 100\n","Epoch: 7005/8000  Traning Loss: 91.21241092681885  Train_Reconstruction: 87.69675922393799  Train_KL: 3.5156524181365967  Validation Loss : 91.10033416748047 Val_Reconstruction : 87.6290054321289 Val_KL : 3.4713293313980103\n","EarlyStopping counter: 3 out of 100\n","Epoch: 7006/8000  Traning Loss: 90.47811985015869  Train_Reconstruction: 86.96011066436768  Train_KL: 3.5180101096630096  Validation Loss : 90.28931427001953 Val_Reconstruction : 86.81377792358398 Val_KL : 3.4755380153656006\n","Epoch: 7007/8000  Traning Loss: 90.0401496887207  Train_Reconstruction: 86.5219316482544  Train_KL: 3.5182185769081116  Validation Loss : 89.85943603515625 Val_Reconstruction : 86.38313674926758 Val_KL : 3.476299285888672\n","Epoch: 7008/8000  Traning Loss: 89.82762908935547  Train_Reconstruction: 86.31031036376953  Train_KL: 3.517319142818451  Validation Loss : 90.05276870727539 Val_Reconstruction : 86.57875061035156 Val_KL : 3.474018931388855\n","EarlyStopping counter: 1 out of 100\n","Epoch: 7009/8000  Traning Loss: 90.06483745574951  Train_Reconstruction: 86.5458345413208  Train_KL: 3.5190032720565796  Validation Loss : 90.76313781738281 Val_Reconstruction : 87.28775787353516 Val_KL : 3.4753804206848145\n","EarlyStopping counter: 2 out of 100\n","Epoch: 7010/8000  Traning Loss: 90.25681400299072  Train_Reconstruction: 86.73513984680176  Train_KL: 3.5216751992702484  Validation Loss : 90.38071060180664 Val_Reconstruction : 86.90145874023438 Val_KL : 3.479252815246582\n","EarlyStopping counter: 3 out of 100\n","Epoch: 7011/8000  Traning Loss: 89.89082050323486  Train_Reconstruction: 86.36610984802246  Train_KL: 3.524709790945053  Validation Loss : 89.7852897644043 Val_Reconstruction : 86.30381774902344 Val_KL : 3.481472134590149\n","Epoch: 7012/8000  Traning Loss: 89.52919960021973  Train_Reconstruction: 86.00626182556152  Train_KL: 3.522939234972  Validation Loss : 89.81367492675781 Val_Reconstruction : 86.33624267578125 Val_KL : 3.4774335622787476\n","EarlyStopping counter: 1 out of 100\n","Epoch: 7013/8000  Traning Loss: 89.69285106658936  Train_Reconstruction: 86.17192935943604  Train_KL: 3.520921140909195  Validation Loss : 90.32005310058594 Val_Reconstruction : 86.84160614013672 Val_KL : 3.4784494638442993\n","EarlyStopping counter: 2 out of 100\n","Epoch: 7014/8000  Traning Loss: 90.15743064880371  Train_Reconstruction: 86.63663291931152  Train_KL: 3.5207978785037994  Validation Loss : 90.35260009765625 Val_Reconstruction : 86.87787246704102 Val_KL : 3.474729537963867\n","EarlyStopping counter: 3 out of 100\n","Epoch: 7015/8000  Traning Loss: 90.00724506378174  Train_Reconstruction: 86.48816299438477  Train_KL: 3.5190835893154144  Validation Loss : 90.27285385131836 Val_Reconstruction : 86.79682540893555 Val_KL : 3.4760305881500244\n","EarlyStopping counter: 4 out of 100\n","Epoch: 7016/8000  Traning Loss: 90.26000690460205  Train_Reconstruction: 86.74209499359131  Train_KL: 3.5179112553596497  Validation Loss : 90.62063980102539 Val_Reconstruction : 87.15417098999023 Val_KL : 3.466469645500183\n","EarlyStopping counter: 5 out of 100\n","Epoch: 7017/8000  Traning Loss: 90.41127490997314  Train_Reconstruction: 86.89877319335938  Train_KL: 3.5125023126602173  Validation Loss : 90.60232162475586 Val_Reconstruction : 87.12472915649414 Val_KL : 3.477591872215271\n","EarlyStopping counter: 6 out of 100\n","Epoch: 7018/8000  Traning Loss: 90.12474918365479  Train_Reconstruction: 86.59842109680176  Train_KL: 3.5263291001319885  Validation Loss : 89.9319076538086 Val_Reconstruction : 86.45291900634766 Val_KL : 3.4789897203445435\n","EarlyStopping counter: 7 out of 100\n","Epoch: 7019/8000  Traning Loss: 90.27465152740479  Train_Reconstruction: 86.74705791473389  Train_KL: 3.5275932252407074  Validation Loss : 90.64614486694336 Val_Reconstruction : 87.16357040405273 Val_KL : 3.482574224472046\n","EarlyStopping counter: 8 out of 100\n","Epoch: 7020/8000  Traning Loss: 90.18988513946533  Train_Reconstruction: 86.66545104980469  Train_KL: 3.5244340002536774  Validation Loss : 90.5237808227539 Val_Reconstruction : 87.04365158081055 Val_KL : 3.480126738548279\n","EarlyStopping counter: 9 out of 100\n","Epoch: 7021/8000  Traning Loss: 90.11885356903076  Train_Reconstruction: 86.59735584259033  Train_KL: 3.5214966535568237  Validation Loss : 90.26596450805664 Val_Reconstruction : 86.78416061401367 Val_KL : 3.481802821159363\n","EarlyStopping counter: 10 out of 100\n","Epoch: 7022/8000  Traning Loss: 89.8207311630249  Train_Reconstruction: 86.29538822174072  Train_KL: 3.525344133377075  Validation Loss : 90.37865447998047 Val_Reconstruction : 86.88969039916992 Val_KL : 3.4889633655548096\n","EarlyStopping counter: 11 out of 100\n","Epoch: 7023/8000  Traning Loss: 90.04869365692139  Train_Reconstruction: 86.53079605102539  Train_KL: 3.51789790391922  Validation Loss : 90.61435317993164 Val_Reconstruction : 87.142822265625 Val_KL : 3.4715300798416138\n","EarlyStopping counter: 12 out of 100\n","Epoch: 7024/8000  Traning Loss: 90.08697032928467  Train_Reconstruction: 86.58005619049072  Train_KL: 3.506914407014847  Validation Loss : 90.23566055297852 Val_Reconstruction : 86.76933670043945 Val_KL : 3.4663256406784058\n","EarlyStopping counter: 13 out of 100\n","Epoch: 7025/8000  Traning Loss: 90.17790699005127  Train_Reconstruction: 86.66052341461182  Train_KL: 3.517383247613907  Validation Loss : 90.53328704833984 Val_Reconstruction : 87.06063842773438 Val_KL : 3.4726465940475464\n","EarlyStopping counter: 14 out of 100\n","Epoch: 7026/8000  Traning Loss: 90.4773302078247  Train_Reconstruction: 86.94807529449463  Train_KL: 3.529255121946335  Validation Loss : 90.64312362670898 Val_Reconstruction : 87.15830993652344 Val_KL : 3.484812378883362\n","EarlyStopping counter: 15 out of 100\n","Epoch: 7027/8000  Traning Loss: 90.40909004211426  Train_Reconstruction: 86.88676357269287  Train_KL: 3.5223251283168793  Validation Loss : 90.84965133666992 Val_Reconstruction : 87.38080215454102 Val_KL : 3.4688483476638794\n","EarlyStopping counter: 16 out of 100\n","Epoch: 7028/8000  Traning Loss: 90.18125247955322  Train_Reconstruction: 86.67154693603516  Train_KL: 3.509706139564514  Validation Loss : 90.17481231689453 Val_Reconstruction : 86.70976638793945 Val_KL : 3.4650440216064453\n","EarlyStopping counter: 17 out of 100\n","Epoch: 7029/8000  Traning Loss: 89.92917346954346  Train_Reconstruction: 86.40748500823975  Train_KL: 3.5216901302337646  Validation Loss : 89.88864517211914 Val_Reconstruction : 86.40567398071289 Val_KL : 3.482973337173462\n","EarlyStopping counter: 18 out of 100\n","Epoch: 7030/8000  Traning Loss: 90.0352430343628  Train_Reconstruction: 86.5028657913208  Train_KL: 3.5323767364025116  Validation Loss : 90.62549591064453 Val_Reconstruction : 87.13860321044922 Val_KL : 3.4868942499160767\n","EarlyStopping counter: 19 out of 100\n","Epoch: 7031/8000  Traning Loss: 90.4039659500122  Train_Reconstruction: 86.8731803894043  Train_KL: 3.5307847261428833  Validation Loss : 90.55646514892578 Val_Reconstruction : 87.06978225708008 Val_KL : 3.4866809844970703\n","EarlyStopping counter: 20 out of 100\n","Epoch: 7032/8000  Traning Loss: 90.20242404937744  Train_Reconstruction: 86.68119621276855  Train_KL: 3.521228849887848  Validation Loss : 90.21355819702148 Val_Reconstruction : 86.7502212524414 Val_KL : 3.4633365869522095\n","EarlyStopping counter: 21 out of 100\n","Epoch: 7033/8000  Traning Loss: 89.93640232086182  Train_Reconstruction: 86.42638111114502  Train_KL: 3.5100210905075073  Validation Loss : 90.41059494018555 Val_Reconstruction : 86.94024658203125 Val_KL : 3.470349669456482\n","EarlyStopping counter: 22 out of 100\n","Epoch: 7034/8000  Traning Loss: 90.14420318603516  Train_Reconstruction: 86.61609745025635  Train_KL: 3.5281061232089996  Validation Loss : 90.09784698486328 Val_Reconstruction : 86.60962677001953 Val_KL : 3.488218903541565\n","EarlyStopping counter: 23 out of 100\n","Epoch: 7035/8000  Traning Loss: 89.86922645568848  Train_Reconstruction: 86.33921146392822  Train_KL: 3.5300157368183136  Validation Loss : 89.95409393310547 Val_Reconstruction : 86.46836853027344 Val_KL : 3.4857252836227417\n","EarlyStopping counter: 24 out of 100\n","Epoch: 7036/8000  Traning Loss: 89.74219036102295  Train_Reconstruction: 86.21710205078125  Train_KL: 3.525088369846344  Validation Loss : 90.11494827270508 Val_Reconstruction : 86.6298599243164 Val_KL : 3.485087752342224\n","EarlyStopping counter: 25 out of 100\n","Epoch: 7037/8000  Traning Loss: 90.04965019226074  Train_Reconstruction: 86.52692222595215  Train_KL: 3.5227282643318176  Validation Loss : 90.56499862670898 Val_Reconstruction : 87.09425735473633 Val_KL : 3.4707412719726562\n","EarlyStopping counter: 26 out of 100\n","Epoch: 7038/8000  Traning Loss: 90.22747898101807  Train_Reconstruction: 86.7139892578125  Train_KL: 3.513489454984665  Validation Loss : 90.5475845336914 Val_Reconstruction : 87.07327270507812 Val_KL : 3.474311113357544\n","EarlyStopping counter: 27 out of 100\n","Epoch: 7039/8000  Traning Loss: 90.23431205749512  Train_Reconstruction: 86.72023010253906  Train_KL: 3.514081746339798  Validation Loss : 90.1137580871582 Val_Reconstruction : 86.63743209838867 Val_KL : 3.4763240814208984\n","EarlyStopping counter: 28 out of 100\n","Epoch: 7040/8000  Traning Loss: 89.92666816711426  Train_Reconstruction: 86.40184307098389  Train_KL: 3.5248245000839233  Validation Loss : 90.09427642822266 Val_Reconstruction : 86.59845733642578 Val_KL : 3.4958170652389526\n","EarlyStopping counter: 29 out of 100\n","Epoch: 7041/8000  Traning Loss: 89.62733459472656  Train_Reconstruction: 86.09741306304932  Train_KL: 3.5299213230609894  Validation Loss : 89.82023620605469 Val_Reconstruction : 86.34286880493164 Val_KL : 3.4773696660995483\n","EarlyStopping counter: 30 out of 100\n","Epoch: 7042/8000  Traning Loss: 90.03278732299805  Train_Reconstruction: 86.51245212554932  Train_KL: 3.5203348994255066  Validation Loss : 90.10339736938477 Val_Reconstruction : 86.62736129760742 Val_KL : 3.4760361909866333\n","EarlyStopping counter: 31 out of 100\n","Epoch: 7043/8000  Traning Loss: 90.1808271408081  Train_Reconstruction: 86.66477680206299  Train_KL: 3.5160512924194336  Validation Loss : 90.0804214477539 Val_Reconstruction : 86.6031723022461 Val_KL : 3.4772472381591797\n","EarlyStopping counter: 32 out of 100\n","Epoch: 7044/8000  Traning Loss: 89.96011638641357  Train_Reconstruction: 86.43844509124756  Train_KL: 3.5216707289218903  Validation Loss : 90.49939727783203 Val_Reconstruction : 87.01709747314453 Val_KL : 3.482300877571106\n","EarlyStopping counter: 33 out of 100\n","Epoch: 7045/8000  Traning Loss: 89.82345962524414  Train_Reconstruction: 86.29509830474854  Train_KL: 3.5283616185188293  Validation Loss : 90.14178848266602 Val_Reconstruction : 86.65294647216797 Val_KL : 3.4888405799865723\n","EarlyStopping counter: 34 out of 100\n","Epoch: 7046/8000  Traning Loss: 89.7028694152832  Train_Reconstruction: 86.17638874053955  Train_KL: 3.526481032371521  Validation Loss : 89.77046203613281 Val_Reconstruction : 86.29710006713867 Val_KL : 3.473362922668457\n","Epoch: 7047/8000  Traning Loss: 89.63446712493896  Train_Reconstruction: 86.11397075653076  Train_KL: 3.520495444536209  Validation Loss : 89.9732437133789 Val_Reconstruction : 86.48837280273438 Val_KL : 3.4848718643188477\n","EarlyStopping counter: 1 out of 100\n","Epoch: 7048/8000  Traning Loss: 89.92500305175781  Train_Reconstruction: 86.40303802490234  Train_KL: 3.521966367959976  Validation Loss : 90.08647155761719 Val_Reconstruction : 86.61436462402344 Val_KL : 3.4721081256866455\n","EarlyStopping counter: 2 out of 100\n","Epoch: 7049/8000  Traning Loss: 90.10423564910889  Train_Reconstruction: 86.59511184692383  Train_KL: 3.509123057126999  Validation Loss : 90.07440185546875 Val_Reconstruction : 86.59793472290039 Val_KL : 3.4764665365219116\n","EarlyStopping counter: 3 out of 100\n","Epoch: 7050/8000  Traning Loss: 89.99334144592285  Train_Reconstruction: 86.47211265563965  Train_KL: 3.5212281346321106  Validation Loss : 90.12481307983398 Val_Reconstruction : 86.64731216430664 Val_KL : 3.477497935295105\n","EarlyStopping counter: 4 out of 100\n","Epoch: 7051/8000  Traning Loss: 89.73908042907715  Train_Reconstruction: 86.22556114196777  Train_KL: 3.513519436120987  Validation Loss : 89.84318923950195 Val_Reconstruction : 86.37522888183594 Val_KL : 3.467961549758911\n","EarlyStopping counter: 5 out of 100\n","Epoch: 7052/8000  Traning Loss: 89.86572265625  Train_Reconstruction: 86.34472370147705  Train_KL: 3.520999014377594  Validation Loss : 90.27459716796875 Val_Reconstruction : 86.78823471069336 Val_KL : 3.486360788345337\n","EarlyStopping counter: 6 out of 100\n","Epoch: 7053/8000  Traning Loss: 89.79284954071045  Train_Reconstruction: 86.26708221435547  Train_KL: 3.5257676243782043  Validation Loss : 89.84208679199219 Val_Reconstruction : 86.36884689331055 Val_KL : 3.473242163658142\n","EarlyStopping counter: 7 out of 100\n","Epoch: 7054/8000  Traning Loss: 89.87465381622314  Train_Reconstruction: 86.35781955718994  Train_KL: 3.5168352127075195  Validation Loss : 90.21091079711914 Val_Reconstruction : 86.73771286010742 Val_KL : 3.4731979370117188\n","EarlyStopping counter: 8 out of 100\n","Epoch: 7055/8000  Traning Loss: 89.892258644104  Train_Reconstruction: 86.36791229248047  Train_KL: 3.5243462324142456  Validation Loss : 90.11372375488281 Val_Reconstruction : 86.63037872314453 Val_KL : 3.4833433628082275\n","EarlyStopping counter: 9 out of 100\n","Epoch: 7056/8000  Traning Loss: 89.65050983428955  Train_Reconstruction: 86.12902164459229  Train_KL: 3.521488666534424  Validation Loss : 89.63753128051758 Val_Reconstruction : 86.1639404296875 Val_KL : 3.4735898971557617\n","Epoch: 7057/8000  Traning Loss: 89.75482177734375  Train_Reconstruction: 86.24258136749268  Train_KL: 3.5122406780719757  Validation Loss : 90.0103759765625 Val_Reconstruction : 86.54460144042969 Val_KL : 3.465775966644287\n","EarlyStopping counter: 1 out of 100\n","Epoch: 7058/8000  Traning Loss: 89.76367092132568  Train_Reconstruction: 86.25325012207031  Train_KL: 3.510420560836792  Validation Loss : 89.93737411499023 Val_Reconstruction : 86.46464538574219 Val_KL : 3.4727253913879395\n","EarlyStopping counter: 2 out of 100\n","Epoch: 7059/8000  Traning Loss: 89.65564155578613  Train_Reconstruction: 86.14039134979248  Train_KL: 3.51525017619133  Validation Loss : 89.84324645996094 Val_Reconstruction : 86.36740112304688 Val_KL : 3.475843906402588\n","EarlyStopping counter: 3 out of 100\n","Epoch: 7060/8000  Traning Loss: 89.65740871429443  Train_Reconstruction: 86.14354991912842  Train_KL: 3.513857275247574  Validation Loss : 89.80860137939453 Val_Reconstruction : 86.3465576171875 Val_KL : 3.462044596672058\n","EarlyStopping counter: 4 out of 100\n","Epoch: 7061/8000  Traning Loss: 89.71457099914551  Train_Reconstruction: 86.20493412017822  Train_KL: 3.5096375644207  Validation Loss : 90.14225006103516 Val_Reconstruction : 86.66899108886719 Val_KL : 3.473260283470154\n","EarlyStopping counter: 5 out of 100\n","Epoch: 7062/8000  Traning Loss: 89.87859535217285  Train_Reconstruction: 86.35976791381836  Train_KL: 3.5188260078430176  Validation Loss : 90.23747634887695 Val_Reconstruction : 86.76102447509766 Val_KL : 3.4764492511749268\n","EarlyStopping counter: 6 out of 100\n","Epoch: 7063/8000  Traning Loss: 90.19974327087402  Train_Reconstruction: 86.67543983459473  Train_KL: 3.5243035554885864  Validation Loss : 90.57324981689453 Val_Reconstruction : 87.09346771240234 Val_KL : 3.479783773422241\n","EarlyStopping counter: 7 out of 100\n","Epoch: 7064/8000  Traning Loss: 90.20866775512695  Train_Reconstruction: 86.69011974334717  Train_KL: 3.5185471773147583  Validation Loss : 90.48965072631836 Val_Reconstruction : 87.01306533813477 Val_KL : 3.4765841960906982\n","EarlyStopping counter: 8 out of 100\n","Epoch: 7065/8000  Traning Loss: 90.02371501922607  Train_Reconstruction: 86.50021457672119  Train_KL: 3.523499935865402  Validation Loss : 89.53742599487305 Val_Reconstruction : 86.06726455688477 Val_KL : 3.470160484313965\n","Epoch: 7066/8000  Traning Loss: 89.66723155975342  Train_Reconstruction: 86.14788341522217  Train_KL: 3.5193455815315247  Validation Loss : 90.22827911376953 Val_Reconstruction : 86.75405502319336 Val_KL : 3.4742225408554077\n","EarlyStopping counter: 1 out of 100\n","Epoch: 7067/8000  Traning Loss: 90.28198051452637  Train_Reconstruction: 86.76179313659668  Train_KL: 3.5201850831508636  Validation Loss : 90.5667610168457 Val_Reconstruction : 87.0865249633789 Val_KL : 3.4802361726760864\n","EarlyStopping counter: 2 out of 100\n","Epoch: 7068/8000  Traning Loss: 90.13638591766357  Train_Reconstruction: 86.59869003295898  Train_KL: 3.5376961529254913  Validation Loss : 90.1842155456543 Val_Reconstruction : 86.68599700927734 Val_KL : 3.4982173442840576\n","EarlyStopping counter: 3 out of 100\n","Epoch: 7069/8000  Traning Loss: 89.90834999084473  Train_Reconstruction: 86.37690162658691  Train_KL: 3.5314483642578125  Validation Loss : 90.1691665649414 Val_Reconstruction : 86.6961898803711 Val_KL : 3.4729751348495483\n","EarlyStopping counter: 4 out of 100\n","Epoch: 7070/8000  Traning Loss: 89.60642051696777  Train_Reconstruction: 86.08727741241455  Train_KL: 3.5191416144371033  Validation Loss : 89.7436408996582 Val_Reconstruction : 86.26728057861328 Val_KL : 3.476362109184265\n","EarlyStopping counter: 5 out of 100\n","Epoch: 7071/8000  Traning Loss: 89.67174053192139  Train_Reconstruction: 86.15353393554688  Train_KL: 3.518204838037491  Validation Loss : 90.00196838378906 Val_Reconstruction : 86.53203964233398 Val_KL : 3.4699305295944214\n","EarlyStopping counter: 6 out of 100\n","Epoch: 7072/8000  Traning Loss: 89.82693099975586  Train_Reconstruction: 86.30424404144287  Train_KL: 3.5226878225803375  Validation Loss : 90.07425689697266 Val_Reconstruction : 86.5932731628418 Val_KL : 3.4809818267822266\n","EarlyStopping counter: 7 out of 100\n","Epoch: 7073/8000  Traning Loss: 89.95127964019775  Train_Reconstruction: 86.43208122253418  Train_KL: 3.5191991925239563  Validation Loss : 90.43851470947266 Val_Reconstruction : 86.96656036376953 Val_KL : 3.4719513654708862\n","EarlyStopping counter: 8 out of 100\n","Epoch: 7074/8000  Traning Loss: 90.27508640289307  Train_Reconstruction: 86.75671863555908  Train_KL: 3.518369287252426  Validation Loss : 90.19309616088867 Val_Reconstruction : 86.71278762817383 Val_KL : 3.480310082435608\n","EarlyStopping counter: 9 out of 100\n","Epoch: 7075/8000  Traning Loss: 90.06496906280518  Train_Reconstruction: 86.53398323059082  Train_KL: 3.53098601102829  Validation Loss : 90.39196395874023 Val_Reconstruction : 86.90353775024414 Val_KL : 3.488426446914673\n","EarlyStopping counter: 10 out of 100\n","Epoch: 7076/8000  Traning Loss: 90.93586444854736  Train_Reconstruction: 87.41211891174316  Train_KL: 3.523747146129608  Validation Loss : 90.93110275268555 Val_Reconstruction : 87.46242904663086 Val_KL : 3.4686745405197144\n","EarlyStopping counter: 11 out of 100\n","Epoch: 7077/8000  Traning Loss: 91.11766815185547  Train_Reconstruction: 87.6005859375  Train_KL: 3.5170831382274628  Validation Loss : 91.20201873779297 Val_Reconstruction : 87.73320388793945 Val_KL : 3.468813180923462\n","EarlyStopping counter: 12 out of 100\n","Epoch: 7078/8000  Traning Loss: 90.09763813018799  Train_Reconstruction: 86.58269309997559  Train_KL: 3.5149450600147247  Validation Loss : 89.93024826049805 Val_Reconstruction : 86.44804382324219 Val_KL : 3.4822049140930176\n","EarlyStopping counter: 13 out of 100\n","Epoch: 7079/8000  Traning Loss: 89.76182746887207  Train_Reconstruction: 86.23766040802002  Train_KL: 3.5241667330265045  Validation Loss : 89.85515213012695 Val_Reconstruction : 86.36798095703125 Val_KL : 3.4871727228164673\n","EarlyStopping counter: 14 out of 100\n","Epoch: 7080/8000  Traning Loss: 89.85688495635986  Train_Reconstruction: 86.33242416381836  Train_KL: 3.5244604349136353  Validation Loss : 89.66262817382812 Val_Reconstruction : 86.18218612670898 Val_KL : 3.4804444313049316\n","EarlyStopping counter: 15 out of 100\n","Epoch: 7081/8000  Traning Loss: 89.98925685882568  Train_Reconstruction: 86.46472930908203  Train_KL: 3.5245274901390076  Validation Loss : 90.03947830200195 Val_Reconstruction : 86.56015014648438 Val_KL : 3.4793285131454468\n","EarlyStopping counter: 16 out of 100\n","Epoch: 7082/8000  Traning Loss: 89.97677326202393  Train_Reconstruction: 86.4525146484375  Train_KL: 3.524258404970169  Validation Loss : 90.70444107055664 Val_Reconstruction : 87.23190689086914 Val_KL : 3.4725335836410522\n","EarlyStopping counter: 17 out of 100\n","Epoch: 7083/8000  Traning Loss: 90.72856998443604  Train_Reconstruction: 87.21354293823242  Train_KL: 3.5150274634361267  Validation Loss : 91.52612686157227 Val_Reconstruction : 88.0557975769043 Val_KL : 3.4703311920166016\n","EarlyStopping counter: 18 out of 100\n","Epoch: 7084/8000  Traning Loss: 90.87202739715576  Train_Reconstruction: 87.35459041595459  Train_KL: 3.5174363553524017  Validation Loss : 91.02440643310547 Val_Reconstruction : 87.54762649536133 Val_KL : 3.476778268814087\n","EarlyStopping counter: 19 out of 100\n","Epoch: 7085/8000  Traning Loss: 90.64913368225098  Train_Reconstruction: 87.13335037231445  Train_KL: 3.5157826244831085  Validation Loss : 90.62529373168945 Val_Reconstruction : 87.1491813659668 Val_KL : 3.4761111736297607\n","EarlyStopping counter: 20 out of 100\n","Epoch: 7086/8000  Traning Loss: 90.39394092559814  Train_Reconstruction: 86.87583637237549  Train_KL: 3.518105059862137  Validation Loss : 90.63388061523438 Val_Reconstruction : 87.15649795532227 Val_KL : 3.477379560470581\n","EarlyStopping counter: 21 out of 100\n","Epoch: 7087/8000  Traning Loss: 90.12995052337646  Train_Reconstruction: 86.61097717285156  Train_KL: 3.5189724564552307  Validation Loss : 89.77909469604492 Val_Reconstruction : 86.30483627319336 Val_KL : 3.4742575883865356\n","EarlyStopping counter: 22 out of 100\n","Epoch: 7088/8000  Traning Loss: 89.73271179199219  Train_Reconstruction: 86.21066284179688  Train_KL: 3.5220498740673065  Validation Loss : 90.26393508911133 Val_Reconstruction : 86.78564071655273 Val_KL : 3.4782968759536743\n","EarlyStopping counter: 23 out of 100\n","Epoch: 7089/8000  Traning Loss: 90.52388954162598  Train_Reconstruction: 87.00235080718994  Train_KL: 3.521538406610489  Validation Loss : 91.09360885620117 Val_Reconstruction : 87.62613677978516 Val_KL : 3.4674718379974365\n","EarlyStopping counter: 24 out of 100\n","Epoch: 7090/8000  Traning Loss: 90.56611442565918  Train_Reconstruction: 87.05137252807617  Train_KL: 3.5147430300712585  Validation Loss : 91.43107604980469 Val_Reconstruction : 87.96402359008789 Val_KL : 3.46705162525177\n","EarlyStopping counter: 25 out of 100\n","Epoch: 7091/8000  Traning Loss: 90.23160648345947  Train_Reconstruction: 86.71458721160889  Train_KL: 3.5170189142227173  Validation Loss : 90.29796981811523 Val_Reconstruction : 86.82682800292969 Val_KL : 3.471139907836914\n","EarlyStopping counter: 26 out of 100\n","Epoch: 7092/8000  Traning Loss: 89.7762451171875  Train_Reconstruction: 86.26404762268066  Train_KL: 3.512197643518448  Validation Loss : 89.89393997192383 Val_Reconstruction : 86.41905212402344 Val_KL : 3.474886178970337\n","EarlyStopping counter: 27 out of 100\n","Epoch: 7093/8000  Traning Loss: 89.86010360717773  Train_Reconstruction: 86.34681224822998  Train_KL: 3.5132922530174255  Validation Loss : 89.82896041870117 Val_Reconstruction : 86.36636734008789 Val_KL : 3.4625940322875977\n","EarlyStopping counter: 28 out of 100\n","Epoch: 7094/8000  Traning Loss: 89.93046569824219  Train_Reconstruction: 86.4160270690918  Train_KL: 3.5144375562667847  Validation Loss : 89.87928009033203 Val_Reconstruction : 86.3963623046875 Val_KL : 3.4829189777374268\n","EarlyStopping counter: 29 out of 100\n","Epoch: 7095/8000  Traning Loss: 89.80297946929932  Train_Reconstruction: 86.27739334106445  Train_KL: 3.5255846977233887  Validation Loss : 89.8226318359375 Val_Reconstruction : 86.34550094604492 Val_KL : 3.4771279096603394\n","EarlyStopping counter: 30 out of 100\n","Epoch: 7096/8000  Traning Loss: 89.87641334533691  Train_Reconstruction: 86.35830974578857  Train_KL: 3.5181030929088593  Validation Loss : 90.20541763305664 Val_Reconstruction : 86.72808074951172 Val_KL : 3.4773367643356323\n","EarlyStopping counter: 31 out of 100\n","Epoch: 7097/8000  Traning Loss: 89.93177318572998  Train_Reconstruction: 86.41573715209961  Train_KL: 3.516035497188568  Validation Loss : 89.92977523803711 Val_Reconstruction : 86.45866012573242 Val_KL : 3.4711148738861084\n","EarlyStopping counter: 32 out of 100\n","Epoch: 7098/8000  Traning Loss: 89.62683963775635  Train_Reconstruction: 86.11204147338867  Train_KL: 3.514798194169998  Validation Loss : 89.73786926269531 Val_Reconstruction : 86.26031875610352 Val_KL : 3.47754967212677\n","EarlyStopping counter: 33 out of 100\n","Epoch: 7099/8000  Traning Loss: 89.57756233215332  Train_Reconstruction: 86.05340957641602  Train_KL: 3.524153411388397  Validation Loss : 89.76803970336914 Val_Reconstruction : 86.28746032714844 Val_KL : 3.480578064918518\n","EarlyStopping counter: 34 out of 100\n","Epoch: 7100/8000  Traning Loss: 89.62729263305664  Train_Reconstruction: 86.10027122497559  Train_KL: 3.5270216166973114  Validation Loss : 90.08863067626953 Val_Reconstruction : 86.60766983032227 Val_KL : 3.480960488319397\n","EarlyStopping counter: 35 out of 100\n","Epoch: 7101/8000  Traning Loss: 89.80788898468018  Train_Reconstruction: 86.27656555175781  Train_KL: 3.5313242375850677  Validation Loss : 90.12667083740234 Val_Reconstruction : 86.63921356201172 Val_KL : 3.4874554872512817\n","EarlyStopping counter: 36 out of 100\n","Epoch: 7102/8000  Traning Loss: 89.86542892456055  Train_Reconstruction: 86.34390926361084  Train_KL: 3.5215191543102264  Validation Loss : 90.51166534423828 Val_Reconstruction : 87.03707122802734 Val_KL : 3.474592924118042\n","EarlyStopping counter: 37 out of 100\n","Epoch: 7103/8000  Traning Loss: 90.24060726165771  Train_Reconstruction: 86.72003841400146  Train_KL: 3.520567625761032  Validation Loss : 90.1738510131836 Val_Reconstruction : 86.68649673461914 Val_KL : 3.4873534440994263\n","EarlyStopping counter: 38 out of 100\n","Epoch: 7104/8000  Traning Loss: 89.8149356842041  Train_Reconstruction: 86.2910566329956  Train_KL: 3.5238795280456543  Validation Loss : 89.8205795288086 Val_Reconstruction : 86.34009170532227 Val_KL : 3.480487108230591\n","EarlyStopping counter: 39 out of 100\n","Epoch: 7105/8000  Traning Loss: 89.75282001495361  Train_Reconstruction: 86.23029136657715  Train_KL: 3.5225285291671753  Validation Loss : 90.10185623168945 Val_Reconstruction : 86.61791229248047 Val_KL : 3.483945608139038\n","EarlyStopping counter: 40 out of 100\n","Epoch: 7106/8000  Traning Loss: 89.7462272644043  Train_Reconstruction: 86.22046852111816  Train_KL: 3.525759369134903  Validation Loss : 90.01993179321289 Val_Reconstruction : 86.52861785888672 Val_KL : 3.4913134574890137\n","EarlyStopping counter: 41 out of 100\n","Epoch: 7107/8000  Traning Loss: 89.68579292297363  Train_Reconstruction: 86.16446208953857  Train_KL: 3.5213299691677094  Validation Loss : 90.0573844909668 Val_Reconstruction : 86.58275604248047 Val_KL : 3.4746302366256714\n","EarlyStopping counter: 42 out of 100\n","Epoch: 7108/8000  Traning Loss: 89.5301742553711  Train_Reconstruction: 86.01488018035889  Train_KL: 3.5152948200702667  Validation Loss : 89.54694366455078 Val_Reconstruction : 86.07287979125977 Val_KL : 3.4740638732910156\n","EarlyStopping counter: 43 out of 100\n","Epoch: 7109/8000  Traning Loss: 89.60535621643066  Train_Reconstruction: 86.08220481872559  Train_KL: 3.5231519043445587  Validation Loss : 89.90296936035156 Val_Reconstruction : 86.42151260375977 Val_KL : 3.4814547300338745\n","EarlyStopping counter: 44 out of 100\n","Epoch: 7110/8000  Traning Loss: 90.15278720855713  Train_Reconstruction: 86.6251745223999  Train_KL: 3.527612805366516  Validation Loss : 90.91222381591797 Val_Reconstruction : 87.43199920654297 Val_KL : 3.480224370956421\n","EarlyStopping counter: 45 out of 100\n","Epoch: 7111/8000  Traning Loss: 90.58365154266357  Train_Reconstruction: 87.06027507781982  Train_KL: 3.5233775973320007  Validation Loss : 90.77332305908203 Val_Reconstruction : 87.29301834106445 Val_KL : 3.480304479598999\n","EarlyStopping counter: 46 out of 100\n","Epoch: 7112/8000  Traning Loss: 89.94217777252197  Train_Reconstruction: 86.40418434143066  Train_KL: 3.537992388010025  Validation Loss : 89.78301620483398 Val_Reconstruction : 86.29215240478516 Val_KL : 3.4908610582351685\n","EarlyStopping counter: 47 out of 100\n","Epoch: 7113/8000  Traning Loss: 89.677565574646  Train_Reconstruction: 86.14301681518555  Train_KL: 3.53454926609993  Validation Loss : 89.90241622924805 Val_Reconstruction : 86.41983413696289 Val_KL : 3.4825809001922607\n","EarlyStopping counter: 48 out of 100\n","Epoch: 7114/8000  Traning Loss: 89.67809104919434  Train_Reconstruction: 86.1525182723999  Train_KL: 3.5255725383758545  Validation Loss : 89.97409057617188 Val_Reconstruction : 86.49461364746094 Val_KL : 3.4794763326644897\n","EarlyStopping counter: 49 out of 100\n","Epoch: 7115/8000  Traning Loss: 89.81431007385254  Train_Reconstruction: 86.28820991516113  Train_KL: 3.526098072528839  Validation Loss : 90.03211212158203 Val_Reconstruction : 86.55310440063477 Val_KL : 3.479008674621582\n","EarlyStopping counter: 50 out of 100\n","Epoch: 7116/8000  Traning Loss: 89.81147480010986  Train_Reconstruction: 86.28936290740967  Train_KL: 3.5221105217933655  Validation Loss : 89.9712028503418 Val_Reconstruction : 86.49154663085938 Val_KL : 3.479657769203186\n","EarlyStopping counter: 51 out of 100\n","Epoch: 7117/8000  Traning Loss: 89.92780494689941  Train_Reconstruction: 86.40203762054443  Train_KL: 3.525766521692276  Validation Loss : 90.12151336669922 Val_Reconstruction : 86.63788223266602 Val_KL : 3.4836326837539673\n","EarlyStopping counter: 52 out of 100\n","Epoch: 7118/8000  Traning Loss: 89.81929016113281  Train_Reconstruction: 86.29053401947021  Train_KL: 3.5287559926509857  Validation Loss : 90.11770629882812 Val_Reconstruction : 86.64298629760742 Val_KL : 3.474720597267151\n","EarlyStopping counter: 53 out of 100\n","Epoch: 7119/8000  Traning Loss: 89.94273662567139  Train_Reconstruction: 86.42436790466309  Train_KL: 3.5183675587177277  Validation Loss : 90.12222671508789 Val_Reconstruction : 86.64933395385742 Val_KL : 3.4728896617889404\n","EarlyStopping counter: 54 out of 100\n","Epoch: 7120/8000  Traning Loss: 89.92719745635986  Train_Reconstruction: 86.40077590942383  Train_KL: 3.5264210999011993  Validation Loss : 90.43912506103516 Val_Reconstruction : 86.94673538208008 Val_KL : 3.4923884868621826\n","EarlyStopping counter: 55 out of 100\n","Epoch: 7121/8000  Traning Loss: 89.81035327911377  Train_Reconstruction: 86.27672386169434  Train_KL: 3.533628225326538  Validation Loss : 90.03996658325195 Val_Reconstruction : 86.55696487426758 Val_KL : 3.4830031394958496\n","EarlyStopping counter: 56 out of 100\n","Epoch: 7122/8000  Traning Loss: 89.6273775100708  Train_Reconstruction: 86.10971546173096  Train_KL: 3.5176627337932587  Validation Loss : 89.74856948852539 Val_Reconstruction : 86.2790412902832 Val_KL : 3.469527840614319\n","EarlyStopping counter: 57 out of 100\n","Epoch: 7123/8000  Traning Loss: 89.66207885742188  Train_Reconstruction: 86.1408052444458  Train_KL: 3.5212734937667847  Validation Loss : 89.88885879516602 Val_Reconstruction : 86.40251922607422 Val_KL : 3.486340284347534\n","EarlyStopping counter: 58 out of 100\n","Epoch: 7124/8000  Traning Loss: 89.7497968673706  Train_Reconstruction: 86.22351169586182  Train_KL: 3.5262863636016846  Validation Loss : 90.13744735717773 Val_Reconstruction : 86.64955520629883 Val_KL : 3.487894892692566\n","EarlyStopping counter: 59 out of 100\n","Epoch: 7125/8000  Traning Loss: 90.23481559753418  Train_Reconstruction: 86.71064567565918  Train_KL: 3.5241684019565582  Validation Loss : 90.71868133544922 Val_Reconstruction : 87.23697280883789 Val_KL : 3.481707811355591\n","EarlyStopping counter: 60 out of 100\n","Epoch: 7126/8000  Traning Loss: 90.82298278808594  Train_Reconstruction: 87.30380916595459  Train_KL: 3.519172817468643  Validation Loss : 90.90378189086914 Val_Reconstruction : 87.4302864074707 Val_KL : 3.4734957218170166\n","EarlyStopping counter: 61 out of 100\n","Epoch: 7127/8000  Traning Loss: 90.0439453125  Train_Reconstruction: 86.52560806274414  Train_KL: 3.518337309360504  Validation Loss : 89.82478332519531 Val_Reconstruction : 86.34080505371094 Val_KL : 3.4839770793914795\n","EarlyStopping counter: 62 out of 100\n","Epoch: 7128/8000  Traning Loss: 89.62040328979492  Train_Reconstruction: 86.0940523147583  Train_KL: 3.5263507664203644  Validation Loss : 89.88433837890625 Val_Reconstruction : 86.40361404418945 Val_KL : 3.480723261833191\n","EarlyStopping counter: 63 out of 100\n","Epoch: 7129/8000  Traning Loss: 89.68369960784912  Train_Reconstruction: 86.16337013244629  Train_KL: 3.5203285217285156  Validation Loss : 90.02632904052734 Val_Reconstruction : 86.55788040161133 Val_KL : 3.46844744682312\n","EarlyStopping counter: 64 out of 100\n","Epoch: 7130/8000  Traning Loss: 89.52599906921387  Train_Reconstruction: 86.0154676437378  Train_KL: 3.5105311274528503  Validation Loss : 89.66051483154297 Val_Reconstruction : 86.18832397460938 Val_KL : 3.472190260887146\n","EarlyStopping counter: 65 out of 100\n","Epoch: 7131/8000  Traning Loss: 89.47628593444824  Train_Reconstruction: 85.95712661743164  Train_KL: 3.519158720970154  Validation Loss : 89.62625885009766 Val_Reconstruction : 86.14620208740234 Val_KL : 3.4800578355789185\n","EarlyStopping counter: 66 out of 100\n","Epoch: 7132/8000  Traning Loss: 89.57399654388428  Train_Reconstruction: 86.04490375518799  Train_KL: 3.5290921926498413  Validation Loss : 89.9465560913086 Val_Reconstruction : 86.46669387817383 Val_KL : 3.479863405227661\n","EarlyStopping counter: 67 out of 100\n","Epoch: 7133/8000  Traning Loss: 89.77629947662354  Train_Reconstruction: 86.25944423675537  Train_KL: 3.5168569684028625  Validation Loss : 90.095703125 Val_Reconstruction : 86.63271713256836 Val_KL : 3.462984085083008\n","EarlyStopping counter: 68 out of 100\n","Epoch: 7134/8000  Traning Loss: 89.87092876434326  Train_Reconstruction: 86.35349655151367  Train_KL: 3.517432302236557  Validation Loss : 90.01669311523438 Val_Reconstruction : 86.53761672973633 Val_KL : 3.479074001312256\n","EarlyStopping counter: 69 out of 100\n","Epoch: 7135/8000  Traning Loss: 89.76369094848633  Train_Reconstruction: 86.23463344573975  Train_KL: 3.5290579795837402  Validation Loss : 89.90528869628906 Val_Reconstruction : 86.42866516113281 Val_KL : 3.476623058319092\n","EarlyStopping counter: 70 out of 100\n","Epoch: 7136/8000  Traning Loss: 89.72100257873535  Train_Reconstruction: 86.2006664276123  Train_KL: 3.5203367173671722  Validation Loss : 90.07672882080078 Val_Reconstruction : 86.59869003295898 Val_KL : 3.478038787841797\n","EarlyStopping counter: 71 out of 100\n","Epoch: 7137/8000  Traning Loss: 89.91091728210449  Train_Reconstruction: 86.39030265808105  Train_KL: 3.5206146240234375  Validation Loss : 90.66748809814453 Val_Reconstruction : 87.19010925292969 Val_KL : 3.4773776531219482\n","EarlyStopping counter: 72 out of 100\n","Epoch: 7138/8000  Traning Loss: 90.69392490386963  Train_Reconstruction: 87.1699047088623  Train_KL: 3.524019420146942  Validation Loss : 91.01580047607422 Val_Reconstruction : 87.5365219116211 Val_KL : 3.479278802871704\n","EarlyStopping counter: 73 out of 100\n","Epoch: 7139/8000  Traning Loss: 91.2501630783081  Train_Reconstruction: 87.72391986846924  Train_KL: 3.5262429118156433  Validation Loss : 91.48862075805664 Val_Reconstruction : 88.00538635253906 Val_KL : 3.483233094215393\n","EarlyStopping counter: 74 out of 100\n","Epoch: 7140/8000  Traning Loss: 90.89521408081055  Train_Reconstruction: 87.37207412719727  Train_KL: 3.523138612508774  Validation Loss : 90.69266891479492 Val_Reconstruction : 87.21044540405273 Val_KL : 3.4822229146957397\n","EarlyStopping counter: 75 out of 100\n","Epoch: 7141/8000  Traning Loss: 90.79086303710938  Train_Reconstruction: 87.26590061187744  Train_KL: 3.524961918592453  Validation Loss : 90.66848754882812 Val_Reconstruction : 87.18955993652344 Val_KL : 3.4789273738861084\n","EarlyStopping counter: 76 out of 100\n","Epoch: 7142/8000  Traning Loss: 89.95293235778809  Train_Reconstruction: 86.44064521789551  Train_KL: 3.5122880935668945  Validation Loss : 89.69065856933594 Val_Reconstruction : 86.2236442565918 Val_KL : 3.4670119285583496\n","EarlyStopping counter: 77 out of 100\n","Epoch: 7143/8000  Traning Loss: 89.80333137512207  Train_Reconstruction: 86.28890991210938  Train_KL: 3.514420598745346  Validation Loss : 90.23548126220703 Val_Reconstruction : 86.75656509399414 Val_KL : 3.4789143800735474\n","EarlyStopping counter: 78 out of 100\n","Epoch: 7144/8000  Traning Loss: 90.38186454772949  Train_Reconstruction: 86.85985946655273  Train_KL: 3.522005796432495  Validation Loss : 90.798095703125 Val_Reconstruction : 87.31797790527344 Val_KL : 3.480114698410034\n","EarlyStopping counter: 79 out of 100\n","Epoch: 7145/8000  Traning Loss: 90.23334312438965  Train_Reconstruction: 86.71913433074951  Train_KL: 3.5142078399658203  Validation Loss : 89.83095932006836 Val_Reconstruction : 86.3649787902832 Val_KL : 3.465980291366577\n","EarlyStopping counter: 80 out of 100\n","Epoch: 7146/8000  Traning Loss: 89.6734848022461  Train_Reconstruction: 86.16226387023926  Train_KL: 3.511220306158066  Validation Loss : 89.74394989013672 Val_Reconstruction : 86.27042388916016 Val_KL : 3.473524808883667\n","EarlyStopping counter: 81 out of 100\n","Epoch: 7147/8000  Traning Loss: 89.522705078125  Train_Reconstruction: 85.99985599517822  Train_KL: 3.522850453853607  Validation Loss : 89.86307907104492 Val_Reconstruction : 86.38586044311523 Val_KL : 3.4772216081619263\n","EarlyStopping counter: 82 out of 100\n","Epoch: 7148/8000  Traning Loss: 89.74950885772705  Train_Reconstruction: 86.22965049743652  Train_KL: 3.51985701918602  Validation Loss : 90.02937316894531 Val_Reconstruction : 86.55438995361328 Val_KL : 3.4749835729599\n","EarlyStopping counter: 83 out of 100\n","Epoch: 7149/8000  Traning Loss: 89.75464248657227  Train_Reconstruction: 86.23678588867188  Train_KL: 3.5178569853305817  Validation Loss : 89.95573043823242 Val_Reconstruction : 86.47833251953125 Val_KL : 3.477396011352539\n","EarlyStopping counter: 84 out of 100\n","Epoch: 7150/8000  Traning Loss: 89.64767742156982  Train_Reconstruction: 86.12109470367432  Train_KL: 3.526583343744278  Validation Loss : 89.92378234863281 Val_Reconstruction : 86.43233871459961 Val_KL : 3.4914432764053345\n","EarlyStopping counter: 85 out of 100\n","Epoch: 7151/8000  Traning Loss: 89.71036911010742  Train_Reconstruction: 86.18415546417236  Train_KL: 3.5262127816677094  Validation Loss : 89.92599868774414 Val_Reconstruction : 86.44786834716797 Val_KL : 3.4781302213668823\n","EarlyStopping counter: 86 out of 100\n","Epoch: 7152/8000  Traning Loss: 89.75486850738525  Train_Reconstruction: 86.23685455322266  Train_KL: 3.5180139243602753  Validation Loss : 90.04207611083984 Val_Reconstruction : 86.56192779541016 Val_KL : 3.4801498651504517\n","EarlyStopping counter: 87 out of 100\n","Epoch: 7153/8000  Traning Loss: 90.04610347747803  Train_Reconstruction: 86.52791690826416  Train_KL: 3.5181859731674194  Validation Loss : 90.46832656860352 Val_Reconstruction : 86.98589706420898 Val_KL : 3.4824286699295044\n","EarlyStopping counter: 88 out of 100\n","Epoch: 7154/8000  Traning Loss: 90.09575939178467  Train_Reconstruction: 86.56651401519775  Train_KL: 3.5292445719242096  Validation Loss : 90.28911972045898 Val_Reconstruction : 86.80509185791016 Val_KL : 3.484029531478882\n","EarlyStopping counter: 89 out of 100\n","Epoch: 7155/8000  Traning Loss: 90.31276226043701  Train_Reconstruction: 86.79009628295898  Train_KL: 3.522665709257126  Validation Loss : 90.99970245361328 Val_Reconstruction : 87.5234146118164 Val_KL : 3.476285934448242\n","EarlyStopping counter: 90 out of 100\n","Epoch: 7156/8000  Traning Loss: 90.41600322723389  Train_Reconstruction: 86.90393447875977  Train_KL: 3.512068808078766  Validation Loss : 90.53742599487305 Val_Reconstruction : 87.07019424438477 Val_KL : 3.4672333002090454\n","EarlyStopping counter: 91 out of 100\n","Epoch: 7157/8000  Traning Loss: 90.36866569519043  Train_Reconstruction: 86.85223960876465  Train_KL: 3.5164251625537872  Validation Loss : 91.10088348388672 Val_Reconstruction : 87.62117004394531 Val_KL : 3.47971510887146\n","EarlyStopping counter: 92 out of 100\n","Epoch: 7158/8000  Traning Loss: 90.33667087554932  Train_Reconstruction: 86.81085777282715  Train_KL: 3.5258139967918396  Validation Loss : 90.40055465698242 Val_Reconstruction : 86.9129753112793 Val_KL : 3.487578511238098\n","EarlyStopping counter: 93 out of 100\n","Epoch: 7159/8000  Traning Loss: 89.96614456176758  Train_Reconstruction: 86.43584251403809  Train_KL: 3.530303120613098  Validation Loss : 89.88653945922852 Val_Reconstruction : 86.39671325683594 Val_KL : 3.4898282289505005\n","EarlyStopping counter: 94 out of 100\n","Epoch: 7160/8000  Traning Loss: 89.90227222442627  Train_Reconstruction: 86.37209892272949  Train_KL: 3.5301729142665863  Validation Loss : 90.0462760925293 Val_Reconstruction : 86.57027053833008 Val_KL : 3.4760024547576904\n","EarlyStopping counter: 95 out of 100\n","Epoch: 7161/8000  Traning Loss: 89.84403991699219  Train_Reconstruction: 86.33029651641846  Train_KL: 3.5137429535388947  Validation Loss : 89.89463806152344 Val_Reconstruction : 86.42865371704102 Val_KL : 3.4659839868545532\n","EarlyStopping counter: 96 out of 100\n","Epoch: 7162/8000  Traning Loss: 89.77348041534424  Train_Reconstruction: 86.25642395019531  Train_KL: 3.5170571506023407  Validation Loss : 89.82452774047852 Val_Reconstruction : 86.34060668945312 Val_KL : 3.4839221239089966\n","EarlyStopping counter: 97 out of 100\n","Epoch: 7163/8000  Traning Loss: 89.7337703704834  Train_Reconstruction: 86.20174217224121  Train_KL: 3.532029241323471  Validation Loss : 89.84473037719727 Val_Reconstruction : 86.35243225097656 Val_KL : 3.4922975301742554\n","EarlyStopping counter: 98 out of 100\n","Epoch: 7164/8000  Traning Loss: 89.59335899353027  Train_Reconstruction: 86.06433868408203  Train_KL: 3.529019743204117  Validation Loss : 89.81510543823242 Val_Reconstruction : 86.32852935791016 Val_KL : 3.4865764379501343\n","EarlyStopping counter: 99 out of 100\n","Epoch: 7165/8000  Traning Loss: 89.6775312423706  Train_Reconstruction: 86.15827941894531  Train_KL: 3.51925328373909  Validation Loss : 89.73540496826172 Val_Reconstruction : 86.27089309692383 Val_KL : 3.4645127058029175\n","EarlyStopping counter: 100 out of 100\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","optimizer1 = torch.optim.Adam\n","optimizer2 = torch.optim.Adadelta\n","\n","batch_size = 1600\n","print(data_input.shape)\n","\n","\n","epoch = 0\n","\n","model = VAE_MSE(z_dim =5 , device = device).to(device)\n","epoch = train(model=model, optimizer=optimizer1, epochs=8000, batch_size = batch_size,\\\n","          lr=0.0005, data_input=data_input, loss_fig_name=\"MSE_z5_0.0005_earlyfrom7000\")\n","     \n","path = \"/home/igari/igari/VAE_本へ/checkpoint.pt\"\n","model = VAE_MSE(z_dim = 5, device = 'cpu')\n","model.load_state_dict(torch.load(path, map_location=torch.device('cpu')))\n","     \n","path = \"learned_model/epoch7000_z5_0.0005_earlyfrom3000_weight.pth\"\n","torch.save(model.to('cpu').state_dict(), path)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["path = \"/home/igari/igari/VAE_本へ/checkpoint.pt\"\n","model = VAE_MSE(z_dim = 5, device = 'cpu')\n","model.load_state_dict(torch.load(path, map_location=torch.device('cpu')))\n","     \n","path = \"learned_model/zdim5_weight.pth\"\n","torch.save(model.to('cpu').state_dict(), path)"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":630,"status":"ok","timestamp":1676625126825,"user":{"displayName":"猪狩高","userId":"00742343053661794311"},"user_tz":-540},"id":"y1ol8Jhp98s2","outputId":"478fe491-1357-4573-be47-9ed2303d981c"},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["#学習済みモデルの読み込み\n","path = \"learned_model/epoch30000_beta100_earlyfrom2000_Adam_weight.pth\"\n","model = VAE_MSE(z_dim = 4, device = 'cpu')\n","model.load_state_dict(torch.load(path, map_location=torch.device('cpu')))\n","#model = torch.load(\"/content/drive/MyDrive/VAE/model_weight.pth\")"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"ename":"RuntimeError","evalue":"Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-41-9506659965e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVAE_MSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m~/.local/share/virtualenvs/VAE_本へ-eDH28Mmx/lib/python3.6/site-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# make a forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# remove these hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/share/virtualenvs/VAE_本へ-eDH28Mmx/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-a459f0a5a155>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreparameterize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/share/virtualenvs/VAE_本へ-eDH28Mmx/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1069\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             for hook in itertools.chain(\n","\u001b[0;32m<ipython-input-10-a459f0a5a155>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn_layer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn_layer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn_layer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/share/virtualenvs/VAE_本へ-eDH28Mmx/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/share/virtualenvs/VAE_本へ-eDH28Mmx/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/share/virtualenvs/VAE_本へ-eDH28Mmx/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1069\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             for hook in itertools.chain(\n","\u001b[0;32m~/.local/share/virtualenvs/VAE_本へ-eDH28Mmx/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/share/virtualenvs/VAE_本へ-eDH28Mmx/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    439\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 440\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"]}],"source":["net = VAE_MSE(z_dim=4, device='cpu').to('cpu')\n","summary(net, (3, 24, 32))"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":472},"executionInfo":{"elapsed":1438,"status":"ok","timestamp":1676625130649,"user":{"displayName":"猪狩高","userId":"00742343053661794311"},"user_tz":-540},"id":"SH3yGWGl98s3","outputId":"3973a5fe-d646-4737-dbc3-9c8d9ac67ab3"},"outputs":[{"name":"stdout","output_type":"stream","text":["input_image\n","(10, 3, 24, 32)\n","0.0\n","1.0\n","renconst_image\n","(10, 3, 24, 32)\n","0.052778944\n","0.9957159\n"]},{"name":"stderr","output_type":"stream","text":["/home/igari/.local/share/virtualenvs/VAE_本へ-eDH28Mmx/lib/python3.6/site-packages/ipykernel_launcher.py:33: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 10 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["generate_xy_Image(model, data_input, fig_name=\"MSEepoch2000_z2_10e5_halfscale_weight.pth\")"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":170497,"status":"ok","timestamp":1676625623592,"user":{"displayName":"猪狩高","userId":"00742343053661794311"},"user_tz":-540},"id":"fdR6PaFEwngI","outputId":"125e917a-f2f0-48af-97fe-28c67cb9ec60"},"outputs":[],"source":["#data(50, 160, 2) -> return latent(160, 2)\n","def cal_step_meanlatent(data):\n","    z_mean_latent = torch.zeros(data.shape[1], 2)\n","    data = torch.from_numpy(data)\n","\n","    for i in range(data.shape[1]):\n","        zs_step_mean = torch.mean(data[:, i, :], axis = 0).reshape([-1, 2])           \n","        z_mean_latent[i] = zs_step_mean\n","\n","    return z_mean_latent#(160, 2)      \n","\n","\n","def cal_task_latent(model, image_data):\n","    device = 'cpu'\n","    \n","    image_data = image_data.reshape(image_data.shape[0]*image_data.shape[1], 3, 24, 32)\n","    model.eval()\n","    image_data = torch.from_numpy(image_data).float()\n","    image_data = image_data.to(device)\n","    with torch.no_grad():\n","        z_latent, _ = model.forward(image_data)#(16000, 2)\n","\n","    z_latent_pca, explain_variance_ratio = pca(z_latent, n_components=2)\n","    \n","    left_task_z_pca = z_latent_pca[:int(z_latent_pca.shape[0]/2), :]#(8000, 2)\n","    right_task_z_pca = z_latent_pca[int(z_latent_pca.shape[0]/2) :, :]#(8000, 2)\n","    left_task_z_pca = left_task_z_pca.reshape(50, 160, 2)\n","    right_task_z_pca = right_task_z_pca.reshape(50, 160, 2)\n","    print(\"z_left_latent_pca\")\n","\n","    left_task_mean_latent = cal_step_meanlatent(left_task_z_pca)\n","    right_task_mean_latent = cal_step_meanlatent(right_task_z_pca)\n","    \n","    return left_task_mean_latent, right_task_mean_latent, explain_variance_ratio\n","\n","def pca(z, n_components):\n","    pca = PCA(n_components).fit(z)\n","    return pca.fit_transform(z), pca.explained_variance_ratio_\n","\n","\n","#散布図にカラーマップ、ラベルをつけたりする\n","# https://python-academia.com/matplotlib-scatter/\n","#scatterとplotの二つを同時に適用できる\n","def visualize_2task_pca(left_pca,right_pca, explain_variance_ratio, fig_name):\n","    labels = [str(10*num) for num in range(1,int(left_pca.shape[0]/10)+1)]\n","    \n","    fig = plt.figure()\n","    ax = fig.add_subplot()\n","    ax.scatter(left_pca[:, 0], left_pca[:, 1])\n","    ax.scatter(right_pca[:, 0], right_pca[:, 1])\n","    ax.plot(left_pca[:, 0], left_pca[:, 1])\n","    ax.plot(right_pca[:, 0], right_pca[:, 1])\n","    ax.set_xlabel(\"first_pricipal_component(variance_ratio : {})\".format(explain_variance_ratio[0]))\n","    ax.set_ylabel(\"second_pricipal_component(variance_ratio : {})\".format(explain_variance_ratio[1]))\n","    ax.legend(['left_task', 'right_task'])\n","    ax.set_title('pca')\n","\n","    for i, label in enumerate(labels):\n","        ax.text(left_pca[10*i, 0], left_pca[10*i, 1],label)\n","        ax.text(right_pca[10*i, 0], right_pca[10*i, 1],label)\n","    plt.savefig(fig_name + \".png\")\n","\n","def visualize_2task_time_horizontal(task1_pca1, task_pca2, fig_name):\n","    fig = plt.figure()\n","    plt.subplots_adjust(hspace=0.6)\n","\n","    ax1 = fig.add_subplot(2, 1, 1)\n","    ax1.set_title(\"first_pricipal_component\")\n","    ax1.set_xlim(0, 160)\n","    # ax1.scatter(task1_pca1[:, 0])\n","    # ax1.scatter(task_pca2[:, 0])\n","    ax1.plot(task1_pca1[:, 0])\n","    ax1.plot(task_pca2[:, 0])\n","    ax1.legend(['task1', 'task2'])\n","\n","    ax2 = fig.add_subplot(2, 1, 2)\n","    ax2.set_title(\"second_pricipal_component\")\n","    ax2.set_xlim(0, 160)\n","    # ax2.scatter(task1_pca1[:, 1])\n","    # ax2.scatter(task_pca2[:, 1])\n","    ax2.plot(task1_pca1[:, 1])\n","    ax2.plot(task_pca2[:, 1])\n","    ax2.legend(['task1', 'task2'])\n","\n","    plt.savefig(fig_name + \"time_step.png\")\n","\n"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"elapsed":761,"status":"ok","timestamp":1676625652926,"user":{"displayName":"猪狩高","userId":"00742343053661794311"},"user_tz":-540},"id":"B1b4oDFAjDnJ","outputId":"5df968d4-5dd8-4a4f-82e1-848ba84145f2"},"outputs":[{"name":"stdout","output_type":"stream","text":["z_left_latent_pca\n","z_left : torch.Size([160, 2])\n","z_right : torch.Size([160, 2])\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["#それぞれのタスクの潜在表現を出力(160ステップ)\n","\n","z_left_latent_pca, z_right_latent_pca, explain_variabce_ratio = cal_task_latent(model, data_narray.reshape(100,160, 3, 24, 32))\n","print(\"z_left : {}\".format(z_left_latent_pca.shape))\n","print(\"z_right : {}\".format(z_right_latent_pca.shape))\n","\n","\n","visualize_2task_pca(z_left_latent_pca,z_right_latent_pca,explain_variabce_ratio,\"z_latent_z5\")"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["visualize_2task_time_horizontal(z_left_latent_pca, z_right_latent_pca, \"z_latent_z5\")"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"VAE_本へ-eDH28Mmx","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"1cb4802709f4f7cbf5a0aa9caa98b680fa8ceb0de261b345f27c3835aad28ada"}}},"nbformat":4,"nbformat_minor":0}
