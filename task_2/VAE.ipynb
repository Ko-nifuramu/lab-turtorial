{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1676627634455,"user":{"displayName":"猪狩高","userId":"00742343053661794311"},"user_tz":-540},"id":"Mv-yPolM98su"},"outputs":[{"name":"stdout","output_type":"stream","text":["cpu\n"]},{"data":{"text/plain":["<torch._C.Generator at 0x1146fe990>"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","import matplotlib.pyplot as plt\n","import math\n","import os\n","from torchsummary import summary\n","from sklearn.decomposition import PCA\n","\n","seed =2\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(device)\n","gpu_id = 0\n","device = torch.device(device, gpu_id)\n","\n","if device =='cuda : 0':\n","    torch.cuda.manual_seed(seed)\n","\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":597,"status":"ok","timestamp":1676627652570,"user":{"displayName":"猪狩高","userId":"00742343053661794311"},"user_tz":-540},"id":"dAqam4Rq98sy","outputId":"028b27ae-ee27-420f-c8d6-001f6f988cf4"},"outputs":[{"name":"stdout","output_type":"stream","text":["original_data.shape = (100, 160, 24, 32, 3)\n","input_data.shape = (16000, 3, 24, 32)\n","1.0\n","0.0\n"]}],"source":["#データの取り込み\n","data_narray = np.load(\"image_states.npy\")\n","print(\"original_data.shape = {}\".format(data_narray.shape))\n","\n","#cnn入力用にreshapeする\n","data_input = np.reshape(data_narray, (-1, 3, 24, 32))\n","print(\"input_data.shape = {}\".format(data_input.shape))\n","print(np.max(data_input))\n","print(np.min(data_input))"]},{"cell_type":"code","execution_count":4,"metadata":{"cellView":"form","executionInfo":{"elapsed":211,"status":"ok","timestamp":1676627654460,"user":{"displayName":"猪狩高","userId":"00742343053661794311"},"user_tz":-540},"id":"kjfvC46H98sz"},"outputs":[],"source":["#@title データ定義\n","class MyDataset(torch.utils.data.Dataset):\n","    def __init__(self, input_data):\n","        self.input_data = torch.from_numpy(input_data).float()\n","\n","    def __len__(self):\n","        return self.input_data.shape[0]\n","\n","    def __getitem__(self, idx):\n","        input_datum = self.input_data[idx]#+torch.normal(mean=0, std = 1, size=self.input_data[idx].shape)<-入力0-1の外の値も入力してしまう\n","        return input_datum\n","\n","def create_dataloader(batch_size, input_data):\n","\n","    indeces = [int(input_data.shape[0] * n) for n in [0.4, 0.4+0.1, 0.9]]\n","    train_data1, val_data1, train_data2, val_data2 = np.split(input_data, indeces, axis=0)\n","    train_data = np.concatenate([train_data1, train_data2], axis=0)\n","    val_data = np.concatenate([val_data1, val_data2], axis=0)\n","\n","    train_dataset = MyDataset(train_data)\n","    val_dataset = MyDataset(val_data)\n","    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n","    \n","    return train_dataloader, val_dataloader\n","\n","def visualize_loss(epochs, dict_eachLoss, fig_name):\n","    fig = plt.figure()\n","    ax = fig.add_subplot()\n","    ax.plot(dict_eachLoss['train_recon'], linestyle=\"solid\")\n","    ax.plot(dict_eachLoss['train_kl'], linestyle=\"dashed\")\n","    ax.plot(dict_eachLoss['train_loss'], linestyle = \"dotted\")\n","    ax.plot(dict_eachLoss['val_recon'], linestyle=\"solid\")\n","    ax.plot(dict_eachLoss['val_kl'], linestyle=\"dashed\")\n","    ax.plot(dict_eachLoss['val_loss'], linestyle = \"dotted\")\n","    ax.set_yscale('log')\n","    ax.legend(['train_recon_loss * weight_lamda', 'train_kld', 'train_loss', 'val_recon * weight_lamda', 'val_kld', 'val_loss'],\\\n","              loc='center left', bbox_to_anchor=(1., .5))\n","    \n","\n","    ax.set_xlim(0, epochs)\n","    ax.set_title('each_loss')\n","    fig.savefig(fig_name + \".png\")\n","\n","\n","    {'train_loss' : [], 'train_recon' : [], 'train_kl' : [], 'val_loss' : [], 'val_recon' : [], 'val_kl' : [],}"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["class EarlyStopping:\n","    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n","    def __init__(self, patience=100, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n","        \"\"\"\n","        Args:\n","            patience (int): How long to wait after last time validation loss improved.\n","                            Default: 7\n","            verbose (bool): If True, prints a message for each validation loss improvement. \n","                            Default: False\n","            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n","                            Default: 0\n","            path (str): Path for the checkpoint to be saved to.\n","                            Default: 'checkpoint.pt'\n","            trace_func (function): trace print function.\n","                            Default: print            \n","        \"\"\"\n","        self.patience = patience\n","        self.verbose = verbose\n","        self.counter = 0\n","        self.best_score = None\n","        self.early_stop = False\n","        self.val_loss_min = np.Inf\n","        self.delta = delta\n","        self.path = path\n","        self.trace_func = trace_func\n","    def __call__(self, val_loss, model):\n","\n","        score = -val_loss\n","\n","        if self.best_score is None:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","        elif score < self.best_score + self.delta:\n","            self.counter += 1\n","            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","        else:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","            self.counter = 0\n","\n","    def save_checkpoint(self, val_loss, model):\n","        '''Saves model when validation loss decrease.'''\n","        if self.verbose:\n","            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n","        torch.save(model.state_dict(), self.path)\n","        self.val_loss_min = val_loss\n"]},{"cell_type":"code","execution_count":6,"metadata":{"cellView":"form","executionInfo":{"elapsed":226,"status":"ok","timestamp":1676627655744,"user":{"displayName":"猪狩高","userId":"00742343053661794311"},"user_tz":-540},"id":"T2Jw0XJ498s0"},"outputs":[],"source":["#@title モデル定義\n","# x -> (encoder) -> z_latent -> (decoder) -> y\n","#method : forward, reparameterize, cal_loss\n","class VAE_BCE(nn.Module):\n","    def __init__(self, z_dim,device):\n","        super(VAE_BCE, self).__init__()\n","        self.device = device\n","        self.encoder = VAE_Encoder(z_dim)\n","        self.decoder = VAE_Decoder(z_dim)\n","        self.z_dim = z_dim\n","\n","    def forward(self, x):\n","        mean, log_var = self.encoder(x)\n","        z = self.reparameterize(mean, log_var)\n","        y = self.decoder(z)\n","        return y, z\n","\n","    def reparameterize(self, mean, var):\n","        z = (mean + torch.mul(torch.sqrt(torch.exp(var)), torch.normal(mean = 0, std=1, size=mean.shape).to(self.device)))\n","        return z\n","\n","    def cal_loss(self, x, criterion = nn.BCELoss()):\n","        \n","        mean, log_var = self.encoder.forward(x)\n","        #print(\"var{}\".format(var))\n","        z = self.reparameterize(mean, log_var)\n","        y = self.decoder.forward(z)\n","        x = x.view(x.size(0), -1)\n","        y = y.view(y.size(0), -1)\n","\n","        #変分下限Lの最大化　-> -Lの最小化\n","        reconstruction = criterion(y , x)\n","        kl = -torch.sum(1+log_var- mean**2 - torch.exp(log_var))/2\n","        #print(\"reconstruction : {}\".format(reconstruction.shape))\n","        #print(\"KL : {}\".format(kl.shape))\n","        loss = reconstruction + kl\n","        return loss, reconstruction, kl \n","\n","class VAE_MSE(nn.Module):\n","    def __init__(self, z_dim,device):\n","        super(VAE_MSE, self).__init__()\n","        self.device = device\n","        self.encoder = VAE_Encoder(z_dim)\n","        self.decoder = VAE_Decoder(z_dim)\n","        self.z_dim = z_dim\n","        self.beta = 1\n","        self.var = 1e-2\n","\n","    def forward(self, x):\n","        mean, log_var = self.encoder(x)\n","        z = self.reparameterize(mean, log_var)\n","        y = self.decoder(z)\n","        return z, y\n","\n","    def reparameterize(self, mean, var):\n","        z = (mean + torch.mul(torch.sqrt(torch.exp(var)), torch.normal(mean = 0, std=1, size=mean.shape).to(self.device)))\n","        return z\n","\n","    def cal_loss(self, x, criterion = nn.MSELoss()):\n","        scale_adjust = x.shape[1] * x.shape[2] *x.shape[3]/ self.z_dim\n","        mean, log_var = self.encoder.forward(x)\n","        #print(\"var{}\".format(var))\n","        z = self.reparameterize(mean, log_var)\n","        y = self.decoder.forward(z)\n","        x = x.view(x.size(0), -1)\n","        y = y.view(y.size(0), -1)\n","\n","        #変分下限Lの最大化　-> -Lの最小化\n","        reconstruction = criterion(y , x) * scale_adjust/(self.var*2)\n","        kl = -torch.mean(1+log_var- mean**2 - torch.exp(log_var))/2#beta_vae\n","        #print(\"reconstruction : {}\".format(reconstruction.shape))\n","        #print(\"KL : {}\".format(kl.shape))\n","        loss =  reconstruction + kl\n","        return loss, reconstruction, kl \n","\n","\n","class VAE_Encoder(nn.Module):\n","    def __init__(self, z_dim):\n","        super(VAE_Encoder, self).__init__()\n","        self.cnn_layer1 = nn.Sequential(\n","            nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(num_features = 8),\n","            nn.Mish(),\n","            \n","        )#->(8, 24, 32)\n","\n","        self.cnn_layer2 = nn.Sequential(\n","            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=2, padding=1),\n","            nn.BatchNorm2d(num_features=16),\n","            nn.Mish(),\n","            \n","        )#->(8, 12, 16)\n","\n","        self.cnn_layer3 = nn.Sequential(\n","            nn.Conv2d(in_channels =16, out_channels=32, kernel_size=3, stride=2,),\n","            nn.BatchNorm2d(num_features=32),\n","            nn.Mish(),\n","            \n","        )#->(16, 5, 7)\n","        \n","        self.mean_layer = nn.Sequential(\n","            nn.Linear(32*5*7, 200),\n","            nn.Mish(),\n","            nn.BatchNorm1d(num_features=200),\n","            nn.Linear(200, z_dim)    \n","        )   \n","        self.log_var_layer = nn.Sequential(\n","            nn.Linear(32*5*7, 150),\n","            nn.Mish(),\n","            nn.BatchNorm1d(num_features=150),\n","            nn.Linear(150, z_dim)\n","            \n","        )   \n","\n","    def forward(self, x):\n","        out = self.cnn_layer1(x)\n","        out = self.cnn_layer2(out)\n","        out = self.cnn_layer3(out)\n","        out = out.view(out.size(0), -1)\n","\n","        mean = self.mean_layer(out)\n","        log_var = self.log_var_layer(out)\n","\n","        return mean, log_var\n","\n","class VAE_Decoder(nn.Module):\n","    def __init__(self, z_dim):\n","        super(VAE_Decoder, self).__init__()\n","        self.fc1 = nn.Linear(z_dim, 100)\n","        self.fc2 = nn.Linear(100, 32*3*4)\n","\n","        self.cnn_layer1 = nn.Sequential(\n","            nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=4, stride=2, padding=1),\n","            nn.BatchNorm2d(num_features = 16),\n","            nn.Mish(),\n","            \n","        )#->(8, 7, 9)\n","\n","        self.cnn_layer2 = nn.Sequential(\n","            nn.ConvTranspose2d(in_channels=16, out_channels=8, kernel_size=4, stride=2,padding=1),\n","            nn.BatchNorm2d(num_features = 8),\n","            nn.Mish(),\n","            \n","        )#->(8, 13, 17)\n","\n","        self.cnn_layer3 = nn.Sequential(\n","            nn.ConvTranspose2d(in_channels=8, out_channels=3, kernel_size=4, stride=2, padding=1),\n","        )#->(3, 24, 32)\n","\n","\n","    def forward(self, z):\n","        out = self.fc1(z)\n","        out = torch.relu(out)\n","        #out = nn.BatchNorm1d(num_features=40)(out)\n","        out = self.fc2(out)\n","        out = torch.relu(out)\n","        out = out.view(-1, 32, 3, 4)\n","        out = self.cnn_layer1(out)\n","        out = self.cnn_layer2(out)\n","        out = self.cnn_layer3(out)\n","        out = torch.sigmoid(out)\n","        \n","        return out\n","        \n","\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"cellView":"form","executionInfo":{"elapsed":2,"status":"ok","timestamp":1676627656673,"user":{"displayName":"猪狩高","userId":"00742343053661794311"},"user_tz":-540},"id":"RmSMlWHF98s1"},"outputs":[],"source":["# earlyStopping、datalaoder\n","def train(model, optimizer, epochs, batch_size, lr, data_input, loss_fig_name):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    train_dataloader, val_dataloader = create_dataloader(batch_size, data_input)\n","    torch.manual_seed(seed=seed)\n","    optimizer = optimizer(model.parameters(), lr=lr)\n","    early_stopping = EarlyStopping()\n","\n","    #勾配クリッピング\n","    #grad_clip = 1\n","\n","    loss_dict = {'train_loss' : [], 'train_recon' : [], 'train_kl' : [], 'val_loss' : [], 'val_recon' : [], 'val_kl' : [],}\n","\n","    for epoch in range(epochs):\n","        train_loss = 0\n","        train_reconstruction = 0\n","        train_kld = 0\n","\n","        for inputs in train_dataloader:\n","\n","            model.train()\n","            inputs = inputs.to(device)\n","            loss, _, _ = model.cal_loss(inputs)\n","            #print(\"loss.shpe {}\".format(loss.shape))\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            #nn.utils.clip_grad_value_(parameters=model.parameters(), clip_value=grad_clip)\n","            optimizer.step()    \n","\n","            model.eval()\n","            with torch.no_grad():\n","                each_train_loss, train_recon, train_kl = model.cal_loss(inputs)\n","                train_loss += each_train_loss.item()\n","                train_reconstruction += train_recon.item()\n","                train_kld += train_kl.item()\n","\n","        model.eval()\n","        with torch.no_grad():\n","            val_loss = 0\n","            val_reconstruction = 0\n","            val_kld = 0\n","            for inputs in val_dataloader:\n","                inputs = inputs.to(device)\n","                each_val_loss, val_recon, val_kl = model.cal_loss(inputs)\n","                val_loss += each_val_loss.item()\n","                val_reconstruction += val_recon.item()\n","                val_kld += val_kl.item()\n","\n","\n","        print(\"Epoch: {}/{} \".format(epoch + 1, epochs),\n","            \"Traning Loss: {} \".format(train_loss/len(train_dataloader)),\n","            \"Train_Reconstruction: {} \".format(train_reconstruction/len(train_dataloader)),\n","            \"Train_KL: {} \".format(train_kld/len(train_dataloader)),\n","            \"Validation Loss : {}\".format(val_loss/len(val_dataloader)),\n","            \"Val_Reconstruction : {}\".format(val_reconstruction/len(val_dataloader)),\n","            \"Val_KL : {}\".format(val_kld/len(val_dataloader)))\n","\n","        loss_dict[\"train_loss\"].append(train_loss/len(train_dataloader))\n","        loss_dict[\"train_recon\"].append(train_reconstruction/len(train_dataloader))\n","        loss_dict[\"train_kl\"].append(train_kld/len(train_dataloader))\n","        loss_dict[\"val_loss\"].append(val_loss/len(val_dataloader))\n","        loss_dict[\"val_recon\"].append(val_reconstruction/len(val_dataloader))\n","        loss_dict[\"val_kl\"].append(val_kld/len(val_dataloader))\n","\n","        if math.isnan(val_loss) and epoch>0:\n","            break\n","        \n","        if epoch > 3000:\n","            early_stopping(val_loss/len(val_dataloader), model) # 最良モデルならモデルパラメータ保存\n","            if early_stopping.early_stop: \n","                        # 一定epochだけval_lossが最低値を更新しなかった場合、ここに入り学習を終了\n","                break   \n","\n","    visualize_loss( epoch,loss_dict,\"image/\"+loss_fig_name+\".png\")\n","    return epoch\n","    "]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["#x -> (encoder) -> z_latent -> (decoder) -> y\n","#16000のデータ化から、ランダムにデータxを１０個生成してx,yを比較\n","def generate_xy_Image(model, data_input, fig_name):\n","    device = torch.device('cpu')\n","    data_input = torch.from_numpy(data_input)\n","    data_input = data_input.to(device)\n","\n","    inputs, outputs = genrate_random_inputAndReconst(model, data_input)\n","\n","    #入力xの表示(10個分)\n","    input_images =inputs.to(device).detach().numpy().copy()\n","    print(\"input_image\")\n","    print(input_images.shape)\n","    print(np.min(input_images))\n","    print(np.max(input_images))\n","    for i, image in enumerate(input_images):\n","        plt.title(\"original\")\n","        plt.subplot(2, 5, i+1)\n","        plt.imshow(image.reshape(24, 32, 3))\n","        plt.axis(\"off\")\n","\n","    plt.tight_layout()\n","    plt.savefig(\"image/inputs\"+fig_name+\".png\")\n","\n","    #出力yの表示(10個)\n","    output_images = (outputs).to(device).detach().numpy().copy()\n","    print(\"renconst_image\")\n","    print(output_images.shape)\n","    print(np.min(output_images))\n","    print(np.max(output_images))\n","    for i, image in enumerate(output_images):\n","        plt.title(\"reconst\")\n","        plt.subplot(2, 5, i+1)\n","        image = image.reshape(24, 32, 3)\n","        plt.imshow(image)\n","        plt.axis(\"off\")\n","\n","    plt.tight_layout()\n","    plt.savefig(\"image/recon\" + fig_name + \".png\")     \n","\n","def genrate_random_inputAndReconst(model, data_input):\n","    device = torch.device(\"cpu\")\n","\n","    #入力データを2で割って学習させたので、画像の生成時には2をかけて元に戻す\n","    data_input_tensor = data_input.clone()\n","    inputs = torch.zeros((10, 3, 24, 32))\n","    outputs = torch.zeros((10, 3, 24, 32))\n","\n","    #xをランダムに生成\n","    torch.manual_seed(seed=2)\n","    random_index = torch.randint(low=0, high=16000, size=(10,))\n","    for i in range(10):\n","        input = data_input_tensor[random_index[i]]\n","        inputs[i] = input\n","    \n","    model = model.to(device)\n","    #yを求める    \n","    model.eval()\n","    with torch.no_grad():\n","        _, outputs = model.forward(inputs.to(device))\n","\n","    return inputs, outputs    "]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":389},"executionInfo":{"elapsed":351,"status":"error","timestamp":1676627658430,"user":{"displayName":"猪狩高","userId":"00742343053661794311"},"user_tz":-540},"id":"mml6Gkoq98s2","outputId":"52175812-3a16-4e0e-ffa9-43056a397bd1"},"outputs":[{"name":"stdout","output_type":"stream","text":["(16000, 3, 24, 32)\n","Epoch: 1/5000  Traning Loss: 420.10516357421875  Train_Reconstruction: 420.09854888916016  Train_KL: 0.006609134419704787  Validation Loss : 414.4334716796875 Val_Reconstruction : 414.4193115234375 Val_KL : 0.01416084636002779\n","Epoch: 2/5000  Traning Loss: 414.4707336425781  Train_Reconstruction: 414.43896865844727  Train_KL: 0.03176386887207627  Validation Loss : 406.1064910888672 Val_Reconstruction : 406.0615234375 Val_KL : 0.04496537335216999\n","Epoch: 3/5000  Traning Loss: 396.5948600769043  Train_Reconstruction: 396.5274238586426  Train_KL: 0.06744172796607018  Validation Loss : 378.10569763183594 Val_Reconstruction : 378.010498046875 Val_KL : 0.0952029898762703\n","Epoch: 4/5000  Traning Loss: 354.63475799560547  Train_Reconstruction: 354.4369926452637  Train_KL: 0.1977612730115652  Validation Loss : 327.7830810546875 Val_Reconstruction : 327.4643859863281 Val_KL : 0.3186857998371124\n","Epoch: 5/5000  Traning Loss: 307.48828887939453  Train_Reconstruction: 306.88430404663086  Train_KL: 0.6039871871471405  Validation Loss : 286.4241027832031 Val_Reconstruction : 285.5601501464844 Val_KL : 0.8639441430568695\n","Epoch: 6/5000  Traning Loss: 275.0335388183594  Train_Reconstruction: 273.8442268371582  Train_KL: 1.1893140003085136  Validation Loss : 259.25477600097656 Val_Reconstruction : 257.7907943725586 Val_KL : 1.4639744758605957\n","Epoch: 7/5000  Traning Loss: 252.7634620666504  Train_Reconstruction: 251.05390548706055  Train_KL: 1.7095565646886826  Validation Loss : 239.89444732666016 Val_Reconstruction : 237.93740844726562 Val_KL : 1.9570366740226746\n","Epoch: 8/5000  Traning Loss: 235.61909294128418  Train_Reconstruction: 233.4997100830078  Train_KL: 2.119382292032242  Validation Loss : 224.02481079101562 Val_Reconstruction : 221.74403381347656 Val_KL : 2.2807750701904297\n","Epoch: 9/5000  Traning Loss: 220.8867702484131  Train_Reconstruction: 218.39562606811523  Train_KL: 2.4911432564258575  Validation Loss : 210.4253692626953 Val_Reconstruction : 207.79981994628906 Val_KL : 2.6255509853363037\n","Epoch: 10/5000  Traning Loss: 208.0066261291504  Train_Reconstruction: 205.18646240234375  Train_KL: 2.820162206888199  Validation Loss : 198.4191665649414 Val_Reconstruction : 195.43836975097656 Val_KL : 2.9807965755462646\n","Epoch: 11/5000  Traning Loss: 196.84487533569336  Train_Reconstruction: 193.6602325439453  Train_KL: 3.184643119573593  Validation Loss : 187.68748474121094 Val_Reconstruction : 184.4249725341797 Val_KL : 3.262507677078247\n","Epoch: 12/5000  Traning Loss: 186.72490119934082  Train_Reconstruction: 183.31450653076172  Train_KL: 3.410395324230194  Validation Loss : 178.33326721191406 Val_Reconstruction : 174.8834228515625 Val_KL : 3.4498467445373535\n","Epoch: 13/5000  Traning Loss: 178.29820823669434  Train_Reconstruction: 174.7932357788086  Train_KL: 3.504971981048584  Validation Loss : 170.66238403320312 Val_Reconstruction : 167.23253631591797 Val_KL : 3.429849624633789\n","Epoch: 14/5000  Traning Loss: 171.15987014770508  Train_Reconstruction: 167.61890602111816  Train_KL: 3.5409638583660126  Validation Loss : 164.06153106689453 Val_Reconstruction : 160.54229736328125 Val_KL : 3.51923406124115\n","Epoch: 15/5000  Traning Loss: 165.11176300048828  Train_Reconstruction: 161.53522300720215  Train_KL: 3.5765397548675537  Validation Loss : 158.4937286376953 Val_Reconstruction : 154.94548797607422 Val_KL : 3.5482397079467773\n","Epoch: 16/5000  Traning Loss: 159.76144218444824  Train_Reconstruction: 156.10533714294434  Train_KL: 3.656108111143112  Validation Loss : 153.56060028076172 Val_Reconstruction : 149.9666976928711 Val_KL : 3.5939048528671265\n","Epoch: 17/5000  Traning Loss: 155.19975090026855  Train_Reconstruction: 151.53644943237305  Train_KL: 3.6633031964302063  Validation Loss : 149.26386260986328 Val_Reconstruction : 145.63993072509766 Val_KL : 3.6239277124404907\n","Epoch: 18/5000  Traning Loss: 151.06353759765625  Train_Reconstruction: 147.3234462738037  Train_KL: 3.7400928735733032  Validation Loss : 145.42877197265625 Val_Reconstruction : 141.78822708129883 Val_KL : 3.640548586845398\n","Epoch: 19/5000  Traning Loss: 147.4232692718506  Train_Reconstruction: 143.70180892944336  Train_KL: 3.721461206674576  Validation Loss : 142.12638092041016 Val_Reconstruction : 138.48296737670898 Val_KL : 3.6434167623519897\n","Epoch: 20/5000  Traning Loss: 144.09020233154297  Train_Reconstruction: 140.32755661010742  Train_KL: 3.7626483142375946  Validation Loss : 138.7708740234375 Val_Reconstruction : 135.08210372924805 Val_KL : 3.6887749433517456\n","Epoch: 21/5000  Traning Loss: 140.90003204345703  Train_Reconstruction: 137.05407524108887  Train_KL: 3.8459572196006775  Validation Loss : 135.76143264770508 Val_Reconstruction : 131.99217224121094 Val_KL : 3.7692633867263794\n","Epoch: 22/5000  Traning Loss: 137.8862018585205  Train_Reconstruction: 134.01608657836914  Train_KL: 3.870115429162979  Validation Loss : 132.88217544555664 Val_Reconstruction : 129.11527633666992 Val_KL : 3.7668994665145874\n","Epoch: 23/5000  Traning Loss: 135.15892601013184  Train_Reconstruction: 131.26174545288086  Train_KL: 3.897180289030075  Validation Loss : 130.24538803100586 Val_Reconstruction : 126.35359954833984 Val_KL : 3.89178729057312\n","Epoch: 24/5000  Traning Loss: 132.43381118774414  Train_Reconstruction: 128.48506450653076  Train_KL: 3.9487464129924774  Validation Loss : 127.75726318359375 Val_Reconstruction : 123.91976928710938 Val_KL : 3.8374924659729004\n","Epoch: 25/5000  Traning Loss: 129.83157348632812  Train_Reconstruction: 125.88094806671143  Train_KL: 3.950623244047165  Validation Loss : 125.23218154907227 Val_Reconstruction : 121.40836715698242 Val_KL : 3.8238167762756348\n","Epoch: 26/5000  Traning Loss: 127.44023895263672  Train_Reconstruction: 123.52004909515381  Train_KL: 3.920189321041107  Validation Loss : 122.81289291381836 Val_Reconstruction : 118.93845748901367 Val_KL : 3.8744384050369263\n","Epoch: 27/5000  Traning Loss: 125.32428455352783  Train_Reconstruction: 121.32390880584717  Train_KL: 4.000374525785446  Validation Loss : 120.78152084350586 Val_Reconstruction : 116.84618377685547 Val_KL : 3.9353342056274414\n","Epoch: 28/5000  Traning Loss: 123.0749626159668  Train_Reconstruction: 119.06612396240234  Train_KL: 4.008838057518005  Validation Loss : 118.66912078857422 Val_Reconstruction : 114.78279495239258 Val_KL : 3.886322855949402\n","Epoch: 29/5000  Traning Loss: 121.1094741821289  Train_Reconstruction: 117.11403846740723  Train_KL: 3.9954354763031006  Validation Loss : 116.79177856445312 Val_Reconstruction : 112.80236053466797 Val_KL : 3.989415764808655\n","Epoch: 30/5000  Traning Loss: 119.17010498046875  Train_Reconstruction: 115.17670345306396  Train_KL: 3.993400901556015  Validation Loss : 114.97640228271484 Val_Reconstruction : 111.11533737182617 Val_KL : 3.8610655069351196\n","Epoch: 31/5000  Traning Loss: 117.38102531433105  Train_Reconstruction: 113.4746789932251  Train_KL: 3.9063456058502197  Validation Loss : 113.14628219604492 Val_Reconstruction : 109.28384399414062 Val_KL : 3.8624374866485596\n","Epoch: 32/5000  Traning Loss: 115.55571460723877  Train_Reconstruction: 111.60960960388184  Train_KL: 3.9461036324501038  Validation Loss : 111.48128890991211 Val_Reconstruction : 107.62036895751953 Val_KL : 3.860918879508972\n","Epoch: 33/5000  Traning Loss: 113.91734600067139  Train_Reconstruction: 110.01189136505127  Train_KL: 3.9054554402828217  Validation Loss : 110.00831604003906 Val_Reconstruction : 106.18940353393555 Val_KL : 3.8189131021499634\n","Epoch: 34/5000  Traning Loss: 112.28563785552979  Train_Reconstruction: 108.38838195800781  Train_KL: 3.8972562551498413  Validation Loss : 108.4129524230957 Val_Reconstruction : 104.57373046875 Val_KL : 3.8392224311828613\n","Epoch: 35/5000  Traning Loss: 110.8120059967041  Train_Reconstruction: 106.9319257736206  Train_KL: 3.8800818026065826  Validation Loss : 106.82081985473633 Val_Reconstruction : 103.02948379516602 Val_KL : 3.7913358211517334\n","Epoch: 36/5000  Traning Loss: 109.21204948425293  Train_Reconstruction: 105.40208339691162  Train_KL: 3.809965193271637  Validation Loss : 105.39640045166016 Val_Reconstruction : 101.68195724487305 Val_KL : 3.714445114135742\n","Epoch: 37/5000  Traning Loss: 107.84829616546631  Train_Reconstruction: 104.13780498504639  Train_KL: 3.7104907035827637  Validation Loss : 103.98999786376953 Val_Reconstruction : 100.33004760742188 Val_KL : 3.6599483489990234\n","Epoch: 38/5000  Traning Loss: 106.71121311187744  Train_Reconstruction: 103.0280408859253  Train_KL: 3.6831717789173126  Validation Loss : 103.09446334838867 Val_Reconstruction : 99.5469741821289 Val_KL : 3.5474891662597656\n","Epoch: 39/5000  Traning Loss: 105.51430702209473  Train_Reconstruction: 101.91063594818115  Train_KL: 3.6036714017391205  Validation Loss : 101.75940704345703 Val_Reconstruction : 98.24314498901367 Val_KL : 3.5162622928619385\n","Epoch: 40/5000  Traning Loss: 104.2533073425293  Train_Reconstruction: 100.66152667999268  Train_KL: 3.5917817056179047  Validation Loss : 100.5254898071289 Val_Reconstruction : 96.97212219238281 Val_KL : 3.5533705949783325\n","Epoch: 41/5000  Traning Loss: 103.0549488067627  Train_Reconstruction: 99.51211357116699  Train_KL: 3.5428349375724792  Validation Loss : 99.31330108642578 Val_Reconstruction : 95.87903213500977 Val_KL : 3.434267997741699\n","Epoch: 42/5000  Traning Loss: 101.95211410522461  Train_Reconstruction: 98.44661235809326  Train_KL: 3.505501240491867  Validation Loss : 98.51215362548828 Val_Reconstruction : 95.07767105102539 Val_KL : 3.4344818592071533\n","Epoch: 43/5000  Traning Loss: 101.06125259399414  Train_Reconstruction: 97.64367866516113  Train_KL: 3.4175745844841003  Validation Loss : 97.51762008666992 Val_Reconstruction : 94.16114807128906 Val_KL : 3.356473684310913\n","Epoch: 44/5000  Traning Loss: 100.04239082336426  Train_Reconstruction: 96.66149997711182  Train_KL: 3.380891054868698  Validation Loss : 96.68471145629883 Val_Reconstruction : 93.4353256225586 Val_KL : 3.2493884563446045\n","Epoch: 45/5000  Traning Loss: 99.05102157592773  Train_Reconstruction: 95.7463493347168  Train_KL: 3.304671883583069  Validation Loss : 95.52942276000977 Val_Reconstruction : 92.2915153503418 Val_KL : 3.2379066944122314\n","Epoch: 46/5000  Traning Loss: 98.16353034973145  Train_Reconstruction: 94.89437580108643  Train_KL: 3.269154369831085  Validation Loss : 94.87145233154297 Val_Reconstruction : 91.67303466796875 Val_KL : 3.198416233062744\n","Epoch: 47/5000  Traning Loss: 97.26724529266357  Train_Reconstruction: 94.04919528961182  Train_KL: 3.2180500626564026  Validation Loss : 93.78772735595703 Val_Reconstruction : 90.65123748779297 Val_KL : 3.1364904642105103\n","Epoch: 48/5000  Traning Loss: 96.17940044403076  Train_Reconstruction: 93.02381324768066  Train_KL: 3.1555852591991425  Validation Loss : 92.82165145874023 Val_Reconstruction : 89.73124313354492 Val_KL : 3.0904070138931274\n","Epoch: 49/5000  Traning Loss: 95.32065105438232  Train_Reconstruction: 92.22962760925293  Train_KL: 3.091024488210678  Validation Loss : 91.78092193603516 Val_Reconstruction : 88.75523376464844 Val_KL : 3.0256892442703247\n","Epoch: 50/5000  Traning Loss: 94.54461669921875  Train_Reconstruction: 91.45342636108398  Train_KL: 3.0911899507045746  Validation Loss : 91.24812316894531 Val_Reconstruction : 88.24232864379883 Val_KL : 3.005796432495117\n","Epoch: 51/5000  Traning Loss: 93.84079360961914  Train_Reconstruction: 90.82116413116455  Train_KL: 3.019630014896393  Validation Loss : 90.37732315063477 Val_Reconstruction : 87.42792892456055 Val_KL : 2.9493924379348755\n","Epoch: 52/5000  Traning Loss: 93.0821533203125  Train_Reconstruction: 90.10531234741211  Train_KL: 2.9768421351909637  Validation Loss : 89.76852798461914 Val_Reconstruction : 86.86718368530273 Val_KL : 2.9013434648513794\n","Epoch: 53/5000  Traning Loss: 92.36881732940674  Train_Reconstruction: 89.44738292694092  Train_KL: 2.921435624361038  Validation Loss : 89.07059478759766 Val_Reconstruction : 86.24176788330078 Val_KL : 2.828827977180481\n","Epoch: 54/5000  Traning Loss: 91.79048442840576  Train_Reconstruction: 88.90748023986816  Train_KL: 2.8830051124095917  Validation Loss : 88.66317367553711 Val_Reconstruction : 85.82181930541992 Val_KL : 2.841351270675659\n","Epoch: 55/5000  Traning Loss: 91.20171737670898  Train_Reconstruction: 88.31464290618896  Train_KL: 2.8870753049850464  Validation Loss : 87.90867233276367 Val_Reconstruction : 85.08191299438477 Val_KL : 2.826760768890381\n","Epoch: 56/5000  Traning Loss: 90.54066371917725  Train_Reconstruction: 87.72363948822021  Train_KL: 2.817025125026703  Validation Loss : 87.16983032226562 Val_Reconstruction : 84.41028213500977 Val_KL : 2.7595467567443848\n","Epoch: 57/5000  Traning Loss: 90.05112075805664  Train_Reconstruction: 87.2421646118164  Train_KL: 2.808956891298294  Validation Loss : 86.83208084106445 Val_Reconstruction : 84.07991409301758 Val_KL : 2.752169132232666\n","Epoch: 58/5000  Traning Loss: 89.49744415283203  Train_Reconstruction: 86.70699119567871  Train_KL: 2.7904535233974457  Validation Loss : 86.32698822021484 Val_Reconstruction : 83.58585739135742 Val_KL : 2.7411328554153442\n","Epoch: 59/5000  Traning Loss: 89.03335189819336  Train_Reconstruction: 86.25397872924805  Train_KL: 2.779370903968811  Validation Loss : 85.66228866577148 Val_Reconstruction : 82.92630004882812 Val_KL : 2.7359893321990967\n","Epoch: 60/5000  Traning Loss: 88.3994607925415  Train_Reconstruction: 85.61946964263916  Train_KL: 2.7799912691116333  Validation Loss : 85.12033462524414 Val_Reconstruction : 82.39287185668945 Val_KL : 2.727462410926819\n","Epoch: 61/5000  Traning Loss: 87.77749156951904  Train_Reconstruction: 85.01657772064209  Train_KL: 2.7609143555164337  Validation Loss : 84.65526580810547 Val_Reconstruction : 81.97125625610352 Val_KL : 2.6840100288391113\n","Epoch: 62/5000  Traning Loss: 87.31963634490967  Train_Reconstruction: 84.60719966888428  Train_KL: 2.7124359905719757  Validation Loss : 84.18781661987305 Val_Reconstruction : 81.51008987426758 Val_KL : 2.677725911140442\n","Epoch: 63/5000  Traning Loss: 86.83677864074707  Train_Reconstruction: 84.13677501678467  Train_KL: 2.7000039517879486  Validation Loss : 83.85894775390625 Val_Reconstruction : 81.22306442260742 Val_KL : 2.635883331298828\n","Epoch: 64/5000  Traning Loss: 86.49208354949951  Train_Reconstruction: 83.81553745269775  Train_KL: 2.6765467822551727  Validation Loss : 83.5638313293457 Val_Reconstruction : 80.91797637939453 Val_KL : 2.645853042602539\n","Epoch: 65/5000  Traning Loss: 86.03512573242188  Train_Reconstruction: 83.36401081085205  Train_KL: 2.6711154878139496  Validation Loss : 82.95796203613281 Val_Reconstruction : 80.35254287719727 Val_KL : 2.6054166555404663\n","Epoch: 66/5000  Traning Loss: 85.40489768981934  Train_Reconstruction: 82.72716808319092  Train_KL: 2.6777305603027344  Validation Loss : 82.24098587036133 Val_Reconstruction : 79.59605026245117 Val_KL : 2.6449384689331055\n","Epoch: 67/5000  Traning Loss: 85.00959014892578  Train_Reconstruction: 82.31728935241699  Train_KL: 2.692300021648407  Validation Loss : 82.16592025756836 Val_Reconstruction : 79.55253982543945 Val_KL : 2.6133809089660645\n","Epoch: 68/5000  Traning Loss: 84.8497428894043  Train_Reconstruction: 82.20154190063477  Train_KL: 2.648201107978821  Validation Loss : 81.81437301635742 Val_Reconstruction : 79.22148895263672 Val_KL : 2.5928831100463867\n","Epoch: 69/5000  Traning Loss: 84.43585300445557  Train_Reconstruction: 81.78680419921875  Train_KL: 2.6490504145622253  Validation Loss : 81.50072479248047 Val_Reconstruction : 78.9117431640625 Val_KL : 2.5889812707901\n","Epoch: 70/5000  Traning Loss: 84.14656734466553  Train_Reconstruction: 81.49689865112305  Train_KL: 2.6496681571006775  Validation Loss : 81.00917053222656 Val_Reconstruction : 78.4135856628418 Val_KL : 2.5955861806869507\n","Epoch: 71/5000  Traning Loss: 83.76889991760254  Train_Reconstruction: 81.11547565460205  Train_KL: 2.6534230709075928  Validation Loss : 80.84199523925781 Val_Reconstruction : 78.24762344360352 Val_KL : 2.5943716764450073\n","Epoch: 72/5000  Traning Loss: 83.41787528991699  Train_Reconstruction: 80.7794771194458  Train_KL: 2.638397753238678  Validation Loss : 80.45164489746094 Val_Reconstruction : 77.86678314208984 Val_KL : 2.58486008644104\n","Epoch: 73/5000  Traning Loss: 83.11272525787354  Train_Reconstruction: 80.47941398620605  Train_KL: 2.63331139087677  Validation Loss : 80.14218521118164 Val_Reconstruction : 77.55215835571289 Val_KL : 2.5900248289108276\n","Epoch: 74/5000  Traning Loss: 82.85563468933105  Train_Reconstruction: 80.22247886657715  Train_KL: 2.6331546008586884  Validation Loss : 79.96866989135742 Val_Reconstruction : 77.38233947753906 Val_KL : 2.586328625679016\n","Epoch: 75/5000  Traning Loss: 82.6977481842041  Train_Reconstruction: 80.073805809021  Train_KL: 2.623941481113434  Validation Loss : 79.96270370483398 Val_Reconstruction : 77.3834342956543 Val_KL : 2.5792709589004517\n","Epoch: 76/5000  Traning Loss: 82.57682132720947  Train_Reconstruction: 79.956374168396  Train_KL: 2.6204470694065094  Validation Loss : 79.34770202636719 Val_Reconstruction : 76.77010726928711 Val_KL : 2.5775935649871826\n","Epoch: 77/5000  Traning Loss: 82.06748008728027  Train_Reconstruction: 79.43506336212158  Train_KL: 2.6324176490306854  Validation Loss : 78.93839645385742 Val_Reconstruction : 76.36293411254883 Val_KL : 2.5754603147506714\n","Epoch: 78/5000  Traning Loss: 81.62876033782959  Train_Reconstruction: 79.00495624542236  Train_KL: 2.623804122209549  Validation Loss : 78.81193161010742 Val_Reconstruction : 76.23990631103516 Val_KL : 2.5720274448394775\n","Epoch: 79/5000  Traning Loss: 81.46520709991455  Train_Reconstruction: 78.83357429504395  Train_KL: 2.631632477045059  Validation Loss : 78.6016845703125 Val_Reconstruction : 76.0202407836914 Val_KL : 2.5814437866210938\n","Epoch: 80/5000  Traning Loss: 81.20493698120117  Train_Reconstruction: 78.57369613647461  Train_KL: 2.6312415599823  Validation Loss : 78.38017654418945 Val_Reconstruction : 75.80834197998047 Val_KL : 2.571836233139038\n","Epoch: 81/5000  Traning Loss: 81.12508296966553  Train_Reconstruction: 78.50854206085205  Train_KL: 2.6165395081043243  Validation Loss : 78.30184555053711 Val_Reconstruction : 75.746337890625 Val_KL : 2.5555078983306885\n","Epoch: 82/5000  Traning Loss: 80.76314163208008  Train_Reconstruction: 78.13609600067139  Train_KL: 2.627045512199402  Validation Loss : 77.77231979370117 Val_Reconstruction : 75.20282745361328 Val_KL : 2.5694929361343384\n","Epoch: 83/5000  Traning Loss: 80.50937461853027  Train_Reconstruction: 77.87982177734375  Train_KL: 2.6295543015003204  Validation Loss : 77.76419830322266 Val_Reconstruction : 75.20405960083008 Val_KL : 2.560138702392578\n","Epoch: 84/5000  Traning Loss: 80.34273338317871  Train_Reconstruction: 77.71587467193604  Train_KL: 2.6268579959869385  Validation Loss : 77.38280868530273 Val_Reconstruction : 74.80050659179688 Val_KL : 2.5823053121566772\n","Epoch: 85/5000  Traning Loss: 80.1369981765747  Train_Reconstruction: 77.50037002563477  Train_KL: 2.63662588596344  Validation Loss : 77.25737380981445 Val_Reconstruction : 74.65946960449219 Val_KL : 2.5979034900665283\n","Epoch: 86/5000  Traning Loss: 79.85325050354004  Train_Reconstruction: 77.21946430206299  Train_KL: 2.6337862610816956  Validation Loss : 76.91447448730469 Val_Reconstruction : 74.33208465576172 Val_KL : 2.582390069961548\n","Epoch: 87/5000  Traning Loss: 79.66314697265625  Train_Reconstruction: 77.02555179595947  Train_KL: 2.637596696615219  Validation Loss : 76.84686279296875 Val_Reconstruction : 74.2729606628418 Val_KL : 2.573899030685425\n","Epoch: 88/5000  Traning Loss: 79.42997646331787  Train_Reconstruction: 76.81437873840332  Train_KL: 2.6155980825424194  Validation Loss : 76.7273063659668 Val_Reconstruction : 74.16501235961914 Val_KL : 2.562296509742737\n","Epoch: 89/5000  Traning Loss: 79.11738586425781  Train_Reconstruction: 76.49760150909424  Train_KL: 2.619784116744995  Validation Loss : 76.47819519042969 Val_Reconstruction : 73.92313766479492 Val_KL : 2.555058479309082\n","Epoch: 90/5000  Traning Loss: 79.12587261199951  Train_Reconstruction: 76.51115798950195  Train_KL: 2.6147134006023407  Validation Loss : 76.39178085327148 Val_Reconstruction : 73.8250503540039 Val_KL : 2.5667308568954468\n","Epoch: 91/5000  Traning Loss: 78.82474327087402  Train_Reconstruction: 76.20835304260254  Train_KL: 2.6163904070854187  Validation Loss : 76.18488311767578 Val_Reconstruction : 73.63091278076172 Val_KL : 2.5539710521698\n","Epoch: 92/5000  Traning Loss: 78.5318431854248  Train_Reconstruction: 75.91450023651123  Train_KL: 2.6173431873321533  Validation Loss : 75.69013595581055 Val_Reconstruction : 73.12163925170898 Val_KL : 2.5684990882873535\n","Epoch: 93/5000  Traning Loss: 78.42447853088379  Train_Reconstruction: 75.79813766479492  Train_KL: 2.626341164112091  Validation Loss : 75.62326049804688 Val_Reconstruction : 73.05054092407227 Val_KL : 2.572719931602478\n","Epoch: 94/5000  Traning Loss: 78.2819471359253  Train_Reconstruction: 75.65079402923584  Train_KL: 2.6311532855033875  Validation Loss : 75.74729919433594 Val_Reconstruction : 73.16059875488281 Val_KL : 2.586698293685913\n","Epoch: 95/5000  Traning Loss: 78.15017509460449  Train_Reconstruction: 75.52131462097168  Train_KL: 2.6288591027259827  Validation Loss : 75.34427642822266 Val_Reconstruction : 72.77115249633789 Val_KL : 2.5731236934661865\n","Epoch: 96/5000  Traning Loss: 77.9845666885376  Train_Reconstruction: 75.33922100067139  Train_KL: 2.645344704389572  Validation Loss : 75.19527053833008 Val_Reconstruction : 72.58847427368164 Val_KL : 2.6067949533462524\n","Epoch: 97/5000  Traning Loss: 77.76052761077881  Train_Reconstruction: 75.11277103424072  Train_KL: 2.647756576538086  Validation Loss : 75.11236572265625 Val_Reconstruction : 72.5296630859375 Val_KL : 2.5827044248580933\n","Epoch: 98/5000  Traning Loss: 77.55626583099365  Train_Reconstruction: 74.91340446472168  Train_KL: 2.6428622603416443  Validation Loss : 74.79201126098633 Val_Reconstruction : 72.19689178466797 Val_KL : 2.595119833946228\n","Epoch: 99/5000  Traning Loss: 77.4651288986206  Train_Reconstruction: 74.8157901763916  Train_KL: 2.6493398249149323  Validation Loss : 74.704833984375 Val_Reconstruction : 72.1186294555664 Val_KL : 2.586205244064331\n","Epoch: 100/5000  Traning Loss: 77.26329708099365  Train_Reconstruction: 74.62069988250732  Train_KL: 2.6425974667072296  Validation Loss : 74.58526611328125 Val_Reconstruction : 71.99948120117188 Val_KL : 2.5857831239700317\n","Epoch: 101/5000  Traning Loss: 77.1170711517334  Train_Reconstruction: 74.48303318023682  Train_KL: 2.6340385377407074  Validation Loss : 74.50136184692383 Val_Reconstruction : 71.90718078613281 Val_KL : 2.594184637069702\n","Epoch: 102/5000  Traning Loss: 77.1024923324585  Train_Reconstruction: 74.44928455352783  Train_KL: 2.6532077491283417  Validation Loss : 74.34303283691406 Val_Reconstruction : 71.74550247192383 Val_KL : 2.5975310802459717\n","Epoch: 103/5000  Traning Loss: 77.10427665710449  Train_Reconstruction: 74.45923709869385  Train_KL: 2.645040988922119  Validation Loss : 74.67190170288086 Val_Reconstruction : 72.07970809936523 Val_KL : 2.5921930074691772\n","Epoch: 104/5000  Traning Loss: 77.1850643157959  Train_Reconstruction: 74.53857803344727  Train_KL: 2.6464851796627045  Validation Loss : 74.53350830078125 Val_Reconstruction : 71.93783187866211 Val_KL : 2.5956755876541138\n","Epoch: 105/5000  Traning Loss: 76.77264213562012  Train_Reconstruction: 74.11508560180664  Train_KL: 2.657556712627411  Validation Loss : 74.0526237487793 Val_Reconstruction : 71.44459533691406 Val_KL : 2.6080280542373657\n","Epoch: 106/5000  Traning Loss: 76.40547943115234  Train_Reconstruction: 73.75303745269775  Train_KL: 2.6524434983730316  Validation Loss : 73.82440948486328 Val_Reconstruction : 71.23202514648438 Val_KL : 2.592382073402405\n","Epoch: 107/5000  Traning Loss: 76.1183967590332  Train_Reconstruction: 73.4677038192749  Train_KL: 2.6506928205490112  Validation Loss : 73.47681045532227 Val_Reconstruction : 70.88687896728516 Val_KL : 2.5899319648742676\n","Epoch: 108/5000  Traning Loss: 76.08357048034668  Train_Reconstruction: 73.44554901123047  Train_KL: 2.638021767139435  Validation Loss : 73.4942741394043 Val_Reconstruction : 70.9018783569336 Val_KL : 2.592397451400757\n","Epoch: 109/5000  Traning Loss: 76.06056785583496  Train_Reconstruction: 73.41057777404785  Train_KL: 2.649990975856781  Validation Loss : 73.29190063476562 Val_Reconstruction : 70.6985855102539 Val_KL : 2.5933157205581665\n","Epoch: 110/5000  Traning Loss: 75.66225147247314  Train_Reconstruction: 73.01234912872314  Train_KL: 2.6499020159244537  Validation Loss : 72.93976211547852 Val_Reconstruction : 70.3398323059082 Val_KL : 2.5999274253845215\n","Epoch: 111/5000  Traning Loss: 75.57375431060791  Train_Reconstruction: 72.92832088470459  Train_KL: 2.6454336047172546  Validation Loss : 72.9336051940918 Val_Reconstruction : 70.3371467590332 Val_KL : 2.59645676612854\n","Epoch: 112/5000  Traning Loss: 75.5247802734375  Train_Reconstruction: 72.86669158935547  Train_KL: 2.65808829665184  Validation Loss : 73.0042953491211 Val_Reconstruction : 70.39859008789062 Val_KL : 2.6057053804397583\n","Epoch: 113/5000  Traning Loss: 75.46826267242432  Train_Reconstruction: 72.81463050842285  Train_KL: 2.653631716966629  Validation Loss : 72.96025466918945 Val_Reconstruction : 70.3754997253418 Val_KL : 2.584754705429077\n","Epoch: 114/5000  Traning Loss: 75.50210762023926  Train_Reconstruction: 72.85080146789551  Train_KL: 2.6513078808784485  Validation Loss : 72.85157012939453 Val_Reconstruction : 70.23626708984375 Val_KL : 2.6153039932250977\n","Epoch: 115/5000  Traning Loss: 75.43514442443848  Train_Reconstruction: 72.7828950881958  Train_KL: 2.652250111103058  Validation Loss : 73.02203369140625 Val_Reconstruction : 70.42193603515625 Val_KL : 2.600096344947815\n","Epoch: 116/5000  Traning Loss: 75.38053131103516  Train_Reconstruction: 72.72530269622803  Train_KL: 2.655228942632675  Validation Loss : 72.87005996704102 Val_Reconstruction : 70.26319885253906 Val_KL : 2.6068599224090576\n","Epoch: 117/5000  Traning Loss: 75.11381244659424  Train_Reconstruction: 72.449631690979  Train_KL: 2.664181798696518  Validation Loss : 72.4629020690918 Val_Reconstruction : 69.84548568725586 Val_KL : 2.617416501045227\n","Epoch: 118/5000  Traning Loss: 74.95746421813965  Train_Reconstruction: 72.28544044494629  Train_KL: 2.6720243096351624  Validation Loss : 72.29434585571289 Val_Reconstruction : 69.68956756591797 Val_KL : 2.604776978492737\n","Epoch: 119/5000  Traning Loss: 74.80770778656006  Train_Reconstruction: 72.1438512802124  Train_KL: 2.6638588905334473  Validation Loss : 72.09961318969727 Val_Reconstruction : 69.4824104309082 Val_KL : 2.6172049045562744\n","Epoch: 120/5000  Traning Loss: 74.71852779388428  Train_Reconstruction: 72.04417037963867  Train_KL: 2.674356997013092  Validation Loss : 72.22295761108398 Val_Reconstruction : 69.60357284545898 Val_KL : 2.6193829774856567\n","Epoch: 121/5000  Traning Loss: 74.73671340942383  Train_Reconstruction: 72.06424140930176  Train_KL: 2.6724716126918793  Validation Loss : 72.23277282714844 Val_Reconstruction : 69.61019897460938 Val_KL : 2.6225754022598267\n","Epoch: 122/5000  Traning Loss: 74.66039848327637  Train_Reconstruction: 71.98165130615234  Train_KL: 2.678747743368149  Validation Loss : 72.2066535949707 Val_Reconstruction : 69.59919357299805 Val_KL : 2.6074588298797607\n","Epoch: 123/5000  Traning Loss: 74.57836437225342  Train_Reconstruction: 71.92626857757568  Train_KL: 2.652096301317215  Validation Loss : 72.18425369262695 Val_Reconstruction : 69.59835815429688 Val_KL : 2.5858945846557617\n","Epoch: 124/5000  Traning Loss: 74.28582668304443  Train_Reconstruction: 71.62612056732178  Train_KL: 2.6597065031528473  Validation Loss : 71.83974075317383 Val_Reconstruction : 69.21914672851562 Val_KL : 2.620592474937439\n","Epoch: 125/5000  Traning Loss: 74.37896633148193  Train_Reconstruction: 71.70015716552734  Train_KL: 2.6788089275360107  Validation Loss : 71.65265274047852 Val_Reconstruction : 69.02555847167969 Val_KL : 2.6270965337753296\n","Epoch: 126/5000  Traning Loss: 74.22145557403564  Train_Reconstruction: 71.54551219940186  Train_KL: 2.675942689180374  Validation Loss : 71.87438583374023 Val_Reconstruction : 69.2537956237793 Val_KL : 2.6205912828445435\n","Epoch: 127/5000  Traning Loss: 74.0613784790039  Train_Reconstruction: 71.3818826675415  Train_KL: 2.679497003555298  Validation Loss : 71.38870239257812 Val_Reconstruction : 68.77070045471191 Val_KL : 2.6180049180984497\n","Epoch: 128/5000  Traning Loss: 74.01861381530762  Train_Reconstruction: 71.33246231079102  Train_KL: 2.6861511766910553  Validation Loss : 71.39279556274414 Val_Reconstruction : 68.76900482177734 Val_KL : 2.6237881183624268\n","Epoch: 129/5000  Traning Loss: 73.91525173187256  Train_Reconstruction: 71.23654460906982  Train_KL: 2.678705871105194  Validation Loss : 71.49120712280273 Val_Reconstruction : 68.87755966186523 Val_KL : 2.6136471033096313\n","Epoch: 130/5000  Traning Loss: 73.87964344024658  Train_Reconstruction: 71.20480251312256  Train_KL: 2.674840956926346  Validation Loss : 71.28258895874023 Val_Reconstruction : 68.65943717956543 Val_KL : 2.623154044151306\n","Epoch: 131/5000  Traning Loss: 73.92622566223145  Train_Reconstruction: 71.24804782867432  Train_KL: 2.678177982568741  Validation Loss : 71.29188919067383 Val_Reconstruction : 68.67816352844238 Val_KL : 2.6137245893478394\n","Epoch: 132/5000  Traning Loss: 73.78346633911133  Train_Reconstruction: 71.10269165039062  Train_KL: 2.6807751059532166  Validation Loss : 71.1071548461914 Val_Reconstruction : 68.50077629089355 Val_KL : 2.6063791513442993\n","Epoch: 133/5000  Traning Loss: 73.75289535522461  Train_Reconstruction: 71.08019638061523  Train_KL: 2.6726990342140198  Validation Loss : 71.2807502746582 Val_Reconstruction : 68.65623474121094 Val_KL : 2.624516010284424\n","Epoch: 134/5000  Traning Loss: 73.49427795410156  Train_Reconstruction: 70.8145523071289  Train_KL: 2.6797254383563995  Validation Loss : 71.03387451171875 Val_Reconstruction : 68.40712547302246 Val_KL : 2.6267491579055786\n","Epoch: 135/5000  Traning Loss: 73.37835884094238  Train_Reconstruction: 70.69949626922607  Train_KL: 2.67886221408844  Validation Loss : 70.80595397949219 Val_Reconstruction : 68.16971206665039 Val_KL : 2.6362403631210327\n","Epoch: 136/5000  Traning Loss: 73.09560108184814  Train_Reconstruction: 70.41465473175049  Train_KL: 2.680945962667465  Validation Loss : 70.62652969360352 Val_Reconstruction : 68.01123809814453 Val_KL : 2.6152912378311157\n","Epoch: 137/5000  Traning Loss: 73.0382947921753  Train_Reconstruction: 70.35641288757324  Train_KL: 2.6818808019161224  Validation Loss : 70.59732437133789 Val_Reconstruction : 67.96911811828613 Val_KL : 2.628206491470337\n","Epoch: 138/5000  Traning Loss: 73.02777767181396  Train_Reconstruction: 70.34555721282959  Train_KL: 2.6822198629379272  Validation Loss : 70.65219497680664 Val_Reconstruction : 68.03443908691406 Val_KL : 2.617756485939026\n","Epoch: 139/5000  Traning Loss: 72.98679447174072  Train_Reconstruction: 70.30509281158447  Train_KL: 2.6817015409469604  Validation Loss : 70.73200225830078 Val_Reconstruction : 68.0933895111084 Val_KL : 2.63861083984375\n","Epoch: 140/5000  Traning Loss: 72.88716411590576  Train_Reconstruction: 70.20346927642822  Train_KL: 2.6836951076984406  Validation Loss : 70.51498794555664 Val_Reconstruction : 67.8889389038086 Val_KL : 2.626047730445862\n","Epoch: 141/5000  Traning Loss: 72.71506023406982  Train_Reconstruction: 70.03311252593994  Train_KL: 2.6819482147693634  Validation Loss : 70.24308776855469 Val_Reconstruction : 67.62166976928711 Val_KL : 2.6214194297790527\n","Epoch: 142/5000  Traning Loss: 72.60458374023438  Train_Reconstruction: 69.90891170501709  Train_KL: 2.6956720650196075  Validation Loss : 70.29191589355469 Val_Reconstruction : 67.66908073425293 Val_KL : 2.6228328943252563\n","Epoch: 143/5000  Traning Loss: 72.59783458709717  Train_Reconstruction: 69.90790557861328  Train_KL: 2.689930021762848  Validation Loss : 70.41122436523438 Val_Reconstruction : 67.7797679901123 Val_KL : 2.6314531564712524\n","Epoch: 144/5000  Traning Loss: 72.53655910491943  Train_Reconstruction: 69.84798908233643  Train_KL: 2.6885701417922974  Validation Loss : 70.02804946899414 Val_Reconstruction : 67.39870834350586 Val_KL : 2.6293418407440186\n","Epoch: 145/5000  Traning Loss: 72.32737636566162  Train_Reconstruction: 69.64216899871826  Train_KL: 2.6852071285247803  Validation Loss : 69.80489730834961 Val_Reconstruction : 67.16621780395508 Val_KL : 2.6386784315109253\n","Epoch: 146/5000  Traning Loss: 72.31250476837158  Train_Reconstruction: 69.62103939056396  Train_KL: 2.6914647817611694  Validation Loss : 70.02091217041016 Val_Reconstruction : 67.40713119506836 Val_KL : 2.613777756690979\n","Epoch: 147/5000  Traning Loss: 72.36305809020996  Train_Reconstruction: 69.67471313476562  Train_KL: 2.688343822956085  Validation Loss : 69.9359130859375 Val_Reconstruction : 67.30213356018066 Val_KL : 2.633778691291809\n","Epoch: 148/5000  Traning Loss: 72.1581563949585  Train_Reconstruction: 69.46639060974121  Train_KL: 2.6917647421360016  Validation Loss : 69.6183090209961 Val_Reconstruction : 67.00257682800293 Val_KL : 2.615734338760376\n","Epoch: 149/5000  Traning Loss: 72.01861000061035  Train_Reconstruction: 69.33239841461182  Train_KL: 2.686211347579956  Validation Loss : 69.66775894165039 Val_Reconstruction : 67.03780937194824 Val_KL : 2.6299480199813843\n","Epoch: 150/5000  Traning Loss: 72.01124668121338  Train_Reconstruction: 69.32319736480713  Train_KL: 2.688049703836441  Validation Loss : 69.49318313598633 Val_Reconstruction : 66.8762378692627 Val_KL : 2.616946220397949\n","Epoch: 151/5000  Traning Loss: 72.16581344604492  Train_Reconstruction: 69.48642349243164  Train_KL: 2.6793903410434723  Validation Loss : 69.65495300292969 Val_Reconstruction : 67.02905464172363 Val_KL : 2.6258994340896606\n","Epoch: 152/5000  Traning Loss: 72.19312191009521  Train_Reconstruction: 69.50316619873047  Train_KL: 2.689955860376358  Validation Loss : 69.85982131958008 Val_Reconstruction : 67.22686576843262 Val_KL : 2.632954955101013\n","Epoch: 153/5000  Traning Loss: 72.32886219024658  Train_Reconstruction: 69.64782905578613  Train_KL: 2.6810333728790283  Validation Loss : 69.96125793457031 Val_Reconstruction : 67.33470344543457 Val_KL : 2.6265549659729004\n","Epoch: 154/5000  Traning Loss: 71.93152904510498  Train_Reconstruction: 69.23817253112793  Train_KL: 2.6933565735816956  Validation Loss : 69.40414810180664 Val_Reconstruction : 66.76504516601562 Val_KL : 2.639099359512329\n","Epoch: 155/5000  Traning Loss: 71.86615753173828  Train_Reconstruction: 69.16634178161621  Train_KL: 2.699816197156906  Validation Loss : 69.47690200805664 Val_Reconstruction : 66.84338569641113 Val_KL : 2.6335145235061646\n","Epoch: 156/5000  Traning Loss: 71.65510845184326  Train_Reconstruction: 68.96577835083008  Train_KL: 2.6893298029899597  Validation Loss : 69.26554870605469 Val_Reconstruction : 66.63432884216309 Val_KL : 2.6312180757522583\n","Epoch: 157/5000  Traning Loss: 71.48128986358643  Train_Reconstruction: 68.78483200073242  Train_KL: 2.696457952260971  Validation Loss : 69.07954406738281 Val_Reconstruction : 66.44059944152832 Val_KL : 2.638946533203125\n","Epoch: 158/5000  Traning Loss: 71.43698596954346  Train_Reconstruction: 68.74316120147705  Train_KL: 2.693825602531433  Validation Loss : 69.08901786804199 Val_Reconstruction : 66.46905708312988 Val_KL : 2.6199618577957153\n","Epoch: 159/5000  Traning Loss: 71.43842029571533  Train_Reconstruction: 68.76534080505371  Train_KL: 2.6730799973011017  Validation Loss : 69.25570678710938 Val_Reconstruction : 66.64242362976074 Val_KL : 2.613281726837158\n","Epoch: 160/5000  Traning Loss: 71.57819652557373  Train_Reconstruction: 68.89620971679688  Train_KL: 2.6819854378700256  Validation Loss : 69.27241516113281 Val_Reconstruction : 66.63628959655762 Val_KL : 2.6361249685287476\n","Epoch: 161/5000  Traning Loss: 71.58851432800293  Train_Reconstruction: 68.89065265655518  Train_KL: 2.6978620290756226  Validation Loss : 69.3009147644043 Val_Reconstruction : 66.6613826751709 Val_KL : 2.639531970024109\n","Epoch: 162/5000  Traning Loss: 71.63744068145752  Train_Reconstruction: 68.93963813781738  Train_KL: 2.6978016793727875  Validation Loss : 69.27214431762695 Val_Reconstruction : 66.63310432434082 Val_KL : 2.639041543006897\n","Epoch: 163/5000  Traning Loss: 71.6641092300415  Train_Reconstruction: 68.96519374847412  Train_KL: 2.6989157497882843  Validation Loss : 69.5602035522461 Val_Reconstruction : 66.90983390808105 Val_KL : 2.650370717048645\n","Epoch: 164/5000  Traning Loss: 71.6062068939209  Train_Reconstruction: 68.90269374847412  Train_KL: 2.703513503074646  Validation Loss : 69.27020645141602 Val_Reconstruction : 66.62014198303223 Val_KL : 2.6500638723373413\n","Epoch: 165/5000  Traning Loss: 71.2847032546997  Train_Reconstruction: 68.57621669769287  Train_KL: 2.708486557006836  Validation Loss : 68.7311897277832 Val_Reconstruction : 66.0882682800293 Val_KL : 2.6429200172424316\n","Epoch: 166/5000  Traning Loss: 71.10930633544922  Train_Reconstruction: 68.40677547454834  Train_KL: 2.7025321424007416  Validation Loss : 68.74175643920898 Val_Reconstruction : 66.09632873535156 Val_KL : 2.6454286575317383\n","Epoch: 167/5000  Traning Loss: 71.10762310028076  Train_Reconstruction: 68.40448188781738  Train_KL: 2.703142821788788  Validation Loss : 68.90533065795898 Val_Reconstruction : 66.25228118896484 Val_KL : 2.653048038482666\n","Epoch: 168/5000  Traning Loss: 71.13768005371094  Train_Reconstruction: 68.43502998352051  Train_KL: 2.7026509940624237  Validation Loss : 68.59845924377441 Val_Reconstruction : 65.96618461608887 Val_KL : 2.6322741508483887\n","Epoch: 169/5000  Traning Loss: 71.00042247772217  Train_Reconstruction: 68.29696655273438  Train_KL: 2.7034555077552795  Validation Loss : 68.71290969848633 Val_Reconstruction : 66.07545852661133 Val_KL : 2.6374499797821045\n","Epoch: 170/5000  Traning Loss: 70.94245719909668  Train_Reconstruction: 68.24394798278809  Train_KL: 2.6985080540180206  Validation Loss : 68.69872665405273 Val_Reconstruction : 66.05844116210938 Val_KL : 2.6402872800827026\n","Epoch: 171/5000  Traning Loss: 70.81963920593262  Train_Reconstruction: 68.11598873138428  Train_KL: 2.703651189804077  Validation Loss : 68.54969787597656 Val_Reconstruction : 65.89659118652344 Val_KL : 2.653104782104492\n","Epoch: 172/5000  Traning Loss: 70.92296695709229  Train_Reconstruction: 68.20981121063232  Train_KL: 2.713155895471573  Validation Loss : 68.64712333679199 Val_Reconstruction : 65.99252700805664 Val_KL : 2.654598355293274\n","Epoch: 173/5000  Traning Loss: 70.94489669799805  Train_Reconstruction: 68.23528385162354  Train_KL: 2.7096117436885834  Validation Loss : 68.70933151245117 Val_Reconstruction : 66.06700325012207 Val_KL : 2.6423259973526\n","Epoch: 174/5000  Traning Loss: 70.67835426330566  Train_Reconstruction: 67.97369861602783  Train_KL: 2.704655885696411  Validation Loss : 68.32779502868652 Val_Reconstruction : 65.67953300476074 Val_KL : 2.648261547088623\n","Epoch: 175/5000  Traning Loss: 70.46812152862549  Train_Reconstruction: 67.76746559143066  Train_KL: 2.700656145811081  Validation Loss : 68.14990615844727 Val_Reconstruction : 65.51944732666016 Val_KL : 2.6304596662521362\n","Epoch: 176/5000  Traning Loss: 70.39910411834717  Train_Reconstruction: 67.69589805603027  Train_KL: 2.7032063603401184  Validation Loss : 68.21688461303711 Val_Reconstruction : 65.57283592224121 Val_KL : 2.6440471410751343\n","Epoch: 177/5000  Traning Loss: 70.4248456954956  Train_Reconstruction: 67.7252950668335  Train_KL: 2.6995503902435303  Validation Loss : 68.10519409179688 Val_Reconstruction : 65.46072959899902 Val_KL : 2.644465684890747\n","Epoch: 178/5000  Traning Loss: 70.42859840393066  Train_Reconstruction: 67.70529556274414  Train_KL: 2.7233035564422607  Validation Loss : 68.3730583190918 Val_Reconstruction : 65.69675064086914 Val_KL : 2.6763081550598145\n","Epoch: 179/5000  Traning Loss: 70.44181156158447  Train_Reconstruction: 67.72063541412354  Train_KL: 2.721174955368042  Validation Loss : 68.17963600158691 Val_Reconstruction : 65.53376960754395 Val_KL : 2.645867109298706\n","Epoch: 180/5000  Traning Loss: 70.41529178619385  Train_Reconstruction: 67.69801139831543  Train_KL: 2.717279702425003  Validation Loss : 68.10311317443848 Val_Reconstruction : 65.44958305358887 Val_KL : 2.6535314321517944\n","Epoch: 181/5000  Traning Loss: 70.19769668579102  Train_Reconstruction: 67.48996543884277  Train_KL: 2.7077334821224213  Validation Loss : 68.13805198669434 Val_Reconstruction : 65.49398422241211 Val_KL : 2.6440682411193848\n","Epoch: 182/5000  Traning Loss: 70.13075923919678  Train_Reconstruction: 67.42798900604248  Train_KL: 2.7027705013751984  Validation Loss : 67.97867584228516 Val_Reconstruction : 65.3377857208252 Val_KL : 2.6408900022506714\n","Epoch: 183/5000  Traning Loss: 70.23290729522705  Train_Reconstruction: 67.53138256072998  Train_KL: 2.7015253007411957  Validation Loss : 67.95621299743652 Val_Reconstruction : 65.31547355651855 Val_KL : 2.6407405138015747\n","Epoch: 184/5000  Traning Loss: 70.2041015625  Train_Reconstruction: 67.49878120422363  Train_KL: 2.7053202986717224  Validation Loss : 67.92971420288086 Val_Reconstruction : 65.28876113891602 Val_KL : 2.6409517526626587\n","Epoch: 185/5000  Traning Loss: 70.31811428070068  Train_Reconstruction: 67.6088171005249  Train_KL: 2.7092963457107544  Validation Loss : 68.17354583740234 Val_Reconstruction : 65.53155517578125 Val_KL : 2.641992449760437\n","Epoch: 186/5000  Traning Loss: 70.02970504760742  Train_Reconstruction: 67.31830596923828  Train_KL: 2.711398333311081  Validation Loss : 67.78164482116699 Val_Reconstruction : 65.11494636535645 Val_KL : 2.6666977405548096\n","Epoch: 187/5000  Traning Loss: 69.77286338806152  Train_Reconstruction: 67.05667781829834  Train_KL: 2.716186225414276  Validation Loss : 67.56147956848145 Val_Reconstruction : 64.9183349609375 Val_KL : 2.6431440114974976\n","Epoch: 188/5000  Traning Loss: 69.98399353027344  Train_Reconstruction: 67.2815933227539  Train_KL: 2.7024011313915253  Validation Loss : 67.94407844543457 Val_Reconstruction : 65.29831314086914 Val_KL : 2.6457661390304565\n","Epoch: 189/5000  Traning Loss: 70.30649280548096  Train_Reconstruction: 67.59696769714355  Train_KL: 2.709525853395462  Validation Loss : 68.3103199005127 Val_Reconstruction : 65.6473159790039 Val_KL : 2.6630032062530518\n","Epoch: 190/5000  Traning Loss: 70.47882652282715  Train_Reconstruction: 67.75411128997803  Train_KL: 2.724715381860733  Validation Loss : 68.36973762512207 Val_Reconstruction : 65.71155738830566 Val_KL : 2.658179759979248\n","Epoch: 191/5000  Traning Loss: 70.53058815002441  Train_Reconstruction: 67.82091331481934  Train_KL: 2.7096753418445587  Validation Loss : 68.05099105834961 Val_Reconstruction : 65.38829612731934 Val_KL : 2.662696957588196\n","Epoch: 192/5000  Traning Loss: 70.14143371582031  Train_Reconstruction: 67.43346214294434  Train_KL: 2.707970470190048  Validation Loss : 67.82443237304688 Val_Reconstruction : 65.18539810180664 Val_KL : 2.6390328407287598\n","Epoch: 193/5000  Traning Loss: 69.98654556274414  Train_Reconstruction: 67.27318859100342  Train_KL: 2.713356524705887  Validation Loss : 68.04888153076172 Val_Reconstruction : 65.39235877990723 Val_KL : 2.6565229892730713\n","Epoch: 194/5000  Traning Loss: 70.12199401855469  Train_Reconstruction: 67.3970251083374  Train_KL: 2.72497022151947  Validation Loss : 67.92768859863281 Val_Reconstruction : 65.26148223876953 Val_KL : 2.6662075519561768\n","Epoch: 195/5000  Traning Loss: 70.02045631408691  Train_Reconstruction: 67.30303382873535  Train_KL: 2.717424154281616  Validation Loss : 67.74646377563477 Val_Reconstruction : 65.0909652709961 Val_KL : 2.6554973125457764\n","Epoch: 196/5000  Traning Loss: 69.88884449005127  Train_Reconstruction: 67.18352508544922  Train_KL: 2.7053193747997284  Validation Loss : 67.64173316955566 Val_Reconstruction : 65.00111389160156 Val_KL : 2.6406197547912598\n","Epoch: 197/5000  Traning Loss: 69.8713960647583  Train_Reconstruction: 67.16312313079834  Train_KL: 2.7082713842391968  Validation Loss : 68.00804328918457 Val_Reconstruction : 65.36104011535645 Val_KL : 2.6470028162002563\n","Epoch: 198/5000  Traning Loss: 70.08287334442139  Train_Reconstruction: 67.38027858734131  Train_KL: 2.7025951147079468  Validation Loss : 67.82220268249512 Val_Reconstruction : 65.17426490783691 Val_KL : 2.6479376554489136\n","Epoch: 199/5000  Traning Loss: 69.90378284454346  Train_Reconstruction: 67.18686485290527  Train_KL: 2.7169159650802612  Validation Loss : 67.5064468383789 Val_Reconstruction : 64.84818077087402 Val_KL : 2.6582647562026978\n","Epoch: 200/5000  Traning Loss: 69.67923831939697  Train_Reconstruction: 66.95747470855713  Train_KL: 2.7217641174793243  Validation Loss : 67.47879600524902 Val_Reconstruction : 64.82305145263672 Val_KL : 2.6557451486587524\n","Epoch: 201/5000  Traning Loss: 69.47163200378418  Train_Reconstruction: 66.74828720092773  Train_KL: 2.723345994949341  Validation Loss : 67.31964874267578 Val_Reconstruction : 64.64439582824707 Val_KL : 2.6752508878707886\n","Epoch: 202/5000  Traning Loss: 69.55104064941406  Train_Reconstruction: 66.82282161712646  Train_KL: 2.728218197822571  Validation Loss : 67.42726135253906 Val_Reconstruction : 64.75247192382812 Val_KL : 2.674790143966675\n","Epoch: 203/5000  Traning Loss: 69.44574165344238  Train_Reconstruction: 66.71361064910889  Train_KL: 2.732130080461502  Validation Loss : 67.2642993927002 Val_Reconstruction : 64.59084892272949 Val_KL : 2.6734507083892822\n","Epoch: 204/5000  Traning Loss: 69.49546241760254  Train_Reconstruction: 66.77325630187988  Train_KL: 2.722204953432083  Validation Loss : 67.2103042602539 Val_Reconstruction : 64.55304336547852 Val_KL : 2.6572600603103638\n","Epoch: 205/5000  Traning Loss: 69.38752746582031  Train_Reconstruction: 66.6711196899414  Train_KL: 2.716408610343933  Validation Loss : 67.24829483032227 Val_Reconstruction : 64.58695220947266 Val_KL : 2.661341428756714\n","Epoch: 206/5000  Traning Loss: 69.51537322998047  Train_Reconstruction: 66.78593063354492  Train_KL: 2.729442685842514  Validation Loss : 67.31359481811523 Val_Reconstruction : 64.63537216186523 Val_KL : 2.678222894668579\n","Epoch: 207/5000  Traning Loss: 69.47297191619873  Train_Reconstruction: 66.75302124023438  Train_KL: 2.719951778650284  Validation Loss : 67.40106773376465 Val_Reconstruction : 64.75041770935059 Val_KL : 2.650649070739746\n","Epoch: 208/5000  Traning Loss: 69.20455169677734  Train_Reconstruction: 66.49225044250488  Train_KL: 2.7123018503189087  Validation Loss : 67.13407707214355 Val_Reconstruction : 64.47299575805664 Val_KL : 2.6610796451568604\n","Epoch: 209/5000  Traning Loss: 69.22231674194336  Train_Reconstruction: 66.50454425811768  Train_KL: 2.7177720069885254  Validation Loss : 67.08580017089844 Val_Reconstruction : 64.42063331604004 Val_KL : 2.6651673316955566\n","Epoch: 210/5000  Traning Loss: 69.18175220489502  Train_Reconstruction: 66.44585037231445  Train_KL: 2.735902726650238  Validation Loss : 67.19942474365234 Val_Reconstruction : 64.51274871826172 Val_KL : 2.686676025390625\n","Epoch: 211/5000  Traning Loss: 69.07880783081055  Train_Reconstruction: 66.34458351135254  Train_KL: 2.734223961830139  Validation Loss : 66.94659423828125 Val_Reconstruction : 64.27265167236328 Val_KL : 2.6739431619644165\n","Epoch: 212/5000  Traning Loss: 68.8848762512207  Train_Reconstruction: 66.15348148345947  Train_KL: 2.731395274400711  Validation Loss : 66.77557373046875 Val_Reconstruction : 64.10647201538086 Val_KL : 2.669099450111389\n","Epoch: 213/5000  Traning Loss: 68.82874774932861  Train_Reconstruction: 66.10567569732666  Train_KL: 2.723072588443756  Validation Loss : 66.68244552612305 Val_Reconstruction : 64.01617622375488 Val_KL : 2.6662710905075073\n","Epoch: 214/5000  Traning Loss: 68.89231395721436  Train_Reconstruction: 66.1680498123169  Train_KL: 2.7242634296417236  Validation Loss : 66.72611427307129 Val_Reconstruction : 64.06628608703613 Val_KL : 2.6598299741744995\n","Epoch: 215/5000  Traning Loss: 68.87166118621826  Train_Reconstruction: 66.15524196624756  Train_KL: 2.716418206691742  Validation Loss : 67.01281929016113 Val_Reconstruction : 64.3520450592041 Val_KL : 2.660776376724243\n","Epoch: 216/5000  Traning Loss: 68.82707595825195  Train_Reconstruction: 66.10364532470703  Train_KL: 2.7234299778938293  Validation Loss : 66.59449768066406 Val_Reconstruction : 63.92829132080078 Val_KL : 2.6662060022354126\n","Epoch: 217/5000  Traning Loss: 68.78775787353516  Train_Reconstruction: 66.0594835281372  Train_KL: 2.728274703025818  Validation Loss : 66.58122825622559 Val_Reconstruction : 63.89815902709961 Val_KL : 2.6830679178237915\n","Epoch: 218/5000  Traning Loss: 68.66989517211914  Train_Reconstruction: 65.93564224243164  Train_KL: 2.734253168106079  Validation Loss : 66.62265586853027 Val_Reconstruction : 63.94843673706055 Val_KL : 2.674219250679016\n","Epoch: 219/5000  Traning Loss: 68.99395179748535  Train_Reconstruction: 66.26449298858643  Train_KL: 2.7294576466083527  Validation Loss : 66.90632438659668 Val_Reconstruction : 64.23751258850098 Val_KL : 2.6688103675842285\n","Epoch: 220/5000  Traning Loss: 68.72157192230225  Train_Reconstruction: 65.99163484573364  Train_KL: 2.7299371659755707  Validation Loss : 66.52425384521484 Val_Reconstruction : 63.85155487060547 Val_KL : 2.672699451446533\n","Epoch: 221/5000  Traning Loss: 68.74253463745117  Train_Reconstruction: 66.02186870574951  Train_KL: 2.720666140317917  Validation Loss : 66.63134956359863 Val_Reconstruction : 63.97275161743164 Val_KL : 2.6585984230041504\n","Epoch: 222/5000  Traning Loss: 68.66654586791992  Train_Reconstruction: 65.93276929855347  Train_KL: 2.7337754666805267  Validation Loss : 66.8610725402832 Val_Reconstruction : 64.18006324768066 Val_KL : 2.681009888648987\n","Epoch: 223/5000  Traning Loss: 68.59548759460449  Train_Reconstruction: 65.86308193206787  Train_KL: 2.7324056029319763  Validation Loss : 66.40149307250977 Val_Reconstruction : 63.72211837768555 Val_KL : 2.679374575614929\n","Epoch: 224/5000  Traning Loss: 68.56699657440186  Train_Reconstruction: 65.83423519134521  Train_KL: 2.73276349902153  Validation Loss : 66.45964050292969 Val_Reconstruction : 63.79110336303711 Val_KL : 2.6685374975204468\n","Epoch: 225/5000  Traning Loss: 68.63078880310059  Train_Reconstruction: 65.89995765686035  Train_KL: 2.730830818414688  Validation Loss : 66.62142181396484 Val_Reconstruction : 63.93684005737305 Val_KL : 2.684581756591797\n","Epoch: 226/5000  Traning Loss: 68.89007186889648  Train_Reconstruction: 66.1504898071289  Train_KL: 2.73958221077919  Validation Loss : 66.86055183410645 Val_Reconstruction : 64.17593765258789 Val_KL : 2.684613585472107\n","Epoch: 227/5000  Traning Loss: 68.78631687164307  Train_Reconstruction: 66.05149555206299  Train_KL: 2.7348205745220184  Validation Loss : 66.87004089355469 Val_Reconstruction : 64.19722557067871 Val_KL : 2.672814130783081\n","Epoch: 228/5000  Traning Loss: 68.57067680358887  Train_Reconstruction: 65.84886837005615  Train_KL: 2.721809506416321  Validation Loss : 66.43699836730957 Val_Reconstruction : 63.779767990112305 Val_KL : 2.657231330871582\n","Epoch: 229/5000  Traning Loss: 68.3865909576416  Train_Reconstruction: 65.6600341796875  Train_KL: 2.7265580892562866  Validation Loss : 66.32398986816406 Val_Reconstruction : 63.64023208618164 Val_KL : 2.683758497238159\n","Epoch: 230/5000  Traning Loss: 68.56311511993408  Train_Reconstruction: 65.83524513244629  Train_KL: 2.7278694212436676  Validation Loss : 66.72928428649902 Val_Reconstruction : 64.0495433807373 Val_KL : 2.6797388792037964\n","Epoch: 231/5000  Traning Loss: 68.6568832397461  Train_Reconstruction: 65.92879676818848  Train_KL: 2.72808638215065  Validation Loss : 66.45434188842773 Val_Reconstruction : 63.793182373046875 Val_KL : 2.6611591577529907\n","Epoch: 232/5000  Traning Loss: 68.36346912384033  Train_Reconstruction: 65.6436710357666  Train_KL: 2.719798892736435  Validation Loss : 66.49444389343262 Val_Reconstruction : 63.82088851928711 Val_KL : 2.67355740070343\n","Epoch: 233/5000  Traning Loss: 68.33492279052734  Train_Reconstruction: 65.59352922439575  Train_KL: 2.7413937151432037  Validation Loss : 66.33519172668457 Val_Reconstruction : 63.645389556884766 Val_KL : 2.6898034811019897\n","Epoch: 234/5000  Traning Loss: 68.29069805145264  Train_Reconstruction: 65.55503368377686  Train_KL: 2.7356653213500977  Validation Loss : 66.4120044708252 Val_Reconstruction : 63.735137939453125 Val_KL : 2.676864743232727\n","Epoch: 235/5000  Traning Loss: 68.27084445953369  Train_Reconstruction: 65.53381538391113  Train_KL: 2.7370302975177765  Validation Loss : 66.23742294311523 Val_Reconstruction : 63.540748596191406 Val_KL : 2.6966755390167236\n","Epoch: 236/5000  Traning Loss: 68.29427433013916  Train_Reconstruction: 65.54991149902344  Train_KL: 2.7443618774414062  Validation Loss : 66.7281665802002 Val_Reconstruction : 64.03912734985352 Val_KL : 2.6890382766723633\n","Epoch: 237/5000  Traning Loss: 68.4702959060669  Train_Reconstruction: 65.7299861907959  Train_KL: 2.7403091490268707  Validation Loss : 66.44709014892578 Val_Reconstruction : 63.757930755615234 Val_KL : 2.689160704612732\n","Epoch: 238/5000  Traning Loss: 68.37120056152344  Train_Reconstruction: 65.63892364501953  Train_KL: 2.7322766184806824  Validation Loss : 66.93357467651367 Val_Reconstruction : 64.26011848449707 Val_KL : 2.673455595970154\n","Epoch: 239/5000  Traning Loss: 68.40357971191406  Train_Reconstruction: 65.66828346252441  Train_KL: 2.7352969348430634  Validation Loss : 66.32138442993164 Val_Reconstruction : 63.62808799743652 Val_KL : 2.6932971477508545\n","Epoch: 240/5000  Traning Loss: 68.32236957550049  Train_Reconstruction: 65.57822561264038  Train_KL: 2.7441458702087402  Validation Loss : 66.52557182312012 Val_Reconstruction : 63.83832931518555 Val_KL : 2.6872435808181763\n","Epoch: 241/5000  Traning Loss: 68.22486782073975  Train_Reconstruction: 65.47732353210449  Train_KL: 2.747545540332794  Validation Loss : 66.22295570373535 Val_Reconstruction : 63.532711029052734 Val_KL : 2.690246105194092\n","Epoch: 242/5000  Traning Loss: 68.08585929870605  Train_Reconstruction: 65.33136320114136  Train_KL: 2.754495620727539  Validation Loss : 66.22890281677246 Val_Reconstruction : 63.520620346069336 Val_KL : 2.7082810401916504\n","Epoch: 243/5000  Traning Loss: 68.14486885070801  Train_Reconstruction: 65.38733386993408  Train_KL: 2.757534295320511  Validation Loss : 66.38743591308594 Val_Reconstruction : 63.68976020812988 Val_KL : 2.69767689704895\n","Epoch: 244/5000  Traning Loss: 67.98085880279541  Train_Reconstruction: 65.23009157180786  Train_KL: 2.75076761841774  Validation Loss : 65.99139785766602 Val_Reconstruction : 63.28903007507324 Val_KL : 2.702367067337036\n","Epoch: 245/5000  Traning Loss: 67.89745330810547  Train_Reconstruction: 65.13206481933594  Train_KL: 2.7653889060020447  Validation Loss : 66.11128234863281 Val_Reconstruction : 63.41170883178711 Val_KL : 2.6995726823806763\n","Epoch: 246/5000  Traning Loss: 67.98268508911133  Train_Reconstruction: 65.22583055496216  Train_KL: 2.756855070590973  Validation Loss : 66.09093475341797 Val_Reconstruction : 63.38322830200195 Val_KL : 2.707708954811096\n","Epoch: 247/5000  Traning Loss: 67.89878940582275  Train_Reconstruction: 65.14780569076538  Train_KL: 2.750984102487564  Validation Loss : 66.02927589416504 Val_Reconstruction : 63.32290458679199 Val_KL : 2.706371307373047\n","Epoch: 248/5000  Traning Loss: 67.92266845703125  Train_Reconstruction: 65.16972732543945  Train_KL: 2.752941370010376  Validation Loss : 65.90941047668457 Val_Reconstruction : 63.21051025390625 Val_KL : 2.69890034198761\n","Epoch: 249/5000  Traning Loss: 67.73655891418457  Train_Reconstruction: 64.98682737350464  Train_KL: 2.7497316002845764  Validation Loss : 65.8848705291748 Val_Reconstruction : 63.192447662353516 Val_KL : 2.692421793937683\n","Epoch: 250/5000  Traning Loss: 68.03738498687744  Train_Reconstruction: 65.29269409179688  Train_KL: 2.7446900606155396  Validation Loss : 66.43785095214844 Val_Reconstruction : 63.74650955200195 Val_KL : 2.6913414001464844\n","Epoch: 251/5000  Traning Loss: 68.09899425506592  Train_Reconstruction: 65.35532760620117  Train_KL: 2.743668109178543  Validation Loss : 66.05551338195801 Val_Reconstruction : 63.373313903808594 Val_KL : 2.6822006702423096\n","Epoch: 252/5000  Traning Loss: 67.93973445892334  Train_Reconstruction: 65.20126914978027  Train_KL: 2.738466054201126  Validation Loss : 65.87464141845703 Val_Reconstruction : 63.19894027709961 Val_KL : 2.6757030487060547\n","Epoch: 253/5000  Traning Loss: 68.05244541168213  Train_Reconstruction: 65.31666660308838  Train_KL: 2.735777586698532  Validation Loss : 65.82785606384277 Val_Reconstruction : 63.12472343444824 Val_KL : 2.703134059906006\n","Epoch: 254/5000  Traning Loss: 67.75942134857178  Train_Reconstruction: 65.01766967773438  Train_KL: 2.7417517006397247  Validation Loss : 65.88486671447754 Val_Reconstruction : 63.192800521850586 Val_KL : 2.6920650005340576\n","Epoch: 255/5000  Traning Loss: 67.8837423324585  Train_Reconstruction: 65.13572216033936  Train_KL: 2.7480205297470093  Validation Loss : 65.74928665161133 Val_Reconstruction : 63.0491886138916 Val_KL : 2.700095534324646\n","Epoch: 256/5000  Traning Loss: 67.69019412994385  Train_Reconstruction: 64.9390001296997  Train_KL: 2.751194417476654  Validation Loss : 65.6637077331543 Val_Reconstruction : 62.954973220825195 Val_KL : 2.708732485771179\n","Epoch: 257/5000  Traning Loss: 67.70757675170898  Train_Reconstruction: 64.95047903060913  Train_KL: 2.7570992708206177  Validation Loss : 65.9679069519043 Val_Reconstruction : 63.255929946899414 Val_KL : 2.711977958679199\n","Epoch: 258/5000  Traning Loss: 67.67336654663086  Train_Reconstruction: 64.91698360443115  Train_KL: 2.756383329629898  Validation Loss : 65.77080726623535 Val_Reconstruction : 63.06282424926758 Val_KL : 2.7079843282699585\n","Epoch: 259/5000  Traning Loss: 67.85186195373535  Train_Reconstruction: 65.1060700416565  Train_KL: 2.7457916140556335  Validation Loss : 66.05641555786133 Val_Reconstruction : 63.36477851867676 Val_KL : 2.691637635231018\n","Epoch: 260/5000  Traning Loss: 67.77273941040039  Train_Reconstruction: 65.02251434326172  Train_KL: 2.7502256631851196  Validation Loss : 65.6630744934082 Val_Reconstruction : 62.96228218078613 Val_KL : 2.7007912397384644\n","Epoch: 261/5000  Traning Loss: 67.64449119567871  Train_Reconstruction: 64.8934440612793  Train_KL: 2.751047194004059  Validation Loss : 65.80360984802246 Val_Reconstruction : 63.11124610900879 Val_KL : 2.6923636198043823\n","Epoch: 262/5000  Traning Loss: 67.84803295135498  Train_Reconstruction: 65.1090612411499  Train_KL: 2.738971531391144  Validation Loss : 66.09135627746582 Val_Reconstruction : 63.3941535949707 Val_KL : 2.697200894355774\n","Epoch: 263/5000  Traning Loss: 68.07964611053467  Train_Reconstruction: 65.33210754394531  Train_KL: 2.7475385069847107  Validation Loss : 66.3134994506836 Val_Reconstruction : 63.620927810668945 Val_KL : 2.692572236061096\n","Epoch: 264/5000  Traning Loss: 67.87553119659424  Train_Reconstruction: 65.13021564483643  Train_KL: 2.745315730571747  Validation Loss : 65.5980396270752 Val_Reconstruction : 62.9053897857666 Val_KL : 2.692649245262146\n","Epoch: 265/5000  Traning Loss: 67.48003005981445  Train_Reconstruction: 64.73305988311768  Train_KL: 2.7469699382781982  Validation Loss : 65.68025398254395 Val_Reconstruction : 62.983802795410156 Val_KL : 2.696453094482422\n","Epoch: 266/5000  Traning Loss: 67.3857069015503  Train_Reconstruction: 64.63471698760986  Train_KL: 2.7509906589984894  Validation Loss : 65.35111618041992 Val_Reconstruction : 62.652923583984375 Val_KL : 2.6981940269470215\n","Epoch: 267/5000  Traning Loss: 67.29423427581787  Train_Reconstruction: 64.53598356246948  Train_KL: 2.758251339197159  Validation Loss : 65.70315170288086 Val_Reconstruction : 62.99810791015625 Val_KL : 2.7050429582595825\n","Epoch: 268/5000  Traning Loss: 67.50619792938232  Train_Reconstruction: 64.75048637390137  Train_KL: 2.755712330341339  Validation Loss : 65.75037956237793 Val_Reconstruction : 63.04854202270508 Val_KL : 2.701836585998535\n","Epoch: 269/5000  Traning Loss: 67.73521041870117  Train_Reconstruction: 64.96741533279419  Train_KL: 2.7677955627441406  Validation Loss : 65.59805679321289 Val_Reconstruction : 62.877220153808594 Val_KL : 2.720837950706482\n","Epoch: 270/5000  Traning Loss: 67.37416648864746  Train_Reconstruction: 64.61165952682495  Train_KL: 2.762506604194641  Validation Loss : 65.27656936645508 Val_Reconstruction : 62.57135772705078 Val_KL : 2.7052100896835327\n","Epoch: 271/5000  Traning Loss: 67.22673416137695  Train_Reconstruction: 64.47397565841675  Train_KL: 2.752757430076599  Validation Loss : 65.33254814147949 Val_Reconstruction : 62.628732681274414 Val_KL : 2.7038142681121826\n","Epoch: 272/5000  Traning Loss: 67.28111171722412  Train_Reconstruction: 64.52079343795776  Train_KL: 2.7603200376033783  Validation Loss : 65.19723510742188 Val_Reconstruction : 62.494224548339844 Val_KL : 2.703009843826294\n","Epoch: 273/5000  Traning Loss: 67.11720085144043  Train_Reconstruction: 64.35757398605347  Train_KL: 2.7596268951892853  Validation Loss : 65.18083953857422 Val_Reconstruction : 62.47401428222656 Val_KL : 2.7068248987197876\n","Epoch: 274/5000  Traning Loss: 67.16504859924316  Train_Reconstruction: 64.41113519668579  Train_KL: 2.753913015127182  Validation Loss : 65.61639213562012 Val_Reconstruction : 62.908287048339844 Val_KL : 2.7081035375595093\n","Epoch: 275/5000  Traning Loss: 67.26848316192627  Train_Reconstruction: 64.5028920173645  Train_KL: 2.765590637922287  Validation Loss : 65.42529487609863 Val_Reconstruction : 62.70948791503906 Val_KL : 2.7158056497573853\n","Epoch: 276/5000  Traning Loss: 67.29286766052246  Train_Reconstruction: 64.52327394485474  Train_KL: 2.7695934176445007  Validation Loss : 65.43653106689453 Val_Reconstruction : 62.72045135498047 Val_KL : 2.7160779237747192\n","Epoch: 277/5000  Traning Loss: 67.0698299407959  Train_Reconstruction: 64.30600309371948  Train_KL: 2.7638265192508698  Validation Loss : 65.23333740234375 Val_Reconstruction : 62.53264617919922 Val_KL : 2.7006893157958984\n","Epoch: 278/5000  Traning Loss: 67.0101671218872  Train_Reconstruction: 64.26013040542603  Train_KL: 2.7500363290309906  Validation Loss : 65.16137504577637 Val_Reconstruction : 62.465946197509766 Val_KL : 2.6954314708709717\n","Epoch: 279/5000  Traning Loss: 67.04084396362305  Train_Reconstruction: 64.29511594772339  Train_KL: 2.745727628469467  Validation Loss : 65.10237884521484 Val_Reconstruction : 62.405540466308594 Val_KL : 2.6968367099761963\n","Epoch: 280/5000  Traning Loss: 66.83993339538574  Train_Reconstruction: 64.07720136642456  Train_KL: 2.7627323865890503  Validation Loss : 64.99276733398438 Val_Reconstruction : 62.27518653869629 Val_KL : 2.7175806760787964\n","Epoch: 281/5000  Traning Loss: 66.88134670257568  Train_Reconstruction: 64.12690830230713  Train_KL: 2.754439562559128  Validation Loss : 65.1310920715332 Val_Reconstruction : 62.4281005859375 Val_KL : 2.7029913663864136\n","Epoch: 282/5000  Traning Loss: 66.83502674102783  Train_Reconstruction: 64.0740761756897  Train_KL: 2.760951727628708  Validation Loss : 64.98922538757324 Val_Reconstruction : 62.28287696838379 Val_KL : 2.7063461542129517\n","Epoch: 283/5000  Traning Loss: 66.93105030059814  Train_Reconstruction: 64.1778450012207  Train_KL: 2.7532056868076324  Validation Loss : 65.24942779541016 Val_Reconstruction : 62.54119110107422 Val_KL : 2.7082356214523315\n","Epoch: 284/5000  Traning Loss: 66.94381332397461  Train_Reconstruction: 64.17389440536499  Train_KL: 2.7699196338653564  Validation Loss : 65.05617904663086 Val_Reconstruction : 62.34206771850586 Val_KL : 2.7141127586364746\n","Epoch: 285/5000  Traning Loss: 66.90547943115234  Train_Reconstruction: 64.14414834976196  Train_KL: 2.7613300681114197  Validation Loss : 65.41430854797363 Val_Reconstruction : 62.69923400878906 Val_KL : 2.7150744199752808\n","Epoch: 286/5000  Traning Loss: 67.26372051239014  Train_Reconstruction: 64.49887466430664  Train_KL: 2.7648465037345886  Validation Loss : 64.93607902526855 Val_Reconstruction : 62.22908973693848 Val_KL : 2.7069884538650513\n","Epoch: 287/5000  Traning Loss: 66.95185852050781  Train_Reconstruction: 64.19398355484009  Train_KL: 2.7578737437725067  Validation Loss : 65.17731666564941 Val_Reconstruction : 62.46021842956543 Val_KL : 2.717099905014038\n","Epoch: 288/5000  Traning Loss: 66.98998355865479  Train_Reconstruction: 64.23350191116333  Train_KL: 2.7564820647239685  Validation Loss : 64.92170715332031 Val_Reconstruction : 62.21552848815918 Val_KL : 2.7061808109283447\n","Epoch: 289/5000  Traning Loss: 66.6992769241333  Train_Reconstruction: 63.944756507873535  Train_KL: 2.7545201182365417  Validation Loss : 64.85984802246094 Val_Reconstruction : 62.15015983581543 Val_KL : 2.7096880674362183\n","Epoch: 290/5000  Traning Loss: 66.81675434112549  Train_Reconstruction: 64.05441188812256  Train_KL: 2.7623420655727386  Validation Loss : 64.80059814453125 Val_Reconstruction : 62.088666915893555 Val_KL : 2.7119321823120117\n","Epoch: 291/5000  Traning Loss: 67.04102039337158  Train_Reconstruction: 64.27378845214844  Train_KL: 2.767232298851013  Validation Loss : 65.40327072143555 Val_Reconstruction : 62.67658805847168 Val_KL : 2.7266838550567627\n","Epoch: 292/5000  Traning Loss: 66.9609546661377  Train_Reconstruction: 64.17921209335327  Train_KL: 2.781741976737976  Validation Loss : 65.23054313659668 Val_Reconstruction : 62.50619697570801 Val_KL : 2.7243467569351196\n","Epoch: 293/5000  Traning Loss: 67.01449584960938  Train_Reconstruction: 64.2450304031372  Train_KL: 2.7694664001464844  Validation Loss : 65.0705394744873 Val_Reconstruction : 62.3537483215332 Val_KL : 2.716790795326233\n","Epoch: 294/5000  Traning Loss: 66.93007946014404  Train_Reconstruction: 64.16578674316406  Train_KL: 2.7642925679683685  Validation Loss : 65.11554527282715 Val_Reconstruction : 62.39894104003906 Val_KL : 2.716603994369507\n","Epoch: 295/5000  Traning Loss: 66.8417329788208  Train_Reconstruction: 64.08090400695801  Train_KL: 2.7608284950256348  Validation Loss : 65.03094482421875 Val_Reconstruction : 62.31117630004883 Val_KL : 2.7197664976119995\n","Epoch: 296/5000  Traning Loss: 66.86935138702393  Train_Reconstruction: 64.09665536880493  Train_KL: 2.7726942896842957  Validation Loss : 64.9856071472168 Val_Reconstruction : 62.27557373046875 Val_KL : 2.7100329399108887\n","Epoch: 297/5000  Traning Loss: 66.63607501983643  Train_Reconstruction: 63.86999702453613  Train_KL: 2.7660782039165497  Validation Loss : 64.80778121948242 Val_Reconstruction : 62.08125114440918 Val_KL : 2.7265321016311646\n","Epoch: 298/5000  Traning Loss: 66.83143138885498  Train_Reconstruction: 64.06233739852905  Train_KL: 2.7690942883491516  Validation Loss : 65.12171363830566 Val_Reconstruction : 62.39277267456055 Val_KL : 2.728940725326538\n","Epoch: 299/5000  Traning Loss: 67.02021884918213  Train_Reconstruction: 64.24248600006104  Train_KL: 2.7777332961559296  Validation Loss : 65.10979270935059 Val_Reconstruction : 62.40519714355469 Val_KL : 2.704597592353821\n","Epoch: 300/5000  Traning Loss: 66.82799434661865  Train_Reconstruction: 64.0739164352417  Train_KL: 2.754078894853592  Validation Loss : 64.74558067321777 Val_Reconstruction : 62.04741287231445 Val_KL : 2.698168158531189\n","Epoch: 301/5000  Traning Loss: 66.52303123474121  Train_Reconstruction: 63.76888036727905  Train_KL: 2.7541509568691254  Validation Loss : 64.61628532409668 Val_Reconstruction : 61.897783279418945 Val_KL : 2.7185001373291016\n","Epoch: 302/5000  Traning Loss: 66.5023832321167  Train_Reconstruction: 63.73207378387451  Train_KL: 2.7703088223934174  Validation Loss : 64.63788223266602 Val_Reconstruction : 61.914323806762695 Val_KL : 2.7235580682754517\n","Epoch: 303/5000  Traning Loss: 66.3562068939209  Train_Reconstruction: 63.58198833465576  Train_KL: 2.774218887090683  Validation Loss : 64.46127128601074 Val_Reconstruction : 61.738521575927734 Val_KL : 2.7227494716644287\n","Epoch: 304/5000  Traning Loss: 66.46756839752197  Train_Reconstruction: 63.70503807067871  Train_KL: 2.762530505657196  Validation Loss : 64.80474853515625 Val_Reconstruction : 62.08109664916992 Val_KL : 2.7236530780792236\n","Epoch: 305/5000  Traning Loss: 66.55892848968506  Train_Reconstruction: 63.79240369796753  Train_KL: 2.766524851322174  Validation Loss : 64.47171401977539 Val_Reconstruction : 61.75563430786133 Val_KL : 2.71607768535614\n","Epoch: 306/5000  Traning Loss: 66.44734001159668  Train_Reconstruction: 63.67005777359009  Train_KL: 2.7772815227508545  Validation Loss : 64.77945709228516 Val_Reconstruction : 62.03301239013672 Val_KL : 2.7464449405670166\n","Epoch: 307/5000  Traning Loss: 66.57929420471191  Train_Reconstruction: 63.79523229598999  Train_KL: 2.784061998128891  Validation Loss : 64.74802017211914 Val_Reconstruction : 62.02341651916504 Val_KL : 2.724601984024048\n","Epoch: 308/5000  Traning Loss: 66.53574752807617  Train_Reconstruction: 63.76500844955444  Train_KL: 2.77073872089386  Validation Loss : 64.75348472595215 Val_Reconstruction : 62.02873229980469 Val_KL : 2.7247501611709595\n","Epoch: 309/5000  Traning Loss: 66.5061388015747  Train_Reconstruction: 63.73732900619507  Train_KL: 2.768809884786606  Validation Loss : 64.3963794708252 Val_Reconstruction : 61.67286682128906 Val_KL : 2.723510980606079\n","Epoch: 310/5000  Traning Loss: 66.39537811279297  Train_Reconstruction: 63.61884117126465  Train_KL: 2.7765369713306427  Validation Loss : 64.70864677429199 Val_Reconstruction : 61.97209548950195 Val_KL : 2.7365490198135376\n","Epoch: 311/5000  Traning Loss: 66.92062187194824  Train_Reconstruction: 64.13442993164062  Train_KL: 2.786191910505295  Validation Loss : 65.70226287841797 Val_Reconstruction : 62.96822547912598 Val_KL : 2.734036684036255\n","Epoch: 312/5000  Traning Loss: 67.05269145965576  Train_Reconstruction: 64.27387237548828  Train_KL: 2.7788201570510864  Validation Loss : 65.0030288696289 Val_Reconstruction : 62.27799987792969 Val_KL : 2.7250272035598755\n","Epoch: 313/5000  Traning Loss: 66.58462524414062  Train_Reconstruction: 63.813247203826904  Train_KL: 2.77137890458107  Validation Loss : 64.48368835449219 Val_Reconstruction : 61.76285934448242 Val_KL : 2.7208292484283447\n","Epoch: 314/5000  Traning Loss: 66.1495714187622  Train_Reconstruction: 63.37765455245972  Train_KL: 2.771918088197708  Validation Loss : 64.42844581604004 Val_Reconstruction : 61.69390106201172 Val_KL : 2.734547257423401\n","Epoch: 315/5000  Traning Loss: 66.27997970581055  Train_Reconstruction: 63.50291347503662  Train_KL: 2.777065396308899  Validation Loss : 64.50782585144043 Val_Reconstruction : 61.77008819580078 Val_KL : 2.7377402782440186\n","Epoch: 316/5000  Traning Loss: 66.42659950256348  Train_Reconstruction: 63.64270544052124  Train_KL: 2.7838935554027557  Validation Loss : 64.6776008605957 Val_Reconstruction : 61.935598373413086 Val_KL : 2.742000937461853\n","Epoch: 317/5000  Traning Loss: 66.41726016998291  Train_Reconstruction: 63.625885009765625  Train_KL: 2.791374087333679  Validation Loss : 64.5302677154541 Val_Reconstruction : 61.784393310546875 Val_KL : 2.745874285697937\n","Epoch: 318/5000  Traning Loss: 66.13692378997803  Train_Reconstruction: 63.34839630126953  Train_KL: 2.788528323173523  Validation Loss : 64.24788665771484 Val_Reconstruction : 61.52126884460449 Val_KL : 2.726616621017456\n","Epoch: 319/5000  Traning Loss: 66.11540460586548  Train_Reconstruction: 63.33864879608154  Train_KL: 2.776755154132843  Validation Loss : 64.23382949829102 Val_Reconstruction : 61.49853706359863 Val_KL : 2.7352927923202515\n","Epoch: 320/5000  Traning Loss: 66.15571689605713  Train_Reconstruction: 63.381571769714355  Train_KL: 2.7741456627845764  Validation Loss : 64.52508735656738 Val_Reconstruction : 61.806894302368164 Val_KL : 2.718191981315613\n","Epoch: 321/5000  Traning Loss: 66.23277759552002  Train_Reconstruction: 63.45925283432007  Train_KL: 2.773524224758148  Validation Loss : 64.49556922912598 Val_Reconstruction : 61.764896392822266 Val_KL : 2.7306735515594482\n","Epoch: 322/5000  Traning Loss: 66.17023277282715  Train_Reconstruction: 63.3910174369812  Train_KL: 2.779214560985565  Validation Loss : 64.3667163848877 Val_Reconstruction : 61.62785530090332 Val_KL : 2.7388592958450317\n","Epoch: 323/5000  Traning Loss: 66.0864782333374  Train_Reconstruction: 63.30058765411377  Train_KL: 2.785891503095627  Validation Loss : 64.24732971191406 Val_Reconstruction : 61.51087951660156 Val_KL : 2.736448287963867\n","Epoch: 324/5000  Traning Loss: 66.02899074554443  Train_Reconstruction: 63.25331449508667  Train_KL: 2.7756763696670532  Validation Loss : 64.26227378845215 Val_Reconstruction : 61.5255241394043 Val_KL : 2.73675000667572\n","Epoch: 325/5000  Traning Loss: 66.07273769378662  Train_Reconstruction: 63.28671932220459  Train_KL: 2.786017566919327  Validation Loss : 64.53232192993164 Val_Reconstruction : 61.78331756591797 Val_KL : 2.7490047216415405\n","Epoch: 326/5000  Traning Loss: 66.1162462234497  Train_Reconstruction: 63.322988510131836  Train_KL: 2.793258547782898  Validation Loss : 64.32465171813965 Val_Reconstruction : 61.58504295349121 Val_KL : 2.739608407020569\n","Epoch: 327/5000  Traning Loss: 66.18890571594238  Train_Reconstruction: 63.400437355041504  Train_KL: 2.7884693443775177  Validation Loss : 64.66832160949707 Val_Reconstruction : 61.92775917053223 Val_KL : 2.74056339263916\n","Epoch: 328/5000  Traning Loss: 66.48317432403564  Train_Reconstruction: 63.692405700683594  Train_KL: 2.7907700538635254  Validation Loss : 65.15964317321777 Val_Reconstruction : 62.41662406921387 Val_KL : 2.7430195808410645\n","Epoch: 329/5000  Traning Loss: 67.03142261505127  Train_Reconstruction: 64.2501311302185  Train_KL: 2.7812909483909607  Validation Loss : 65.64035415649414 Val_Reconstruction : 62.90611457824707 Val_KL : 2.7342416048049927\n","Epoch: 330/5000  Traning Loss: 66.9046459197998  Train_Reconstruction: 64.12304735183716  Train_KL: 2.781598597764969  Validation Loss : 64.92740249633789 Val_Reconstruction : 62.1900691986084 Val_KL : 2.737331748008728\n","Epoch: 331/5000  Traning Loss: 66.40501499176025  Train_Reconstruction: 63.623531341552734  Train_KL: 2.7814841866493225  Validation Loss : 64.47378540039062 Val_Reconstruction : 61.72258949279785 Val_KL : 2.751197099685669\n","Epoch: 332/5000  Traning Loss: 65.9790153503418  Train_Reconstruction: 63.182557106018066  Train_KL: 2.7964577674865723  Validation Loss : 64.01403617858887 Val_Reconstruction : 61.26961898803711 Val_KL : 2.744416832923889\n","Epoch: 333/5000  Traning Loss: 65.73764038085938  Train_Reconstruction: 62.95704889297485  Train_KL: 2.7805911004543304  Validation Loss : 64.02242088317871 Val_Reconstruction : 61.28013038635254 Val_KL : 2.7422913312911987\n","Epoch: 334/5000  Traning Loss: 65.84213590621948  Train_Reconstruction: 63.06845808029175  Train_KL: 2.773676425218582  Validation Loss : 64.2595157623291 Val_Reconstruction : 61.53554916381836 Val_KL : 2.7239686250686646\n","Epoch: 335/5000  Traning Loss: 66.14090347290039  Train_Reconstruction: 63.360435962677  Train_KL: 2.7804671823978424  Validation Loss : 64.83045387268066 Val_Reconstruction : 62.09196090698242 Val_KL : 2.7384936809539795\n","Epoch: 336/5000  Traning Loss: 66.06262874603271  Train_Reconstruction: 63.27562093734741  Train_KL: 2.7870065569877625  Validation Loss : 64.06644821166992 Val_Reconstruction : 61.316070556640625 Val_KL : 2.750378131866455\n","Epoch: 337/5000  Traning Loss: 66.09409427642822  Train_Reconstruction: 63.2935905456543  Train_KL: 2.800503581762314  Validation Loss : 64.51699829101562 Val_Reconstruction : 61.756181716918945 Val_KL : 2.760815978050232\n","Epoch: 338/5000  Traning Loss: 66.25015068054199  Train_Reconstruction: 63.4558162689209  Train_KL: 2.794335126876831  Validation Loss : 64.45602035522461 Val_Reconstruction : 61.723060607910156 Val_KL : 2.7329577207565308\n","Epoch: 339/5000  Traning Loss: 65.91826152801514  Train_Reconstruction: 63.129456996917725  Train_KL: 2.7888049483299255  Validation Loss : 64.06154441833496 Val_Reconstruction : 61.3096981048584 Val_KL : 2.7518439292907715\n","Epoch: 340/5000  Traning Loss: 65.91356754302979  Train_Reconstruction: 63.1275429725647  Train_KL: 2.7860240042209625  Validation Loss : 64.29106140136719 Val_Reconstruction : 61.558244705200195 Val_KL : 2.732818365097046\n","Epoch: 341/5000  Traning Loss: 66.28188037872314  Train_Reconstruction: 63.49616479873657  Train_KL: 2.7857156693935394  Validation Loss : 64.1807746887207 Val_Reconstruction : 61.442155838012695 Val_KL : 2.738617777824402\n","Epoch: 342/5000  Traning Loss: 66.10623598098755  Train_Reconstruction: 63.31637096405029  Train_KL: 2.789865344762802  Validation Loss : 64.10035133361816 Val_Reconstruction : 61.350189208984375 Val_KL : 2.750162720680237\n","Epoch: 343/5000  Traning Loss: 65.9806900024414  Train_Reconstruction: 63.184587478637695  Train_KL: 2.796103298664093  Validation Loss : 64.29384994506836 Val_Reconstruction : 61.54915809631348 Val_KL : 2.7446905374526978\n","Epoch: 344/5000  Traning Loss: 66.15428352355957  Train_Reconstruction: 63.36905288696289  Train_KL: 2.785230815410614  Validation Loss : 64.33990097045898 Val_Reconstruction : 61.59769630432129 Val_KL : 2.7422062158584595\n","Epoch: 345/5000  Traning Loss: 65.89085292816162  Train_Reconstruction: 63.09652805328369  Train_KL: 2.7943246364593506  Validation Loss : 64.30667495727539 Val_Reconstruction : 61.558380126953125 Val_KL : 2.7482948303222656\n","Epoch: 346/5000  Traning Loss: 66.13432693481445  Train_Reconstruction: 63.34847593307495  Train_KL: 2.7858511805534363  Validation Loss : 64.60437965393066 Val_Reconstruction : 61.86176300048828 Val_KL : 2.742616295814514\n","Epoch: 347/5000  Traning Loss: 66.15342903137207  Train_Reconstruction: 63.37042570114136  Train_KL: 2.7830042839050293  Validation Loss : 64.29193878173828 Val_Reconstruction : 61.551042556762695 Val_KL : 2.7408971786499023\n","Epoch: 348/5000  Traning Loss: 66.1892318725586  Train_Reconstruction: 63.41011047363281  Train_KL: 2.7791217863559723  Validation Loss : 64.52774429321289 Val_Reconstruction : 61.787431716918945 Val_KL : 2.7403149604797363\n","Epoch: 349/5000  Traning Loss: 66.21057510375977  Train_Reconstruction: 63.41219186782837  Train_KL: 2.798383414745331  Validation Loss : 64.62462043762207 Val_Reconstruction : 61.854570388793945 Val_KL : 2.7700499296188354\n","Epoch: 350/5000  Traning Loss: 65.76769256591797  Train_Reconstruction: 62.96674203872681  Train_KL: 2.8009505569934845  Validation Loss : 63.95341682434082 Val_Reconstruction : 61.19865798950195 Val_KL : 2.7547576427459717\n","Epoch: 351/5000  Traning Loss: 65.6246280670166  Train_Reconstruction: 62.82221698760986  Train_KL: 2.8024103343486786  Validation Loss : 63.963623046875 Val_Reconstruction : 61.19888114929199 Val_KL : 2.764740824699402\n","Epoch: 352/5000  Traning Loss: 65.70745849609375  Train_Reconstruction: 62.908838748931885  Train_KL: 2.798620581626892  Validation Loss : 63.97268867492676 Val_Reconstruction : 61.22810173034668 Val_KL : 2.7445859909057617\n","Epoch: 353/5000  Traning Loss: 65.71030521392822  Train_Reconstruction: 62.92149019241333  Train_KL: 2.788814127445221  Validation Loss : 64.14274215698242 Val_Reconstruction : 61.39365577697754 Val_KL : 2.7490859031677246\n","Epoch: 354/5000  Traning Loss: 65.99031352996826  Train_Reconstruction: 63.20044946670532  Train_KL: 2.7898636162281036  Validation Loss : 64.89798164367676 Val_Reconstruction : 62.164180755615234 Val_KL : 2.7337993383407593\n","Epoch: 355/5000  Traning Loss: 66.01685619354248  Train_Reconstruction: 63.22264909744263  Train_KL: 2.79420804977417  Validation Loss : 64.45783233642578 Val_Reconstruction : 61.699615478515625 Val_KL : 2.758217215538025\n","Epoch: 356/5000  Traning Loss: 66.28407287597656  Train_Reconstruction: 63.48409461975098  Train_KL: 2.799976795911789  Validation Loss : 64.6143741607666 Val_Reconstruction : 61.86613082885742 Val_KL : 2.748242139816284\n","Epoch: 357/5000  Traning Loss: 66.08209228515625  Train_Reconstruction: 63.28180265426636  Train_KL: 2.8002902269363403  Validation Loss : 64.57377624511719 Val_Reconstruction : 61.81256675720215 Val_KL : 2.7612112760543823\n","Epoch: 358/5000  Traning Loss: 65.79741477966309  Train_Reconstruction: 63.00583744049072  Train_KL: 2.7915771901607513  Validation Loss : 64.07054710388184 Val_Reconstruction : 61.32809066772461 Val_KL : 2.7424581050872803\n","Epoch: 359/5000  Traning Loss: 65.56227731704712  Train_Reconstruction: 62.770716190338135  Train_KL: 2.791561037302017  Validation Loss : 63.987091064453125 Val_Reconstruction : 61.23703956604004 Val_KL : 2.7500516176223755\n","Epoch: 360/5000  Traning Loss: 65.64204788208008  Train_Reconstruction: 62.85235023498535  Train_KL: 2.789697676897049  Validation Loss : 63.81381607055664 Val_Reconstruction : 61.06942558288574 Val_KL : 2.7443920373916626\n","Epoch: 361/5000  Traning Loss: 65.61230659484863  Train_Reconstruction: 62.81313371658325  Train_KL: 2.7991744577884674  Validation Loss : 63.934823989868164 Val_Reconstruction : 61.16736030578613 Val_KL : 2.7674659490585327\n","Epoch: 362/5000  Traning Loss: 65.52488279342651  Train_Reconstruction: 62.715965270996094  Train_KL: 2.8089180290699005  Validation Loss : 63.66867446899414 Val_Reconstruction : 60.90940284729004 Val_KL : 2.7592718601226807\n","Epoch: 363/5000  Traning Loss: 65.44838619232178  Train_Reconstruction: 62.64951467514038  Train_KL: 2.798871874809265  Validation Loss : 63.93385696411133 Val_Reconstruction : 61.19412422180176 Val_KL : 2.739731192588806\n","Epoch: 364/5000  Traning Loss: 65.73974895477295  Train_Reconstruction: 62.95992994308472  Train_KL: 2.7798184156417847  Validation Loss : 64.2117862701416 Val_Reconstruction : 61.4763240814209 Val_KL : 2.735463500022888\n","Epoch: 365/5000  Traning Loss: 65.66728973388672  Train_Reconstruction: 62.88690996170044  Train_KL: 2.780379891395569  Validation Loss : 63.820064544677734 Val_Reconstruction : 61.08617401123047 Val_KL : 2.7338894605636597\n","Epoch: 366/5000  Traning Loss: 65.61411190032959  Train_Reconstruction: 62.817877769470215  Train_KL: 2.7962340116500854  Validation Loss : 63.920650482177734 Val_Reconstruction : 61.16607093811035 Val_KL : 2.754581570625305\n","Epoch: 367/5000  Traning Loss: 65.42756938934326  Train_Reconstruction: 62.62721395492554  Train_KL: 2.8003557920455933  Validation Loss : 63.71020317077637 Val_Reconstruction : 60.95320701599121 Val_KL : 2.756994843482971\n","Epoch: 368/5000  Traning Loss: 65.50698947906494  Train_Reconstruction: 62.705522537231445  Train_KL: 2.801467180252075  Validation Loss : 63.76027488708496 Val_Reconstruction : 61.00530815124512 Val_KL : 2.754964828491211\n","Epoch: 369/5000  Traning Loss: 65.46707344055176  Train_Reconstruction: 62.67098379135132  Train_KL: 2.796089917421341  Validation Loss : 63.79488563537598 Val_Reconstruction : 61.03711700439453 Val_KL : 2.7577685117721558\n","Epoch: 370/5000  Traning Loss: 65.23474407196045  Train_Reconstruction: 62.438748359680176  Train_KL: 2.795995980501175  Validation Loss : 63.52943420410156 Val_Reconstruction : 60.77576446533203 Val_KL : 2.753668785095215\n","Epoch: 371/5000  Traning Loss: 65.23199081420898  Train_Reconstruction: 62.43764400482178  Train_KL: 2.79434671998024  Validation Loss : 63.55663871765137 Val_Reconstruction : 60.80751037597656 Val_KL : 2.7491272687911987\n","Epoch: 372/5000  Traning Loss: 65.45355606079102  Train_Reconstruction: 62.65838861465454  Train_KL: 2.795167565345764  Validation Loss : 63.6439094543457 Val_Reconstruction : 60.901039123535156 Val_KL : 2.742872714996338\n","Epoch: 373/5000  Traning Loss: 65.61105251312256  Train_Reconstruction: 62.817522048950195  Train_KL: 2.793530762195587  Validation Loss : 64.22921562194824 Val_Reconstruction : 61.47803497314453 Val_KL : 2.7511788606643677\n","Epoch: 374/5000  Traning Loss: 65.60225582122803  Train_Reconstruction: 62.80794382095337  Train_KL: 2.7943131029605865  Validation Loss : 63.762550354003906 Val_Reconstruction : 61.01249122619629 Val_KL : 2.7500596046447754\n","Epoch: 375/5000  Traning Loss: 65.64731121063232  Train_Reconstruction: 62.84749460220337  Train_KL: 2.7998173236846924  Validation Loss : 64.02561950683594 Val_Reconstruction : 61.254371643066406 Val_KL : 2.7712485790252686\n","Epoch: 376/5000  Traning Loss: 65.21752834320068  Train_Reconstruction: 62.40792179107666  Train_KL: 2.8096064925193787  Validation Loss : 63.509023666381836 Val_Reconstruction : 60.7581672668457 Val_KL : 2.7508561611175537\n","Epoch: 377/5000  Traning Loss: 65.27959775924683  Train_Reconstruction: 62.48052453994751  Train_KL: 2.799073487520218  Validation Loss : 63.448495864868164 Val_Reconstruction : 60.690670013427734 Val_KL : 2.7578258514404297\n","Epoch: 378/5000  Traning Loss: 65.28568935394287  Train_Reconstruction: 62.48843193054199  Train_KL: 2.797257274389267  Validation Loss : 63.87687301635742 Val_Reconstruction : 61.13154983520508 Val_KL : 2.745323657989502\n","Epoch: 379/5000  Traning Loss: 65.1893424987793  Train_Reconstruction: 62.39118576049805  Train_KL: 2.7981584072113037  Validation Loss : 63.47432327270508 Val_Reconstruction : 60.71822929382324 Val_KL : 2.756094813346863\n","Epoch: 380/5000  Traning Loss: 65.43811988830566  Train_Reconstruction: 62.63200235366821  Train_KL: 2.8061183989048004  Validation Loss : 63.89955139160156 Val_Reconstruction : 61.12528610229492 Val_KL : 2.774263620376587\n","Epoch: 381/5000  Traning Loss: 66.04854965209961  Train_Reconstruction: 63.23940420150757  Train_KL: 2.809145450592041  Validation Loss : 64.52585792541504 Val_Reconstruction : 61.77102851867676 Val_KL : 2.754832148551941\n","Epoch: 382/5000  Traning Loss: 65.90890407562256  Train_Reconstruction: 63.1137638092041  Train_KL: 2.79513943195343  Validation Loss : 63.87885665893555 Val_Reconstruction : 61.123687744140625 Val_KL : 2.7551695108413696\n","Epoch: 383/5000  Traning Loss: 65.43902587890625  Train_Reconstruction: 62.641597270965576  Train_KL: 2.7974285185337067  Validation Loss : 63.83303642272949 Val_Reconstruction : 61.0687313079834 Val_KL : 2.7643059492111206\n","Epoch: 384/5000  Traning Loss: 65.3475399017334  Train_Reconstruction: 62.535441875457764  Train_KL: 2.812098652124405  Validation Loss : 63.40333557128906 Val_Reconstruction : 60.62981986999512 Val_KL : 2.773516058921814\n","Epoch: 385/5000  Traning Loss: 65.5329008102417  Train_Reconstruction: 62.71680736541748  Train_KL: 2.8160944283008575  Validation Loss : 63.988197326660156 Val_Reconstruction : 61.21562576293945 Val_KL : 2.77256977558136\n","Epoch: 386/5000  Traning Loss: 66.05286979675293  Train_Reconstruction: 63.239262104034424  Train_KL: 2.813608467578888  Validation Loss : 63.997684478759766 Val_Reconstruction : 61.22226142883301 Val_KL : 2.7754253149032593\n","Epoch: 387/5000  Traning Loss: 66.20035171508789  Train_Reconstruction: 63.38815116882324  Train_KL: 2.81220018863678  Validation Loss : 64.81917953491211 Val_Reconstruction : 62.05915832519531 Val_KL : 2.760019302368164\n","Epoch: 388/5000  Traning Loss: 66.24530982971191  Train_Reconstruction: 63.439091205596924  Train_KL: 2.8062201142311096  Validation Loss : 64.36947250366211 Val_Reconstruction : 61.59951400756836 Val_KL : 2.7699562311172485\n","Epoch: 389/5000  Traning Loss: 65.99545574188232  Train_Reconstruction: 63.18733072280884  Train_KL: 2.8081252574920654  Validation Loss : 64.135498046875 Val_Reconstruction : 61.390878677368164 Val_KL : 2.744617223739624\n","Epoch: 390/5000  Traning Loss: 65.75517559051514  Train_Reconstruction: 62.95216608047485  Train_KL: 2.8030090630054474  Validation Loss : 63.65951919555664 Val_Reconstruction : 60.88930320739746 Val_KL : 2.770214319229126\n","Epoch: 391/5000  Traning Loss: 65.4454574584961  Train_Reconstruction: 62.646058082580566  Train_KL: 2.7993991374969482  Validation Loss : 63.556814193725586 Val_Reconstruction : 60.79931449890137 Val_KL : 2.757500648498535\n","Epoch: 392/5000  Traning Loss: 65.07682704925537  Train_Reconstruction: 62.2708101272583  Train_KL: 2.8060167133808136  Validation Loss : 63.23366165161133 Val_Reconstruction : 60.47075843811035 Val_KL : 2.762904405593872\n","Epoch: 393/5000  Traning Loss: 64.88143730163574  Train_Reconstruction: 62.08871078491211  Train_KL: 2.792727679014206  Validation Loss : 63.35794448852539 Val_Reconstruction : 60.60187530517578 Val_KL : 2.7560691833496094\n","Epoch: 394/5000  Traning Loss: 65.22997713088989  Train_Reconstruction: 62.418964862823486  Train_KL: 2.8110125958919525  Validation Loss : 63.54268836975098 Val_Reconstruction : 60.76310157775879 Val_KL : 2.779587149620056\n","Epoch: 395/5000  Traning Loss: 65.2030839920044  Train_Reconstruction: 62.38842821121216  Train_KL: 2.814656287431717  Validation Loss : 63.427757263183594 Val_Reconstruction : 60.65522384643555 Val_KL : 2.7725335359573364\n","Epoch: 396/5000  Traning Loss: 65.35950946807861  Train_Reconstruction: 62.55674409866333  Train_KL: 2.802765190601349  Validation Loss : 63.98541259765625 Val_Reconstruction : 61.22418785095215 Val_KL : 2.7612252235412598\n","Epoch: 397/5000  Traning Loss: 65.29657888412476  Train_Reconstruction: 62.489174365997314  Train_KL: 2.8074026703834534  Validation Loss : 63.597293853759766 Val_Reconstruction : 60.831552505493164 Val_KL : 2.765742301940918\n","Epoch: 398/5000  Traning Loss: 65.09975481033325  Train_Reconstruction: 62.30624771118164  Train_KL: 2.79350745677948  Validation Loss : 63.28300666809082 Val_Reconstruction : 60.54970359802246 Val_KL : 2.733304023742676\n","Epoch: 399/5000  Traning Loss: 64.95976781845093  Train_Reconstruction: 62.15850639343262  Train_KL: 2.801260828971863  Validation Loss : 63.22821044921875 Val_Reconstruction : 60.46004676818848 Val_KL : 2.7681655883789062\n","Epoch: 400/5000  Traning Loss: 64.84892559051514  Train_Reconstruction: 62.04594612121582  Train_KL: 2.8029803037643433  Validation Loss : 63.30203819274902 Val_Reconstruction : 60.53232955932617 Val_KL : 2.76970899105072\n","Epoch: 401/5000  Traning Loss: 64.96941804885864  Train_Reconstruction: 62.145395278930664  Train_KL: 2.8240228295326233  Validation Loss : 63.131805419921875 Val_Reconstruction : 60.349544525146484 Val_KL : 2.78226101398468\n","Epoch: 402/5000  Traning Loss: 65.18369150161743  Train_Reconstruction: 62.37089204788208  Train_KL: 2.8127987682819366  Validation Loss : 63.58933067321777 Val_Reconstruction : 60.81323051452637 Val_KL : 2.7760998010635376\n","Epoch: 403/5000  Traning Loss: 65.32823753356934  Train_Reconstruction: 62.50193786621094  Train_KL: 2.826300472021103  Validation Loss : 63.88287162780762 Val_Reconstruction : 61.09460258483887 Val_KL : 2.7882673740386963\n","Epoch: 404/5000  Traning Loss: 65.4151725769043  Train_Reconstruction: 62.59458637237549  Train_KL: 2.8205858170986176  Validation Loss : 63.759674072265625 Val_Reconstruction : 60.98049354553223 Val_KL : 2.779180645942688\n","Epoch: 405/5000  Traning Loss: 65.12033176422119  Train_Reconstruction: 62.30636978149414  Train_KL: 2.813961297273636  Validation Loss : 63.56629943847656 Val_Reconstruction : 60.79651069641113 Val_KL : 2.769791007041931\n","Epoch: 406/5000  Traning Loss: 65.06445264816284  Train_Reconstruction: 62.252768993377686  Train_KL: 2.8116833865642548  Validation Loss : 63.53535270690918 Val_Reconstruction : 60.763017654418945 Val_KL : 2.772333264350891\n","Epoch: 407/5000  Traning Loss: 65.05085229873657  Train_Reconstruction: 62.24641180038452  Train_KL: 2.804440438747406  Validation Loss : 63.29477500915527 Val_Reconstruction : 60.53284454345703 Val_KL : 2.761932134628296\n","Epoch: 408/5000  Traning Loss: 65.03895664215088  Train_Reconstruction: 62.228052616119385  Train_KL: 2.810904175043106  Validation Loss : 63.62660217285156 Val_Reconstruction : 60.85097885131836 Val_KL : 2.775622606277466\n","Epoch: 409/5000  Traning Loss: 65.00073528289795  Train_Reconstruction: 62.19270324707031  Train_KL: 2.808030813932419  Validation Loss : 63.24582862854004 Val_Reconstruction : 60.48532485961914 Val_KL : 2.760504722595215\n","Epoch: 410/5000  Traning Loss: 64.73401927947998  Train_Reconstruction: 61.931700706481934  Train_KL: 2.8023176193237305  Validation Loss : 63.028358459472656 Val_Reconstruction : 60.26570701599121 Val_KL : 2.762649416923523\n","Epoch: 411/5000  Traning Loss: 64.90171384811401  Train_Reconstruction: 62.08570671081543  Train_KL: 2.81600683927536  Validation Loss : 63.38356399536133 Val_Reconstruction : 60.60023880004883 Val_KL : 2.783323049545288\n","Epoch: 412/5000  Traning Loss: 64.99506282806396  Train_Reconstruction: 62.16582489013672  Train_KL: 2.829239398241043  Validation Loss : 63.42582893371582 Val_Reconstruction : 60.63846206665039 Val_KL : 2.7873659133911133\n","Epoch: 413/5000  Traning Loss: 64.85490131378174  Train_Reconstruction: 62.04637861251831  Train_KL: 2.8085228502750397  Validation Loss : 62.9985237121582 Val_Reconstruction : 60.24154472351074 Val_KL : 2.7569799423217773\n","Epoch: 414/5000  Traning Loss: 64.88890266418457  Train_Reconstruction: 62.07818555831909  Train_KL: 2.8107173144817352  Validation Loss : 63.3017635345459 Val_Reconstruction : 60.52549362182617 Val_KL : 2.776269555091858\n","Epoch: 415/5000  Traning Loss: 65.07518815994263  Train_Reconstruction: 62.263997077941895  Train_KL: 2.811191290616989  Validation Loss : 63.43268013000488 Val_Reconstruction : 60.663551330566406 Val_KL : 2.7691274881362915\n","Epoch: 416/5000  Traning Loss: 65.01094055175781  Train_Reconstruction: 62.19851207733154  Train_KL: 2.8124286234378815  Validation Loss : 63.68311882019043 Val_Reconstruction : 60.90409278869629 Val_KL : 2.7790251970291138\n","Epoch: 417/5000  Traning Loss: 65.13892030715942  Train_Reconstruction: 62.32018280029297  Train_KL: 2.818738132715225  Validation Loss : 63.6646785736084 Val_Reconstruction : 60.88423156738281 Val_KL : 2.780445694923401\n","Epoch: 418/5000  Traning Loss: 64.91534662246704  Train_Reconstruction: 62.10233688354492  Train_KL: 2.8130103647708893  Validation Loss : 63.301918029785156 Val_Reconstruction : 60.53680419921875 Val_KL : 2.7651147842407227\n","Epoch: 419/5000  Traning Loss: 64.87358140945435  Train_Reconstruction: 62.05823755264282  Train_KL: 2.8153437674045563  Validation Loss : 63.3160400390625 Val_Reconstruction : 60.54543685913086 Val_KL : 2.7706023454666138\n","Epoch: 420/5000  Traning Loss: 64.86851930618286  Train_Reconstruction: 62.059532165527344  Train_KL: 2.8089863061904907  Validation Loss : 63.260398864746094 Val_Reconstruction : 60.49128341674805 Val_KL : 2.7691149711608887\n","Epoch: 421/5000  Traning Loss: 64.70893287658691  Train_Reconstruction: 61.89118528366089  Train_KL: 2.817747801542282  Validation Loss : 62.95449256896973 Val_Reconstruction : 60.17784118652344 Val_KL : 2.7766510248184204\n","Epoch: 422/5000  Traning Loss: 64.58695077896118  Train_Reconstruction: 61.77579927444458  Train_KL: 2.8111511170864105  Validation Loss : 62.91525650024414 Val_Reconstruction : 60.135005950927734 Val_KL : 2.780250310897827\n","Epoch: 423/5000  Traning Loss: 65.11263370513916  Train_Reconstruction: 62.304808139801025  Train_KL: 2.8078253269195557  Validation Loss : 64.0191478729248 Val_Reconstruction : 61.25362014770508 Val_KL : 2.7655287981033325\n","Epoch: 424/5000  Traning Loss: 64.88866901397705  Train_Reconstruction: 62.07905578613281  Train_KL: 2.8096126914024353  Validation Loss : 63.21857833862305 Val_Reconstruction : 60.45119094848633 Val_KL : 2.767386555671692\n","Epoch: 425/5000  Traning Loss: 64.74177646636963  Train_Reconstruction: 61.93767595291138  Train_KL: 2.804099977016449  Validation Loss : 63.13797569274902 Val_Reconstruction : 60.37873840332031 Val_KL : 2.7592352628707886\n","Epoch: 426/5000  Traning Loss: 64.66554260253906  Train_Reconstruction: 61.86174488067627  Train_KL: 2.8037965893745422  Validation Loss : 62.999961853027344 Val_Reconstruction : 60.233787536621094 Val_KL : 2.7661739587783813\n","Epoch: 427/5000  Traning Loss: 64.9364881515503  Train_Reconstruction: 62.124659061431885  Train_KL: 2.8118282854557037  Validation Loss : 63.25177001953125 Val_Reconstruction : 60.48713302612305 Val_KL : 2.764636278152466\n","Epoch: 428/5000  Traning Loss: 64.63015556335449  Train_Reconstruction: 61.82233285903931  Train_KL: 2.807821750640869  Validation Loss : 62.96198081970215 Val_Reconstruction : 60.18033409118652 Val_KL : 2.781648874282837\n","Epoch: 429/5000  Traning Loss: 64.65411043167114  Train_Reconstruction: 61.83314800262451  Train_KL: 2.820962369441986  Validation Loss : 63.13505744934082 Val_Reconstruction : 60.3548469543457 Val_KL : 2.780208945274353\n","Epoch: 430/5000  Traning Loss: 64.81558036804199  Train_Reconstruction: 61.989293575286865  Train_KL: 2.82628670334816  Validation Loss : 63.380855560302734 Val_Reconstruction : 60.594085693359375 Val_KL : 2.7867714166641235\n","Epoch: 431/5000  Traning Loss: 65.03224468231201  Train_Reconstruction: 62.213441371917725  Train_KL: 2.81880322098732  Validation Loss : 63.23592948913574 Val_Reconstruction : 60.45436477661133 Val_KL : 2.781565546989441\n","Epoch: 432/5000  Traning Loss: 64.83698511123657  Train_Reconstruction: 62.01420974731445  Train_KL: 2.822776108980179  Validation Loss : 63.2742977142334 Val_Reconstruction : 60.491363525390625 Val_KL : 2.782935380935669\n","Epoch: 433/5000  Traning Loss: 64.75228881835938  Train_Reconstruction: 61.933098793029785  Train_KL: 2.8191912174224854  Validation Loss : 63.182838439941406 Val_Reconstruction : 60.405799865722656 Val_KL : 2.777036666870117\n","Epoch: 434/5000  Traning Loss: 64.87554597854614  Train_Reconstruction: 62.062416553497314  Train_KL: 2.81312894821167  Validation Loss : 63.39042854309082 Val_Reconstruction : 60.62783622741699 Val_KL : 2.7625941038131714\n","Epoch: 435/5000  Traning Loss: 64.60976648330688  Train_Reconstruction: 61.79777145385742  Train_KL: 2.811994045972824  Validation Loss : 62.976985931396484 Val_Reconstruction : 60.19712448120117 Val_KL : 2.779861330986023\n","Epoch: 436/5000  Traning Loss: 64.80448579788208  Train_Reconstruction: 61.98213577270508  Train_KL: 2.8223498463630676  Validation Loss : 63.35080337524414 Val_Reconstruction : 60.568668365478516 Val_KL : 2.7821356058120728\n","Epoch: 437/5000  Traning Loss: 64.79790544509888  Train_Reconstruction: 61.96698188781738  Train_KL: 2.8309231996536255  Validation Loss : 63.150814056396484 Val_Reconstruction : 60.35205841064453 Val_KL : 2.7987537384033203\n","Epoch: 438/5000  Traning Loss: 64.70407295227051  Train_Reconstruction: 61.87773370742798  Train_KL: 2.8263394832611084  Validation Loss : 63.40720558166504 Val_Reconstruction : 60.63317108154297 Val_KL : 2.774035334587097\n","Epoch: 439/5000  Traning Loss: 64.77102327346802  Train_Reconstruction: 61.94567060470581  Train_KL: 2.825351893901825  Validation Loss : 63.2174072265625 Val_Reconstruction : 60.42691421508789 Val_KL : 2.790491819381714\n","Epoch: 440/5000  Traning Loss: 64.84229469299316  Train_Reconstruction: 62.00967454910278  Train_KL: 2.8326205909252167  Validation Loss : 63.332061767578125 Val_Reconstruction : 60.53194618225098 Val_KL : 2.800113320350647\n","Epoch: 441/5000  Traning Loss: 64.95099925994873  Train_Reconstruction: 62.119539737701416  Train_KL: 2.831457942724228  Validation Loss : 63.34233856201172 Val_Reconstruction : 60.56081581115723 Val_KL : 2.781522274017334\n","Epoch: 442/5000  Traning Loss: 64.90880012512207  Train_Reconstruction: 62.083351612091064  Train_KL: 2.825449377298355  Validation Loss : 62.832794189453125 Val_Reconstruction : 60.02181816101074 Val_KL : 2.8109771013259888\n","Epoch: 443/5000  Traning Loss: 64.63714981079102  Train_Reconstruction: 61.79960012435913  Train_KL: 2.8375495076179504  Validation Loss : 62.86688423156738 Val_Reconstruction : 60.080848693847656 Val_KL : 2.786036491394043\n","Epoch: 444/5000  Traning Loss: 64.59214401245117  Train_Reconstruction: 61.765021324157715  Train_KL: 2.8271225094795227  Validation Loss : 62.856279373168945 Val_Reconstruction : 60.07011032104492 Val_KL : 2.786167860031128\n","Epoch: 445/5000  Traning Loss: 64.59079837799072  Train_Reconstruction: 61.76644849777222  Train_KL: 2.8243494629859924  Validation Loss : 63.25370979309082 Val_Reconstruction : 60.4794807434082 Val_KL : 2.774228096008301\n","Epoch: 446/5000  Traning Loss: 64.74823665618896  Train_Reconstruction: 61.91415309906006  Train_KL: 2.834082692861557  Validation Loss : 63.16202163696289 Val_Reconstruction : 60.36654281616211 Val_KL : 2.7954790592193604\n","Epoch: 447/5000  Traning Loss: 64.38711357116699  Train_Reconstruction: 61.550050258636475  Train_KL: 2.837063878774643  Validation Loss : 62.71205139160156 Val_Reconstruction : 59.91539192199707 Val_KL : 2.7966620922088623\n","Epoch: 448/5000  Traning Loss: 64.57926559448242  Train_Reconstruction: 61.75139570236206  Train_KL: 2.8278703689575195  Validation Loss : 63.41114044189453 Val_Reconstruction : 60.628835678100586 Val_KL : 2.7823057174682617\n","Epoch: 449/5000  Traning Loss: 64.85671710968018  Train_Reconstruction: 62.0398588180542  Train_KL: 2.8168593049049377  Validation Loss : 63.4802303314209 Val_Reconstruction : 60.702964782714844 Val_KL : 2.777266502380371\n","Epoch: 450/5000  Traning Loss: 64.95938110351562  Train_Reconstruction: 62.13362693786621  Train_KL: 2.8257530629634857  Validation Loss : 63.23512268066406 Val_Reconstruction : 60.45151138305664 Val_KL : 2.783610701560974\n","Epoch: 451/5000  Traning Loss: 64.68981409072876  Train_Reconstruction: 61.864177227020264  Train_KL: 2.825636565685272  Validation Loss : 63.20045852661133 Val_Reconstruction : 60.40928268432617 Val_KL : 2.791174054145813\n","Epoch: 452/5000  Traning Loss: 64.607834815979  Train_Reconstruction: 61.779632568359375  Train_KL: 2.8282008469104767  Validation Loss : 63.01566696166992 Val_Reconstruction : 60.22727394104004 Val_KL : 2.788391947746277\n","Epoch: 453/5000  Traning Loss: 64.56215953826904  Train_Reconstruction: 61.74797821044922  Train_KL: 2.8141812682151794  Validation Loss : 63.02315330505371 Val_Reconstruction : 60.25447082519531 Val_KL : 2.768681764602661\n","Epoch: 454/5000  Traning Loss: 64.59601306915283  Train_Reconstruction: 61.7811918258667  Train_KL: 2.8148213922977448  Validation Loss : 62.923343658447266 Val_Reconstruction : 60.15335464477539 Val_KL : 2.769989252090454\n","Epoch: 455/5000  Traning Loss: 64.34939002990723  Train_Reconstruction: 61.54367733001709  Train_KL: 2.8057126104831696  Validation Loss : 62.83587837219238 Val_Reconstruction : 60.07440185546875 Val_KL : 2.761475086212158\n","Epoch: 456/5000  Traning Loss: 64.68880414962769  Train_Reconstruction: 61.86710500717163  Train_KL: 2.821699410676956  Validation Loss : 63.16057205200195 Val_Reconstruction : 60.366641998291016 Val_KL : 2.7939289808273315\n","Epoch: 457/5000  Traning Loss: 64.69647121429443  Train_Reconstruction: 61.87438631057739  Train_KL: 2.8220843374729156  Validation Loss : 62.963823318481445 Val_Reconstruction : 60.17789077758789 Val_KL : 2.7859342098236084\n","Epoch: 458/5000  Traning Loss: 64.5172290802002  Train_Reconstruction: 61.69070816040039  Train_KL: 2.8265202939510345  Validation Loss : 62.827125549316406 Val_Reconstruction : 60.025978088378906 Val_KL : 2.8011488914489746\n","Epoch: 459/5000  Traning Loss: 64.2723240852356  Train_Reconstruction: 61.440736293792725  Train_KL: 2.8315875232219696  Validation Loss : 62.610103607177734 Val_Reconstruction : 59.82040214538574 Val_KL : 2.7897013425827026\n","Epoch: 460/5000  Traning Loss: 64.48241710662842  Train_Reconstruction: 61.65746736526489  Train_KL: 2.8249495029449463  Validation Loss : 63.01198959350586 Val_Reconstruction : 60.228736877441406 Val_KL : 2.783253788948059\n","Epoch: 461/5000  Traning Loss: 64.37034320831299  Train_Reconstruction: 61.55107927322388  Train_KL: 2.819263994693756  Validation Loss : 62.72383499145508 Val_Reconstruction : 59.92832565307617 Val_KL : 2.7955098152160645\n","Epoch: 462/5000  Traning Loss: 64.18266010284424  Train_Reconstruction: 61.35796403884888  Train_KL: 2.824694573879242  Validation Loss : 62.52413749694824 Val_Reconstruction : 59.7445068359375 Val_KL : 2.7796294689178467\n","Epoch: 463/5000  Traning Loss: 64.20857763290405  Train_Reconstruction: 61.39155435562134  Train_KL: 2.8170244991779327  Validation Loss : 62.88240432739258 Val_Reconstruction : 60.093170166015625 Val_KL : 2.7892322540283203\n","Epoch: 464/5000  Traning Loss: 64.23982667922974  Train_Reconstruction: 61.40734052658081  Train_KL: 2.832485944032669  Validation Loss : 62.56576156616211 Val_Reconstruction : 59.779685974121094 Val_KL : 2.7860753536224365\n","Epoch: 465/5000  Traning Loss: 64.19584703445435  Train_Reconstruction: 61.382463455200195  Train_KL: 2.813382625579834  Validation Loss : 62.86812400817871 Val_Reconstruction : 60.09619331359863 Val_KL : 2.7719295024871826\n","Epoch: 466/5000  Traning Loss: 64.2519588470459  Train_Reconstruction: 61.432106494903564  Train_KL: 2.8198517560958862  Validation Loss : 62.98284912109375 Val_Reconstruction : 60.20387077331543 Val_KL : 2.778980851173401\n","Epoch: 467/5000  Traning Loss: 64.51135635375977  Train_Reconstruction: 61.67956304550171  Train_KL: 2.8317929208278656  Validation Loss : 63.06096267700195 Val_Reconstruction : 60.257041931152344 Val_KL : 2.8039225339889526\n","Epoch: 468/5000  Traning Loss: 64.35824060440063  Train_Reconstruction: 61.53056716918945  Train_KL: 2.8276743590831757  Validation Loss : 62.69584083557129 Val_Reconstruction : 59.8999080657959 Val_KL : 2.795931339263916\n","Epoch: 469/5000  Traning Loss: 64.47664642333984  Train_Reconstruction: 61.64136600494385  Train_KL: 2.8352806866168976  Validation Loss : 62.89266014099121 Val_Reconstruction : 60.090383529663086 Val_KL : 2.802277088165283\n","Epoch: 470/5000  Traning Loss: 64.38561248779297  Train_Reconstruction: 61.55445861816406  Train_KL: 2.8311547338962555  Validation Loss : 62.862165451049805 Val_Reconstruction : 60.0710506439209 Val_KL : 2.791115880012512\n","Epoch: 471/5000  Traning Loss: 64.453293800354  Train_Reconstruction: 61.616727352142334  Train_KL: 2.8365656435489655  Validation Loss : 63.08733558654785 Val_Reconstruction : 60.29145050048828 Val_KL : 2.795883059501648\n","Epoch: 472/5000  Traning Loss: 64.4636082649231  Train_Reconstruction: 61.63500928878784  Train_KL: 2.8285994827747345  Validation Loss : 62.667367935180664 Val_Reconstruction : 59.871646881103516 Val_KL : 2.7957197427749634\n","Epoch: 473/5000  Traning Loss: 64.22042798995972  Train_Reconstruction: 61.38326835632324  Train_KL: 2.837161272764206  Validation Loss : 62.79867362976074 Val_Reconstruction : 60.00489616394043 Val_KL : 2.793778419494629\n","Epoch: 474/5000  Traning Loss: 64.25663328170776  Train_Reconstruction: 61.424856185913086  Train_KL: 2.8317769169807434  Validation Loss : 62.72820854187012 Val_Reconstruction : 59.93185615539551 Val_KL : 2.796353816986084\n","Epoch: 475/5000  Traning Loss: 64.33631372451782  Train_Reconstruction: 61.506239891052246  Train_KL: 2.8300736844539642  Validation Loss : 62.79276084899902 Val_Reconstruction : 59.99845314025879 Val_KL : 2.7943068742752075\n","Epoch: 476/5000  Traning Loss: 64.34691333770752  Train_Reconstruction: 61.51626539230347  Train_KL: 2.8306486010551453  Validation Loss : 62.63372993469238 Val_Reconstruction : 59.837486267089844 Val_KL : 2.7962435483932495\n","Epoch: 477/5000  Traning Loss: 64.23511600494385  Train_Reconstruction: 61.40437173843384  Train_KL: 2.830744504928589  Validation Loss : 62.754255294799805 Val_Reconstruction : 59.95372009277344 Val_KL : 2.8005340099334717\n","Epoch: 478/5000  Traning Loss: 64.21437072753906  Train_Reconstruction: 61.38684368133545  Train_KL: 2.8275261521339417  Validation Loss : 62.789756774902344 Val_Reconstruction : 60.003347396850586 Val_KL : 2.786409020423889\n","Epoch: 479/5000  Traning Loss: 64.11757230758667  Train_Reconstruction: 61.29010200500488  Train_KL: 2.8274713456630707  Validation Loss : 62.3546257019043 Val_Reconstruction : 59.5538330078125 Val_KL : 2.8007915019989014\n","Epoch: 480/5000  Traning Loss: 64.07213163375854  Train_Reconstruction: 61.24101448059082  Train_KL: 2.831117630004883  Validation Loss : 62.65359306335449 Val_Reconstruction : 59.8564453125 Val_KL : 2.7971482276916504\n","Epoch: 481/5000  Traning Loss: 64.34362125396729  Train_Reconstruction: 61.49777889251709  Train_KL: 2.845842331647873  Validation Loss : 62.93558692932129 Val_Reconstruction : 60.13113021850586 Val_KL : 2.804456114768982\n","Epoch: 482/5000  Traning Loss: 64.49840497970581  Train_Reconstruction: 61.66335725784302  Train_KL: 2.8350479900836945  Validation Loss : 62.946462631225586 Val_Reconstruction : 60.15627479553223 Val_KL : 2.7901893854141235\n","Epoch: 483/5000  Traning Loss: 64.50598526000977  Train_Reconstruction: 61.679237842559814  Train_KL: 2.826747179031372  Validation Loss : 62.46598434448242 Val_Reconstruction : 59.6860294342041 Val_KL : 2.7799559831619263\n","Epoch: 484/5000  Traning Loss: 64.03166770935059  Train_Reconstruction: 61.215250968933105  Train_KL: 2.816416084766388  Validation Loss : 62.458927154541016 Val_Reconstruction : 59.68869590759277 Val_KL : 2.770230770111084\n","Epoch: 485/5000  Traning Loss: 64.30272054672241  Train_Reconstruction: 61.47905492782593  Train_KL: 2.8236654102802277  Validation Loss : 62.78630065917969 Val_Reconstruction : 59.99269676208496 Val_KL : 2.793605089187622\n","Epoch: 486/5000  Traning Loss: 64.4628701210022  Train_Reconstruction: 61.6332950592041  Train_KL: 2.8295753598213196  Validation Loss : 62.843528747558594 Val_Reconstruction : 60.04328155517578 Val_KL : 2.800244927406311\n","Epoch: 487/5000  Traning Loss: 64.37451314926147  Train_Reconstruction: 61.54343366622925  Train_KL: 2.831080675125122  Validation Loss : 63.0292911529541 Val_Reconstruction : 60.2391471862793 Val_KL : 2.790145516395569\n","Epoch: 488/5000  Traning Loss: 64.17201900482178  Train_Reconstruction: 61.3413667678833  Train_KL: 2.83065128326416  Validation Loss : 62.67215156555176 Val_Reconstruction : 59.88358688354492 Val_KL : 2.7885656356811523\n","Epoch: 489/5000  Traning Loss: 64.12842226028442  Train_Reconstruction: 61.2947473526001  Train_KL: 2.833673447370529  Validation Loss : 62.619476318359375 Val_Reconstruction : 59.81947135925293 Val_KL : 2.800005555152893\n","Epoch: 490/5000  Traning Loss: 64.21428918838501  Train_Reconstruction: 61.35998725891113  Train_KL: 2.8543020486831665  Validation Loss : 62.39857482910156 Val_Reconstruction : 59.57430839538574 Val_KL : 2.8242686986923218\n","Epoch: 491/5000  Traning Loss: 64.11257696151733  Train_Reconstruction: 61.265034198760986  Train_KL: 2.8475433588027954  Validation Loss : 62.3923397064209 Val_Reconstruction : 59.58685874938965 Val_KL : 2.8054823875427246\n","Epoch: 492/5000  Traning Loss: 64.3073787689209  Train_Reconstruction: 61.46568298339844  Train_KL: 2.841694802045822  Validation Loss : 62.96490669250488 Val_Reconstruction : 60.14678955078125 Val_KL : 2.818117618560791\n","Epoch: 493/5000  Traning Loss: 64.530846118927  Train_Reconstruction: 61.686095237731934  Train_KL: 2.844751626253128  Validation Loss : 63.068904876708984 Val_Reconstruction : 60.2712287902832 Val_KL : 2.7976765632629395\n","Epoch: 494/5000  Traning Loss: 64.2345495223999  Train_Reconstruction: 61.39591455459595  Train_KL: 2.838634967803955  Validation Loss : 62.44950866699219 Val_Reconstruction : 59.64175796508789 Val_KL : 2.8077484369277954\n","Epoch: 495/5000  Traning Loss: 63.91330528259277  Train_Reconstruction: 61.073923110961914  Train_KL: 2.8393822014331818  Validation Loss : 62.44392776489258 Val_Reconstruction : 59.64556694030762 Val_KL : 2.798359990119934\n","Epoch: 496/5000  Traning Loss: 64.14863777160645  Train_Reconstruction: 61.30377674102783  Train_KL: 2.84485986828804  Validation Loss : 62.58214569091797 Val_Reconstruction : 59.778724670410156 Val_KL : 2.8034228086471558\n","Epoch: 497/5000  Traning Loss: 64.3884506225586  Train_Reconstruction: 61.5553240776062  Train_KL: 2.833126276731491  Validation Loss : 62.567344665527344 Val_Reconstruction : 59.782522201538086 Val_KL : 2.784821391105652\n","Epoch: 498/5000  Traning Loss: 64.05723094940186  Train_Reconstruction: 61.22954082489014  Train_KL: 2.8276911675930023  Validation Loss : 62.553524017333984 Val_Reconstruction : 59.75885772705078 Val_KL : 2.7946667671203613\n","Epoch: 499/5000  Traning Loss: 63.91770267486572  Train_Reconstruction: 61.08099937438965  Train_KL: 2.8367024660110474  Validation Loss : 62.55202293395996 Val_Reconstruction : 59.7454891204834 Val_KL : 2.8065348863601685\n","Epoch: 500/5000  Traning Loss: 64.09114789962769  Train_Reconstruction: 61.26384687423706  Train_KL: 2.827299803495407  Validation Loss : 62.68063545227051 Val_Reconstruction : 59.889461517333984 Val_KL : 2.791174054145813\n","Epoch: 501/5000  Traning Loss: 64.13294506072998  Train_Reconstruction: 61.299046993255615  Train_KL: 2.8338984549045563  Validation Loss : 62.38454627990723 Val_Reconstruction : 59.589805603027344 Val_KL : 2.7947407960891724\n","Epoch: 502/5000  Traning Loss: 64.15328741073608  Train_Reconstruction: 61.30540704727173  Train_KL: 2.8478802144527435  Validation Loss : 62.5892448425293 Val_Reconstruction : 59.78311729431152 Val_KL : 2.8061269521713257\n","Epoch: 503/5000  Traning Loss: 64.11695289611816  Train_Reconstruction: 61.27156400680542  Train_KL: 2.8453888297080994  Validation Loss : 62.64204025268555 Val_Reconstruction : 59.82941246032715 Val_KL : 2.8126285076141357\n","Epoch: 504/5000  Traning Loss: 64.27230072021484  Train_Reconstruction: 61.42804002761841  Train_KL: 2.8442612290382385  Validation Loss : 62.88310623168945 Val_Reconstruction : 60.08144950866699 Val_KL : 2.801654100418091\n","Epoch: 505/5000  Traning Loss: 64.29253387451172  Train_Reconstruction: 61.4565224647522  Train_KL: 2.8360117077827454  Validation Loss : 62.445810317993164 Val_Reconstruction : 59.643850326538086 Val_KL : 2.8019585609436035\n","Epoch: 506/5000  Traning Loss: 63.94000482559204  Train_Reconstruction: 61.09318733215332  Train_KL: 2.8468169271945953  Validation Loss : 62.39554977416992 Val_Reconstruction : 59.594892501831055 Val_KL : 2.800655961036682\n","Epoch: 507/5000  Traning Loss: 63.8390154838562  Train_Reconstruction: 60.99584913253784  Train_KL: 2.843166798353195  Validation Loss : 62.254276275634766 Val_Reconstruction : 59.446128845214844 Val_KL : 2.8081459999084473\n","Epoch: 508/5000  Traning Loss: 63.79713582992554  Train_Reconstruction: 60.949564933776855  Train_KL: 2.8475722670555115  Validation Loss : 62.39663505554199 Val_Reconstruction : 59.58433151245117 Val_KL : 2.812303304672241\n","Epoch: 509/5000  Traning Loss: 63.90100908279419  Train_Reconstruction: 61.05368232727051  Train_KL: 2.8473265767097473  Validation Loss : 62.360069274902344 Val_Reconstruction : 59.54388427734375 Val_KL : 2.816183924674988\n","Epoch: 510/5000  Traning Loss: 63.868165493011475  Train_Reconstruction: 61.01846265792847  Train_KL: 2.849702000617981  Validation Loss : 62.19403648376465 Val_Reconstruction : 59.38222122192383 Val_KL : 2.8118152618408203\n","Epoch: 511/5000  Traning Loss: 63.887662410736084  Train_Reconstruction: 61.04274559020996  Train_KL: 2.84491690993309  Validation Loss : 62.5195369720459 Val_Reconstruction : 59.71389961242676 Val_KL : 2.805638313293457\n","Epoch: 512/5000  Traning Loss: 64.04352331161499  Train_Reconstruction: 61.202603816986084  Train_KL: 2.840918928384781  Validation Loss : 62.42632865905762 Val_Reconstruction : 59.627201080322266 Val_KL : 2.799129366874695\n","Epoch: 513/5000  Traning Loss: 64.14267444610596  Train_Reconstruction: 61.31080770492554  Train_KL: 2.8318671882152557  Validation Loss : 62.54866027832031 Val_Reconstruction : 59.751678466796875 Val_KL : 2.796980619430542\n","Epoch: 514/5000  Traning Loss: 63.89894199371338  Train_Reconstruction: 61.06660985946655  Train_KL: 2.832332342863083  Validation Loss : 62.233293533325195 Val_Reconstruction : 59.433332443237305 Val_KL : 2.7999627590179443\n","Epoch: 515/5000  Traning Loss: 63.70584154129028  Train_Reconstruction: 60.87091064453125  Train_KL: 2.8349306881427765  Validation Loss : 62.03525733947754 Val_Reconstruction : 59.23575019836426 Val_KL : 2.7995080947875977\n","Epoch: 516/5000  Traning Loss: 63.75457572937012  Train_Reconstruction: 60.925010204315186  Train_KL: 2.8295657336711884  Validation Loss : 62.16040802001953 Val_Reconstruction : 59.37373924255371 Val_KL : 2.7866684198379517\n","Epoch: 517/5000  Traning Loss: 63.70673751831055  Train_Reconstruction: 60.872848987579346  Train_KL: 2.833887994289398  Validation Loss : 62.254207611083984 Val_Reconstruction : 59.45754623413086 Val_KL : 2.7966607809066772\n","Epoch: 518/5000  Traning Loss: 63.919692516326904  Train_Reconstruction: 61.09055042266846  Train_KL: 2.8291417360305786  Validation Loss : 62.59996223449707 Val_Reconstruction : 59.80660438537598 Val_KL : 2.7933590412139893\n","Epoch: 519/5000  Traning Loss: 64.00354194641113  Train_Reconstruction: 61.17325210571289  Train_KL: 2.8302899599075317  Validation Loss : 62.51211929321289 Val_Reconstruction : 59.71447563171387 Val_KL : 2.797644853591919\n","Epoch: 520/5000  Traning Loss: 64.0442385673523  Train_Reconstruction: 61.19974660873413  Train_KL: 2.8444916009902954  Validation Loss : 62.56390190124512 Val_Reconstruction : 59.7442512512207 Val_KL : 2.8196531534194946\n","Epoch: 521/5000  Traning Loss: 63.80368614196777  Train_Reconstruction: 60.972984313964844  Train_KL: 2.8307012021541595  Validation Loss : 62.36492919921875 Val_Reconstruction : 59.57704162597656 Val_KL : 2.78788685798645\n","Epoch: 522/5000  Traning Loss: 63.84931230545044  Train_Reconstruction: 61.01790142059326  Train_KL: 2.83141028881073  Validation Loss : 62.38121032714844 Val_Reconstruction : 59.580474853515625 Val_KL : 2.800735116004944\n","Epoch: 523/5000  Traning Loss: 63.79753494262695  Train_Reconstruction: 60.97443866729736  Train_KL: 2.823097050189972  Validation Loss : 62.16755676269531 Val_Reconstruction : 59.36585807800293 Val_KL : 2.8016971349716187\n","Epoch: 524/5000  Traning Loss: 63.583452224731445  Train_Reconstruction: 60.7346887588501  Train_KL: 2.848763644695282  Validation Loss : 62.12534141540527 Val_Reconstruction : 59.32506561279297 Val_KL : 2.8002785444259644\n","Epoch: 525/5000  Traning Loss: 63.668424129486084  Train_Reconstruction: 60.836230754852295  Train_KL: 2.8321923911571503  Validation Loss : 62.11165237426758 Val_Reconstruction : 59.30901527404785 Val_KL : 2.8026387691497803\n","Epoch: 526/5000  Traning Loss: 63.912890911102295  Train_Reconstruction: 61.072099685668945  Train_KL: 2.840791165828705  Validation Loss : 62.611839294433594 Val_Reconstruction : 59.82846641540527 Val_KL : 2.783374786376953\n","Epoch: 527/5000  Traning Loss: 64.08333778381348  Train_Reconstruction: 61.23815631866455  Train_KL: 2.845182031393051  Validation Loss : 62.563276290893555 Val_Reconstruction : 59.749114990234375 Val_KL : 2.81415855884552\n","Epoch: 528/5000  Traning Loss: 63.954617977142334  Train_Reconstruction: 61.1047306060791  Train_KL: 2.8498884737491608  Validation Loss : 62.413434982299805 Val_Reconstruction : 59.5915641784668 Val_KL : 2.821869969367981\n","Epoch: 529/5000  Traning Loss: 63.78581476211548  Train_Reconstruction: 60.93503427505493  Train_KL: 2.850780725479126  Validation Loss : 62.41790008544922 Val_Reconstruction : 59.60993766784668 Val_KL : 2.8079617023468018\n","Epoch: 530/5000  Traning Loss: 63.830652713775635  Train_Reconstruction: 60.982033252716064  Train_KL: 2.8486204147338867  Validation Loss : 62.37979507446289 Val_Reconstruction : 59.56924819946289 Val_KL : 2.810548782348633\n","Epoch: 531/5000  Traning Loss: 63.7376275062561  Train_Reconstruction: 60.886826515197754  Train_KL: 2.850801169872284  Validation Loss : 62.25667762756348 Val_Reconstruction : 59.44851303100586 Val_KL : 2.80816650390625\n","Epoch: 532/5000  Traning Loss: 63.64077711105347  Train_Reconstruction: 60.80143594741821  Train_KL: 2.8393413424491882  Validation Loss : 62.24179267883301 Val_Reconstruction : 59.43907165527344 Val_KL : 2.8027219772338867\n","Epoch: 533/5000  Traning Loss: 63.90360641479492  Train_Reconstruction: 61.054386138916016  Train_KL: 2.849219799041748  Validation Loss : 62.486507415771484 Val_Reconstruction : 59.67280578613281 Val_KL : 2.8136996030807495\n","Epoch: 534/5000  Traning Loss: 64.15993309020996  Train_Reconstruction: 61.321425437927246  Train_KL: 2.8385069966316223  Validation Loss : 62.65620803833008 Val_Reconstruction : 59.84828186035156 Val_KL : 2.80792498588562\n","Epoch: 535/5000  Traning Loss: 64.2227349281311  Train_Reconstruction: 61.374491691589355  Train_KL: 2.8482439517974854  Validation Loss : 62.53405952453613 Val_Reconstruction : 59.734519958496094 Val_KL : 2.79953932762146\n","Epoch: 536/5000  Traning Loss: 63.86131954193115  Train_Reconstruction: 61.02273178100586  Train_KL: 2.838587313890457  Validation Loss : 62.430015563964844 Val_Reconstruction : 59.635597229003906 Val_KL : 2.794418454170227\n","Epoch: 537/5000  Traning Loss: 63.6275634765625  Train_Reconstruction: 60.7829704284668  Train_KL: 2.8445935547351837  Validation Loss : 62.116939544677734 Val_Reconstruction : 59.31503486633301 Val_KL : 2.8019036054611206\n","Epoch: 538/5000  Traning Loss: 63.680280685424805  Train_Reconstruction: 60.84060287475586  Train_KL: 2.839678168296814  Validation Loss : 62.35347557067871 Val_Reconstruction : 59.5526180267334 Val_KL : 2.8008580207824707\n","Epoch: 539/5000  Traning Loss: 63.752277851104736  Train_Reconstruction: 60.90866708755493  Train_KL: 2.8436101377010345  Validation Loss : 62.24662971496582 Val_Reconstruction : 59.434303283691406 Val_KL : 2.812326192855835\n","Epoch: 540/5000  Traning Loss: 63.73907661437988  Train_Reconstruction: 60.88905715942383  Train_KL: 2.850020110607147  Validation Loss : 62.34448051452637 Val_Reconstruction : 59.52197265625 Val_KL : 2.822508454322815\n","Epoch: 541/5000  Traning Loss: 63.703622817993164  Train_Reconstruction: 60.85455894470215  Train_KL: 2.8490634858608246  Validation Loss : 62.356191635131836 Val_Reconstruction : 59.55033493041992 Val_KL : 2.805858612060547\n","Epoch: 542/5000  Traning Loss: 63.821598529815674  Train_Reconstruction: 60.977022647857666  Train_KL: 2.844575971364975  Validation Loss : 62.29048156738281 Val_Reconstruction : 59.46862602233887 Val_KL : 2.8218557834625244\n","Epoch: 543/5000  Traning Loss: 63.65992975234985  Train_Reconstruction: 60.818336963653564  Train_KL: 2.841591626405716  Validation Loss : 61.96164131164551 Val_Reconstruction : 59.17770004272461 Val_KL : 2.783939838409424\n","Epoch: 544/5000  Traning Loss: 63.61320924758911  Train_Reconstruction: 60.78309154510498  Train_KL: 2.83011794090271  Validation Loss : 62.29150390625 Val_Reconstruction : 59.49116516113281 Val_KL : 2.800338387489319\n","Epoch: 545/5000  Traning Loss: 63.48222780227661  Train_Reconstruction: 60.63289833068848  Train_KL: 2.849329888820648  Validation Loss : 62.027984619140625 Val_Reconstruction : 59.22811317443848 Val_KL : 2.7998729944229126\n","Epoch: 546/5000  Traning Loss: 63.40035152435303  Train_Reconstruction: 60.553324699401855  Train_KL: 2.8470269441604614  Validation Loss : 61.86813735961914 Val_Reconstruction : 59.056907653808594 Val_KL : 2.8112281560897827\n","Epoch: 547/5000  Traning Loss: 63.74906063079834  Train_Reconstruction: 60.90490245819092  Train_KL: 2.8441583812236786  Validation Loss : 62.08415412902832 Val_Reconstruction : 59.27001190185547 Val_KL : 2.8141428232192993\n","Epoch: 548/5000  Traning Loss: 63.583653926849365  Train_Reconstruction: 60.7328577041626  Train_KL: 2.8507952988147736  Validation Loss : 62.44392395019531 Val_Reconstruction : 59.63797187805176 Val_KL : 2.8059511184692383\n","Epoch: 549/5000  Traning Loss: 64.28226280212402  Train_Reconstruction: 61.44283676147461  Train_KL: 2.839425653219223  Validation Loss : 62.98367118835449 Val_Reconstruction : 60.18931770324707 Val_KL : 2.7943544387817383\n","Epoch: 550/5000  Traning Loss: 64.31364107131958  Train_Reconstruction: 61.469650745391846  Train_KL: 2.8439916372299194  Validation Loss : 62.918203353881836 Val_Reconstruction : 60.10667419433594 Val_KL : 2.8115289211273193\n","Epoch: 551/5000  Traning Loss: 64.10254144668579  Train_Reconstruction: 61.2510666847229  Train_KL: 2.8514742255210876  Validation Loss : 62.41863441467285 Val_Reconstruction : 59.60405158996582 Val_KL : 2.8145835399627686\n","Epoch: 552/5000  Traning Loss: 63.84709024429321  Train_Reconstruction: 61.00621318817139  Train_KL: 2.8408763110637665  Validation Loss : 62.17990493774414 Val_Reconstruction : 59.372344970703125 Val_KL : 2.8075613975524902\n","Epoch: 553/5000  Traning Loss: 63.59754228591919  Train_Reconstruction: 60.74324703216553  Train_KL: 2.854294717311859  Validation Loss : 62.242509841918945 Val_Reconstruction : 59.438438415527344 Val_KL : 2.804073452949524\n","Epoch: 554/5000  Traning Loss: 63.602420806884766  Train_Reconstruction: 60.76116418838501  Train_KL: 2.841256320476532  Validation Loss : 62.20181083679199 Val_Reconstruction : 59.38767623901367 Val_KL : 2.8141331672668457\n","Epoch: 555/5000  Traning Loss: 63.46017503738403  Train_Reconstruction: 60.62080764770508  Train_KL: 2.8393674492836  Validation Loss : 61.85968208312988 Val_Reconstruction : 59.06808853149414 Val_KL : 2.7915940284729004\n","Epoch: 556/5000  Traning Loss: 63.433979988098145  Train_Reconstruction: 60.600624561309814  Train_KL: 2.833354890346527  Validation Loss : 62.14896774291992 Val_Reconstruction : 59.35881805419922 Val_KL : 2.7901490926742554\n","Epoch: 557/5000  Traning Loss: 63.57415151596069  Train_Reconstruction: 60.751830101013184  Train_KL: 2.8223211765289307  Validation Loss : 62.05952453613281 Val_Reconstruction : 59.27280235290527 Val_KL : 2.786722779273987\n","Epoch: 558/5000  Traning Loss: 63.545284271240234  Train_Reconstruction: 60.705140113830566  Train_KL: 2.8401437401771545  Validation Loss : 61.92671775817871 Val_Reconstruction : 59.12127494812012 Val_KL : 2.805442452430725\n","Epoch: 559/5000  Traning Loss: 63.71529817581177  Train_Reconstruction: 60.87966537475586  Train_KL: 2.835632234811783  Validation Loss : 62.34483337402344 Val_Reconstruction : 59.53873062133789 Val_KL : 2.806101083755493\n","Epoch: 560/5000  Traning Loss: 63.644179821014404  Train_Reconstruction: 60.802228927612305  Train_KL: 2.8419510424137115  Validation Loss : 62.2036018371582 Val_Reconstruction : 59.4031867980957 Val_KL : 2.8004143238067627\n","Epoch: 561/5000  Traning Loss: 63.57707405090332  Train_Reconstruction: 60.72218608856201  Train_KL: 2.854888141155243  Validation Loss : 62.14589500427246 Val_Reconstruction : 59.318328857421875 Val_KL : 2.827566146850586\n","Epoch: 562/5000  Traning Loss: 63.452510833740234  Train_Reconstruction: 60.58925437927246  Train_KL: 2.8632563650608063  Validation Loss : 61.97909355163574 Val_Reconstruction : 59.14469337463379 Val_KL : 2.8343989849090576\n","Epoch: 563/5000  Traning Loss: 63.26998996734619  Train_Reconstruction: 60.413007736206055  Train_KL: 2.856982558965683  Validation Loss : 61.87349319458008 Val_Reconstruction : 59.05926704406738 Val_KL : 2.8142253160476685\n","Epoch: 564/5000  Traning Loss: 63.23193979263306  Train_Reconstruction: 60.38011360168457  Train_KL: 2.8518253564834595  Validation Loss : 61.7729434967041 Val_Reconstruction : 58.947763442993164 Val_KL : 2.8251789808273315\n","Epoch: 565/5000  Traning Loss: 63.48676252365112  Train_Reconstruction: 60.634799003601074  Train_KL: 2.851962924003601  Validation Loss : 62.539947509765625 Val_Reconstruction : 59.73188400268555 Val_KL : 2.808062195777893\n","Epoch: 566/5000  Traning Loss: 63.421897888183594  Train_Reconstruction: 60.566542625427246  Train_KL: 2.8553555011749268  Validation Loss : 61.94430923461914 Val_Reconstruction : 59.12814521789551 Val_KL : 2.8161656856536865\n","Epoch: 567/5000  Traning Loss: 63.41196060180664  Train_Reconstruction: 60.56736421585083  Train_KL: 2.844596028327942  Validation Loss : 61.94479942321777 Val_Reconstruction : 59.137460708618164 Val_KL : 2.807338833808899\n","Epoch: 568/5000  Traning Loss: 63.1707124710083  Train_Reconstruction: 60.3317813873291  Train_KL: 2.8389316499233246  Validation Loss : 61.84211349487305 Val_Reconstruction : 59.0327091217041 Val_KL : 2.8094035387039185\n","Epoch: 569/5000  Traning Loss: 63.29352140426636  Train_Reconstruction: 60.43743133544922  Train_KL: 2.856090724468231  Validation Loss : 62.02153968811035 Val_Reconstruction : 59.20901679992676 Val_KL : 2.812524199485779\n","Epoch: 570/5000  Traning Loss: 63.58735990524292  Train_Reconstruction: 60.75259494781494  Train_KL: 2.8347648680210114  Validation Loss : 61.986711502075195 Val_Reconstruction : 59.18869400024414 Val_KL : 2.7980167865753174\n","Epoch: 571/5000  Traning Loss: 63.20451021194458  Train_Reconstruction: 60.36083745956421  Train_KL: 2.8436723053455353  Validation Loss : 61.819793701171875 Val_Reconstruction : 59.005380630493164 Val_KL : 2.8144140243530273\n","Epoch: 572/5000  Traning Loss: 63.270076751708984  Train_Reconstruction: 60.41099834442139  Train_KL: 2.8590793311595917  Validation Loss : 61.79326820373535 Val_Reconstruction : 58.97496032714844 Val_KL : 2.81830894947052\n","Epoch: 573/5000  Traning Loss: 63.368157386779785  Train_Reconstruction: 60.51305150985718  Train_KL: 2.855106830596924  Validation Loss : 61.87203407287598 Val_Reconstruction : 59.050193786621094 Val_KL : 2.821839928627014\n","Epoch: 574/5000  Traning Loss: 63.548940658569336  Train_Reconstruction: 60.69625902175903  Train_KL: 2.8526819944381714  Validation Loss : 62.22793960571289 Val_Reconstruction : 59.41984558105469 Val_KL : 2.808095335960388\n","Epoch: 575/5000  Traning Loss: 63.633485317230225  Train_Reconstruction: 60.79340982437134  Train_KL: 2.840075522661209  Validation Loss : 61.92043113708496 Val_Reconstruction : 59.13340187072754 Val_KL : 2.787027359008789\n","Epoch: 576/5000  Traning Loss: 63.415576457977295  Train_Reconstruction: 60.57808589935303  Train_KL: 2.8374900221824646  Validation Loss : 62.14223861694336 Val_Reconstruction : 59.33346748352051 Val_KL : 2.8087724447250366\n","Epoch: 577/5000  Traning Loss: 63.3511848449707  Train_Reconstruction: 60.497241497039795  Train_KL: 2.853943794965744  Validation Loss : 61.92625617980957 Val_Reconstruction : 59.114532470703125 Val_KL : 2.811725616455078\n","Epoch: 578/5000  Traning Loss: 63.45779085159302  Train_Reconstruction: 60.60061502456665  Train_KL: 2.857176274061203  Validation Loss : 61.9963264465332 Val_Reconstruction : 59.1669921875 Val_KL : 2.829336166381836\n","Epoch: 579/5000  Traning Loss: 63.288086891174316  Train_Reconstruction: 60.445701122283936  Train_KL: 2.842385560274124  Validation Loss : 61.77845764160156 Val_Reconstruction : 58.982194900512695 Val_KL : 2.7962642908096313\n","Epoch: 580/5000  Traning Loss: 63.223803997039795  Train_Reconstruction: 60.38544178009033  Train_KL: 2.838362008333206  Validation Loss : 61.90165901184082 Val_Reconstruction : 59.10150718688965 Val_KL : 2.8001534938812256\n","Epoch: 581/5000  Traning Loss: 63.193190574645996  Train_Reconstruction: 60.351683616638184  Train_KL: 2.841507077217102  Validation Loss : 61.47847557067871 Val_Reconstruction : 58.66309928894043 Val_KL : 2.815376043319702\n","Epoch: 582/5000  Traning Loss: 63.02335166931152  Train_Reconstruction: 60.17157554626465  Train_KL: 2.851776272058487  Validation Loss : 61.57071495056152 Val_Reconstruction : 58.75645446777344 Val_KL : 2.81426203250885\n","Epoch: 583/5000  Traning Loss: 63.0992112159729  Train_Reconstruction: 60.25673484802246  Train_KL: 2.842477172613144  Validation Loss : 61.80402755737305 Val_Reconstruction : 58.99804496765137 Val_KL : 2.8059816360473633\n","Epoch: 584/5000  Traning Loss: 63.30941438674927  Train_Reconstruction: 60.45055866241455  Train_KL: 2.858855277299881  Validation Loss : 62.10137939453125 Val_Reconstruction : 59.275102615356445 Val_KL : 2.826275110244751\n","Epoch: 585/5000  Traning Loss: 63.520347595214844  Train_Reconstruction: 60.66574192047119  Train_KL: 2.854605257511139  Validation Loss : 62.08109092712402 Val_Reconstruction : 59.26856994628906 Val_KL : 2.8125215768814087\n","Epoch: 586/5000  Traning Loss: 63.336055755615234  Train_Reconstruction: 60.494182109832764  Train_KL: 2.8418736159801483  Validation Loss : 61.81752395629883 Val_Reconstruction : 59.000022888183594 Val_KL : 2.817499279975891\n","Epoch: 587/5000  Traning Loss: 63.39400100708008  Train_Reconstruction: 60.53666162490845  Train_KL: 2.8573395907878876  Validation Loss : 62.37158203125 Val_Reconstruction : 59.54155349731445 Val_KL : 2.8300271034240723\n","Epoch: 588/5000  Traning Loss: 63.65880870819092  Train_Reconstruction: 60.79995822906494  Train_KL: 2.858850806951523  Validation Loss : 62.40917778015137 Val_Reconstruction : 59.56662178039551 Val_KL : 2.8425562381744385\n","Epoch: 589/5000  Traning Loss: 63.7670955657959  Train_Reconstruction: 60.89515209197998  Train_KL: 2.871943086385727  Validation Loss : 62.131866455078125 Val_Reconstruction : 59.30377388000488 Val_KL : 2.8280946016311646\n","Epoch: 590/5000  Traning Loss: 63.558998107910156  Train_Reconstruction: 60.700944900512695  Train_KL: 2.8580530881881714  Validation Loss : 62.03286552429199 Val_Reconstruction : 59.21805381774902 Val_KL : 2.814809203147888\n","Epoch: 591/5000  Traning Loss: 63.1630220413208  Train_Reconstruction: 60.310306549072266  Train_KL: 2.852715492248535  Validation Loss : 61.7299690246582 Val_Reconstruction : 58.90083885192871 Val_KL : 2.829131007194519\n","Epoch: 592/5000  Traning Loss: 63.07962417602539  Train_Reconstruction: 60.22529888153076  Train_KL: 2.854325532913208  Validation Loss : 61.75013542175293 Val_Reconstruction : 58.93185997009277 Val_KL : 2.8182767629623413\n","Epoch: 593/5000  Traning Loss: 63.077292919158936  Train_Reconstruction: 60.21560096740723  Train_KL: 2.8616916835308075  Validation Loss : 61.67951011657715 Val_Reconstruction : 58.86119842529297 Val_KL : 2.818312168121338\n","Epoch: 594/5000  Traning Loss: 63.01154613494873  Train_Reconstruction: 60.15770149230957  Train_KL: 2.853844702243805  Validation Loss : 61.57583427429199 Val_Reconstruction : 58.75679016113281 Val_KL : 2.81904399394989\n","Epoch: 595/5000  Traning Loss: 63.0421257019043  Train_Reconstruction: 60.19973659515381  Train_KL: 2.8423888087272644  Validation Loss : 61.64090347290039 Val_Reconstruction : 58.83944320678711 Val_KL : 2.8014594316482544\n","Epoch: 596/5000  Traning Loss: 63.04858636856079  Train_Reconstruction: 60.195380210876465  Train_KL: 2.853206157684326  Validation Loss : 61.78071975708008 Val_Reconstruction : 58.96420097351074 Val_KL : 2.816520929336548\n","Epoch: 597/5000  Traning Loss: 63.08838987350464  Train_Reconstruction: 60.243544578552246  Train_KL: 2.844845950603485  Validation Loss : 61.508392333984375 Val_Reconstruction : 58.69749641418457 Val_KL : 2.8108943700790405\n","Epoch: 598/5000  Traning Loss: 63.05991458892822  Train_Reconstruction: 60.214027881622314  Train_KL: 2.8458865880966187  Validation Loss : 61.758962631225586 Val_Reconstruction : 58.95332717895508 Val_KL : 2.805635690689087\n","Epoch: 599/5000  Traning Loss: 63.17887306213379  Train_Reconstruction: 60.33089208602905  Train_KL: 2.847980946302414  Validation Loss : 61.714298248291016 Val_Reconstruction : 58.901472091674805 Val_KL : 2.8128262758255005\n","Epoch: 600/5000  Traning Loss: 63.32227945327759  Train_Reconstruction: 60.47433662414551  Train_KL: 2.8479418456554413  Validation Loss : 62.20588302612305 Val_Reconstruction : 59.40854835510254 Val_KL : 2.79733407497406\n","Epoch: 601/5000  Traning Loss: 63.567423820495605  Train_Reconstruction: 60.72679090499878  Train_KL: 2.8406329452991486  Validation Loss : 62.47605895996094 Val_Reconstruction : 59.650617599487305 Val_KL : 2.8254411220550537\n","Epoch: 602/5000  Traning Loss: 63.72922611236572  Train_Reconstruction: 60.88371276855469  Train_KL: 2.8455125093460083  Validation Loss : 62.10167694091797 Val_Reconstruction : 59.30950927734375 Val_KL : 2.792167067527771\n","Epoch: 603/5000  Traning Loss: 63.61676549911499  Train_Reconstruction: 60.76649332046509  Train_KL: 2.8502713441848755  Validation Loss : 62.084394454956055 Val_Reconstruction : 59.26065444946289 Val_KL : 2.823739528656006\n","Epoch: 604/5000  Traning Loss: 63.42988443374634  Train_Reconstruction: 60.5796103477478  Train_KL: 2.8502740561962128  Validation Loss : 61.917741775512695 Val_Reconstruction : 59.100730895996094 Val_KL : 2.8170101642608643\n","Epoch: 605/5000  Traning Loss: 63.12160015106201  Train_Reconstruction: 60.270676612854004  Train_KL: 2.8509241342544556  Validation Loss : 61.760690689086914 Val_Reconstruction : 58.94063949584961 Val_KL : 2.8200520277023315\n","Epoch: 606/5000  Traning Loss: 62.98068380355835  Train_Reconstruction: 60.118454933166504  Train_KL: 2.8622286915779114  Validation Loss : 61.70012283325195 Val_Reconstruction : 58.875694274902344 Val_KL : 2.8244279623031616\n","Epoch: 607/5000  Traning Loss: 63.022339820861816  Train_Reconstruction: 60.164700984954834  Train_KL: 2.8576384484767914  Validation Loss : 61.74663543701172 Val_Reconstruction : 58.92652702331543 Val_KL : 2.8201074600219727\n","Epoch: 608/5000  Traning Loss: 63.106361389160156  Train_Reconstruction: 60.2595648765564  Train_KL: 2.846797078847885  Validation Loss : 61.587860107421875 Val_Reconstruction : 58.79257583618164 Val_KL : 2.7952839136123657\n","Epoch: 609/5000  Traning Loss: 63.21122169494629  Train_Reconstruction: 60.3742938041687  Train_KL: 2.8369281589984894  Validation Loss : 61.87959098815918 Val_Reconstruction : 59.06963920593262 Val_KL : 2.8099530935287476\n","Epoch: 610/5000  Traning Loss: 63.18987560272217  Train_Reconstruction: 60.344658851623535  Train_KL: 2.8452160358428955  Validation Loss : 61.76021957397461 Val_Reconstruction : 58.95731735229492 Val_KL : 2.8029019832611084\n","Epoch: 611/5000  Traning Loss: 63.466843128204346  Train_Reconstruction: 60.61825895309448  Train_KL: 2.8485834896564484  Validation Loss : 62.1055908203125 Val_Reconstruction : 59.28782844543457 Val_KL : 2.817762017250061\n","Epoch: 612/5000  Traning Loss: 63.66009569168091  Train_Reconstruction: 60.79713821411133  Train_KL: 2.862957388162613  Validation Loss : 61.92326354980469 Val_Reconstruction : 59.09079933166504 Val_KL : 2.832464575767517\n","Epoch: 613/5000  Traning Loss: 63.05230093002319  Train_Reconstruction: 60.19066286087036  Train_KL: 2.8616381883621216  Validation Loss : 61.5211296081543 Val_Reconstruction : 58.70602226257324 Val_KL : 2.8151073455810547\n","Epoch: 614/5000  Traning Loss: 62.882160663604736  Train_Reconstruction: 60.03174638748169  Train_KL: 2.8504143059253693  Validation Loss : 61.30472755432129 Val_Reconstruction : 58.48837852478027 Val_KL : 2.8163493871688843\n","Epoch: 615/5000  Traning Loss: 62.969059467315674  Train_Reconstruction: 60.11236572265625  Train_KL: 2.8566938042640686  Validation Loss : 61.52937698364258 Val_Reconstruction : 58.697526931762695 Val_KL : 2.8318485021591187\n","Epoch: 616/5000  Traning Loss: 63.158263206481934  Train_Reconstruction: 60.29444122314453  Train_KL: 2.8638216257095337  Validation Loss : 61.82294464111328 Val_Reconstruction : 58.97477722167969 Val_KL : 2.8481667041778564\n","Epoch: 617/5000  Traning Loss: 63.02858018875122  Train_Reconstruction: 60.16586112976074  Train_KL: 2.8627188205718994  Validation Loss : 61.95977783203125 Val_Reconstruction : 59.14823913574219 Val_KL : 2.811539053916931\n","Epoch: 618/5000  Traning Loss: 63.25829553604126  Train_Reconstruction: 60.40867853164673  Train_KL: 2.849616914987564  Validation Loss : 61.97041702270508 Val_Reconstruction : 59.151594161987305 Val_KL : 2.818821907043457\n","Epoch: 619/5000  Traning Loss: 63.31940984725952  Train_Reconstruction: 60.463642597198486  Train_KL: 2.8557664155960083  Validation Loss : 62.16088676452637 Val_Reconstruction : 59.33270454406738 Val_KL : 2.8281824588775635\n","Epoch: 620/5000  Traning Loss: 63.30331802368164  Train_Reconstruction: 60.448227405548096  Train_KL: 2.855090767145157  Validation Loss : 61.95072937011719 Val_Reconstruction : 59.133602142333984 Val_KL : 2.817127823829651\n","Epoch: 621/5000  Traning Loss: 63.0876727104187  Train_Reconstruction: 60.23381853103638  Train_KL: 2.8538540601730347  Validation Loss : 61.71267127990723 Val_Reconstruction : 58.89537048339844 Val_KL : 2.8173006772994995\n","Epoch: 622/5000  Traning Loss: 62.97832012176514  Train_Reconstruction: 60.124953269958496  Train_KL: 2.8533662855625153  Validation Loss : 61.66586494445801 Val_Reconstruction : 58.84910202026367 Val_KL : 2.8167645931243896\n","Epoch: 623/5000  Traning Loss: 62.96787738800049  Train_Reconstruction: 60.10624027252197  Train_KL: 2.8616364300251007  Validation Loss : 61.71951866149902 Val_Reconstruction : 58.89580535888672 Val_KL : 2.823715090751648\n","Epoch: 624/5000  Traning Loss: 62.95607900619507  Train_Reconstruction: 60.111374378204346  Train_KL: 2.844704508781433  Validation Loss : 61.380178451538086 Val_Reconstruction : 58.57314109802246 Val_KL : 2.807037830352783\n","Epoch: 625/5000  Traning Loss: 62.92235040664673  Train_Reconstruction: 60.07032108306885  Train_KL: 2.8520294427871704  Validation Loss : 61.83055305480957 Val_Reconstruction : 59.0390510559082 Val_KL : 2.7915014028549194\n","Epoch: 626/5000  Traning Loss: 63.34605026245117  Train_Reconstruction: 60.49973917007446  Train_KL: 2.846310406923294  Validation Loss : 62.09242248535156 Val_Reconstruction : 59.26372528076172 Val_KL : 2.828696370124817\n","Epoch: 627/5000  Traning Loss: 63.43769454956055  Train_Reconstruction: 60.58169984817505  Train_KL: 2.8559946417808533  Validation Loss : 61.800472259521484 Val_Reconstruction : 58.97933006286621 Val_KL : 2.821142315864563\n","Epoch: 628/5000  Traning Loss: 63.06889533996582  Train_Reconstruction: 60.19551372528076  Train_KL: 2.873381733894348  Validation Loss : 61.80976867675781 Val_Reconstruction : 58.978816986083984 Val_KL : 2.8309508562088013\n","Epoch: 629/5000  Traning Loss: 63.05558633804321  Train_Reconstruction: 60.1967453956604  Train_KL: 2.858840674161911  Validation Loss : 62.21750068664551 Val_Reconstruction : 59.39119338989258 Val_KL : 2.8263087272644043\n","Epoch: 630/5000  Traning Loss: 63.33122158050537  Train_Reconstruction: 60.473912715911865  Train_KL: 2.857308804988861  Validation Loss : 62.15018081665039 Val_Reconstruction : 59.332298278808594 Val_KL : 2.8178839683532715\n","Epoch: 631/5000  Traning Loss: 64.05736064910889  Train_Reconstruction: 61.2049036026001  Train_KL: 2.8524570167064667  Validation Loss : 62.330177307128906 Val_Reconstruction : 59.5216121673584 Val_KL : 2.8085652589797974\n","Epoch: 632/5000  Traning Loss: 63.36333703994751  Train_Reconstruction: 60.52175235748291  Train_KL: 2.8415833711624146  Validation Loss : 61.75988578796387 Val_Reconstruction : 58.9466552734375 Val_KL : 2.8132320642471313\n","Epoch: 633/5000  Traning Loss: 62.951332092285156  Train_Reconstruction: 60.098172664642334  Train_KL: 2.853158712387085  Validation Loss : 61.71199035644531 Val_Reconstruction : 58.896379470825195 Val_KL : 2.815613031387329\n","Epoch: 634/5000  Traning Loss: 62.914382457733154  Train_Reconstruction: 60.06901788711548  Train_KL: 2.8453649282455444  Validation Loss : 61.49325370788574 Val_Reconstruction : 58.687320709228516 Val_KL : 2.805930495262146\n","Epoch: 635/5000  Traning Loss: 63.01122045516968  Train_Reconstruction: 60.15686655044556  Train_KL: 2.8543534874916077  Validation Loss : 61.81051826477051 Val_Reconstruction : 58.9754524230957 Val_KL : 2.835066556930542\n","Epoch: 636/5000  Traning Loss: 62.96994972229004  Train_Reconstruction: 60.1170539855957  Train_KL: 2.8528959453105927  Validation Loss : 61.45895004272461 Val_Reconstruction : 58.63549613952637 Val_KL : 2.8234522342681885\n","Epoch: 637/5000  Traning Loss: 62.91785907745361  Train_Reconstruction: 60.04797124862671  Train_KL: 2.869887560606003  Validation Loss : 61.596391677856445 Val_Reconstruction : 58.79242134094238 Val_KL : 2.8039697408676147\n","Epoch: 638/5000  Traning Loss: 62.89075946807861  Train_Reconstruction: 60.05104303359985  Train_KL: 2.8397166430950165  Validation Loss : 61.58308410644531 Val_Reconstruction : 58.757192611694336 Val_KL : 2.8258910179138184\n","Epoch: 639/5000  Traning Loss: 63.22799015045166  Train_Reconstruction: 60.367225646972656  Train_KL: 2.860764116048813  Validation Loss : 61.828330993652344 Val_Reconstruction : 59.01178550720215 Val_KL : 2.8165477514266968\n","Epoch: 640/5000  Traning Loss: 63.24284648895264  Train_Reconstruction: 60.373380184173584  Train_KL: 2.8694656789302826  Validation Loss : 61.9392204284668 Val_Reconstruction : 59.088829040527344 Val_KL : 2.8503907918930054\n","Epoch: 641/5000  Traning Loss: 62.90288257598877  Train_Reconstruction: 60.0319299697876  Train_KL: 2.870952934026718  Validation Loss : 61.767751693725586 Val_Reconstruction : 58.9294376373291 Val_KL : 2.8383136987686157\n","Epoch: 642/5000  Traning Loss: 62.92172193527222  Train_Reconstruction: 60.06842660903931  Train_KL: 2.853295713663101  Validation Loss : 61.530982971191406 Val_Reconstruction : 58.715829849243164 Val_KL : 2.8151509761810303\n","Epoch: 643/5000  Traning Loss: 62.790595054626465  Train_Reconstruction: 59.93757343292236  Train_KL: 2.853022485971451  Validation Loss : 61.4864387512207 Val_Reconstruction : 58.655683517456055 Val_KL : 2.8307576179504395\n","Epoch: 644/5000  Traning Loss: 62.84621477127075  Train_Reconstruction: 59.99708652496338  Train_KL: 2.849128305912018  Validation Loss : 61.70790672302246 Val_Reconstruction : 58.90693283081055 Val_KL : 2.8009718656539917\n","Epoch: 645/5000  Traning Loss: 63.29550266265869  Train_Reconstruction: 60.45735502243042  Train_KL: 2.8381485641002655  Validation Loss : 61.55746650695801 Val_Reconstruction : 58.739402770996094 Val_KL : 2.8180617094039917\n","Epoch: 646/5000  Traning Loss: 62.982590198516846  Train_Reconstruction: 60.11861991882324  Train_KL: 2.863970309495926  Validation Loss : 61.55542755126953 Val_Reconstruction : 58.73025703430176 Val_KL : 2.825171113014221\n","Epoch: 647/5000  Traning Loss: 62.742785930633545  Train_Reconstruction: 59.88924503326416  Train_KL: 2.853540688753128  Validation Loss : 61.3186092376709 Val_Reconstruction : 58.497467041015625 Val_KL : 2.8211437463760376\n","Epoch: 648/5000  Traning Loss: 62.80873727798462  Train_Reconstruction: 59.94480657577515  Train_KL: 2.8639305233955383  Validation Loss : 61.64383888244629 Val_Reconstruction : 58.82029914855957 Val_KL : 2.823541522026062\n","Epoch: 649/5000  Traning Loss: 62.824421405792236  Train_Reconstruction: 59.96386480331421  Train_KL: 2.8605567514896393  Validation Loss : 61.50751876831055 Val_Reconstruction : 58.67283630371094 Val_KL : 2.834682822227478\n","Epoch: 650/5000  Traning Loss: 62.7093071937561  Train_Reconstruction: 59.84066820144653  Train_KL: 2.868638515472412  Validation Loss : 61.33494567871094 Val_Reconstruction : 58.495323181152344 Val_KL : 2.8396220207214355\n","Epoch: 651/5000  Traning Loss: 62.75116157531738  Train_Reconstruction: 59.88855504989624  Train_KL: 2.8626067340373993  Validation Loss : 61.989248275756836 Val_Reconstruction : 59.169504165649414 Val_KL : 2.819744110107422\n","Epoch: 652/5000  Traning Loss: 63.073081970214844  Train_Reconstruction: 60.21041202545166  Train_KL: 2.8626693785190582  Validation Loss : 61.935768127441406 Val_Reconstruction : 59.1193904876709 Val_KL : 2.8163766860961914\n","Epoch: 653/5000  Traning Loss: 63.1109504699707  Train_Reconstruction: 60.26098823547363  Train_KL: 2.849962443113327  Validation Loss : 61.791303634643555 Val_Reconstruction : 58.9782600402832 Val_KL : 2.813045382499695\n","Epoch: 654/5000  Traning Loss: 62.82112741470337  Train_Reconstruction: 59.9646635055542  Train_KL: 2.8564637303352356  Validation Loss : 61.467559814453125 Val_Reconstruction : 58.648794174194336 Val_KL : 2.818765640258789\n","Epoch: 655/5000  Traning Loss: 62.786277770996094  Train_Reconstruction: 59.944103717803955  Train_KL: 2.842173606157303  Validation Loss : 61.50991630554199 Val_Reconstruction : 58.688493728637695 Val_KL : 2.8214240074157715\n","Epoch: 656/5000  Traning Loss: 62.88457536697388  Train_Reconstruction: 60.021663188934326  Train_KL: 2.8629120588302612  Validation Loss : 61.61639595031738 Val_Reconstruction : 58.78525352478027 Val_KL : 2.831143021583557\n","Epoch: 657/5000  Traning Loss: 63.17476224899292  Train_Reconstruction: 60.30322074890137  Train_KL: 2.8715403974056244  Validation Loss : 62.28240394592285 Val_Reconstruction : 59.4284553527832 Val_KL : 2.8539481163024902\n","Epoch: 658/5000  Traning Loss: 63.15504026412964  Train_Reconstruction: 60.274388790130615  Train_KL: 2.880650579929352  Validation Loss : 61.57645225524902 Val_Reconstruction : 58.75065994262695 Val_KL : 2.825792074203491\n","Epoch: 659/5000  Traning Loss: 63.043354511260986  Train_Reconstruction: 60.18367862701416  Train_KL: 2.8596759736537933  Validation Loss : 61.82114219665527 Val_Reconstruction : 58.99478530883789 Val_KL : 2.8263579607009888\n","Epoch: 660/5000  Traning Loss: 63.13361978530884  Train_Reconstruction: 60.27417278289795  Train_KL: 2.8594472110271454  Validation Loss : 62.22834587097168 Val_Reconstruction : 59.39353942871094 Val_KL : 2.8348045349121094\n","Epoch: 661/5000  Traning Loss: 63.309146881103516  Train_Reconstruction: 60.446152687072754  Train_KL: 2.8629945516586304  Validation Loss : 62.142784118652344 Val_Reconstruction : 59.31461524963379 Val_KL : 2.82817006111145\n","Epoch: 662/5000  Traning Loss: 62.87997055053711  Train_Reconstruction: 60.01258993148804  Train_KL: 2.8673816919326782  Validation Loss : 61.429391860961914 Val_Reconstruction : 58.60830879211426 Val_KL : 2.8210816383361816\n","Epoch: 663/5000  Traning Loss: 62.848482608795166  Train_Reconstruction: 59.99698495864868  Train_KL: 2.8514981269836426  Validation Loss : 61.581092834472656 Val_Reconstruction : 58.75095748901367 Val_KL : 2.8301366567611694\n","Epoch: 664/5000  Traning Loss: 62.844868659973145  Train_Reconstruction: 59.97675371170044  Train_KL: 2.868115097284317  Validation Loss : 61.546945571899414 Val_Reconstruction : 58.71689414978027 Val_KL : 2.8300517797470093\n","Epoch: 665/5000  Traning Loss: 62.77497434616089  Train_Reconstruction: 59.92332410812378  Train_KL: 2.8516502678394318  Validation Loss : 61.5482177734375 Val_Reconstruction : 58.73830223083496 Val_KL : 2.809914708137512\n","Epoch: 666/5000  Traning Loss: 62.776607513427734  Train_Reconstruction: 59.9187536239624  Train_KL: 2.857853412628174  Validation Loss : 61.328372955322266 Val_Reconstruction : 58.49310111999512 Val_KL : 2.8352737426757812\n","Epoch: 667/5000  Traning Loss: 62.77876663208008  Train_Reconstruction: 59.91440153121948  Train_KL: 2.8643662333488464  Validation Loss : 61.58921813964844 Val_Reconstruction : 58.77324295043945 Val_KL : 2.815973401069641\n","Epoch: 668/5000  Traning Loss: 62.714393615722656  Train_Reconstruction: 59.85569095611572  Train_KL: 2.8587021231651306  Validation Loss : 61.40909957885742 Val_Reconstruction : 58.587881088256836 Val_KL : 2.8212190866470337\n","Epoch: 669/5000  Traning Loss: 62.623108863830566  Train_Reconstruction: 59.77222919464111  Train_KL: 2.8508796393871307  Validation Loss : 61.33404541015625 Val_Reconstruction : 58.5224552154541 Val_KL : 2.81158983707428\n","Epoch: 670/5000  Traning Loss: 62.55201292037964  Train_Reconstruction: 59.69871759414673  Train_KL: 2.8532952666282654  Validation Loss : 61.41293144226074 Val_Reconstruction : 58.59918212890625 Val_KL : 2.8137497901916504\n","Epoch: 671/5000  Traning Loss: 62.689122676849365  Train_Reconstruction: 59.82744836807251  Train_KL: 2.8616742193698883  Validation Loss : 61.385568618774414 Val_Reconstruction : 58.55662536621094 Val_KL : 2.828942060470581\n","Epoch: 672/5000  Traning Loss: 62.53865051269531  Train_Reconstruction: 59.686720848083496  Train_KL: 2.8519293665885925  Validation Loss : 61.142181396484375 Val_Reconstruction : 58.33634376525879 Val_KL : 2.805836319923401\n","Epoch: 673/5000  Traning Loss: 62.49960470199585  Train_Reconstruction: 59.64354372024536  Train_KL: 2.8560606241226196  Validation Loss : 61.13173866271973 Val_Reconstruction : 58.30236625671387 Val_KL : 2.8293726444244385\n","Epoch: 674/5000  Traning Loss: 62.52339696884155  Train_Reconstruction: 59.665945053100586  Train_KL: 2.8574514985084534  Validation Loss : 61.00607872009277 Val_Reconstruction : 58.17840385437012 Val_KL : 2.8276727199554443\n","Epoch: 675/5000  Traning Loss: 62.593077182769775  Train_Reconstruction: 59.73570251464844  Train_KL: 2.857374995946884  Validation Loss : 61.18295097351074 Val_Reconstruction : 58.34445762634277 Val_KL : 2.8384939432144165\n","Epoch: 676/5000  Traning Loss: 62.993205547332764  Train_Reconstruction: 60.11568737030029  Train_KL: 2.8775190114974976  Validation Loss : 62.11224555969238 Val_Reconstruction : 59.26805877685547 Val_KL : 2.8441883325576782\n","Epoch: 677/5000  Traning Loss: 63.34875297546387  Train_Reconstruction: 60.4853515625  Train_KL: 2.8634007275104523  Validation Loss : 61.8787841796875 Val_Reconstruction : 59.061723709106445 Val_KL : 2.8170605897903442\n","Epoch: 678/5000  Traning Loss: 63.09069013595581  Train_Reconstruction: 60.23144292831421  Train_KL: 2.8592459559440613  Validation Loss : 61.704833984375 Val_Reconstruction : 58.87480354309082 Val_KL : 2.83003032207489\n","Epoch: 679/5000  Traning Loss: 63.05639028549194  Train_Reconstruction: 60.20118522644043  Train_KL: 2.8552048802375793  Validation Loss : 62.03843688964844 Val_Reconstruction : 59.22350311279297 Val_KL : 2.8149349689483643\n","Epoch: 680/5000  Traning Loss: 62.97669315338135  Train_Reconstruction: 60.119133949279785  Train_KL: 2.8575586080551147  Validation Loss : 61.30705451965332 Val_Reconstruction : 58.491167068481445 Val_KL : 2.81588876247406\n","Epoch: 681/5000  Traning Loss: 63.33741760253906  Train_Reconstruction: 60.48453426361084  Train_KL: 2.8528831601142883  Validation Loss : 61.879438400268555 Val_Reconstruction : 59.05288124084473 Val_KL : 2.82655668258667\n","Epoch: 682/5000  Traning Loss: 63.36361742019653  Train_Reconstruction: 60.510026931762695  Train_KL: 2.853589951992035  Validation Loss : 61.45873832702637 Val_Reconstruction : 58.642587661743164 Val_KL : 2.816149115562439\n","Epoch: 683/5000  Traning Loss: 63.7588791847229  Train_Reconstruction: 60.88434600830078  Train_KL: 2.8745336532592773  Validation Loss : 62.35764122009277 Val_Reconstruction : 59.51134490966797 Val_KL : 2.8462947607040405\n","Epoch: 684/5000  Traning Loss: 63.54837989807129  Train_Reconstruction: 60.68937921524048  Train_KL: 2.8590003848075867  Validation Loss : 61.64344024658203 Val_Reconstruction : 58.827924728393555 Val_KL : 2.815515875816345\n","Epoch: 685/5000  Traning Loss: 63.23846626281738  Train_Reconstruction: 60.38422918319702  Train_KL: 2.854236662387848  Validation Loss : 62.02561569213867 Val_Reconstruction : 59.19849395751953 Val_KL : 2.8271219730377197\n","Epoch: 686/5000  Traning Loss: 62.86790609359741  Train_Reconstruction: 60.00186252593994  Train_KL: 2.8660442233085632  Validation Loss : 61.36267280578613 Val_Reconstruction : 58.52906608581543 Val_KL : 2.8336058855056763\n","Epoch: 687/5000  Traning Loss: 62.57685899734497  Train_Reconstruction: 59.70866584777832  Train_KL: 2.86819326877594  Validation Loss : 61.00715446472168 Val_Reconstruction : 58.16237258911133 Val_KL : 2.844782829284668\n","Epoch: 688/5000  Traning Loss: 62.55410623550415  Train_Reconstruction: 59.67980623245239  Train_KL: 2.8742994368076324  Validation Loss : 61.32700538635254 Val_Reconstruction : 58.478681564331055 Val_KL : 2.8483232259750366\n","Epoch: 689/5000  Traning Loss: 62.752875328063965  Train_Reconstruction: 59.88784885406494  Train_KL: 2.865026831626892  Validation Loss : 61.51371765136719 Val_Reconstruction : 58.682308197021484 Val_KL : 2.831407308578491\n","Epoch: 690/5000  Traning Loss: 62.430222511291504  Train_Reconstruction: 59.56286811828613  Train_KL: 2.867354243993759  Validation Loss : 60.89316177368164 Val_Reconstruction : 58.05624198913574 Val_KL : 2.8369191884994507\n","Epoch: 691/5000  Traning Loss: 62.49779796600342  Train_Reconstruction: 59.629403591156006  Train_KL: 2.8683942258358  Validation Loss : 61.10171318054199 Val_Reconstruction : 58.25011444091797 Val_KL : 2.8515989780426025\n","Epoch: 692/5000  Traning Loss: 63.273540019989014  Train_Reconstruction: 60.407156467437744  Train_KL: 2.866383135318756  Validation Loss : 61.730621337890625 Val_Reconstruction : 58.90297889709473 Val_KL : 2.82764208316803\n","Epoch: 693/5000  Traning Loss: 63.36350774765015  Train_Reconstruction: 60.49453544616699  Train_KL: 2.8689718544483185  Validation Loss : 61.75593376159668 Val_Reconstruction : 58.92512130737305 Val_KL : 2.8308123350143433\n","Epoch: 694/5000  Traning Loss: 62.906198024749756  Train_Reconstruction: 60.043367862701416  Train_KL: 2.862830102443695  Validation Loss : 61.46293830871582 Val_Reconstruction : 58.64359474182129 Val_KL : 2.8193432092666626\n","Epoch: 695/5000  Traning Loss: 62.73086595535278  Train_Reconstruction: 59.87880516052246  Train_KL: 2.852060854434967  Validation Loss : 61.654584884643555 Val_Reconstruction : 58.83894157409668 Val_KL : 2.8156445026397705\n","Epoch: 696/5000  Traning Loss: 62.73848009109497  Train_Reconstruction: 59.886351108551025  Train_KL: 2.8521286249160767  Validation Loss : 61.20034217834473 Val_Reconstruction : 58.39273262023926 Val_KL : 2.8076106309890747\n","Epoch: 697/5000  Traning Loss: 62.47903347015381  Train_Reconstruction: 59.60814714431763  Train_KL: 2.8708865344524384  Validation Loss : 61.180572509765625 Val_Reconstruction : 58.342620849609375 Val_KL : 2.8379513025283813\n","Epoch: 698/5000  Traning Loss: 62.346450328826904  Train_Reconstruction: 59.4862699508667  Train_KL: 2.860180288553238  Validation Loss : 61.1375789642334 Val_Reconstruction : 58.31168174743652 Val_KL : 2.8258968591690063\n","Epoch: 699/5000  Traning Loss: 62.74219465255737  Train_Reconstruction: 59.87003135681152  Train_KL: 2.872162848711014  Validation Loss : 61.200923919677734 Val_Reconstruction : 58.3625431060791 Val_KL : 2.838379740715027\n","Epoch: 700/5000  Traning Loss: 62.66964340209961  Train_Reconstruction: 59.81167411804199  Train_KL: 2.8579697012901306  Validation Loss : 61.469858169555664 Val_Reconstruction : 58.64426231384277 Val_KL : 2.8255947828292847\n","Epoch: 701/5000  Traning Loss: 62.679739475250244  Train_Reconstruction: 59.8027982711792  Train_KL: 2.876940816640854  Validation Loss : 61.1590690612793 Val_Reconstruction : 58.31699752807617 Val_KL : 2.8420698642730713\n","Epoch: 702/5000  Traning Loss: 62.409568786621094  Train_Reconstruction: 59.549099922180176  Train_KL: 2.860468566417694  Validation Loss : 61.02908134460449 Val_Reconstruction : 58.21187210083008 Val_KL : 2.8172069787979126\n","Epoch: 703/5000  Traning Loss: 62.58457612991333  Train_Reconstruction: 59.71307182312012  Train_KL: 2.8715047240257263  Validation Loss : 61.596473693847656 Val_Reconstruction : 58.747467041015625 Val_KL : 2.849008321762085\n","Epoch: 704/5000  Traning Loss: 62.73796224594116  Train_Reconstruction: 59.875715255737305  Train_KL: 2.8622471392154694  Validation Loss : 61.52983856201172 Val_Reconstruction : 58.7090950012207 Val_KL : 2.8207411766052246\n","Epoch: 705/5000  Traning Loss: 62.45683574676514  Train_Reconstruction: 59.597628116607666  Train_KL: 2.8592076003551483  Validation Loss : 61.01885795593262 Val_Reconstruction : 58.1940803527832 Val_KL : 2.8247779607772827\n","Epoch: 706/5000  Traning Loss: 62.38230657577515  Train_Reconstruction: 59.507179260253906  Train_KL: 2.875126898288727  Validation Loss : 61.081398010253906 Val_Reconstruction : 58.24199295043945 Val_KL : 2.8394073247909546\n","Epoch: 707/5000  Traning Loss: 62.51397085189819  Train_Reconstruction: 59.64318037033081  Train_KL: 2.87079057097435  Validation Loss : 61.12164306640625 Val_Reconstruction : 58.27865409851074 Val_KL : 2.8429882526397705\n","Epoch: 708/5000  Traning Loss: 62.72776460647583  Train_Reconstruction: 59.860058307647705  Train_KL: 2.8677063584327698  Validation Loss : 61.28602981567383 Val_Reconstruction : 58.454864501953125 Val_KL : 2.831166386604309\n","Epoch: 709/5000  Traning Loss: 62.92654371261597  Train_Reconstruction: 60.07335090637207  Train_KL: 2.85319322347641  Validation Loss : 61.457496643066406 Val_Reconstruction : 58.63607597351074 Val_KL : 2.8214218616485596\n","Epoch: 710/5000  Traning Loss: 62.823455810546875  Train_Reconstruction: 59.968196392059326  Train_KL: 2.85525980591774  Validation Loss : 61.533395767211914 Val_Reconstruction : 58.69797134399414 Val_KL : 2.8354233503341675\n","Epoch: 711/5000  Traning Loss: 62.57239818572998  Train_Reconstruction: 59.70557451248169  Train_KL: 2.8668234646320343  Validation Loss : 61.362470626831055 Val_Reconstruction : 58.53594779968262 Val_KL : 2.826524257659912\n","Epoch: 712/5000  Traning Loss: 62.7675986289978  Train_Reconstruction: 59.904847145080566  Train_KL: 2.862751841545105  Validation Loss : 61.31986999511719 Val_Reconstruction : 58.4848575592041 Val_KL : 2.835011839866638\n","Epoch: 713/5000  Traning Loss: 62.69961929321289  Train_Reconstruction: 59.836122035980225  Train_KL: 2.8634974658489227  Validation Loss : 61.074419021606445 Val_Reconstruction : 58.25152778625488 Val_KL : 2.822891592979431\n","Epoch: 714/5000  Traning Loss: 62.778146266937256  Train_Reconstruction: 59.91369295120239  Train_KL: 2.86445352435112  Validation Loss : 61.40843200683594 Val_Reconstruction : 58.58552551269531 Val_KL : 2.8229072093963623\n","Epoch: 715/5000  Traning Loss: 62.87160587310791  Train_Reconstruction: 60.00573396682739  Train_KL: 2.8658712208271027  Validation Loss : 61.36279106140137 Val_Reconstruction : 58.52181816101074 Val_KL : 2.84097421169281\n","Epoch: 716/5000  Traning Loss: 62.489055156707764  Train_Reconstruction: 59.62309503555298  Train_KL: 2.865960657596588  Validation Loss : 61.17468452453613 Val_Reconstruction : 58.34233474731445 Val_KL : 2.832350730895996\n","Epoch: 717/5000  Traning Loss: 62.423044204711914  Train_Reconstruction: 59.56223821640015  Train_KL: 2.8608058094978333  Validation Loss : 61.127410888671875 Val_Reconstruction : 58.287933349609375 Val_KL : 2.8394757509231567\n","Epoch: 718/5000  Traning Loss: 62.46116065979004  Train_Reconstruction: 59.596911907196045  Train_KL: 2.8642484545707703  Validation Loss : 61.39151191711426 Val_Reconstruction : 58.57968330383301 Val_KL : 2.8118276596069336\n","Epoch: 719/5000  Traning Loss: 62.51870393753052  Train_Reconstruction: 59.651901721954346  Train_KL: 2.866802603006363  Validation Loss : 61.318031311035156 Val_Reconstruction : 58.48405456542969 Val_KL : 2.8339779376983643\n","Epoch: 720/5000  Traning Loss: 62.529046058654785  Train_Reconstruction: 59.664459228515625  Train_KL: 2.864587038755417  Validation Loss : 61.25709915161133 Val_Reconstruction : 58.42038917541504 Val_KL : 2.8367096185684204\n","Epoch: 721/5000  Traning Loss: 62.544116497039795  Train_Reconstruction: 59.67107629776001  Train_KL: 2.873040109872818  Validation Loss : 61.18879318237305 Val_Reconstruction : 58.3538761138916 Val_KL : 2.8349181413650513\n","Epoch: 722/5000  Traning Loss: 62.44051790237427  Train_Reconstruction: 59.57973575592041  Train_KL: 2.8607819974422455  Validation Loss : 61.28657531738281 Val_Reconstruction : 58.4529914855957 Val_KL : 2.833585023880005\n","Epoch: 723/5000  Traning Loss: 62.61673593521118  Train_Reconstruction: 59.743650913238525  Train_KL: 2.8730850219726562  Validation Loss : 61.240516662597656 Val_Reconstruction : 58.403079986572266 Val_KL : 2.83743679523468\n","Epoch: 724/5000  Traning Loss: 62.632492542266846  Train_Reconstruction: 59.762351512908936  Train_KL: 2.8701407611370087  Validation Loss : 61.30180549621582 Val_Reconstruction : 58.47502899169922 Val_KL : 2.8267757892608643\n","Epoch: 725/5000  Traning Loss: 62.525197982788086  Train_Reconstruction: 59.67175626754761  Train_KL: 2.8534419536590576  Validation Loss : 61.32649612426758 Val_Reconstruction : 58.508514404296875 Val_KL : 2.817980408668518\n","Epoch: 726/5000  Traning Loss: 62.445054054260254  Train_Reconstruction: 59.585750102996826  Train_KL: 2.8593043088912964  Validation Loss : 61.274423599243164 Val_Reconstruction : 58.45315361022949 Val_KL : 2.8212705850601196\n","Epoch: 727/5000  Traning Loss: 62.39970397949219  Train_Reconstruction: 59.549540996551514  Train_KL: 2.850162446498871  Validation Loss : 61.31597328186035 Val_Reconstruction : 58.49468231201172 Val_KL : 2.821289539337158\n","Epoch: 728/5000  Traning Loss: 62.32568073272705  Train_Reconstruction: 59.46804094314575  Train_KL: 2.857639789581299  Validation Loss : 60.88030433654785 Val_Reconstruction : 58.05404853820801 Val_KL : 2.8262566328048706\n","Epoch: 729/5000  Traning Loss: 62.1109037399292  Train_Reconstruction: 59.253180503845215  Train_KL: 2.857723832130432  Validation Loss : 60.96409797668457 Val_Reconstruction : 58.13871383666992 Val_KL : 2.8253833055496216\n","Epoch: 730/5000  Traning Loss: 62.459022998809814  Train_Reconstruction: 59.59748411178589  Train_KL: 2.8615387678146362  Validation Loss : 61.17717170715332 Val_Reconstruction : 58.36092948913574 Val_KL : 2.8162405490875244\n","Epoch: 731/5000  Traning Loss: 62.66076326370239  Train_Reconstruction: 59.81319999694824  Train_KL: 2.84756338596344  Validation Loss : 61.39354133605957 Val_Reconstruction : 58.5794563293457 Val_KL : 2.8140861988067627\n","Epoch: 732/5000  Traning Loss: 62.58734130859375  Train_Reconstruction: 59.727619647979736  Train_KL: 2.8597209453582764  Validation Loss : 61.22223472595215 Val_Reconstruction : 58.38186264038086 Val_KL : 2.840373396873474\n","Epoch: 733/5000  Traning Loss: 62.265583992004395  Train_Reconstruction: 59.39138746261597  Train_KL: 2.8741964995861053  Validation Loss : 60.96107864379883 Val_Reconstruction : 58.116519927978516 Val_KL : 2.844559907913208\n","Epoch: 734/5000  Traning Loss: 62.213876724243164  Train_Reconstruction: 59.33696460723877  Train_KL: 2.876911461353302  Validation Loss : 60.94180870056152 Val_Reconstruction : 58.0958366394043 Val_KL : 2.8459709882736206\n","Epoch: 735/5000  Traning Loss: 62.286030769348145  Train_Reconstruction: 59.413959980010986  Train_KL: 2.872070014476776  Validation Loss : 60.85141181945801 Val_Reconstruction : 58.012685775756836 Val_KL : 2.838727355003357\n","Epoch: 736/5000  Traning Loss: 62.06187343597412  Train_Reconstruction: 59.201032638549805  Train_KL: 2.8608402609825134  Validation Loss : 60.847028732299805 Val_Reconstruction : 58.01877975463867 Val_KL : 2.8282487392425537\n","Epoch: 737/5000  Traning Loss: 62.048795223236084  Train_Reconstruction: 59.18414926528931  Train_KL: 2.8646462857723236  Validation Loss : 60.79670333862305 Val_Reconstruction : 57.973093032836914 Val_KL : 2.8236087560653687\n","Epoch: 738/5000  Traning Loss: 62.36970663070679  Train_Reconstruction: 59.49411153793335  Train_KL: 2.8755946159362793  Validation Loss : 61.3113899230957 Val_Reconstruction : 58.45343017578125 Val_KL : 2.8579612970352173\n","Epoch: 739/5000  Traning Loss: 62.310487270355225  Train_Reconstruction: 59.42642021179199  Train_KL: 2.884066253900528  Validation Loss : 60.940547943115234 Val_Reconstruction : 58.09900665283203 Val_KL : 2.8415411710739136\n","Epoch: 740/5000  Traning Loss: 62.61243391036987  Train_Reconstruction: 59.74841070175171  Train_KL: 2.8640230000019073  Validation Loss : 61.5682430267334 Val_Reconstruction : 58.72717475891113 Val_KL : 2.8410699367523193\n","Epoch: 741/5000  Traning Loss: 62.60647010803223  Train_Reconstruction: 59.738107681274414  Train_KL: 2.8683617413043976  Validation Loss : 61.72262191772461 Val_Reconstruction : 58.90650939941406 Val_KL : 2.816113591194153\n","Epoch: 742/5000  Traning Loss: 62.54784154891968  Train_Reconstruction: 59.67441701889038  Train_KL: 2.873424917459488  Validation Loss : 61.43233680725098 Val_Reconstruction : 58.600425720214844 Val_KL : 2.8319103717803955\n","Epoch: 743/5000  Traning Loss: 62.5454216003418  Train_Reconstruction: 59.68425130844116  Train_KL: 2.861170083284378  Validation Loss : 61.1241340637207 Val_Reconstruction : 58.29882621765137 Val_KL : 2.8253060579299927\n","Epoch: 744/5000  Traning Loss: 62.41961908340454  Train_Reconstruction: 59.559438705444336  Train_KL: 2.860180377960205  Validation Loss : 61.031185150146484 Val_Reconstruction : 58.196739196777344 Val_KL : 2.8344435691833496\n","Epoch: 745/5000  Traning Loss: 62.62230062484741  Train_Reconstruction: 59.74539136886597  Train_KL: 2.876909017562866  Validation Loss : 61.57009315490723 Val_Reconstruction : 58.737483978271484 Val_KL : 2.832608222961426\n","Epoch: 746/5000  Traning Loss: 62.6184024810791  Train_Reconstruction: 59.73773241043091  Train_KL: 2.8806698620319366  Validation Loss : 61.34278678894043 Val_Reconstruction : 58.49630546569824 Val_KL : 2.846481680870056\n","Epoch: 747/5000  Traning Loss: 62.34868907928467  Train_Reconstruction: 59.4752516746521  Train_KL: 2.8734371960163116  Validation Loss : 61.07712173461914 Val_Reconstruction : 58.24721908569336 Val_KL : 2.829902768135071\n","Epoch: 748/5000  Traning Loss: 62.16850471496582  Train_Reconstruction: 59.28348207473755  Train_KL: 2.885022521018982  Validation Loss : 60.759090423583984 Val_Reconstruction : 57.90684509277344 Val_KL : 2.8522465229034424\n","Epoch: 749/5000  Traning Loss: 62.11445951461792  Train_Reconstruction: 59.228771686553955  Train_KL: 2.885687619447708  Validation Loss : 60.88411521911621 Val_Reconstruction : 58.03261375427246 Val_KL : 2.8515015840530396\n","Epoch: 750/5000  Traning Loss: 62.38484525680542  Train_Reconstruction: 59.50997829437256  Train_KL: 2.8748671412467957  Validation Loss : 61.24643135070801 Val_Reconstruction : 58.424325942993164 Val_KL : 2.8221046924591064\n","Epoch: 751/5000  Traning Loss: 62.333980560302734  Train_Reconstruction: 59.477612018585205  Train_KL: 2.85636830329895  Validation Loss : 60.86081123352051 Val_Reconstruction : 58.03922462463379 Val_KL : 2.821587920188904\n","Epoch: 752/5000  Traning Loss: 62.36275243759155  Train_Reconstruction: 59.483726978302  Train_KL: 2.8790260553359985  Validation Loss : 61.11098861694336 Val_Reconstruction : 58.27653884887695 Val_KL : 2.8344502449035645\n","Epoch: 753/5000  Traning Loss: 62.08096790313721  Train_Reconstruction: 59.21607255935669  Train_KL: 2.8648954033851624  Validation Loss : 60.838483810424805 Val_Reconstruction : 58.008670806884766 Val_KL : 2.8298124074935913\n","Epoch: 754/5000  Traning Loss: 62.21718406677246  Train_Reconstruction: 59.35268259048462  Train_KL: 2.8645009994506836  Validation Loss : 60.86092567443848 Val_Reconstruction : 58.054832458496094 Val_KL : 2.8060933351516724\n","Epoch: 755/5000  Traning Loss: 61.98295497894287  Train_Reconstruction: 59.12297201156616  Train_KL: 2.859983056783676  Validation Loss : 60.67789649963379 Val_Reconstruction : 57.85044288635254 Val_KL : 2.827452063560486\n","Epoch: 756/5000  Traning Loss: 62.04567861557007  Train_Reconstruction: 59.18268346786499  Train_KL: 2.8629955649375916  Validation Loss : 60.89589309692383 Val_Reconstruction : 58.072139739990234 Val_KL : 2.8237524032592773\n","Epoch: 757/5000  Traning Loss: 62.4291672706604  Train_Reconstruction: 59.558887004852295  Train_KL: 2.870279461145401  Validation Loss : 61.739078521728516 Val_Reconstruction : 58.89591407775879 Val_KL : 2.8431657552719116\n","Epoch: 758/5000  Traning Loss: 62.48090887069702  Train_Reconstruction: 59.59230661392212  Train_KL: 2.888601988554001  Validation Loss : 61.07847595214844 Val_Reconstruction : 58.23353576660156 Val_KL : 2.844940662384033\n","Epoch: 759/5000  Traning Loss: 62.073646068573  Train_Reconstruction: 59.2004714012146  Train_KL: 2.873174339532852  Validation Loss : 60.63916206359863 Val_Reconstruction : 57.80940818786621 Val_KL : 2.829752564430237\n","Epoch: 760/5000  Traning Loss: 62.058470249176025  Train_Reconstruction: 59.18926477432251  Train_KL: 2.8692056238651276  Validation Loss : 60.72335433959961 Val_Reconstruction : 57.90103530883789 Val_KL : 2.8223206996917725\n","Epoch: 761/5000  Traning Loss: 62.41150951385498  Train_Reconstruction: 59.56381130218506  Train_KL: 2.8476981818675995  Validation Loss : 61.462839126586914 Val_Reconstruction : 58.643022537231445 Val_KL : 2.8198187351226807\n","Epoch: 762/5000  Traning Loss: 62.56014156341553  Train_Reconstruction: 59.6938853263855  Train_KL: 2.866255760192871  Validation Loss : 60.98824882507324 Val_Reconstruction : 58.149898529052734 Val_KL : 2.8383491039276123\n","Epoch: 763/5000  Traning Loss: 62.138118743896484  Train_Reconstruction: 59.250476360321045  Train_KL: 2.8876422345638275  Validation Loss : 60.69525909423828 Val_Reconstruction : 57.84675598144531 Val_KL : 2.8485015630722046\n","Epoch: 764/5000  Traning Loss: 61.95597505569458  Train_Reconstruction: 59.0954966545105  Train_KL: 2.8604786694049835  Validation Loss : 60.73151206970215 Val_Reconstruction : 57.90897750854492 Val_KL : 2.822533369064331\n","Epoch: 765/5000  Traning Loss: 62.02854537963867  Train_Reconstruction: 59.15290975570679  Train_KL: 2.8756361305713654  Validation Loss : 60.643306732177734 Val_Reconstruction : 57.79069519042969 Val_KL : 2.8526134490966797\n","Epoch: 766/5000  Traning Loss: 62.11417102813721  Train_Reconstruction: 59.242034912109375  Train_KL: 2.8721359968185425  Validation Loss : 60.70171356201172 Val_Reconstruction : 57.867074966430664 Val_KL : 2.834640622138977\n","Epoch: 767/5000  Traning Loss: 61.92090034484863  Train_Reconstruction: 59.04512929916382  Train_KL: 2.8757706582546234  Validation Loss : 60.57244110107422 Val_Reconstruction : 57.73836326599121 Val_KL : 2.8340765237808228\n","Epoch: 768/5000  Traning Loss: 61.89850378036499  Train_Reconstruction: 59.02439594268799  Train_KL: 2.874107450246811  Validation Loss : 60.85098075866699 Val_Reconstruction : 58.00648880004883 Val_KL : 2.8444916009902954\n","Epoch: 769/5000  Traning Loss: 61.892189025878906  Train_Reconstruction: 59.0283145904541  Train_KL: 2.8638747334480286  Validation Loss : 60.692691802978516 Val_Reconstruction : 57.8775520324707 Val_KL : 2.8151413202285767\n","Epoch: 770/5000  Traning Loss: 61.88435506820679  Train_Reconstruction: 59.03193807601929  Train_KL: 2.8524169921875  Validation Loss : 60.49106407165527 Val_Reconstruction : 57.663536071777344 Val_KL : 2.82752788066864\n","Epoch: 771/5000  Traning Loss: 61.9525408744812  Train_Reconstruction: 59.07592821121216  Train_KL: 2.876612424850464  Validation Loss : 60.771223068237305 Val_Reconstruction : 57.91585731506348 Val_KL : 2.855365514755249\n","Epoch: 772/5000  Traning Loss: 61.97511863708496  Train_Reconstruction: 59.083730697631836  Train_KL: 2.8913879990577698  Validation Loss : 60.81821250915527 Val_Reconstruction : 57.98381996154785 Val_KL : 2.834393262863159\n","Epoch: 773/5000  Traning Loss: 62.582366943359375  Train_Reconstruction: 59.71366024017334  Train_KL: 2.868706911802292  Validation Loss : 60.98521041870117 Val_Reconstruction : 58.149539947509766 Val_KL : 2.8356701135635376\n","Epoch: 774/5000  Traning Loss: 62.322612285614014  Train_Reconstruction: 59.46680688858032  Train_KL: 2.8558053374290466  Validation Loss : 60.96028709411621 Val_Reconstruction : 58.12841606140137 Val_KL : 2.831870198249817\n","Epoch: 775/5000  Traning Loss: 62.18247127532959  Train_Reconstruction: 59.318761348724365  Train_KL: 2.863710343837738  Validation Loss : 61.03131294250488 Val_Reconstruction : 58.22172164916992 Val_KL : 2.809592366218567\n","Epoch: 776/5000  Traning Loss: 62.43137741088867  Train_Reconstruction: 59.55788993835449  Train_KL: 2.8734877705574036  Validation Loss : 61.092864990234375 Val_Reconstruction : 58.230743408203125 Val_KL : 2.8621230125427246\n","Epoch: 777/5000  Traning Loss: 62.14931392669678  Train_Reconstruction: 59.26661205291748  Train_KL: 2.8827025294303894  Validation Loss : 61.2519645690918 Val_Reconstruction : 58.41825866699219 Val_KL : 2.8337056636810303\n","Epoch: 778/5000  Traning Loss: 61.93201160430908  Train_Reconstruction: 59.04657173156738  Train_KL: 2.8854400515556335  Validation Loss : 60.63682746887207 Val_Reconstruction : 57.7719841003418 Val_KL : 2.8648418188095093\n","Epoch: 779/5000  Traning Loss: 61.90321636199951  Train_Reconstruction: 59.01839590072632  Train_KL: 2.8848204612731934  Validation Loss : 60.763614654541016 Val_Reconstruction : 57.92653274536133 Val_KL : 2.8370829820632935\n","Epoch: 780/5000  Traning Loss: 61.9640736579895  Train_Reconstruction: 59.08808469772339  Train_KL: 2.8759891390800476  Validation Loss : 60.66240882873535 Val_Reconstruction : 57.8263053894043 Val_KL : 2.836103677749634\n","Epoch: 781/5000  Traning Loss: 62.11826944351196  Train_Reconstruction: 59.252830028533936  Train_KL: 2.8654388189315796  Validation Loss : 60.919734954833984 Val_Reconstruction : 58.07973098754883 Val_KL : 2.8400031328201294\n","Epoch: 782/5000  Traning Loss: 62.44526481628418  Train_Reconstruction: 59.57162857055664  Train_KL: 2.873636394739151  Validation Loss : 61.00127601623535 Val_Reconstruction : 58.16197204589844 Val_KL : 2.8393030166625977\n","Epoch: 783/5000  Traning Loss: 62.29576826095581  Train_Reconstruction: 59.41833019256592  Train_KL: 2.8774376809597015  Validation Loss : 60.765892028808594 Val_Reconstruction : 57.92246437072754 Val_KL : 2.843426823616028\n","Epoch: 784/5000  Traning Loss: 61.925353050231934  Train_Reconstruction: 59.04519033432007  Train_KL: 2.880162298679352  Validation Loss : 60.772958755493164 Val_Reconstruction : 57.932621002197266 Val_KL : 2.8403362035751343\n","Epoch: 785/5000  Traning Loss: 61.8902473449707  Train_Reconstruction: 59.015525341033936  Train_KL: 2.8747227787971497  Validation Loss : 60.6865119934082 Val_Reconstruction : 57.83417510986328 Val_KL : 2.852337121963501\n","Epoch: 786/5000  Traning Loss: 62.03436088562012  Train_Reconstruction: 59.161202907562256  Train_KL: 2.8731579780578613  Validation Loss : 61.18735885620117 Val_Reconstruction : 58.339128494262695 Val_KL : 2.8482314348220825\n","Epoch: 787/5000  Traning Loss: 62.17062759399414  Train_Reconstruction: 59.28805208206177  Train_KL: 2.8825754821300507  Validation Loss : 60.94495391845703 Val_Reconstruction : 58.10173225402832 Val_KL : 2.843223452568054\n","Epoch: 788/5000  Traning Loss: 62.165276527404785  Train_Reconstruction: 59.29444122314453  Train_KL: 2.870834857225418  Validation Loss : 61.0444278717041 Val_Reconstruction : 58.21358871459961 Val_KL : 2.8308383226394653\n","Epoch: 789/5000  Traning Loss: 62.15729999542236  Train_Reconstruction: 59.29179859161377  Train_KL: 2.8655014634132385  Validation Loss : 60.9824104309082 Val_Reconstruction : 58.143571853637695 Val_KL : 2.838839054107666\n","Epoch: 790/5000  Traning Loss: 62.203096866607666  Train_Reconstruction: 59.33076190948486  Train_KL: 2.8723353445529938  Validation Loss : 60.70977973937988 Val_Reconstruction : 57.86236381530762 Val_KL : 2.84741747379303\n","Epoch: 791/5000  Traning Loss: 61.870455741882324  Train_Reconstruction: 58.98193550109863  Train_KL: 2.888520061969757  Validation Loss : 60.54237365722656 Val_Reconstruction : 57.70258331298828 Val_KL : 2.8397899866104126\n","Epoch: 792/5000  Traning Loss: 61.87222337722778  Train_Reconstruction: 59.00499963760376  Train_KL: 2.867223173379898  Validation Loss : 60.570804595947266 Val_Reconstruction : 57.74179267883301 Val_KL : 2.8290131092071533\n","Epoch: 793/5000  Traning Loss: 61.924683570861816  Train_Reconstruction: 59.04023599624634  Train_KL: 2.88444784283638  Validation Loss : 60.884334564208984 Val_Reconstruction : 58.04884719848633 Val_KL : 2.8354870080947876\n","Epoch: 794/5000  Traning Loss: 62.04301929473877  Train_Reconstruction: 59.179980754852295  Train_KL: 2.863038092851639  Validation Loss : 61.07832145690918 Val_Reconstruction : 58.245962142944336 Val_KL : 2.832359552383423\n","Epoch: 795/5000  Traning Loss: 62.16155290603638  Train_Reconstruction: 59.279706954956055  Train_KL: 2.8818461894989014  Validation Loss : 61.46124267578125 Val_Reconstruction : 58.62864303588867 Val_KL : 2.832598567008972\n","Epoch: 796/5000  Traning Loss: 62.33577585220337  Train_Reconstruction: 59.4759840965271  Train_KL: 2.8597915172576904  Validation Loss : 61.360443115234375 Val_Reconstruction : 58.52789115905762 Val_KL : 2.8325517177581787\n","Epoch: 797/5000  Traning Loss: 62.095356941223145  Train_Reconstruction: 59.227559089660645  Train_KL: 2.8677978217601776  Validation Loss : 60.87787055969238 Val_Reconstruction : 58.04717826843262 Val_KL : 2.830691337585449\n","Epoch: 798/5000  Traning Loss: 62.043960094451904  Train_Reconstruction: 59.16861438751221  Train_KL: 2.875345766544342  Validation Loss : 60.76359939575195 Val_Reconstruction : 57.92581367492676 Val_KL : 2.8377845287323\n","Epoch: 799/5000  Traning Loss: 61.71593761444092  Train_Reconstruction: 58.8535361289978  Train_KL: 2.8624014258384705  Validation Loss : 60.599021911621094 Val_Reconstruction : 57.78215789794922 Val_KL : 2.816862106323242\n","Epoch: 800/5000  Traning Loss: 61.87934494018555  Train_Reconstruction: 59.01375436782837  Train_KL: 2.8655915558338165  Validation Loss : 61.001197814941406 Val_Reconstruction : 58.1616325378418 Val_KL : 2.8395659923553467\n","Epoch: 801/5000  Traning Loss: 61.98779106140137  Train_Reconstruction: 59.104125022888184  Train_KL: 2.8836662769317627  Validation Loss : 60.74624252319336 Val_Reconstruction : 57.897132873535156 Val_KL : 2.8491082191467285\n","Epoch: 802/5000  Traning Loss: 62.05144643783569  Train_Reconstruction: 59.16804838180542  Train_KL: 2.8833976686000824  Validation Loss : 61.044334411621094 Val_Reconstruction : 58.21250915527344 Val_KL : 2.831826686859131\n","Epoch: 803/5000  Traning Loss: 61.98957347869873  Train_Reconstruction: 59.11029624938965  Train_KL: 2.879277318716049  Validation Loss : 60.87296676635742 Val_Reconstruction : 58.03269577026367 Val_KL : 2.84027099609375\n","Epoch: 804/5000  Traning Loss: 61.93977499008179  Train_Reconstruction: 59.06107950210571  Train_KL: 2.8786949515342712  Validation Loss : 60.61005783081055 Val_Reconstruction : 57.77134323120117 Val_KL : 2.8387151956558228\n","Epoch: 805/5000  Traning Loss: 61.90948438644409  Train_Reconstruction: 59.04119300842285  Train_KL: 2.86829149723053  Validation Loss : 60.85460662841797 Val_Reconstruction : 58.00755310058594 Val_KL : 2.8470538854599\n","Epoch: 806/5000  Traning Loss: 62.09944772720337  Train_Reconstruction: 59.21668195724487  Train_KL: 2.88276669383049  Validation Loss : 60.82400894165039 Val_Reconstruction : 57.989173889160156 Val_KL : 2.8348352909088135\n","Epoch: 807/5000  Traning Loss: 62.243813037872314  Train_Reconstruction: 59.37357568740845  Train_KL: 2.8702370822429657  Validation Loss : 61.238277435302734 Val_Reconstruction : 58.38641548156738 Val_KL : 2.8518632650375366\n","Epoch: 808/5000  Traning Loss: 62.442832469940186  Train_Reconstruction: 59.55478572845459  Train_KL: 2.888046830892563  Validation Loss : 61.023681640625 Val_Reconstruction : 58.17522430419922 Val_KL : 2.8484578132629395\n","Epoch: 809/5000  Traning Loss: 62.02324914932251  Train_Reconstruction: 59.146037578582764  Train_KL: 2.8772114515304565  Validation Loss : 60.70998191833496 Val_Reconstruction : 57.84883117675781 Val_KL : 2.8611501455307007\n","Epoch: 810/5000  Traning Loss: 62.00676202774048  Train_Reconstruction: 59.12743616104126  Train_KL: 2.879326343536377  Validation Loss : 60.734418869018555 Val_Reconstruction : 57.895809173583984 Val_KL : 2.8386110067367554\n","Epoch: 811/5000  Traning Loss: 61.78419637680054  Train_Reconstruction: 58.91547203063965  Train_KL: 2.86872461438179  Validation Loss : 60.451629638671875 Val_Reconstruction : 57.61945152282715 Val_KL : 2.832176089286804\n","Epoch: 812/5000  Traning Loss: 61.72083330154419  Train_Reconstruction: 58.84471321105957  Train_KL: 2.8761206567287445  Validation Loss : 60.53089141845703 Val_Reconstruction : 57.68489646911621 Val_KL : 2.845992684364319\n","Epoch: 813/5000  Traning Loss: 61.76145839691162  Train_Reconstruction: 58.88818407058716  Train_KL: 2.8732745349407196  Validation Loss : 60.53067970275879 Val_Reconstruction : 57.70037841796875 Val_KL : 2.8303017616271973\n","Epoch: 814/5000  Traning Loss: 61.78215169906616  Train_Reconstruction: 58.89433002471924  Train_KL: 2.8878217339515686  Validation Loss : 60.532880783081055 Val_Reconstruction : 57.694318771362305 Val_KL : 2.838563919067383\n","Epoch: 815/5000  Traning Loss: 61.799827098846436  Train_Reconstruction: 58.929121017456055  Train_KL: 2.8707060515880585  Validation Loss : 60.51135063171387 Val_Reconstruction : 57.67743682861328 Val_KL : 2.833911895751953\n","Epoch: 816/5000  Traning Loss: 61.67080211639404  Train_Reconstruction: 58.80180597305298  Train_KL: 2.8689965307712555  Validation Loss : 60.495201110839844 Val_Reconstruction : 57.655460357666016 Val_KL : 2.8397425413131714\n","Epoch: 817/5000  Traning Loss: 61.83336305618286  Train_Reconstruction: 58.95989274978638  Train_KL: 2.873469829559326  Validation Loss : 60.75560188293457 Val_Reconstruction : 57.91953086853027 Val_KL : 2.8360711336135864\n","Epoch: 818/5000  Traning Loss: 61.93342590332031  Train_Reconstruction: 59.05605983734131  Train_KL: 2.8773660957813263  Validation Loss : 60.77692794799805 Val_Reconstruction : 57.92898941040039 Val_KL : 2.847938299179077\n","Epoch: 819/5000  Traning Loss: 61.96654462814331  Train_Reconstruction: 59.09032487869263  Train_KL: 2.876219391822815  Validation Loss : 60.76852035522461 Val_Reconstruction : 57.92776679992676 Val_KL : 2.8407540321350098\n","Epoch: 820/5000  Traning Loss: 62.48698329925537  Train_Reconstruction: 59.60362386703491  Train_KL: 2.883359372615814  Validation Loss : 61.55646324157715 Val_Reconstruction : 58.71191596984863 Val_KL : 2.8445470333099365\n","Epoch: 821/5000  Traning Loss: 62.66005325317383  Train_Reconstruction: 59.78400707244873  Train_KL: 2.8760460317134857  Validation Loss : 61.52649116516113 Val_Reconstruction : 58.68293380737305 Val_KL : 2.843557596206665\n","Epoch: 822/5000  Traning Loss: 62.355735778808594  Train_Reconstruction: 59.487038135528564  Train_KL: 2.868697941303253  Validation Loss : 60.7254524230957 Val_Reconstruction : 57.88945007324219 Val_KL : 2.8360031843185425\n","Epoch: 823/5000  Traning Loss: 61.91035175323486  Train_Reconstruction: 59.04164123535156  Train_KL: 2.868710309267044  Validation Loss : 60.939842224121094 Val_Reconstruction : 58.10703086853027 Val_KL : 2.832810163497925\n","Epoch: 824/5000  Traning Loss: 62.05518674850464  Train_Reconstruction: 59.17374801635742  Train_KL: 2.8814385533332825  Validation Loss : 61.18008041381836 Val_Reconstruction : 58.32044982910156 Val_KL : 2.8596303462982178\n","Epoch: 825/5000  Traning Loss: 62.03636837005615  Train_Reconstruction: 59.157490730285645  Train_KL: 2.8788774013519287  Validation Loss : 60.852128982543945 Val_Reconstruction : 58.016483306884766 Val_KL : 2.835647225379944\n","Epoch: 826/5000  Traning Loss: 62.19089221954346  Train_Reconstruction: 59.31206130981445  Train_KL: 2.878831058740616  Validation Loss : 61.039676666259766 Val_Reconstruction : 58.180477142333984 Val_KL : 2.8591995239257812\n","Epoch: 827/5000  Traning Loss: 62.05824613571167  Train_Reconstruction: 59.17225217819214  Train_KL: 2.885994017124176  Validation Loss : 60.62190246582031 Val_Reconstruction : 57.77251625061035 Val_KL : 2.8493865728378296\n","Epoch: 828/5000  Traning Loss: 61.82973337173462  Train_Reconstruction: 58.934452056884766  Train_KL: 2.895281106233597  Validation Loss : 60.73193168640137 Val_Reconstruction : 57.87547302246094 Val_KL : 2.856458067893982\n","Epoch: 829/5000  Traning Loss: 62.11886930465698  Train_Reconstruction: 59.23897123336792  Train_KL: 2.879898339509964  Validation Loss : 61.140607833862305 Val_Reconstruction : 58.30108833312988 Val_KL : 2.8395183086395264\n","Epoch: 830/5000  Traning Loss: 62.64716291427612  Train_Reconstruction: 59.77356433868408  Train_KL: 2.8735989928245544  Validation Loss : 61.18516540527344 Val_Reconstruction : 58.33530807495117 Val_KL : 2.849857211112976\n","Epoch: 831/5000  Traning Loss: 62.86667490005493  Train_Reconstruction: 59.98929691314697  Train_KL: 2.877377539873123  Validation Loss : 61.22487258911133 Val_Reconstruction : 58.38955497741699 Val_KL : 2.8353172540664673\n","Epoch: 832/5000  Traning Loss: 62.031320095062256  Train_Reconstruction: 59.157204151153564  Train_KL: 2.8741158843040466  Validation Loss : 60.556325912475586 Val_Reconstruction : 57.70730018615723 Val_KL : 2.849027156829834\n","Epoch: 833/5000  Traning Loss: 61.679383754730225  Train_Reconstruction: 58.797439098358154  Train_KL: 2.881943941116333  Validation Loss : 60.56549644470215 Val_Reconstruction : 57.730838775634766 Val_KL : 2.834656238555908\n","Epoch: 834/5000  Traning Loss: 61.53580045700073  Train_Reconstruction: 58.655200481414795  Train_KL: 2.880599945783615  Validation Loss : 60.23053741455078 Val_Reconstruction : 57.3787899017334 Val_KL : 2.8517487049102783\n","Epoch: 835/5000  Traning Loss: 61.4883508682251  Train_Reconstruction: 58.6023907661438  Train_KL: 2.885960340499878  Validation Loss : 60.423295974731445 Val_Reconstruction : 57.573015213012695 Val_KL : 2.85028076171875\n","Epoch: 836/5000  Traning Loss: 61.60904359817505  Train_Reconstruction: 58.728840351104736  Train_KL: 2.880202978849411  Validation Loss : 60.482675552368164 Val_Reconstruction : 57.65377235412598 Val_KL : 2.8289036750793457\n","Epoch: 837/5000  Traning Loss: 61.65709209442139  Train_Reconstruction: 58.78462314605713  Train_KL: 2.872468888759613  Validation Loss : 60.47178077697754 Val_Reconstruction : 57.61486625671387 Val_KL : 2.8569164276123047\n","Epoch: 838/5000  Traning Loss: 61.78040075302124  Train_Reconstruction: 58.88948965072632  Train_KL: 2.890911251306534  Validation Loss : 60.53332328796387 Val_Reconstruction : 57.682416915893555 Val_KL : 2.850907325744629\n","Epoch: 839/5000  Traning Loss: 61.68609046936035  Train_Reconstruction: 58.809391021728516  Train_KL: 2.876699984073639  Validation Loss : 60.7952766418457 Val_Reconstruction : 57.95727729797363 Val_KL : 2.838000535964966\n","Epoch: 840/5000  Traning Loss: 61.93173694610596  Train_Reconstruction: 59.06330871582031  Train_KL: 2.86842879652977  Validation Loss : 61.0124454498291 Val_Reconstruction : 58.17837333679199 Val_KL : 2.834074020385742\n","Epoch: 841/5000  Traning Loss: 61.79887390136719  Train_Reconstruction: 58.94060468673706  Train_KL: 2.85826912522316  Validation Loss : 60.52484703063965 Val_Reconstruction : 57.694393157958984 Val_KL : 2.830452561378479\n","Epoch: 842/5000  Traning Loss: 61.69691324234009  Train_Reconstruction: 58.823554039001465  Train_KL: 2.873359262943268  Validation Loss : 60.61400604248047 Val_Reconstruction : 57.78931427001953 Val_KL : 2.824693202972412\n","Epoch: 843/5000  Traning Loss: 61.78034067153931  Train_Reconstruction: 58.92334985733032  Train_KL: 2.85699063539505  Validation Loss : 60.54035568237305 Val_Reconstruction : 57.692697525024414 Val_KL : 2.847658634185791\n","Epoch: 844/5000  Traning Loss: 61.722647190093994  Train_Reconstruction: 58.84481763839722  Train_KL: 2.877829134464264  Validation Loss : 60.80676078796387 Val_Reconstruction : 57.985788345336914 Val_KL : 2.820971965789795\n","Epoch: 845/5000  Traning Loss: 61.881959438323975  Train_Reconstruction: 59.0144829750061  Train_KL: 2.8674757182598114  Validation Loss : 60.75997543334961 Val_Reconstruction : 57.91920471191406 Val_KL : 2.8407691717147827\n","Epoch: 846/5000  Traning Loss: 61.98514699935913  Train_Reconstruction: 59.11939334869385  Train_KL: 2.865753322839737  Validation Loss : 60.719587326049805 Val_Reconstruction : 57.89560127258301 Val_KL : 2.8239874839782715\n","Epoch: 847/5000  Traning Loss: 61.61458396911621  Train_Reconstruction: 58.74443244934082  Train_KL: 2.8701518774032593  Validation Loss : 60.257463455200195 Val_Reconstruction : 57.424089431762695 Val_KL : 2.8333736658096313\n","Epoch: 848/5000  Traning Loss: 61.47839975357056  Train_Reconstruction: 58.60700702667236  Train_KL: 2.8713929057121277  Validation Loss : 60.22503089904785 Val_Reconstruction : 57.37243461608887 Val_KL : 2.852594017982483\n","Epoch: 849/5000  Traning Loss: 61.46445178985596  Train_Reconstruction: 58.58683395385742  Train_KL: 2.8776178658008575  Validation Loss : 60.320960998535156 Val_Reconstruction : 57.47928237915039 Val_KL : 2.8416783809661865\n","Epoch: 850/5000  Traning Loss: 61.55808162689209  Train_Reconstruction: 58.66931486129761  Train_KL: 2.888767033815384  Validation Loss : 60.356651306152344 Val_Reconstruction : 57.507102966308594 Val_KL : 2.8495486974716187\n","Epoch: 851/5000  Traning Loss: 62.04287147521973  Train_Reconstruction: 59.16267490386963  Train_KL: 2.8801969289779663  Validation Loss : 61.221181869506836 Val_Reconstruction : 58.3558464050293 Val_KL : 2.8653335571289062\n","Epoch: 852/5000  Traning Loss: 61.94831895828247  Train_Reconstruction: 59.0633602142334  Train_KL: 2.8849581480026245  Validation Loss : 61.064369201660156 Val_Reconstruction : 58.216691970825195 Val_KL : 2.8476758003234863\n","Epoch: 853/5000  Traning Loss: 62.147257804870605  Train_Reconstruction: 59.26595640182495  Train_KL: 2.881301611661911  Validation Loss : 60.900421142578125 Val_Reconstruction : 58.057682037353516 Val_KL : 2.842737078666687\n","Epoch: 854/5000  Traning Loss: 61.67338562011719  Train_Reconstruction: 58.80182361602783  Train_KL: 2.8715622425079346  Validation Loss : 60.41202163696289 Val_Reconstruction : 57.57040023803711 Val_KL : 2.8416213989257812\n","Epoch: 855/5000  Traning Loss: 61.75712490081787  Train_Reconstruction: 58.875619888305664  Train_KL: 2.8815050423145294  Validation Loss : 60.948659896850586 Val_Reconstruction : 58.09095764160156 Val_KL : 2.857703447341919\n","Epoch: 856/5000  Traning Loss: 62.167616844177246  Train_Reconstruction: 59.277838706970215  Train_KL: 2.889778286218643  Validation Loss : 61.004207611083984 Val_Reconstruction : 58.14962196350098 Val_KL : 2.8545867204666138\n","Epoch: 857/5000  Traning Loss: 61.82377624511719  Train_Reconstruction: 58.942662715911865  Train_KL: 2.881113886833191  Validation Loss : 60.40761184692383 Val_Reconstruction : 57.55636405944824 Val_KL : 2.851247191429138\n","Epoch: 858/5000  Traning Loss: 61.80411338806152  Train_Reconstruction: 58.927358627319336  Train_KL: 2.8767552077770233  Validation Loss : 60.40383720397949 Val_Reconstruction : 57.56105995178223 Val_KL : 2.842779517173767\n","Epoch: 859/5000  Traning Loss: 61.823951721191406  Train_Reconstruction: 58.94821548461914  Train_KL: 2.875735729932785  Validation Loss : 60.713138580322266 Val_Reconstruction : 57.87739181518555 Val_KL : 2.8357458114624023\n","Epoch: 860/5000  Traning Loss: 61.94285488128662  Train_Reconstruction: 59.05825328826904  Train_KL: 2.884601980447769  Validation Loss : 60.7035026550293 Val_Reconstruction : 57.847089767456055 Val_KL : 2.856413722038269\n","Epoch: 861/5000  Traning Loss: 61.80660676956177  Train_Reconstruction: 58.92544937133789  Train_KL: 2.8811571896076202  Validation Loss : 60.42039680480957 Val_Reconstruction : 57.577396392822266 Val_KL : 2.8430010080337524\n","Epoch: 862/5000  Traning Loss: 61.90970277786255  Train_Reconstruction: 59.01561260223389  Train_KL: 2.8940902650356293  Validation Loss : 60.60496520996094 Val_Reconstruction : 57.75318908691406 Val_KL : 2.8517757654190063\n","Epoch: 863/5000  Traning Loss: 61.909526348114014  Train_Reconstruction: 59.02333068847656  Train_KL: 2.8861956894397736  Validation Loss : 60.55722427368164 Val_Reconstruction : 57.709306716918945 Val_KL : 2.8479156494140625\n","Epoch: 864/5000  Traning Loss: 62.1692681312561  Train_Reconstruction: 59.28652286529541  Train_KL: 2.882745236158371  Validation Loss : 60.62931823730469 Val_Reconstruction : 57.79532814025879 Val_KL : 2.833988666534424\n","Epoch: 865/5000  Traning Loss: 61.71043872833252  Train_Reconstruction: 58.8334321975708  Train_KL: 2.877006411552429  Validation Loss : 60.37225914001465 Val_Reconstruction : 57.53805923461914 Val_KL : 2.834198832511902\n","Epoch: 866/5000  Traning Loss: 61.50688171386719  Train_Reconstruction: 58.64247369766235  Train_KL: 2.864408493041992  Validation Loss : 60.3735466003418 Val_Reconstruction : 57.53318977355957 Val_KL : 2.840358257293701\n","Epoch: 867/5000  Traning Loss: 61.5815634727478  Train_Reconstruction: 58.70064163208008  Train_KL: 2.8809213638305664  Validation Loss : 60.50177192687988 Val_Reconstruction : 57.65659141540527 Val_KL : 2.845179557800293\n","Epoch: 868/5000  Traning Loss: 61.66380739212036  Train_Reconstruction: 58.79002285003662  Train_KL: 2.8737850189208984  Validation Loss : 60.6916618347168 Val_Reconstruction : 57.84463310241699 Val_KL : 2.8470290899276733\n","Epoch: 869/5000  Traning Loss: 61.876362323760986  Train_Reconstruction: 58.99954319000244  Train_KL: 2.876818984746933  Validation Loss : 60.91166687011719 Val_Reconstruction : 58.076133728027344 Val_KL : 2.835531711578369\n","Epoch: 870/5000  Traning Loss: 61.89551258087158  Train_Reconstruction: 59.010079860687256  Train_KL: 2.88543239235878  Validation Loss : 60.32477378845215 Val_Reconstruction : 57.474388122558594 Val_KL : 2.8503832817077637\n","Epoch: 871/5000  Traning Loss: 61.54756736755371  Train_Reconstruction: 58.655351638793945  Train_KL: 2.892215520143509  Validation Loss : 60.27261924743652 Val_Reconstruction : 57.40452575683594 Val_KL : 2.8680940866470337\n","Epoch: 872/5000  Traning Loss: 61.48090076446533  Train_Reconstruction: 58.586726665496826  Train_KL: 2.8941747546195984  Validation Loss : 60.44390296936035 Val_Reconstruction : 57.597801208496094 Val_KL : 2.8461004495620728\n","Epoch: 873/5000  Traning Loss: 61.462409019470215  Train_Reconstruction: 58.5898756980896  Train_KL: 2.8725337386131287  Validation Loss : 60.29583740234375 Val_Reconstruction : 57.45746040344238 Val_KL : 2.8383768796920776\n","Epoch: 874/5000  Traning Loss: 61.65318775177002  Train_Reconstruction: 58.77153396606445  Train_KL: 2.8816542625427246  Validation Loss : 60.47560119628906 Val_Reconstruction : 57.62189292907715 Val_KL : 2.853706955909729\n","Epoch: 875/5000  Traning Loss: 61.58431529998779  Train_Reconstruction: 58.69747304916382  Train_KL: 2.8868422508239746  Validation Loss : 60.44802474975586 Val_Reconstruction : 57.59016227722168 Val_KL : 2.857862114906311\n","Epoch: 876/5000  Traning Loss: 61.418734073638916  Train_Reconstruction: 58.54614782333374  Train_KL: 2.87258642911911  Validation Loss : 60.37250518798828 Val_Reconstruction : 57.54803657531738 Val_KL : 2.824469566345215\n","Epoch: 877/5000  Traning Loss: 61.559561252593994  Train_Reconstruction: 58.699323654174805  Train_KL: 2.8602369129657745  Validation Loss : 60.477638244628906 Val_Reconstruction : 57.643693923950195 Val_KL : 2.833943724632263\n","Epoch: 878/5000  Traning Loss: 61.761237144470215  Train_Reconstruction: 58.87951612472534  Train_KL: 2.881720691919327  Validation Loss : 60.417524337768555 Val_Reconstruction : 57.572933197021484 Val_KL : 2.8445894718170166\n","Epoch: 879/5000  Traning Loss: 61.535420417785645  Train_Reconstruction: 58.651901721954346  Train_KL: 2.8835185170173645  Validation Loss : 60.36581611633301 Val_Reconstruction : 57.520050048828125 Val_KL : 2.8457638025283813\n","Epoch: 880/5000  Traning Loss: 61.522032737731934  Train_Reconstruction: 58.633020877838135  Train_KL: 2.8890117704868317  Validation Loss : 60.47046661376953 Val_Reconstruction : 57.62565612792969 Val_KL : 2.844809412956238\n","Epoch: 881/5000  Traning Loss: 61.724122047424316  Train_Reconstruction: 58.84564256668091  Train_KL: 2.8784801363945007  Validation Loss : 60.51511192321777 Val_Reconstruction : 57.665321350097656 Val_KL : 2.84978985786438\n","Epoch: 882/5000  Traning Loss: 61.626768589019775  Train_Reconstruction: 58.76002359390259  Train_KL: 2.866744965314865  Validation Loss : 60.45231056213379 Val_Reconstruction : 57.611623764038086 Val_KL : 2.8406851291656494\n","Epoch: 883/5000  Traning Loss: 61.63178873062134  Train_Reconstruction: 58.74655485153198  Train_KL: 2.885234445333481  Validation Loss : 60.41943359375 Val_Reconstruction : 57.56850242614746 Val_KL : 2.8509339094161987\n","Epoch: 884/5000  Traning Loss: 61.416457653045654  Train_Reconstruction: 58.53045988082886  Train_KL: 2.8859982192516327  Validation Loss : 60.23099708557129 Val_Reconstruction : 57.37935829162598 Val_KL : 2.8516401052474976\n","Epoch: 885/5000  Traning Loss: 61.42364740371704  Train_Reconstruction: 58.53914022445679  Train_KL: 2.884507268667221  Validation Loss : 60.33817100524902 Val_Reconstruction : 57.50615882873535 Val_KL : 2.8320107460021973\n","Epoch: 886/5000  Traning Loss: 61.779820919036865  Train_Reconstruction: 58.90586805343628  Train_KL: 2.873953491449356  Validation Loss : 60.894338607788086 Val_Reconstruction : 58.031667709350586 Val_KL : 2.8626707792282104\n","Epoch: 887/5000  Traning Loss: 61.83080291748047  Train_Reconstruction: 58.94217109680176  Train_KL: 2.8886326253414154  Validation Loss : 61.26122856140137 Val_Reconstruction : 58.424489974975586 Val_KL : 2.8367393016815186\n","Epoch: 888/5000  Traning Loss: 61.90307140350342  Train_Reconstruction: 59.04076433181763  Train_KL: 2.8623067438602448  Validation Loss : 60.79119300842285 Val_Reconstruction : 57.96761703491211 Val_KL : 2.8235775232315063\n","Epoch: 889/5000  Traning Loss: 61.457892417907715  Train_Reconstruction: 58.58746147155762  Train_KL: 2.870431214570999  Validation Loss : 60.136600494384766 Val_Reconstruction : 57.29379653930664 Val_KL : 2.8428043127059937\n","Epoch: 890/5000  Traning Loss: 61.31139659881592  Train_Reconstruction: 58.4312424659729  Train_KL: 2.8801541924476624  Validation Loss : 60.20353126525879 Val_Reconstruction : 57.35070991516113 Val_KL : 2.8528202772140503\n","Epoch: 891/5000  Traning Loss: 61.57702827453613  Train_Reconstruction: 58.70872974395752  Train_KL: 2.8682977855205536  Validation Loss : 60.556766510009766 Val_Reconstruction : 57.71749305725098 Val_KL : 2.8392730951309204\n","Epoch: 892/5000  Traning Loss: 62.02735137939453  Train_Reconstruction: 59.14092302322388  Train_KL: 2.8864279091358185  Validation Loss : 61.07777976989746 Val_Reconstruction : 58.2211971282959 Val_KL : 2.8565837144851685\n","Epoch: 893/5000  Traning Loss: 61.70054388046265  Train_Reconstruction: 58.80455303192139  Train_KL: 2.8959908485412598  Validation Loss : 60.36805534362793 Val_Reconstruction : 57.5059700012207 Val_KL : 2.8620840311050415\n","Epoch: 894/5000  Traning Loss: 61.471923828125  Train_Reconstruction: 58.59023189544678  Train_KL: 2.881691575050354  Validation Loss : 60.31534194946289 Val_Reconstruction : 57.48541069030762 Val_KL : 2.8299293518066406\n","Epoch: 895/5000  Traning Loss: 61.30116319656372  Train_Reconstruction: 58.425811767578125  Train_KL: 2.8753513395786285  Validation Loss : 60.11987113952637 Val_Reconstruction : 57.284719467163086 Val_KL : 2.83515202999115\n","Epoch: 896/5000  Traning Loss: 61.256914138793945  Train_Reconstruction: 58.37923574447632  Train_KL: 2.877678483724594  Validation Loss : 59.98898887634277 Val_Reconstruction : 57.140480041503906 Val_KL : 2.8485087156295776\n","Epoch: 897/5000  Traning Loss: 61.265685081481934  Train_Reconstruction: 58.37888431549072  Train_KL: 2.8868008255958557  Validation Loss : 60.37766647338867 Val_Reconstruction : 57.522207260131836 Val_KL : 2.8554601669311523\n","Epoch: 898/5000  Traning Loss: 61.4307074546814  Train_Reconstruction: 58.54553985595703  Train_KL: 2.885167956352234  Validation Loss : 60.22426795959473 Val_Reconstruction : 57.364187240600586 Val_KL : 2.8600798845291138\n","Epoch: 899/5000  Traning Loss: 61.39656114578247  Train_Reconstruction: 58.50404071807861  Train_KL: 2.8925209641456604  Validation Loss : 60.38734436035156 Val_Reconstruction : 57.527099609375 Val_KL : 2.860243797302246\n","Epoch: 900/5000  Traning Loss: 61.583176612854004  Train_Reconstruction: 58.685245513916016  Train_KL: 2.897930920124054  Validation Loss : 60.65277671813965 Val_Reconstruction : 57.78485298156738 Val_KL : 2.867922782897949\n","Epoch: 901/5000  Traning Loss: 61.32709264755249  Train_Reconstruction: 58.43867015838623  Train_KL: 2.888421982526779  Validation Loss : 60.2044677734375 Val_Reconstruction : 57.37193489074707 Val_KL : 2.8325302600860596\n","Epoch: 902/5000  Traning Loss: 61.32218408584595  Train_Reconstruction: 58.45073843002319  Train_KL: 2.8714459240436554  Validation Loss : 60.35857963562012 Val_Reconstruction : 57.52289581298828 Val_KL : 2.8356815576553345\n","Epoch: 903/5000  Traning Loss: 61.56150674819946  Train_Reconstruction: 58.68794393539429  Train_KL: 2.8735627830028534  Validation Loss : 60.92411422729492 Val_Reconstruction : 58.07849311828613 Val_KL : 2.845621109008789\n","Epoch: 904/5000  Traning Loss: 61.736674785614014  Train_Reconstruction: 58.85346746444702  Train_KL: 2.883207857608795  Validation Loss : 60.77656936645508 Val_Reconstruction : 57.93037986755371 Val_KL : 2.846189498901367\n","Epoch: 905/5000  Traning Loss: 61.501792430877686  Train_Reconstruction: 58.626136302948  Train_KL: 2.8756556510925293  Validation Loss : 60.29989433288574 Val_Reconstruction : 57.4490909576416 Val_KL : 2.8508052825927734\n","Epoch: 906/5000  Traning Loss: 61.46666479110718  Train_Reconstruction: 58.58246994018555  Train_KL: 2.8841947317123413  Validation Loss : 60.396968841552734 Val_Reconstruction : 57.55388832092285 Val_KL : 2.843082308769226\n","Epoch: 907/5000  Traning Loss: 61.34055948257446  Train_Reconstruction: 58.46535634994507  Train_KL: 2.8752034306526184  Validation Loss : 60.35926818847656 Val_Reconstruction : 57.533485412597656 Val_KL : 2.8257845640182495\n","Epoch: 908/5000  Traning Loss: 61.38727521896362  Train_Reconstruction: 58.51013422012329  Train_KL: 2.877140134572983  Validation Loss : 60.522016525268555 Val_Reconstruction : 57.680606842041016 Val_KL : 2.841408610343933\n","Epoch: 909/5000  Traning Loss: 61.65853786468506  Train_Reconstruction: 58.770495891571045  Train_KL: 2.8880423605442047  Validation Loss : 60.62810516357422 Val_Reconstruction : 57.781097412109375 Val_KL : 2.847007393836975\n","Epoch: 910/5000  Traning Loss: 61.61234426498413  Train_Reconstruction: 58.72317123413086  Train_KL: 2.8891727328300476  Validation Loss : 60.11924171447754 Val_Reconstruction : 57.26882553100586 Val_KL : 2.85041606426239\n","Epoch: 911/5000  Traning Loss: 61.54203224182129  Train_Reconstruction: 58.660953521728516  Train_KL: 2.881078064441681  Validation Loss : 60.50883483886719 Val_Reconstruction : 57.66407775878906 Val_KL : 2.8447558879852295\n","Epoch: 912/5000  Traning Loss: 61.267826557159424  Train_Reconstruction: 58.38951015472412  Train_KL: 2.878316253423691  Validation Loss : 60.17919158935547 Val_Reconstruction : 57.32525825500488 Val_KL : 2.853933095932007\n","Epoch: 913/5000  Traning Loss: 61.40145206451416  Train_Reconstruction: 58.51781940460205  Train_KL: 2.883632779121399  Validation Loss : 60.65836715698242 Val_Reconstruction : 57.82017707824707 Val_KL : 2.8381881713867188\n","Epoch: 914/5000  Traning Loss: 61.64146423339844  Train_Reconstruction: 58.7677903175354  Train_KL: 2.873674154281616  Validation Loss : 60.38179588317871 Val_Reconstruction : 57.52257537841797 Val_KL : 2.859220504760742\n","Epoch: 915/5000  Traning Loss: 61.56587743759155  Train_Reconstruction: 58.683974742889404  Train_KL: 2.8819026947021484  Validation Loss : 60.592267990112305 Val_Reconstruction : 57.74491500854492 Val_KL : 2.847350835800171\n","Epoch: 916/5000  Traning Loss: 61.54554080963135  Train_Reconstruction: 58.661441802978516  Train_KL: 2.8840991258621216  Validation Loss : 60.349117279052734 Val_Reconstruction : 57.486833572387695 Val_KL : 2.8622840642929077\n","Epoch: 917/5000  Traning Loss: 61.4678316116333  Train_Reconstruction: 58.571659564971924  Train_KL: 2.8961722254753113  Validation Loss : 60.238651275634766 Val_Reconstruction : 57.38528633117676 Val_KL : 2.853363871574402\n","Epoch: 918/5000  Traning Loss: 61.31769609451294  Train_Reconstruction: 58.43752098083496  Train_KL: 2.8801754117012024  Validation Loss : 60.35564422607422 Val_Reconstruction : 57.5095272064209 Val_KL : 2.8461191654205322\n","Epoch: 919/5000  Traning Loss: 61.43059825897217  Train_Reconstruction: 58.555323123931885  Train_KL: 2.875276029109955  Validation Loss : 60.25657272338867 Val_Reconstruction : 57.41349220275879 Val_KL : 2.843079090118408\n","Epoch: 920/5000  Traning Loss: 61.50826835632324  Train_Reconstruction: 58.63192701339722  Train_KL: 2.876341313123703  Validation Loss : 60.406211853027344 Val_Reconstruction : 57.5634765625 Val_KL : 2.842736005783081\n","Epoch: 921/5000  Traning Loss: 61.474087715148926  Train_Reconstruction: 58.59232425689697  Train_KL: 2.8817643523216248  Validation Loss : 60.329917907714844 Val_Reconstruction : 57.47983741760254 Val_KL : 2.850080728530884\n","Epoch: 922/5000  Traning Loss: 61.70827913284302  Train_Reconstruction: 58.8359580039978  Train_KL: 2.872320532798767  Validation Loss : 60.352163314819336 Val_Reconstruction : 57.5165958404541 Val_KL : 2.8355679512023926\n","Epoch: 923/5000  Traning Loss: 61.891008377075195  Train_Reconstruction: 59.01079845428467  Train_KL: 2.8802096247673035  Validation Loss : 60.38396453857422 Val_Reconstruction : 57.52650451660156 Val_KL : 2.857460379600525\n","Epoch: 924/5000  Traning Loss: 61.593228340148926  Train_Reconstruction: 58.69883918762207  Train_KL: 2.89438933134079  Validation Loss : 60.428213119506836 Val_Reconstruction : 57.56918144226074 Val_KL : 2.8590327501296997\n","Epoch: 925/5000  Traning Loss: 61.44083213806152  Train_Reconstruction: 58.556066036224365  Train_KL: 2.884766101837158  Validation Loss : 60.43426513671875 Val_Reconstruction : 57.58158874511719 Val_KL : 2.8526777029037476\n","Epoch: 926/5000  Traning Loss: 61.46400213241577  Train_Reconstruction: 58.575984954833984  Train_KL: 2.888017386198044  Validation Loss : 60.48037910461426 Val_Reconstruction : 57.62743949890137 Val_KL : 2.852941155433655\n","Epoch: 927/5000  Traning Loss: 61.62800884246826  Train_Reconstruction: 58.72940254211426  Train_KL: 2.898606061935425  Validation Loss : 60.336421966552734 Val_Reconstruction : 57.480573654174805 Val_KL : 2.855845808982849\n","Epoch: 928/5000  Traning Loss: 61.54065227508545  Train_Reconstruction: 58.65101337432861  Train_KL: 2.889638513326645  Validation Loss : 60.606895446777344 Val_Reconstruction : 57.76064109802246 Val_KL : 2.8462538719177246\n","Epoch: 929/5000  Traning Loss: 61.73626136779785  Train_Reconstruction: 58.86261224746704  Train_KL: 2.8736487925052643  Validation Loss : 60.7037296295166 Val_Reconstruction : 57.86287498474121 Val_KL : 2.8408565521240234\n","Epoch: 930/5000  Traning Loss: 61.53489923477173  Train_Reconstruction: 58.6461386680603  Train_KL: 2.8887608647346497  Validation Loss : 60.39727973937988 Val_Reconstruction : 57.54729461669922 Val_KL : 2.8499852418899536\n","Epoch: 931/5000  Traning Loss: 61.441561222076416  Train_Reconstruction: 58.56652593612671  Train_KL: 2.8750350177288055  Validation Loss : 60.119951248168945 Val_Reconstruction : 57.27061462402344 Val_KL : 2.8493354320526123\n","Epoch: 932/5000  Traning Loss: 61.289785861968994  Train_Reconstruction: 58.40651226043701  Train_KL: 2.8832736909389496  Validation Loss : 60.05930137634277 Val_Reconstruction : 57.21277046203613 Val_KL : 2.846529006958008\n","Epoch: 933/5000  Traning Loss: 61.12665319442749  Train_Reconstruction: 58.24962520599365  Train_KL: 2.8770284950733185  Validation Loss : 60.04866409301758 Val_Reconstruction : 57.18987464904785 Val_KL : 2.858788847923279\n","Epoch: 934/5000  Traning Loss: 61.26859664916992  Train_Reconstruction: 58.381755352020264  Train_KL: 2.886840671300888  Validation Loss : 59.91369819641113 Val_Reconstruction : 57.060720443725586 Val_KL : 2.852978825569153\n","Epoch: 935/5000  Traning Loss: 60.95623826980591  Train_Reconstruction: 58.06545543670654  Train_KL: 2.8907826840877533  Validation Loss : 59.8352165222168 Val_Reconstruction : 56.97072410583496 Val_KL : 2.8644917011260986\n","Epoch: 936/5000  Traning Loss: 60.9644136428833  Train_Reconstruction: 58.06852960586548  Train_KL: 2.895884096622467  Validation Loss : 59.7552490234375 Val_Reconstruction : 56.90603256225586 Val_KL : 2.84921658039093\n","Epoch: 937/5000  Traning Loss: 61.05607557296753  Train_Reconstruction: 58.192824840545654  Train_KL: 2.863250643014908  Validation Loss : 60.0920467376709 Val_Reconstruction : 57.27557182312012 Val_KL : 2.816475749015808\n","Epoch: 938/5000  Traning Loss: 61.33671236038208  Train_Reconstruction: 58.46848011016846  Train_KL: 2.8682318925857544  Validation Loss : 60.3934326171875 Val_Reconstruction : 57.551218032836914 Val_KL : 2.8422127962112427\n","Epoch: 939/5000  Traning Loss: 61.42205762863159  Train_Reconstruction: 58.53112506866455  Train_KL: 2.8909329175949097  Validation Loss : 60.09834098815918 Val_Reconstruction : 57.241994857788086 Val_KL : 2.8563462495803833\n","Epoch: 940/5000  Traning Loss: 61.19894218444824  Train_Reconstruction: 58.31951713562012  Train_KL: 2.879424422979355  Validation Loss : 60.02813911437988 Val_Reconstruction : 57.17424774169922 Val_KL : 2.8538910150527954\n","Epoch: 941/5000  Traning Loss: 61.11739253997803  Train_Reconstruction: 58.225215435028076  Train_KL: 2.8921772837638855  Validation Loss : 60.18929672241211 Val_Reconstruction : 57.34556579589844 Val_KL : 2.8437329530715942\n","Epoch: 942/5000  Traning Loss: 61.320680141448975  Train_Reconstruction: 58.448853492736816  Train_KL: 2.8718269169330597  Validation Loss : 60.02885627746582 Val_Reconstruction : 57.18882751464844 Val_KL : 2.840028762817383\n","Epoch: 943/5000  Traning Loss: 61.18419170379639  Train_Reconstruction: 58.313448429107666  Train_KL: 2.870743453502655  Validation Loss : 60.14213943481445 Val_Reconstruction : 57.3147087097168 Val_KL : 2.827431559562683\n","Epoch: 944/5000  Traning Loss: 61.40192651748657  Train_Reconstruction: 58.52396202087402  Train_KL: 2.8779637217521667  Validation Loss : 60.58712387084961 Val_Reconstruction : 57.72371482849121 Val_KL : 2.8634109497070312\n","Epoch: 945/5000  Traning Loss: 61.490304946899414  Train_Reconstruction: 58.61853790283203  Train_KL: 2.871766746044159  Validation Loss : 60.41587257385254 Val_Reconstruction : 57.58790397644043 Val_KL : 2.827967882156372\n","Epoch: 946/5000  Traning Loss: 61.595765590667725  Train_Reconstruction: 58.71240997314453  Train_KL: 2.883355438709259  Validation Loss : 60.28288650512695 Val_Reconstruction : 57.42658996582031 Val_KL : 2.856299042701721\n","Epoch: 947/5000  Traning Loss: 61.644628047943115  Train_Reconstruction: 58.7673864364624  Train_KL: 2.8772419095039368  Validation Loss : 60.25556564331055 Val_Reconstruction : 57.399967193603516 Val_KL : 2.855596899986267\n","Epoch: 948/5000  Traning Loss: 61.334303855895996  Train_Reconstruction: 58.45050239562988  Train_KL: 2.8838011026382446  Validation Loss : 60.27530288696289 Val_Reconstruction : 57.42878341674805 Val_KL : 2.8465218544006348\n","Epoch: 949/5000  Traning Loss: 61.29624843597412  Train_Reconstruction: 58.4250226020813  Train_KL: 2.871226191520691  Validation Loss : 60.06027412414551 Val_Reconstruction : 57.214263916015625 Val_KL : 2.846009612083435\n","Epoch: 950/5000  Traning Loss: 61.13560247421265  Train_Reconstruction: 58.24518346786499  Train_KL: 2.890418291091919  Validation Loss : 60.119293212890625 Val_Reconstruction : 57.26279640197754 Val_KL : 2.856497645378113\n","Epoch: 951/5000  Traning Loss: 61.064870834350586  Train_Reconstruction: 58.18247604370117  Train_KL: 2.8823949694633484  Validation Loss : 59.77569389343262 Val_Reconstruction : 56.92924880981445 Val_KL : 2.8464454412460327\n","Epoch: 952/5000  Traning Loss: 61.10627317428589  Train_Reconstruction: 58.217416763305664  Train_KL: 2.8888564109802246  Validation Loss : 60.23069190979004 Val_Reconstruction : 57.364328384399414 Val_KL : 2.8663634061813354\n","Epoch: 953/5000  Traning Loss: 61.143953800201416  Train_Reconstruction: 58.24634647369385  Train_KL: 2.8976071774959564  Validation Loss : 60.17004585266113 Val_Reconstruction : 57.31588363647461 Val_KL : 2.8541635274887085\n","Epoch: 954/5000  Traning Loss: 61.02417993545532  Train_Reconstruction: 58.130072593688965  Train_KL: 2.8941070437431335  Validation Loss : 59.77215003967285 Val_Reconstruction : 56.91551971435547 Val_KL : 2.8566293716430664\n","Epoch: 955/5000  Traning Loss: 60.99387836456299  Train_Reconstruction: 58.11798906326294  Train_KL: 2.8758886754512787  Validation Loss : 59.79461860656738 Val_Reconstruction : 56.94868850708008 Val_KL : 2.8459300994873047\n","Epoch: 956/5000  Traning Loss: 61.129971504211426  Train_Reconstruction: 58.25353717803955  Train_KL: 2.8764339983463287  Validation Loss : 60.00704383850098 Val_Reconstruction : 57.15227127075195 Val_KL : 2.8547749519348145\n","Epoch: 957/5000  Traning Loss: 61.232656478881836  Train_Reconstruction: 58.3352632522583  Train_KL: 2.897393077611923  Validation Loss : 60.18039321899414 Val_Reconstruction : 57.311588287353516 Val_KL : 2.868805408477783\n","Epoch: 958/5000  Traning Loss: 61.27836608886719  Train_Reconstruction: 58.396236419677734  Train_KL: 2.882129728794098  Validation Loss : 60.14411735534668 Val_Reconstruction : 57.29073143005371 Val_KL : 2.8533846139907837\n","Epoch: 959/5000  Traning Loss: 61.261184215545654  Train_Reconstruction: 58.373018741607666  Train_KL: 2.8881650865077972  Validation Loss : 59.921142578125 Val_Reconstruction : 57.06180763244629 Val_KL : 2.859334707260132\n","Epoch: 960/5000  Traning Loss: 61.03954458236694  Train_Reconstruction: 58.158270835876465  Train_KL: 2.8812734484672546  Validation Loss : 59.966339111328125 Val_Reconstruction : 57.13045692443848 Val_KL : 2.8358817100524902\n","Epoch: 961/5000  Traning Loss: 61.026795387268066  Train_Reconstruction: 58.13453435897827  Train_KL: 2.8922607600688934  Validation Loss : 59.843971252441406 Val_Reconstruction : 56.97428512573242 Val_KL : 2.8696861267089844\n","Epoch: 962/5000  Traning Loss: 60.84640550613403  Train_Reconstruction: 57.94772720336914  Train_KL: 2.89867827296257  Validation Loss : 59.68093681335449 Val_Reconstruction : 56.82913017272949 Val_KL : 2.851807117462158\n","Epoch: 963/5000  Traning Loss: 60.95794200897217  Train_Reconstruction: 58.076430797576904  Train_KL: 2.8815105259418488  Validation Loss : 59.8186149597168 Val_Reconstruction : 56.95627403259277 Val_KL : 2.86233913898468\n","Epoch: 964/5000  Traning Loss: 61.03008270263672  Train_Reconstruction: 58.14885425567627  Train_KL: 2.881229132413864  Validation Loss : 60.352060317993164 Val_Reconstruction : 57.515451431274414 Val_KL : 2.8366082906723022\n","Epoch: 965/5000  Traning Loss: 61.036099433898926  Train_Reconstruction: 58.14802408218384  Train_KL: 2.8880750238895416  Validation Loss : 60.31930923461914 Val_Reconstruction : 57.45115661621094 Val_KL : 2.8681509494781494\n","Epoch: 966/5000  Traning Loss: 61.4902081489563  Train_Reconstruction: 58.604069232940674  Train_KL: 2.8861387968063354  Validation Loss : 60.68926811218262 Val_Reconstruction : 57.848052978515625 Val_KL : 2.8412152528762817\n","Epoch: 967/5000  Traning Loss: 61.40586996078491  Train_Reconstruction: 58.52552270889282  Train_KL: 2.880346953868866  Validation Loss : 60.346092224121094 Val_Reconstruction : 57.49327278137207 Val_KL : 2.8528192043304443\n","Epoch: 968/5000  Traning Loss: 61.03356409072876  Train_Reconstruction: 58.15716886520386  Train_KL: 2.8763946294784546  Validation Loss : 60.0629997253418 Val_Reconstruction : 57.21415328979492 Val_KL : 2.8488444089889526\n","Epoch: 969/5000  Traning Loss: 60.987051010131836  Train_Reconstruction: 58.091055393218994  Train_KL: 2.8959959745407104  Validation Loss : 59.83871078491211 Val_Reconstruction : 56.983699798583984 Val_KL : 2.8550111055374146\n","Epoch: 970/5000  Traning Loss: 61.588908672332764  Train_Reconstruction: 58.70393657684326  Train_KL: 2.884972631931305  Validation Loss : 60.10518836975098 Val_Reconstruction : 57.24889373779297 Val_KL : 2.856294631958008\n","Epoch: 971/5000  Traning Loss: 61.209439277648926  Train_Reconstruction: 58.32617521286011  Train_KL: 2.883263796567917  Validation Loss : 60.428924560546875 Val_Reconstruction : 57.58925437927246 Val_KL : 2.8396694660186768\n","Epoch: 972/5000  Traning Loss: 61.25813102722168  Train_Reconstruction: 58.36379098892212  Train_KL: 2.8943401277065277  Validation Loss : 60.31413459777832 Val_Reconstruction : 57.45358657836914 Val_KL : 2.86054790019989\n","Epoch: 973/5000  Traning Loss: 61.14761829376221  Train_Reconstruction: 58.246164321899414  Train_KL: 2.9014540016651154  Validation Loss : 60.01347732543945 Val_Reconstruction : 57.15151786804199 Val_KL : 2.861960768699646\n","Epoch: 974/5000  Traning Loss: 61.236695766448975  Train_Reconstruction: 58.360679149627686  Train_KL: 2.876016527414322  Validation Loss : 60.201194763183594 Val_Reconstruction : 57.361080169677734 Val_KL : 2.8401135206222534\n","Epoch: 975/5000  Traning Loss: 61.31896686553955  Train_Reconstruction: 58.42548418045044  Train_KL: 2.893482506275177  Validation Loss : 60.20242118835449 Val_Reconstruction : 57.333553314208984 Val_KL : 2.868868589401245\n","Epoch: 976/5000  Traning Loss: 61.571053981781006  Train_Reconstruction: 58.67161178588867  Train_KL: 2.8994424641132355  Validation Loss : 60.066476821899414 Val_Reconstruction : 57.190481185913086 Val_KL : 2.8759936094284058\n","Epoch: 977/5000  Traning Loss: 61.15302324295044  Train_Reconstruction: 58.25940465927124  Train_KL: 2.8936188519001007  Validation Loss : 60.06035232543945 Val_Reconstruction : 57.207284927368164 Val_KL : 2.8530678749084473\n","Epoch: 978/5000  Traning Loss: 61.0887713432312  Train_Reconstruction: 58.207719802856445  Train_KL: 2.8810515999794006  Validation Loss : 60.082454681396484 Val_Reconstruction : 57.2263240814209 Val_KL : 2.8561294078826904\n","Epoch: 979/5000  Traning Loss: 60.95648956298828  Train_Reconstruction: 58.073286056518555  Train_KL: 2.883202940225601  Validation Loss : 59.85537338256836 Val_Reconstruction : 56.99326133728027 Val_KL : 2.8621121644973755\n","Epoch: 980/5000  Traning Loss: 60.95142364501953  Train_Reconstruction: 58.063090801239014  Train_KL: 2.8883326947689056  Validation Loss : 59.83675003051758 Val_Reconstruction : 56.97869873046875 Val_KL : 2.8580511808395386\n","Epoch: 981/5000  Traning Loss: 61.06225395202637  Train_Reconstruction: 58.186806201934814  Train_KL: 2.8754481077194214  Validation Loss : 60.13370895385742 Val_Reconstruction : 57.2984619140625 Val_KL : 2.83524751663208\n","Epoch: 982/5000  Traning Loss: 61.24921798706055  Train_Reconstruction: 58.37752389907837  Train_KL: 2.8716940581798553  Validation Loss : 60.19841957092285 Val_Reconstruction : 57.356679916381836 Val_KL : 2.8417413234710693\n","Epoch: 983/5000  Traning Loss: 61.2558159828186  Train_Reconstruction: 58.37450838088989  Train_KL: 2.8813081681728363  Validation Loss : 60.27939987182617 Val_Reconstruction : 57.41232109069824 Val_KL : 2.867076873779297\n","Epoch: 984/5000  Traning Loss: 61.54051494598389  Train_Reconstruction: 58.651416301727295  Train_KL: 2.889098644256592  Validation Loss : 60.42553520202637 Val_Reconstruction : 57.56433296203613 Val_KL : 2.8612024784088135\n","Epoch: 985/5000  Traning Loss: 61.4586968421936  Train_Reconstruction: 58.568814754486084  Train_KL: 2.8898820281028748  Validation Loss : 59.90074348449707 Val_Reconstruction : 57.044960021972656 Val_KL : 2.855783462524414\n","Epoch: 986/5000  Traning Loss: 61.13971948623657  Train_Reconstruction: 58.25467014312744  Train_KL: 2.885049194097519  Validation Loss : 59.93877983093262 Val_Reconstruction : 57.09726524353027 Val_KL : 2.8415130376815796\n","Epoch: 987/5000  Traning Loss: 60.985193729400635  Train_Reconstruction: 58.08928394317627  Train_KL: 2.895910382270813  Validation Loss : 59.67878341674805 Val_Reconstruction : 56.8083438873291 Val_KL : 2.8704391717910767\n","Epoch: 988/5000  Traning Loss: 61.131235122680664  Train_Reconstruction: 58.244266986846924  Train_KL: 2.8869685530662537  Validation Loss : 60.185848236083984 Val_Reconstruction : 57.33460998535156 Val_KL : 2.8512368202209473\n","Epoch: 989/5000  Traning Loss: 61.19531297683716  Train_Reconstruction: 58.311747550964355  Train_KL: 2.883565753698349  Validation Loss : 60.21235466003418 Val_Reconstruction : 57.36746406555176 Val_KL : 2.8448915481567383\n","Epoch: 990/5000  Traning Loss: 61.3494439125061  Train_Reconstruction: 58.47681427001953  Train_KL: 2.8726307153701782  Validation Loss : 60.155046463012695 Val_Reconstruction : 57.31274604797363 Val_KL : 2.8422999382019043\n","Epoch: 991/5000  Traning Loss: 61.47352457046509  Train_Reconstruction: 58.60402202606201  Train_KL: 2.869502514600754  Validation Loss : 60.191993713378906 Val_Reconstruction : 57.352874755859375 Val_KL : 2.8391185998916626\n","Epoch: 992/5000  Traning Loss: 61.19127893447876  Train_Reconstruction: 58.30226993560791  Train_KL: 2.889008969068527  Validation Loss : 59.90181922912598 Val_Reconstruction : 57.0412654876709 Val_KL : 2.8605529069900513\n","Epoch: 993/5000  Traning Loss: 61.0051965713501  Train_Reconstruction: 58.11735248565674  Train_KL: 2.8878436982631683  Validation Loss : 59.991472244262695 Val_Reconstruction : 57.12325859069824 Val_KL : 2.8682141304016113\n","Epoch: 994/5000  Traning Loss: 61.13633584976196  Train_Reconstruction: 58.23644399642944  Train_KL: 2.899891495704651  Validation Loss : 59.9675350189209 Val_Reconstruction : 57.10157775878906 Val_KL : 2.8659573793411255\n","Epoch: 995/5000  Traning Loss: 61.166000843048096  Train_Reconstruction: 58.276893615722656  Train_KL: 2.8891075551509857  Validation Loss : 60.03266525268555 Val_Reconstruction : 57.18714141845703 Val_KL : 2.8455246686935425\n","Epoch: 996/5000  Traning Loss: 61.15029430389404  Train_Reconstruction: 58.26108741760254  Train_KL: 2.889207661151886  Validation Loss : 60.37208557128906 Val_Reconstruction : 57.518226623535156 Val_KL : 2.853858232498169\n","Epoch: 997/5000  Traning Loss: 61.36611795425415  Train_Reconstruction: 58.487736225128174  Train_KL: 2.878381907939911  Validation Loss : 60.05035591125488 Val_Reconstruction : 57.21320343017578 Val_KL : 2.837151885032654\n","Epoch: 998/5000  Traning Loss: 61.065441608428955  Train_Reconstruction: 58.18257141113281  Train_KL: 2.882870227098465  Validation Loss : 59.90372276306152 Val_Reconstruction : 57.04331970214844 Val_KL : 2.8604036569595337\n","Epoch: 999/5000  Traning Loss: 61.07150745391846  Train_Reconstruction: 58.181135177612305  Train_KL: 2.8903728723526  Validation Loss : 60.28879737854004 Val_Reconstruction : 57.43376922607422 Val_KL : 2.8550286293029785\n","Epoch: 1000/5000  Traning Loss: 61.56056070327759  Train_Reconstruction: 58.67854976654053  Train_KL: 2.882010906934738  Validation Loss : 60.10656929016113 Val_Reconstruction : 57.252323150634766 Val_KL : 2.854247212409973\n","Epoch: 1001/5000  Traning Loss: 61.89720869064331  Train_Reconstruction: 59.01330614089966  Train_KL: 2.8839031159877777  Validation Loss : 61.336708068847656 Val_Reconstruction : 58.47051811218262 Val_KL : 2.8661919832229614\n","Epoch: 1002/5000  Traning Loss: 61.7821888923645  Train_Reconstruction: 58.897961139678955  Train_KL: 2.8842268586158752  Validation Loss : 60.72166442871094 Val_Reconstruction : 57.87844276428223 Val_KL : 2.8432213068008423\n","Epoch: 1003/5000  Traning Loss: 61.58474588394165  Train_Reconstruction: 58.70193958282471  Train_KL: 2.8828056156635284  Validation Loss : 60.7682991027832 Val_Reconstruction : 57.90575981140137 Val_KL : 2.8625370264053345\n","Epoch: 1004/5000  Traning Loss: 61.32361316680908  Train_Reconstruction: 58.434518337249756  Train_KL: 2.8890950083732605  Validation Loss : 59.94850730895996 Val_Reconstruction : 57.08917045593262 Val_KL : 2.859337329864502\n","Epoch: 1005/5000  Traning Loss: 61.07878398895264  Train_Reconstruction: 58.185303688049316  Train_KL: 2.8934804797172546  Validation Loss : 60.0041389465332 Val_Reconstruction : 57.14886283874512 Val_KL : 2.855276346206665\n","Epoch: 1006/5000  Traning Loss: 60.86695671081543  Train_Reconstruction: 57.98388481140137  Train_KL: 2.8830715715885162  Validation Loss : 59.86868858337402 Val_Reconstruction : 57.037906646728516 Val_KL : 2.8307814598083496\n","Epoch: 1007/5000  Traning Loss: 60.845359802246094  Train_Reconstruction: 57.98271131515503  Train_KL: 2.8626491725444794  Validation Loss : 59.65392303466797 Val_Reconstruction : 56.806358337402344 Val_KL : 2.8475661277770996\n","Epoch: 1008/5000  Traning Loss: 60.85109043121338  Train_Reconstruction: 57.96539068222046  Train_KL: 2.885699510574341  Validation Loss : 59.846078872680664 Val_Reconstruction : 56.99687194824219 Val_KL : 2.849207282066345\n","Epoch: 1009/5000  Traning Loss: 60.81252670288086  Train_Reconstruction: 57.939712047576904  Train_KL: 2.8728140890598297  Validation Loss : 59.983848571777344 Val_Reconstruction : 57.13590621948242 Val_KL : 2.847941279411316\n","Epoch: 1010/5000  Traning Loss: 60.7872748374939  Train_Reconstruction: 57.896681785583496  Train_KL: 2.8905925154685974  Validation Loss : 59.68643379211426 Val_Reconstruction : 56.835079193115234 Val_KL : 2.8513548374176025\n","Epoch: 1011/5000  Traning Loss: 60.87342977523804  Train_Reconstruction: 57.97576856613159  Train_KL: 2.8976611495018005  Validation Loss : 59.80051612854004 Val_Reconstruction : 56.93121337890625 Val_KL : 2.869302749633789\n","Epoch: 1012/5000  Traning Loss: 60.90889263153076  Train_Reconstruction: 58.02094221115112  Train_KL: 2.88795068860054  Validation Loss : 60.08561706542969 Val_Reconstruction : 57.23329734802246 Val_KL : 2.852319598197937\n","Epoch: 1013/5000  Traning Loss: 61.60422706604004  Train_Reconstruction: 58.716936111450195  Train_KL: 2.887291193008423  Validation Loss : 60.215782165527344 Val_Reconstruction : 57.36955642700195 Val_KL : 2.8462246656417847\n","Epoch: 1014/5000  Traning Loss: 61.77511978149414  Train_Reconstruction: 58.901962757110596  Train_KL: 2.873156875371933  Validation Loss : 60.817935943603516 Val_Reconstruction : 57.97386932373047 Val_KL : 2.8440645933151245\n","Epoch: 1015/5000  Traning Loss: 61.783379554748535  Train_Reconstruction: 58.89537286758423  Train_KL: 2.888006627559662  Validation Loss : 60.12306785583496 Val_Reconstruction : 57.261383056640625 Val_KL : 2.861685037612915\n","Epoch: 1016/5000  Traning Loss: 60.9723596572876  Train_Reconstruction: 58.079105854034424  Train_KL: 2.8932542502880096  Validation Loss : 59.80747032165527 Val_Reconstruction : 56.95383262634277 Val_KL : 2.8536367416381836\n","Epoch: 1017/5000  Traning Loss: 60.891966342926025  Train_Reconstruction: 58.00581502914429  Train_KL: 2.8861520886421204  Validation Loss : 59.74194145202637 Val_Reconstruction : 56.875118255615234 Val_KL : 2.8668235540390015\n","Epoch: 1018/5000  Traning Loss: 60.991586208343506  Train_Reconstruction: 58.094489097595215  Train_KL: 2.8970967829227448  Validation Loss : 59.83188247680664 Val_Reconstruction : 56.964040756225586 Val_KL : 2.867842674255371\n","Epoch: 1019/5000  Traning Loss: 61.32315635681152  Train_Reconstruction: 58.4420862197876  Train_KL: 2.881070226430893  Validation Loss : 60.0197811126709 Val_Reconstruction : 57.15827941894531 Val_KL : 2.8615020513534546\n","Epoch: 1020/5000  Traning Loss: 61.64011764526367  Train_Reconstruction: 58.75721788406372  Train_KL: 2.8828997015953064  Validation Loss : 60.01093864440918 Val_Reconstruction : 57.1629524230957 Val_KL : 2.8479868173599243\n","Epoch: 1021/5000  Traning Loss: 60.94455671310425  Train_Reconstruction: 58.06556415557861  Train_KL: 2.8789922297000885  Validation Loss : 59.669029235839844 Val_Reconstruction : 56.826459884643555 Val_KL : 2.842568874359131\n","Epoch: 1022/5000  Traning Loss: 60.90183210372925  Train_Reconstruction: 58.014060974121094  Train_KL: 2.887771427631378  Validation Loss : 59.896780014038086 Val_Reconstruction : 57.03012657165527 Val_KL : 2.866653323173523\n","Epoch: 1023/5000  Traning Loss: 60.880337715148926  Train_Reconstruction: 57.98508358001709  Train_KL: 2.8952545821666718  Validation Loss : 59.9880256652832 Val_Reconstruction : 57.144493103027344 Val_KL : 2.843532681465149\n","Epoch: 1024/5000  Traning Loss: 60.71959161758423  Train_Reconstruction: 57.8341965675354  Train_KL: 2.885394901037216  Validation Loss : 59.53720474243164 Val_Reconstruction : 56.69448471069336 Val_KL : 2.8427191972732544\n","Epoch: 1025/5000  Traning Loss: 60.62440490722656  Train_Reconstruction: 57.74390745162964  Train_KL: 2.8804971873760223  Validation Loss : 59.53230857849121 Val_Reconstruction : 56.67856407165527 Val_KL : 2.8537460565567017\n","Epoch: 1026/5000  Traning Loss: 60.78608274459839  Train_Reconstruction: 57.89903688430786  Train_KL: 2.8870449662208557  Validation Loss : 59.599334716796875 Val_Reconstruction : 56.7444953918457 Val_KL : 2.8548396825790405\n","Epoch: 1027/5000  Traning Loss: 60.95587635040283  Train_Reconstruction: 58.06449604034424  Train_KL: 2.8913803100585938  Validation Loss : 60.17056846618652 Val_Reconstruction : 57.30730438232422 Val_KL : 2.8632625341415405\n","Epoch: 1028/5000  Traning Loss: 61.11449718475342  Train_Reconstruction: 58.23232841491699  Train_KL: 2.8821693062782288  Validation Loss : 59.7086181640625 Val_Reconstruction : 56.86397171020508 Val_KL : 2.8446460962295532\n","Epoch: 1029/5000  Traning Loss: 60.78048038482666  Train_Reconstruction: 57.89596509933472  Train_KL: 2.8845147788524628  Validation Loss : 59.721317291259766 Val_Reconstruction : 56.84563636779785 Val_KL : 2.8756810426712036\n","Epoch: 1030/5000  Traning Loss: 60.71797227859497  Train_Reconstruction: 57.82906103134155  Train_KL: 2.8889112770557404  Validation Loss : 59.45853614807129 Val_Reconstruction : 56.623077392578125 Val_KL : 2.8354586362838745\n","Epoch: 1031/5000  Traning Loss: 60.70530080795288  Train_Reconstruction: 57.81447744369507  Train_KL: 2.890823721885681  Validation Loss : 59.74272346496582 Val_Reconstruction : 56.873544692993164 Val_KL : 2.8691790103912354\n","Epoch: 1032/5000  Traning Loss: 61.06748628616333  Train_Reconstruction: 58.17040967941284  Train_KL: 2.8970765471458435  Validation Loss : 60.17004203796387 Val_Reconstruction : 57.309003829956055 Val_KL : 2.861039638519287\n","Epoch: 1033/5000  Traning Loss: 61.199368476867676  Train_Reconstruction: 58.30552005767822  Train_KL: 2.893849164247513  Validation Loss : 60.225751876831055 Val_Reconstruction : 57.36757850646973 Val_KL : 2.8581740856170654\n","Epoch: 1034/5000  Traning Loss: 61.203043937683105  Train_Reconstruction: 58.32191467285156  Train_KL: 2.881128579378128  Validation Loss : 60.117591857910156 Val_Reconstruction : 57.27459526062012 Val_KL : 2.8429982662200928\n","Epoch: 1035/5000  Traning Loss: 61.16134023666382  Train_Reconstruction: 58.267226219177246  Train_KL: 2.894114524126053  Validation Loss : 60.08085060119629 Val_Reconstruction : 57.212242126464844 Val_KL : 2.8686097860336304\n","Epoch: 1036/5000  Traning Loss: 60.96448373794556  Train_Reconstruction: 58.06459617614746  Train_KL: 2.899886906147003  Validation Loss : 59.74295234680176 Val_Reconstruction : 56.880393981933594 Val_KL : 2.8625588417053223\n","Epoch: 1037/5000  Traning Loss: 61.01870012283325  Train_Reconstruction: 58.1313214302063  Train_KL: 2.88737815618515  Validation Loss : 59.96352767944336 Val_Reconstruction : 57.10785102844238 Val_KL : 2.8556764125823975\n","Epoch: 1038/5000  Traning Loss: 61.030667781829834  Train_Reconstruction: 58.1424674987793  Train_KL: 2.888200342655182  Validation Loss : 59.926748275756836 Val_Reconstruction : 57.06040954589844 Val_KL : 2.8663387298583984\n","Epoch: 1039/5000  Traning Loss: 61.017348766326904  Train_Reconstruction: 58.12755632400513  Train_KL: 2.889792561531067  Validation Loss : 59.92807579040527 Val_Reconstruction : 57.08447265625 Val_KL : 2.843603491783142\n","Epoch: 1040/5000  Traning Loss: 61.32616329193115  Train_Reconstruction: 58.43738126754761  Train_KL: 2.8887819349765778  Validation Loss : 60.388153076171875 Val_Reconstruction : 57.544721603393555 Val_KL : 2.8434298038482666\n","Epoch: 1041/5000  Traning Loss: 61.79232406616211  Train_Reconstruction: 58.9034161567688  Train_KL: 2.888908267021179  Validation Loss : 60.30228042602539 Val_Reconstruction : 57.45221138000488 Val_KL : 2.850069761276245\n","Epoch: 1042/5000  Traning Loss: 61.3374137878418  Train_Reconstruction: 58.45613765716553  Train_KL: 2.881275773048401  Validation Loss : 60.4337272644043 Val_Reconstruction : 57.58418655395508 Val_KL : 2.8495419025421143\n","Epoch: 1043/5000  Traning Loss: 61.693589210510254  Train_Reconstruction: 58.80171346664429  Train_KL: 2.8918757140636444  Validation Loss : 60.278038024902344 Val_Reconstruction : 57.43388557434082 Val_KL : 2.8441511392593384\n","Epoch: 1044/5000  Traning Loss: 61.58488178253174  Train_Reconstruction: 58.70572805404663  Train_KL: 2.879153847694397  Validation Loss : 59.834245681762695 Val_Reconstruction : 56.980873107910156 Val_KL : 2.853372573852539\n","Epoch: 1045/5000  Traning Loss: 60.90865516662598  Train_Reconstruction: 58.008164405822754  Train_KL: 2.9004898071289062  Validation Loss : 59.793325424194336 Val_Reconstruction : 56.93247032165527 Val_KL : 2.8608542680740356\n","Epoch: 1046/5000  Traning Loss: 60.66712760925293  Train_Reconstruction: 57.77026987075806  Train_KL: 2.896857440471649  Validation Loss : 59.532732009887695 Val_Reconstruction : 56.679391860961914 Val_KL : 2.8533390760421753\n","Epoch: 1047/5000  Traning Loss: 60.75251007080078  Train_Reconstruction: 57.87450122833252  Train_KL: 2.878009557723999  Validation Loss : 59.66965103149414 Val_Reconstruction : 56.823734283447266 Val_KL : 2.8459161520004272\n","Epoch: 1048/5000  Traning Loss: 61.0458083152771  Train_Reconstruction: 58.15994882583618  Train_KL: 2.885859400033951  Validation Loss : 60.14017677307129 Val_Reconstruction : 57.2956600189209 Val_KL : 2.8445169925689697\n","Epoch: 1049/5000  Traning Loss: 60.96328926086426  Train_Reconstruction: 58.09169626235962  Train_KL: 2.871592491865158  Validation Loss : 59.78325843811035 Val_Reconstruction : 56.93663787841797 Val_KL : 2.8466200828552246\n","Epoch: 1050/5000  Traning Loss: 60.81800413131714  Train_Reconstruction: 57.927491188049316  Train_KL: 2.8905129730701447  Validation Loss : 59.66133117675781 Val_Reconstruction : 56.796857833862305 Val_KL : 2.864473581314087\n","Epoch: 1051/5000  Traning Loss: 60.883079528808594  Train_Reconstruction: 57.991126537323  Train_KL: 2.8919523656368256  Validation Loss : 59.94390678405762 Val_Reconstruction : 57.08832359313965 Val_KL : 2.8555831909179688\n","Epoch: 1052/5000  Traning Loss: 61.229576587677  Train_Reconstruction: 58.341132164001465  Train_KL: 2.888444185256958  Validation Loss : 60.45365333557129 Val_Reconstruction : 57.60087585449219 Val_KL : 2.852776050567627\n","Epoch: 1053/5000  Traning Loss: 61.40594959259033  Train_Reconstruction: 58.52385663986206  Train_KL: 2.882093131542206  Validation Loss : 60.35281181335449 Val_Reconstruction : 57.50679016113281 Val_KL : 2.8460224866867065\n","Epoch: 1054/5000  Traning Loss: 61.47691869735718  Train_Reconstruction: 58.60545301437378  Train_KL: 2.871465742588043  Validation Loss : 60.17948341369629 Val_Reconstruction : 57.348758697509766 Val_KL : 2.8307238817214966\n","Epoch: 1055/5000  Traning Loss: 60.94876956939697  Train_Reconstruction: 58.06722831726074  Train_KL: 2.8815410435199738  Validation Loss : 60.34980583190918 Val_Reconstruction : 57.48983573913574 Val_KL : 2.8599698543548584\n","Epoch: 1056/5000  Traning Loss: 60.727285861968994  Train_Reconstruction: 57.83259439468384  Train_KL: 2.8946909308433533  Validation Loss : 59.73493003845215 Val_Reconstruction : 56.88042068481445 Val_KL : 2.8545087575912476\n","Epoch: 1057/5000  Traning Loss: 60.86103010177612  Train_Reconstruction: 57.96413803100586  Train_KL: 2.8968921303749084  Validation Loss : 59.80364227294922 Val_Reconstruction : 56.94543647766113 Val_KL : 2.8582050800323486\n","Epoch: 1058/5000  Traning Loss: 60.69654035568237  Train_Reconstruction: 57.822571754455566  Train_KL: 2.873967796564102  Validation Loss : 59.48763847351074 Val_Reconstruction : 56.641048431396484 Val_KL : 2.8465906381607056\n","Epoch: 1059/5000  Traning Loss: 60.63591003417969  Train_Reconstruction: 57.76470470428467  Train_KL: 2.8712055385112762  Validation Loss : 59.66061019897461 Val_Reconstruction : 56.8245792388916 Val_KL : 2.8360300064086914\n","Epoch: 1060/5000  Traning Loss: 60.657573223114014  Train_Reconstruction: 57.78296184539795  Train_KL: 2.874611735343933  Validation Loss : 59.61517906188965 Val_Reconstruction : 56.760629653930664 Val_KL : 2.8545496463775635\n","Epoch: 1061/5000  Traning Loss: 60.777310848236084  Train_Reconstruction: 57.88593339920044  Train_KL: 2.891377806663513  Validation Loss : 59.76735305786133 Val_Reconstruction : 56.90655326843262 Val_KL : 2.8607999086380005\n","Epoch: 1062/5000  Traning Loss: 60.97630023956299  Train_Reconstruction: 58.087411403656006  Train_KL: 2.888888508081436  Validation Loss : 59.46290969848633 Val_Reconstruction : 56.60014724731445 Val_KL : 2.8627610206604004\n","Epoch: 1063/5000  Traning Loss: 60.68926811218262  Train_Reconstruction: 57.78867197036743  Train_KL: 2.9005958139896393  Validation Loss : 59.51339149475098 Val_Reconstruction : 56.645402908325195 Val_KL : 2.86798894405365\n","Epoch: 1064/5000  Traning Loss: 60.69216537475586  Train_Reconstruction: 57.80279207229614  Train_KL: 2.8893735110759735  Validation Loss : 60.05620002746582 Val_Reconstruction : 57.200124740600586 Val_KL : 2.8560760021209717\n","Epoch: 1065/5000  Traning Loss: 60.90528154373169  Train_Reconstruction: 58.01701307296753  Train_KL: 2.8882685005664825  Validation Loss : 59.85261154174805 Val_Reconstruction : 56.99140739440918 Val_KL : 2.861204743385315\n","Epoch: 1066/5000  Traning Loss: 60.984778881073  Train_Reconstruction: 58.09405994415283  Train_KL: 2.890719175338745  Validation Loss : 59.91902732849121 Val_Reconstruction : 57.06145095825195 Val_KL : 2.8575772047042847\n","Epoch: 1067/5000  Traning Loss: 60.71128034591675  Train_Reconstruction: 57.81879901885986  Train_KL: 2.892481178045273  Validation Loss : 59.45558738708496 Val_Reconstruction : 56.58858871459961 Val_KL : 2.8669973611831665\n","Epoch: 1068/5000  Traning Loss: 60.542723178863525  Train_Reconstruction: 57.64701843261719  Train_KL: 2.8957051634788513  Validation Loss : 59.410194396972656 Val_Reconstruction : 56.556440353393555 Val_KL : 2.853753924369812\n","Epoch: 1069/5000  Traning Loss: 60.70865535736084  Train_Reconstruction: 57.81452989578247  Train_KL: 2.8941256403923035  Validation Loss : 59.95898246765137 Val_Reconstruction : 57.07479476928711 Val_KL : 2.8841878175735474\n","Epoch: 1070/5000  Traning Loss: 60.80641031265259  Train_Reconstruction: 57.90707063674927  Train_KL: 2.8993395268917084  Validation Loss : 59.4390983581543 Val_Reconstruction : 56.5696964263916 Val_KL : 2.8694018125534058\n","Epoch: 1071/5000  Traning Loss: 60.85603046417236  Train_Reconstruction: 57.95677042007446  Train_KL: 2.899259716272354  Validation Loss : 59.71992111206055 Val_Reconstruction : 56.842674255371094 Val_KL : 2.877248525619507\n","Epoch: 1072/5000  Traning Loss: 60.745715618133545  Train_Reconstruction: 57.84610939025879  Train_KL: 2.8996065855026245  Validation Loss : 59.52555847167969 Val_Reconstruction : 56.688228607177734 Val_KL : 2.837328791618347\n","Epoch: 1073/5000  Traning Loss: 61.00509786605835  Train_Reconstruction: 58.126216888427734  Train_KL: 2.8788812458515167  Validation Loss : 59.7955207824707 Val_Reconstruction : 56.933868408203125 Val_KL : 2.8616535663604736\n","Epoch: 1074/5000  Traning Loss: 61.08275556564331  Train_Reconstruction: 58.179919719696045  Train_KL: 2.902835726737976  Validation Loss : 60.25857353210449 Val_Reconstruction : 57.387882232666016 Val_KL : 2.870691180229187\n","Epoch: 1075/5000  Traning Loss: 61.26741981506348  Train_Reconstruction: 58.366312980651855  Train_KL: 2.9011074006557465  Validation Loss : 60.31083869934082 Val_Reconstruction : 57.44015884399414 Val_KL : 2.870678186416626\n","Epoch: 1076/5000  Traning Loss: 61.06773614883423  Train_Reconstruction: 58.17011213302612  Train_KL: 2.897624284029007  Validation Loss : 60.02765464782715 Val_Reconstruction : 57.17027282714844 Val_KL : 2.8573834896087646\n","Epoch: 1077/5000  Traning Loss: 60.96059513092041  Train_Reconstruction: 58.06942319869995  Train_KL: 2.891171395778656  Validation Loss : 59.50436019897461 Val_Reconstruction : 56.644113540649414 Val_KL : 2.860247015953064\n","Epoch: 1078/5000  Traning Loss: 60.89574146270752  Train_Reconstruction: 57.994038581848145  Train_KL: 2.901703327894211  Validation Loss : 59.910959243774414 Val_Reconstruction : 57.05464744567871 Val_KL : 2.8563129901885986\n","Epoch: 1079/5000  Traning Loss: 60.723140239715576  Train_Reconstruction: 57.834959506988525  Train_KL: 2.888180732727051  Validation Loss : 59.79619789123535 Val_Reconstruction : 56.947675704956055 Val_KL : 2.848523497581482\n","Epoch: 1080/5000  Traning Loss: 60.67476797103882  Train_Reconstruction: 57.790947914123535  Train_KL: 2.8838208317756653  Validation Loss : 59.59325408935547 Val_Reconstruction : 56.760643005371094 Val_KL : 2.832611560821533\n","Epoch: 1081/5000  Traning Loss: 60.56132221221924  Train_Reconstruction: 57.68129253387451  Train_KL: 2.8800300657749176  Validation Loss : 59.523263931274414 Val_Reconstruction : 56.68134689331055 Val_KL : 2.8419164419174194\n","Epoch: 1082/5000  Traning Loss: 60.67238426208496  Train_Reconstruction: 57.78764009475708  Train_KL: 2.8847440481185913  Validation Loss : 59.704904556274414 Val_Reconstruction : 56.856924057006836 Val_KL : 2.8479796648025513\n","Epoch: 1083/5000  Traning Loss: 60.817325592041016  Train_Reconstruction: 57.93090105056763  Train_KL: 2.8864239752292633  Validation Loss : 59.92460250854492 Val_Reconstruction : 57.06217575073242 Val_KL : 2.862426996231079\n","Epoch: 1084/5000  Traning Loss: 60.838366985321045  Train_Reconstruction: 57.93827724456787  Train_KL: 2.9000898599624634  Validation Loss : 59.990116119384766 Val_Reconstruction : 57.12576484680176 Val_KL : 2.864351987838745\n","Epoch: 1085/5000  Traning Loss: 60.92785453796387  Train_Reconstruction: 58.039273262023926  Train_KL: 2.888580322265625  Validation Loss : 59.74448585510254 Val_Reconstruction : 56.904232025146484 Val_KL : 2.8402528762817383\n","Epoch: 1086/5000  Traning Loss: 60.72468423843384  Train_Reconstruction: 57.83350229263306  Train_KL: 2.8911821842193604  Validation Loss : 59.45468521118164 Val_Reconstruction : 56.58807182312012 Val_KL : 2.866613030433655\n","Epoch: 1087/5000  Traning Loss: 60.61196517944336  Train_Reconstruction: 57.707590103149414  Train_KL: 2.9043754637241364  Validation Loss : 59.424421310424805 Val_Reconstruction : 56.55929183959961 Val_KL : 2.865129828453064\n","Epoch: 1088/5000  Traning Loss: 60.63577365875244  Train_Reconstruction: 57.748849391937256  Train_KL: 2.8869248628616333  Validation Loss : 59.62284851074219 Val_Reconstruction : 56.76213455200195 Val_KL : 2.860714077949524\n","Epoch: 1089/5000  Traning Loss: 60.87151622772217  Train_Reconstruction: 57.987648010253906  Train_KL: 2.883867710828781  Validation Loss : 59.70949363708496 Val_Reconstruction : 56.877281188964844 Val_KL : 2.8322116136550903\n","Epoch: 1090/5000  Traning Loss: 60.92714977264404  Train_Reconstruction: 58.047444343566895  Train_KL: 2.8797056078910828  Validation Loss : 59.690616607666016 Val_Reconstruction : 56.82831954956055 Val_KL : 2.8622976541519165\n","Epoch: 1091/5000  Traning Loss: 60.74909162521362  Train_Reconstruction: 57.85679531097412  Train_KL: 2.8922962844371796  Validation Loss : 59.66339874267578 Val_Reconstruction : 56.812469482421875 Val_KL : 2.850930094718933\n","Epoch: 1092/5000  Traning Loss: 60.84022665023804  Train_Reconstruction: 57.94722890853882  Train_KL: 2.892998367547989  Validation Loss : 59.748159408569336 Val_Reconstruction : 56.86338996887207 Val_KL : 2.8847687244415283\n","Epoch: 1093/5000  Traning Loss: 60.986013889312744  Train_Reconstruction: 58.078309059143066  Train_KL: 2.9077053666114807  Validation Loss : 59.93377494812012 Val_Reconstruction : 57.06921195983887 Val_KL : 2.8645623922348022\n","Epoch: 1094/5000  Traning Loss: 61.01945114135742  Train_Reconstruction: 58.11751317977905  Train_KL: 2.9019381403923035  Validation Loss : 60.13315010070801 Val_Reconstruction : 57.25350761413574 Val_KL : 2.8796428442001343\n","Epoch: 1095/5000  Traning Loss: 60.835143089294434  Train_Reconstruction: 57.9295334815979  Train_KL: 2.9056097865104675  Validation Loss : 59.517072677612305 Val_Reconstruction : 56.661237716674805 Val_KL : 2.855835199356079\n","Epoch: 1096/5000  Traning Loss: 60.8190279006958  Train_Reconstruction: 57.92370653152466  Train_KL: 2.8953219950199127  Validation Loss : 59.944353103637695 Val_Reconstruction : 57.077993392944336 Val_KL : 2.8663591146469116\n","Epoch: 1097/5000  Traning Loss: 60.75505256652832  Train_Reconstruction: 57.8630633354187  Train_KL: 2.8919894993305206  Validation Loss : 59.823665618896484 Val_Reconstruction : 56.969268798828125 Val_KL : 2.8543964624404907\n","Epoch: 1098/5000  Traning Loss: 60.565282344818115  Train_Reconstruction: 57.6557354927063  Train_KL: 2.9095470309257507  Validation Loss : 59.374704360961914 Val_Reconstruction : 56.49550247192383 Val_KL : 2.879202365875244\n","Epoch: 1099/5000  Traning Loss: 60.53916025161743  Train_Reconstruction: 57.63668775558472  Train_KL: 2.902472883462906  Validation Loss : 59.690032958984375 Val_Reconstruction : 56.845685958862305 Val_KL : 2.8443477153778076\n","Epoch: 1100/5000  Traning Loss: 60.8620924949646  Train_Reconstruction: 57.97082853317261  Train_KL: 2.8912642896175385  Validation Loss : 59.70315170288086 Val_Reconstruction : 56.84162712097168 Val_KL : 2.861524224281311\n","Epoch: 1101/5000  Traning Loss: 60.9503436088562  Train_Reconstruction: 58.057148456573486  Train_KL: 2.8931958973407745  Validation Loss : 59.940542221069336 Val_Reconstruction : 57.07643699645996 Val_KL : 2.864105701446533\n","Epoch: 1102/5000  Traning Loss: 60.96376705169678  Train_Reconstruction: 58.07120704650879  Train_KL: 2.892560452222824  Validation Loss : 59.90721893310547 Val_Reconstruction : 57.044620513916016 Val_KL : 2.8625991344451904\n","Epoch: 1103/5000  Traning Loss: 60.98487043380737  Train_Reconstruction: 58.08304166793823  Train_KL: 2.9018293023109436  Validation Loss : 60.113393783569336 Val_Reconstruction : 57.2469482421875 Val_KL : 2.8664458990097046\n","Epoch: 1104/5000  Traning Loss: 61.01526927947998  Train_Reconstruction: 58.124839305877686  Train_KL: 2.8904301822185516  Validation Loss : 60.25787353515625 Val_Reconstruction : 57.413713455200195 Val_KL : 2.8441622257232666\n","Epoch: 1105/5000  Traning Loss: 60.80195093154907  Train_Reconstruction: 57.91024160385132  Train_KL: 2.891709119081497  Validation Loss : 59.924734115600586 Val_Reconstruction : 57.07335662841797 Val_KL : 2.8513758182525635\n","Epoch: 1106/5000  Traning Loss: 60.734561920166016  Train_Reconstruction: 57.85089874267578  Train_KL: 2.8836623430252075  Validation Loss : 59.312599182128906 Val_Reconstruction : 56.4641170501709 Val_KL : 2.848482608795166\n","Epoch: 1107/5000  Traning Loss: 60.75895929336548  Train_Reconstruction: 57.859548568725586  Train_KL: 2.899410754442215  Validation Loss : 59.87512969970703 Val_Reconstruction : 57.021718978881836 Val_KL : 2.853410005569458\n","Epoch: 1108/5000  Traning Loss: 60.58602285385132  Train_Reconstruction: 57.71789264678955  Train_KL: 2.8681304454803467  Validation Loss : 59.7244930267334 Val_Reconstruction : 56.885311126708984 Val_KL : 2.839180827140808\n","Epoch: 1109/5000  Traning Loss: 60.692214488983154  Train_Reconstruction: 57.815378189086914  Train_KL: 2.876836746931076  Validation Loss : 59.82641410827637 Val_Reconstruction : 56.98122978210449 Val_KL : 2.8451842069625854\n","Epoch: 1110/5000  Traning Loss: 60.633399963378906  Train_Reconstruction: 57.75395727157593  Train_KL: 2.8794423043727875  Validation Loss : 59.52802085876465 Val_Reconstruction : 56.6834831237793 Val_KL : 2.844538688659668\n","Epoch: 1111/5000  Traning Loss: 60.54662799835205  Train_Reconstruction: 57.66016626358032  Train_KL: 2.8864617347717285  Validation Loss : 59.566585540771484 Val_Reconstruction : 56.70779037475586 Val_KL : 2.8587963581085205\n","Epoch: 1112/5000  Traning Loss: 60.4245662689209  Train_Reconstruction: 57.5425329208374  Train_KL: 2.882033348083496  Validation Loss : 59.40957832336426 Val_Reconstruction : 56.55855751037598 Val_KL : 2.8510210514068604\n","Epoch: 1113/5000  Traning Loss: 60.61005926132202  Train_Reconstruction: 57.71036434173584  Train_KL: 2.8996945321559906  Validation Loss : 59.62213897705078 Val_Reconstruction : 56.73663139343262 Val_KL : 2.8855080604553223\n","Epoch: 1114/5000  Traning Loss: 60.73399829864502  Train_Reconstruction: 57.835466384887695  Train_KL: 2.898531347513199  Validation Loss : 59.80735397338867 Val_Reconstruction : 56.94955062866211 Val_KL : 2.8578038215637207\n","Epoch: 1115/5000  Traning Loss: 60.718647480010986  Train_Reconstruction: 57.837138175964355  Train_KL: 2.8815087974071503  Validation Loss : 59.468467712402344 Val_Reconstruction : 56.6041145324707 Val_KL : 2.86435329914093\n","Epoch: 1116/5000  Traning Loss: 60.55482721328735  Train_Reconstruction: 57.65452480316162  Train_KL: 2.9003018736839294  Validation Loss : 59.478858947753906 Val_Reconstruction : 56.62214469909668 Val_KL : 2.8567150831222534\n","Epoch: 1117/5000  Traning Loss: 60.46883201599121  Train_Reconstruction: 57.57745409011841  Train_KL: 2.8913772106170654  Validation Loss : 59.60639572143555 Val_Reconstruction : 56.74413871765137 Val_KL : 2.862257242202759\n","Epoch: 1118/5000  Traning Loss: 60.524611949920654  Train_Reconstruction: 57.64004182815552  Train_KL: 2.884569823741913  Validation Loss : 59.56369590759277 Val_Reconstruction : 56.71854782104492 Val_KL : 2.8451476097106934\n","Epoch: 1119/5000  Traning Loss: 60.70590353012085  Train_Reconstruction: 57.80845308303833  Train_KL: 2.8974505364894867  Validation Loss : 59.44907760620117 Val_Reconstruction : 56.58382797241211 Val_KL : 2.865248918533325\n","Epoch: 1120/5000  Traning Loss: 60.6693058013916  Train_Reconstruction: 57.7776255607605  Train_KL: 2.891680598258972  Validation Loss : 60.035526275634766 Val_Reconstruction : 57.16262245178223 Val_KL : 2.872902989387512\n","Epoch: 1121/5000  Traning Loss: 60.71402311325073  Train_Reconstruction: 57.80313062667847  Train_KL: 2.910892754793167  Validation Loss : 59.82765007019043 Val_Reconstruction : 56.952816009521484 Val_KL : 2.8748334646224976\n","Epoch: 1122/5000  Traning Loss: 60.784574031829834  Train_Reconstruction: 57.88468265533447  Train_KL: 2.8998923003673553  Validation Loss : 59.68391799926758 Val_Reconstruction : 56.812408447265625 Val_KL : 2.87151038646698\n","Epoch: 1123/5000  Traning Loss: 60.76501274108887  Train_Reconstruction: 57.86529588699341  Train_KL: 2.899716466665268  Validation Loss : 60.051889419555664 Val_Reconstruction : 57.1772518157959 Val_KL : 2.874638319015503\n","Epoch: 1124/5000  Traning Loss: 61.19995594024658  Train_Reconstruction: 58.30567502975464  Train_KL: 2.894280791282654  Validation Loss : 60.562416076660156 Val_Reconstruction : 57.7122688293457 Val_KL : 2.8501466512680054\n","Epoch: 1125/5000  Traning Loss: 60.74664783477783  Train_Reconstruction: 57.862003803253174  Train_KL: 2.8846442699432373  Validation Loss : 59.54521369934082 Val_Reconstruction : 56.69576644897461 Val_KL : 2.849447011947632\n","Epoch: 1126/5000  Traning Loss: 60.67204809188843  Train_Reconstruction: 57.79151630401611  Train_KL: 2.8805322349071503  Validation Loss : 59.516366958618164 Val_Reconstruction : 56.667287826538086 Val_KL : 2.8490787744522095\n","Epoch: 1127/5000  Traning Loss: 60.36353015899658  Train_Reconstruction: 57.47613477706909  Train_KL: 2.887395739555359  Validation Loss : 59.35809326171875 Val_Reconstruction : 56.50021934509277 Val_KL : 2.8578736782073975\n","Epoch: 1128/5000  Traning Loss: 60.33005380630493  Train_Reconstruction: 57.43616819381714  Train_KL: 2.8938855826854706  Validation Loss : 59.30647659301758 Val_Reconstruction : 56.44425392150879 Val_KL : 2.862222671508789\n","Epoch: 1129/5000  Traning Loss: 60.70613431930542  Train_Reconstruction: 57.81446886062622  Train_KL: 2.8916649520397186  Validation Loss : 59.67026901245117 Val_Reconstruction : 56.815378189086914 Val_KL : 2.85489022731781\n","Epoch: 1130/5000  Traning Loss: 60.586756229400635  Train_Reconstruction: 57.69968938827515  Train_KL: 2.887065887451172  Validation Loss : 59.45045471191406 Val_Reconstruction : 56.58588409423828 Val_KL : 2.8645710945129395\n","Epoch: 1131/5000  Traning Loss: 60.46749258041382  Train_Reconstruction: 57.58221387863159  Train_KL: 2.885278344154358  Validation Loss : 59.481266021728516 Val_Reconstruction : 56.619258880615234 Val_KL : 2.8620073795318604\n","Epoch: 1132/5000  Traning Loss: 60.58625888824463  Train_Reconstruction: 57.6934380531311  Train_KL: 2.8928210735321045  Validation Loss : 59.56529235839844 Val_Reconstruction : 56.712533950805664 Val_KL : 2.852758288383484\n","Epoch: 1133/5000  Traning Loss: 60.5726957321167  Train_Reconstruction: 57.6897759437561  Train_KL: 2.882919877767563  Validation Loss : 59.6481876373291 Val_Reconstruction : 56.790388107299805 Val_KL : 2.8577985763549805\n","Epoch: 1134/5000  Traning Loss: 60.63999557495117  Train_Reconstruction: 57.7538800239563  Train_KL: 2.886115849018097  Validation Loss : 59.81014442443848 Val_Reconstruction : 56.960561752319336 Val_KL : 2.8495826721191406\n","Epoch: 1135/5000  Traning Loss: 60.703335762023926  Train_Reconstruction: 57.81952095031738  Train_KL: 2.8838140666484833  Validation Loss : 59.45996284484863 Val_Reconstruction : 56.60581588745117 Val_KL : 2.8541476726531982\n","Epoch: 1136/5000  Traning Loss: 60.52601146697998  Train_Reconstruction: 57.640408515930176  Train_KL: 2.88560289144516  Validation Loss : 59.62375259399414 Val_Reconstruction : 56.760210037231445 Val_KL : 2.863542914390564\n","Epoch: 1137/5000  Traning Loss: 60.707417011260986  Train_Reconstruction: 57.81869029998779  Train_KL: 2.8887266516685486  Validation Loss : 59.66821479797363 Val_Reconstruction : 56.809627532958984 Val_KL : 2.858588218688965\n","Epoch: 1138/5000  Traning Loss: 60.84121370315552  Train_Reconstruction: 57.949360370635986  Train_KL: 2.8918533623218536  Validation Loss : 59.58652687072754 Val_Reconstruction : 56.73933219909668 Val_KL : 2.8471946716308594\n","Epoch: 1139/5000  Traning Loss: 61.07614040374756  Train_Reconstruction: 58.18818187713623  Train_KL: 2.8879586458206177  Validation Loss : 60.111324310302734 Val_Reconstruction : 57.24179649353027 Val_KL : 2.869527220726013\n","Epoch: 1140/5000  Traning Loss: 61.04018831253052  Train_Reconstruction: 58.14160680770874  Train_KL: 2.8985812664031982  Validation Loss : 59.52180862426758 Val_Reconstruction : 56.656959533691406 Val_KL : 2.8648505210876465\n","Epoch: 1141/5000  Traning Loss: 61.0658221244812  Train_Reconstruction: 58.14992094039917  Train_KL: 2.915901154279709  Validation Loss : 59.91684341430664 Val_Reconstruction : 57.038164138793945 Val_KL : 2.8786802291870117\n","Epoch: 1142/5000  Traning Loss: 61.1834716796875  Train_Reconstruction: 58.281392097473145  Train_KL: 2.902079463005066  Validation Loss : 60.11278533935547 Val_Reconstruction : 57.24654197692871 Val_KL : 2.866245150566101\n","Epoch: 1143/5000  Traning Loss: 61.27215576171875  Train_Reconstruction: 58.3588662147522  Train_KL: 2.913289427757263  Validation Loss : 60.122352600097656 Val_Reconstruction : 57.247589111328125 Val_KL : 2.874763011932373\n","Epoch: 1144/5000  Traning Loss: 61.20496463775635  Train_Reconstruction: 58.307342529296875  Train_KL: 2.8976215422153473  Validation Loss : 60.07309341430664 Val_Reconstruction : 57.22733116149902 Val_KL : 2.8457624912261963\n","Epoch: 1145/5000  Traning Loss: 61.027403354644775  Train_Reconstruction: 58.138643741607666  Train_KL: 2.8887593150138855  Validation Loss : 60.18976974487305 Val_Reconstruction : 57.34554100036621 Val_KL : 2.844228982925415\n","Epoch: 1146/5000  Traning Loss: 60.8617205619812  Train_Reconstruction: 57.971882820129395  Train_KL: 2.8898373544216156  Validation Loss : 60.21551704406738 Val_Reconstruction : 57.34152793884277 Val_KL : 2.873989462852478\n","Epoch: 1147/5000  Traning Loss: 60.637768268585205  Train_Reconstruction: 57.72996759414673  Train_KL: 2.907801032066345  Validation Loss : 59.561445236206055 Val_Reconstruction : 56.70195007324219 Val_KL : 2.8594958782196045\n","Epoch: 1148/5000  Traning Loss: 60.453104972839355  Train_Reconstruction: 57.56843423843384  Train_KL: 2.8846705853939056  Validation Loss : 59.50188636779785 Val_Reconstruction : 56.65216064453125 Val_KL : 2.8497244119644165\n","Epoch: 1149/5000  Traning Loss: 60.52978992462158  Train_Reconstruction: 57.62843465805054  Train_KL: 2.901355355978012  Validation Loss : 59.73790168762207 Val_Reconstruction : 56.868757247924805 Val_KL : 2.8691436052322388\n","Epoch: 1150/5000  Traning Loss: 60.39666223526001  Train_Reconstruction: 57.50614261627197  Train_KL: 2.8905193507671356  Validation Loss : 59.24232864379883 Val_Reconstruction : 56.401798248291016 Val_KL : 2.840530514717102\n","Epoch: 1151/5000  Traning Loss: 60.39127826690674  Train_Reconstruction: 57.510910511016846  Train_KL: 2.880367189645767  Validation Loss : 59.390140533447266 Val_Reconstruction : 56.537235260009766 Val_KL : 2.852906107902527\n","Epoch: 1152/5000  Traning Loss: 60.61250352859497  Train_Reconstruction: 57.726322650909424  Train_KL: 2.886180192232132  Validation Loss : 59.803768157958984 Val_Reconstruction : 56.95898628234863 Val_KL : 2.8447823524475098\n","Epoch: 1153/5000  Traning Loss: 60.8715181350708  Train_Reconstruction: 57.97838258743286  Train_KL: 2.8931352496147156  Validation Loss : 60.040767669677734 Val_Reconstruction : 57.1695499420166 Val_KL : 2.871214985847473\n","Epoch: 1154/5000  Traning Loss: 60.861655712127686  Train_Reconstruction: 57.96807622909546  Train_KL: 2.89357990026474  Validation Loss : 59.83411979675293 Val_Reconstruction : 56.99288558959961 Val_KL : 2.841233730316162\n","Epoch: 1155/5000  Traning Loss: 60.46533441543579  Train_Reconstruction: 57.57108545303345  Train_KL: 2.8942482471466064  Validation Loss : 59.49017906188965 Val_Reconstruction : 56.62948989868164 Val_KL : 2.8606876134872437\n","Epoch: 1156/5000  Traning Loss: 60.32869863510132  Train_Reconstruction: 57.44487380981445  Train_KL: 2.8838247656822205  Validation Loss : 59.214059829711914 Val_Reconstruction : 56.37889099121094 Val_KL : 2.835169196128845\n","Epoch: 1157/5000  Traning Loss: 60.38455390930176  Train_Reconstruction: 57.49612760543823  Train_KL: 2.888426572084427  Validation Loss : 59.7962532043457 Val_Reconstruction : 56.94248008728027 Val_KL : 2.85377299785614\n","Epoch: 1158/5000  Traning Loss: 60.8235559463501  Train_Reconstruction: 57.9434175491333  Train_KL: 2.880138397216797  Validation Loss : 60.25002670288086 Val_Reconstruction : 57.405935287475586 Val_KL : 2.8440892696380615\n","Epoch: 1159/5000  Traning Loss: 60.648197174072266  Train_Reconstruction: 57.76145029067993  Train_KL: 2.886746436357498  Validation Loss : 59.511260986328125 Val_Reconstruction : 56.64749717712402 Val_KL : 2.8637638092041016\n","Epoch: 1160/5000  Traning Loss: 60.543803215026855  Train_Reconstruction: 57.63810110092163  Train_KL: 2.905701607465744  Validation Loss : 59.627389907836914 Val_Reconstruction : 56.75436592102051 Val_KL : 2.873023748397827\n","Epoch: 1161/5000  Traning Loss: 60.488672733306885  Train_Reconstruction: 57.594985008239746  Train_KL: 2.8936875760555267  Validation Loss : 59.55226135253906 Val_Reconstruction : 56.68736267089844 Val_KL : 2.8649001121520996\n","Epoch: 1162/5000  Traning Loss: 60.491599559783936  Train_Reconstruction: 57.60928726196289  Train_KL: 2.8823122382164  Validation Loss : 59.72187423706055 Val_Reconstruction : 56.87297248840332 Val_KL : 2.8489019870758057\n","Epoch: 1163/5000  Traning Loss: 60.59779930114746  Train_Reconstruction: 57.70547151565552  Train_KL: 2.8923279643058777  Validation Loss : 59.93099021911621 Val_Reconstruction : 57.07682228088379 Val_KL : 2.85416841506958\n","Epoch: 1164/5000  Traning Loss: 60.447697162628174  Train_Reconstruction: 57.55705451965332  Train_KL: 2.8906431794166565  Validation Loss : 59.409528732299805 Val_Reconstruction : 56.54510307312012 Val_KL : 2.8644254207611084\n","Epoch: 1165/5000  Traning Loss: 60.377968311309814  Train_Reconstruction: 57.48407745361328  Train_KL: 2.893891453742981  Validation Loss : 59.326765060424805 Val_Reconstruction : 56.46036148071289 Val_KL : 2.8664029836654663\n","Epoch: 1166/5000  Traning Loss: 60.35788297653198  Train_Reconstruction: 57.448482513427734  Train_KL: 2.9094002544879913  Validation Loss : 59.208906173706055 Val_Reconstruction : 56.343746185302734 Val_KL : 2.865159273147583\n","Epoch: 1167/5000  Traning Loss: 60.24421262741089  Train_Reconstruction: 57.35663461685181  Train_KL: 2.887578397989273  Validation Loss : 59.1018180847168 Val_Reconstruction : 56.24361610412598 Val_KL : 2.858201026916504\n","Epoch: 1168/5000  Traning Loss: 60.29906988143921  Train_Reconstruction: 57.40165710449219  Train_KL: 2.897412747144699  Validation Loss : 59.27291297912598 Val_Reconstruction : 56.42117881774902 Val_KL : 2.8517353534698486\n","Epoch: 1169/5000  Traning Loss: 60.28190040588379  Train_Reconstruction: 57.392385482788086  Train_KL: 2.8895144760608673  Validation Loss : 59.190622329711914 Val_Reconstruction : 56.34572792053223 Val_KL : 2.8448938131332397\n","Epoch: 1170/5000  Traning Loss: 60.227832317352295  Train_Reconstruction: 57.34900188446045  Train_KL: 2.8788300454616547  Validation Loss : 59.243106842041016 Val_Reconstruction : 56.39629554748535 Val_KL : 2.8468124866485596\n","Epoch: 1171/5000  Traning Loss: 60.3329119682312  Train_Reconstruction: 57.4357476234436  Train_KL: 2.897164672613144  Validation Loss : 59.54440498352051 Val_Reconstruction : 56.671302795410156 Val_KL : 2.87310254573822\n","Epoch: 1172/5000  Traning Loss: 60.291213035583496  Train_Reconstruction: 57.38193368911743  Train_KL: 2.9092791974544525  Validation Loss : 59.49847412109375 Val_Reconstruction : 56.626678466796875 Val_KL : 2.87179696559906\n","Epoch: 1173/5000  Traning Loss: 60.387224197387695  Train_Reconstruction: 57.49163818359375  Train_KL: 2.8955852389335632  Validation Loss : 59.364253997802734 Val_Reconstruction : 56.49859046936035 Val_KL : 2.8656649589538574\n","Epoch: 1174/5000  Traning Loss: 60.489013671875  Train_Reconstruction: 57.597676277160645  Train_KL: 2.891337275505066  Validation Loss : 59.47014045715332 Val_Reconstruction : 56.61497116088867 Val_KL : 2.8551697731018066\n","Epoch: 1175/5000  Traning Loss: 60.44481945037842  Train_Reconstruction: 57.55170011520386  Train_KL: 2.8931197226047516  Validation Loss : 59.50577354431152 Val_Reconstruction : 56.651824951171875 Val_KL : 2.8539494276046753\n","Epoch: 1176/5000  Traning Loss: 60.4761266708374  Train_Reconstruction: 57.58699131011963  Train_KL: 2.889135390520096  Validation Loss : 59.4908332824707 Val_Reconstruction : 56.63335609436035 Val_KL : 2.857475519180298\n","Epoch: 1177/5000  Traning Loss: 60.50355100631714  Train_Reconstruction: 57.61384439468384  Train_KL: 2.8897062242031097  Validation Loss : 59.300811767578125 Val_Reconstruction : 56.44728660583496 Val_KL : 2.85352623462677\n","Epoch: 1178/5000  Traning Loss: 60.199440002441406  Train_Reconstruction: 57.315216064453125  Train_KL: 2.8842234313488007  Validation Loss : 59.19989776611328 Val_Reconstruction : 56.33763122558594 Val_KL : 2.8622658252716064\n","Epoch: 1179/5000  Traning Loss: 60.477341651916504  Train_Reconstruction: 57.57580804824829  Train_KL: 2.9015338122844696  Validation Loss : 59.659889221191406 Val_Reconstruction : 56.7956600189209 Val_KL : 2.8642284870147705\n","Epoch: 1180/5000  Traning Loss: 60.32445049285889  Train_Reconstruction: 57.425652503967285  Train_KL: 2.898797959089279  Validation Loss : 59.278913497924805 Val_Reconstruction : 56.4121150970459 Val_KL : 2.8667986392974854\n","Epoch: 1181/5000  Traning Loss: 60.221906661987305  Train_Reconstruction: 57.326534271240234  Train_KL: 2.895372211933136  Validation Loss : 59.24309730529785 Val_Reconstruction : 56.37752342224121 Val_KL : 2.865573525428772\n","Epoch: 1182/5000  Traning Loss: 60.1061487197876  Train_Reconstruction: 57.21933841705322  Train_KL: 2.8868108093738556  Validation Loss : 59.076202392578125 Val_Reconstruction : 56.22490882873535 Val_KL : 2.85129451751709\n","Epoch: 1183/5000  Traning Loss: 60.18120098114014  Train_Reconstruction: 57.29483461380005  Train_KL: 2.886366158723831  Validation Loss : 59.33333969116211 Val_Reconstruction : 56.47300338745117 Val_KL : 2.860336184501648\n","Epoch: 1184/5000  Traning Loss: 60.31741428375244  Train_Reconstruction: 57.43142795562744  Train_KL: 2.885987162590027  Validation Loss : 59.51779747009277 Val_Reconstruction : 56.6651496887207 Val_KL : 2.8526476621627808\n","Epoch: 1185/5000  Traning Loss: 60.41910123825073  Train_Reconstruction: 57.533700942993164  Train_KL: 2.8853999972343445  Validation Loss : 59.32339859008789 Val_Reconstruction : 56.48138999938965 Val_KL : 2.842008948326111\n","Epoch: 1186/5000  Traning Loss: 60.14561700820923  Train_Reconstruction: 57.25835990905762  Train_KL: 2.887257158756256  Validation Loss : 59.158626556396484 Val_Reconstruction : 56.296804428100586 Val_KL : 2.861822009086609\n","Epoch: 1187/5000  Traning Loss: 60.086201190948486  Train_Reconstruction: 57.18546438217163  Train_KL: 2.900736153125763  Validation Loss : 59.06950378417969 Val_Reconstruction : 56.1849479675293 Val_KL : 2.8845568895339966\n","Epoch: 1188/5000  Traning Loss: 60.235981941223145  Train_Reconstruction: 57.33132839202881  Train_KL: 2.90465384721756  Validation Loss : 59.22592544555664 Val_Reconstruction : 56.37253189086914 Val_KL : 2.853394031524658\n","Epoch: 1189/5000  Traning Loss: 60.2639045715332  Train_Reconstruction: 57.37560558319092  Train_KL: 2.8882991671562195  Validation Loss : 59.18637657165527 Val_Reconstruction : 56.31455993652344 Val_KL : 2.871816039085388\n","Epoch: 1190/5000  Traning Loss: 60.345765113830566  Train_Reconstruction: 57.453479290008545  Train_KL: 2.8922857344150543  Validation Loss : 59.378536224365234 Val_Reconstruction : 56.53572463989258 Val_KL : 2.8428109884262085\n","Epoch: 1191/5000  Traning Loss: 60.78373050689697  Train_Reconstruction: 57.90487766265869  Train_KL: 2.8788527250289917  Validation Loss : 59.76482582092285 Val_Reconstruction : 56.91349220275879 Val_KL : 2.851334810256958\n","Epoch: 1192/5000  Traning Loss: 60.6729040145874  Train_Reconstruction: 57.77615690231323  Train_KL: 2.896747440099716  Validation Loss : 59.429067611694336 Val_Reconstruction : 56.56662178039551 Val_KL : 2.862446904182434\n","Epoch: 1193/5000  Traning Loss: 60.30078363418579  Train_Reconstruction: 57.40670299530029  Train_KL: 2.894081085920334  Validation Loss : 59.35178565979004 Val_Reconstruction : 56.487945556640625 Val_KL : 2.8638405799865723\n","Epoch: 1194/5000  Traning Loss: 60.21634864807129  Train_Reconstruction: 57.317180156707764  Train_KL: 2.8991684019565582  Validation Loss : 59.32100296020508 Val_Reconstruction : 56.45259666442871 Val_KL : 2.8684074878692627\n","Epoch: 1195/5000  Traning Loss: 60.354137897491455  Train_Reconstruction: 57.45266103744507  Train_KL: 2.9014765918254852  Validation Loss : 59.318695068359375 Val_Reconstruction : 56.46315002441406 Val_KL : 2.8555456399917603\n","Epoch: 1196/5000  Traning Loss: 60.316551208496094  Train_Reconstruction: 57.41023015975952  Train_KL: 2.9063216149806976  Validation Loss : 59.29509353637695 Val_Reconstruction : 56.435163497924805 Val_KL : 2.859930992126465\n","Epoch: 1197/5000  Traning Loss: 60.53279209136963  Train_Reconstruction: 57.638033866882324  Train_KL: 2.8947587609291077  Validation Loss : 59.52420997619629 Val_Reconstruction : 56.66187858581543 Val_KL : 2.8623311519622803\n","Epoch: 1198/5000  Traning Loss: 61.06268072128296  Train_Reconstruction: 58.16885471343994  Train_KL: 2.8938262462615967  Validation Loss : 60.115699768066406 Val_Reconstruction : 57.24831581115723 Val_KL : 2.8673843145370483\n","Epoch: 1199/5000  Traning Loss: 61.27676057815552  Train_Reconstruction: 58.36026668548584  Train_KL: 2.9164937138557434  Validation Loss : 59.88932228088379 Val_Reconstruction : 57.00604248046875 Val_KL : 2.8832801580429077\n","Epoch: 1200/5000  Traning Loss: 60.72272491455078  Train_Reconstruction: 57.81956195831299  Train_KL: 2.9031632244586945  Validation Loss : 59.78471755981445 Val_Reconstruction : 56.91980743408203 Val_KL : 2.8649096488952637\n","Epoch: 1201/5000  Traning Loss: 60.38621711730957  Train_Reconstruction: 57.49242115020752  Train_KL: 2.893795073032379  Validation Loss : 59.15060806274414 Val_Reconstruction : 56.28997230529785 Val_KL : 2.8606356382369995\n","Epoch: 1202/5000  Traning Loss: 60.38691568374634  Train_Reconstruction: 57.48521184921265  Train_KL: 2.9017032384872437  Validation Loss : 59.41119194030762 Val_Reconstruction : 56.53882026672363 Val_KL : 2.8723723888397217\n","Epoch: 1203/5000  Traning Loss: 60.5094952583313  Train_Reconstruction: 57.623472690582275  Train_KL: 2.8860224783420563  Validation Loss : 59.463823318481445 Val_Reconstruction : 56.60533332824707 Val_KL : 2.85849130153656\n","Epoch: 1204/5000  Traning Loss: 60.17560863494873  Train_Reconstruction: 57.27741861343384  Train_KL: 2.898190051317215  Validation Loss : 59.125505447387695 Val_Reconstruction : 56.26032066345215 Val_KL : 2.8651844263076782\n","Epoch: 1205/5000  Traning Loss: 60.30866003036499  Train_Reconstruction: 57.41838502883911  Train_KL: 2.8902748227119446  Validation Loss : 59.404197692871094 Val_Reconstruction : 56.54427719116211 Val_KL : 2.859919548034668\n","Epoch: 1206/5000  Traning Loss: 60.81464147567749  Train_Reconstruction: 57.91106367111206  Train_KL: 2.9035772383213043  Validation Loss : 60.312490463256836 Val_Reconstruction : 57.44347953796387 Val_KL : 2.8690117597579956\n","Epoch: 1207/5000  Traning Loss: 60.550076484680176  Train_Reconstruction: 57.66743993759155  Train_KL: 2.882636457681656  Validation Loss : 59.50997543334961 Val_Reconstruction : 56.66751480102539 Val_KL : 2.8424618244171143\n","Epoch: 1208/5000  Traning Loss: 60.39456844329834  Train_Reconstruction: 57.48871088027954  Train_KL: 2.905857414007187  Validation Loss : 59.23777961730957 Val_Reconstruction : 56.346670150756836 Val_KL : 2.891108274459839\n","Epoch: 1209/5000  Traning Loss: 60.32459259033203  Train_Reconstruction: 57.42805814743042  Train_KL: 2.8965342938899994  Validation Loss : 59.09114646911621 Val_Reconstruction : 56.23741149902344 Val_KL : 2.8537356853485107\n","Epoch: 1210/5000  Traning Loss: 60.30995273590088  Train_Reconstruction: 57.42940330505371  Train_KL: 2.8805497884750366  Validation Loss : 59.66230392456055 Val_Reconstruction : 56.81364440917969 Val_KL : 2.848660111427307\n","Epoch: 1211/5000  Traning Loss: 60.7109260559082  Train_Reconstruction: 57.82945775985718  Train_KL: 2.8814693093299866  Validation Loss : 59.270416259765625 Val_Reconstruction : 56.420528411865234 Val_KL : 2.8498870134353638\n","Epoch: 1212/5000  Traning Loss: 60.439395904541016  Train_Reconstruction: 57.546934604644775  Train_KL: 2.89246141910553  Validation Loss : 59.30862998962402 Val_Reconstruction : 56.45640754699707 Val_KL : 2.852221965789795\n","Epoch: 1213/5000  Traning Loss: 60.35844039916992  Train_Reconstruction: 57.46912670135498  Train_KL: 2.8893133103847504  Validation Loss : 59.410526275634766 Val_Reconstruction : 56.55294609069824 Val_KL : 2.857580780982971\n","Epoch: 1214/5000  Traning Loss: 60.760112285614014  Train_Reconstruction: 57.86884880065918  Train_KL: 2.891263484954834  Validation Loss : 59.96675109863281 Val_Reconstruction : 57.11180877685547 Val_KL : 2.854942202568054\n","Epoch: 1215/5000  Traning Loss: 61.185415267944336  Train_Reconstruction: 58.28513956069946  Train_KL: 2.9002758860588074  Validation Loss : 59.53278732299805 Val_Reconstruction : 56.669233322143555 Val_KL : 2.8635544776916504\n","Epoch: 1216/5000  Traning Loss: 60.49432849884033  Train_Reconstruction: 57.587634563446045  Train_KL: 2.9066942632198334  Validation Loss : 59.39578437805176 Val_Reconstruction : 56.52192497253418 Val_KL : 2.873859167098999\n","Epoch: 1217/5000  Traning Loss: 60.27345371246338  Train_Reconstruction: 57.37496471405029  Train_KL: 2.8984889090061188  Validation Loss : 59.18211364746094 Val_Reconstruction : 56.32885932922363 Val_KL : 2.853253960609436\n","Epoch: 1218/5000  Traning Loss: 60.090243339538574  Train_Reconstruction: 57.20127010345459  Train_KL: 2.888973593711853  Validation Loss : 59.14896202087402 Val_Reconstruction : 56.294342041015625 Val_KL : 2.8546191453933716\n","Epoch: 1219/5000  Traning Loss: 60.045445919036865  Train_Reconstruction: 57.14360761642456  Train_KL: 2.9018385112285614  Validation Loss : 59.13704872131348 Val_Reconstruction : 56.26638221740723 Val_KL : 2.8706655502319336\n","Epoch: 1220/5000  Traning Loss: 60.306437969207764  Train_Reconstruction: 57.40028381347656  Train_KL: 2.9061545729637146  Validation Loss : 59.5048770904541 Val_Reconstruction : 56.64179039001465 Val_KL : 2.8630865812301636\n","Epoch: 1221/5000  Traning Loss: 60.55916738510132  Train_Reconstruction: 57.665098667144775  Train_KL: 2.8940684497356415  Validation Loss : 59.35031318664551 Val_Reconstruction : 56.48555374145508 Val_KL : 2.8647592067718506\n","Epoch: 1222/5000  Traning Loss: 60.865610122680664  Train_Reconstruction: 57.957026958465576  Train_KL: 2.9085828065872192  Validation Loss : 59.53803253173828 Val_Reconstruction : 56.66494560241699 Val_KL : 2.8730852603912354\n","Epoch: 1223/5000  Traning Loss: 60.29991865158081  Train_Reconstruction: 57.397831439971924  Train_KL: 2.902087390422821  Validation Loss : 59.10964584350586 Val_Reconstruction : 56.2254753112793 Val_KL : 2.884169101715088\n","Epoch: 1224/5000  Traning Loss: 60.25632572174072  Train_Reconstruction: 57.3459358215332  Train_KL: 2.910390019416809  Validation Loss : 59.15499687194824 Val_Reconstruction : 56.29290008544922 Val_KL : 2.8620961904525757\n","Epoch: 1225/5000  Traning Loss: 60.15932750701904  Train_Reconstruction: 57.27031707763672  Train_KL: 2.8890105485916138  Validation Loss : 59.09305381774902 Val_Reconstruction : 56.23155212402344 Val_KL : 2.8615005016326904\n","Epoch: 1226/5000  Traning Loss: 60.11907148361206  Train_Reconstruction: 57.22720527648926  Train_KL: 2.8918658196926117  Validation Loss : 59.05121994018555 Val_Reconstruction : 56.19333267211914 Val_KL : 2.8578869104385376\n","Epoch: 1227/5000  Traning Loss: 60.3478102684021  Train_Reconstruction: 57.44424915313721  Train_KL: 2.9035609364509583  Validation Loss : 59.42742919921875 Val_Reconstruction : 56.56323432922363 Val_KL : 2.864194393157959\n","Epoch: 1228/5000  Traning Loss: 60.50292491912842  Train_Reconstruction: 57.61011219024658  Train_KL: 2.892812967300415  Validation Loss : 59.555789947509766 Val_Reconstruction : 56.69655227661133 Val_KL : 2.8592382669448853\n","Epoch: 1229/5000  Traning Loss: 60.6655387878418  Train_Reconstruction: 57.76057767868042  Train_KL: 2.9049609005451202  Validation Loss : 59.892099380493164 Val_Reconstruction : 57.02498245239258 Val_KL : 2.8671175241470337\n","Epoch: 1230/5000  Traning Loss: 60.610440254211426  Train_Reconstruction: 57.703115463256836  Train_KL: 2.9073240160942078  Validation Loss : 59.31164741516113 Val_Reconstruction : 56.43907928466797 Val_KL : 2.8725682497024536\n","Epoch: 1231/5000  Traning Loss: 60.423919677734375  Train_Reconstruction: 57.51627731323242  Train_KL: 2.90764257311821  Validation Loss : 59.196048736572266 Val_Reconstruction : 56.31808662414551 Val_KL : 2.8779629468917847\n","Epoch: 1232/5000  Traning Loss: 60.36667776107788  Train_Reconstruction: 57.469905376434326  Train_KL: 2.8967728912830353  Validation Loss : 59.52673530578613 Val_Reconstruction : 56.658416748046875 Val_KL : 2.86831796169281\n","Epoch: 1233/5000  Traning Loss: 60.418139934539795  Train_Reconstruction: 57.515732288360596  Train_KL: 2.9024078249931335  Validation Loss : 59.15532684326172 Val_Reconstruction : 56.283172607421875 Val_KL : 2.872153401374817\n","Epoch: 1234/5000  Traning Loss: 60.11358451843262  Train_Reconstruction: 57.21949768066406  Train_KL: 2.894086480140686  Validation Loss : 59.14071464538574 Val_Reconstruction : 56.27459907531738 Val_KL : 2.86611545085907\n","Epoch: 1235/5000  Traning Loss: 60.07815742492676  Train_Reconstruction: 57.18995809555054  Train_KL: 2.888198971748352  Validation Loss : 59.14034652709961 Val_Reconstruction : 56.292423248291016 Val_KL : 2.8479219675064087\n","Epoch: 1236/5000  Traning Loss: 60.17564392089844  Train_Reconstruction: 57.29129695892334  Train_KL: 2.8843472003936768  Validation Loss : 59.17369842529297 Val_Reconstruction : 56.31702995300293 Val_KL : 2.856669783592224\n","Epoch: 1237/5000  Traning Loss: 60.15686559677124  Train_Reconstruction: 57.24836587905884  Train_KL: 2.9085004031658173  Validation Loss : 59.231706619262695 Val_Reconstruction : 56.36391830444336 Val_KL : 2.867787718772888\n","Epoch: 1238/5000  Traning Loss: 60.26214027404785  Train_Reconstruction: 57.36601781845093  Train_KL: 2.896122694015503  Validation Loss : 59.165056228637695 Val_Reconstruction : 56.3070011138916 Val_KL : 2.858054041862488\n","Epoch: 1239/5000  Traning Loss: 60.198856353759766  Train_Reconstruction: 57.298285484313965  Train_KL: 2.9005700647830963  Validation Loss : 59.11745071411133 Val_Reconstruction : 56.24216079711914 Val_KL : 2.8752899169921875\n","Epoch: 1240/5000  Traning Loss: 60.022725105285645  Train_Reconstruction: 57.10992956161499  Train_KL: 2.9127953946590424  Validation Loss : 59.17994499206543 Val_Reconstruction : 56.289506912231445 Val_KL : 2.89043927192688\n","Epoch: 1241/5000  Traning Loss: 59.96137237548828  Train_Reconstruction: 57.05268049240112  Train_KL: 2.908692479133606  Validation Loss : 59.16050910949707 Val_Reconstruction : 56.2929801940918 Val_KL : 2.867528200149536\n","Epoch: 1242/5000  Traning Loss: 60.25533103942871  Train_Reconstruction: 57.35420894622803  Train_KL: 2.9011221826076508  Validation Loss : 58.97307205200195 Val_Reconstruction : 56.10157775878906 Val_KL : 2.871493339538574\n","Epoch: 1243/5000  Traning Loss: 60.017446994781494  Train_Reconstruction: 57.1183648109436  Train_KL: 2.8990820944309235  Validation Loss : 59.07550239562988 Val_Reconstruction : 56.22536277770996 Val_KL : 2.8501389026641846\n","Epoch: 1244/5000  Traning Loss: 60.1303014755249  Train_Reconstruction: 57.24454641342163  Train_KL: 2.885754942893982  Validation Loss : 59.12627983093262 Val_Reconstruction : 56.27007484436035 Val_KL : 2.85620379447937\n","Epoch: 1245/5000  Traning Loss: 60.32318830490112  Train_Reconstruction: 57.421884536743164  Train_KL: 2.90130352973938  Validation Loss : 59.693471908569336 Val_Reconstruction : 56.82119369506836 Val_KL : 2.872278094291687\n","Epoch: 1246/5000  Traning Loss: 60.304362297058105  Train_Reconstruction: 57.401716232299805  Train_KL: 2.902646780014038  Validation Loss : 59.34609794616699 Val_Reconstruction : 56.486602783203125 Val_KL : 2.8594952821731567\n","Epoch: 1247/5000  Traning Loss: 60.342559814453125  Train_Reconstruction: 57.45974636077881  Train_KL: 2.882813185453415  Validation Loss : 59.36303901672363 Val_Reconstruction : 56.51284408569336 Val_KL : 2.850194215774536\n","Epoch: 1248/5000  Traning Loss: 60.15466833114624  Train_Reconstruction: 57.25797510147095  Train_KL: 2.896692544221878  Validation Loss : 59.36372756958008 Val_Reconstruction : 56.505775451660156 Val_KL : 2.8579518795013428\n","Epoch: 1249/5000  Traning Loss: 60.159626483917236  Train_Reconstruction: 57.263465881347656  Train_KL: 2.896160304546356  Validation Loss : 59.212738037109375 Val_Reconstruction : 56.36367225646973 Val_KL : 2.8490655422210693\n","Epoch: 1250/5000  Traning Loss: 60.10229253768921  Train_Reconstruction: 57.220152378082275  Train_KL: 2.8821407854557037  Validation Loss : 59.21858596801758 Val_Reconstruction : 56.37485885620117 Val_KL : 2.8437278270721436\n","Epoch: 1251/5000  Traning Loss: 60.03858470916748  Train_Reconstruction: 57.15744352340698  Train_KL: 2.881141632795334  Validation Loss : 58.99255180358887 Val_Reconstruction : 56.13049507141113 Val_KL : 2.8620564937591553\n","Epoch: 1252/5000  Traning Loss: 60.01382398605347  Train_Reconstruction: 57.122941970825195  Train_KL: 2.8908822536468506  Validation Loss : 59.04420471191406 Val_Reconstruction : 56.18330764770508 Val_KL : 2.860898017883301\n","Epoch: 1253/5000  Traning Loss: 60.1543664932251  Train_Reconstruction: 57.24452018737793  Train_KL: 2.9098458290100098  Validation Loss : 59.28874206542969 Val_Reconstruction : 56.41077423095703 Val_KL : 2.8779670000076294\n","Epoch: 1254/5000  Traning Loss: 60.21515607833862  Train_Reconstruction: 57.30055046081543  Train_KL: 2.9146053791046143  Validation Loss : 58.98322677612305 Val_Reconstruction : 56.1132698059082 Val_KL : 2.869957447052002\n","Epoch: 1255/5000  Traning Loss: 60.1104793548584  Train_Reconstruction: 57.204720973968506  Train_KL: 2.9057587683200836  Validation Loss : 59.00402641296387 Val_Reconstruction : 56.13801193237305 Val_KL : 2.866013526916504\n","Epoch: 1256/5000  Traning Loss: 60.19231700897217  Train_Reconstruction: 57.29359579086304  Train_KL: 2.8987211287021637  Validation Loss : 59.07498550415039 Val_Reconstruction : 56.220863342285156 Val_KL : 2.8541218042373657\n","Epoch: 1257/5000  Traning Loss: 60.170963287353516  Train_Reconstruction: 57.28174448013306  Train_KL: 2.8892188370227814  Validation Loss : 59.04554557800293 Val_Reconstruction : 56.19892501831055 Val_KL : 2.8466213941574097\n","Epoch: 1258/5000  Traning Loss: 60.01529407501221  Train_Reconstruction: 57.127277851104736  Train_KL: 2.8880167603492737  Validation Loss : 59.09146308898926 Val_Reconstruction : 56.235328674316406 Val_KL : 2.8561339378356934\n","Epoch: 1259/5000  Traning Loss: 60.236910343170166  Train_Reconstruction: 57.33940315246582  Train_KL: 2.897506833076477  Validation Loss : 59.28592300415039 Val_Reconstruction : 56.423574447631836 Val_KL : 2.8623486757278442\n","Epoch: 1260/5000  Traning Loss: 60.49118137359619  Train_Reconstruction: 57.59868097305298  Train_KL: 2.892500013113022  Validation Loss : 59.8332576751709 Val_Reconstruction : 56.975847244262695 Val_KL : 2.8574098348617554\n","Epoch: 1261/5000  Traning Loss: 60.88785743713379  Train_Reconstruction: 57.98293972015381  Train_KL: 2.9049174785614014  Validation Loss : 59.3653507232666 Val_Reconstruction : 56.50954627990723 Val_KL : 2.855805516242981\n","Epoch: 1262/5000  Traning Loss: 60.63389348983765  Train_Reconstruction: 57.74340200424194  Train_KL: 2.8904912173748016  Validation Loss : 59.592350006103516 Val_Reconstruction : 56.73576545715332 Val_KL : 2.8565847873687744\n","Epoch: 1263/5000  Traning Loss: 61.14488363265991  Train_Reconstruction: 58.234702587127686  Train_KL: 2.9101808965206146  Validation Loss : 60.64716720581055 Val_Reconstruction : 57.76914405822754 Val_KL : 2.8780219554901123\n","Epoch: 1264/5000  Traning Loss: 60.676995277404785  Train_Reconstruction: 57.77812051773071  Train_KL: 2.898875266313553  Validation Loss : 59.197893142700195 Val_Reconstruction : 56.33847618103027 Val_KL : 2.859418272972107\n","Epoch: 1265/5000  Traning Loss: 60.21241092681885  Train_Reconstruction: 57.325170040130615  Train_KL: 2.8872411251068115  Validation Loss : 59.471866607666016 Val_Reconstruction : 56.621530532836914 Val_KL : 2.8503352403640747\n","Epoch: 1266/5000  Traning Loss: 60.51794099807739  Train_Reconstruction: 57.63401222229004  Train_KL: 2.883929044008255  Validation Loss : 60.26276969909668 Val_Reconstruction : 57.43362998962402 Val_KL : 2.829140782356262\n","Epoch: 1267/5000  Traning Loss: 60.97077655792236  Train_Reconstruction: 58.0954704284668  Train_KL: 2.8753061294555664  Validation Loss : 59.91562080383301 Val_Reconstruction : 57.07176399230957 Val_KL : 2.843856692314148\n","Epoch: 1268/5000  Traning Loss: 60.66242599487305  Train_Reconstruction: 57.77633047103882  Train_KL: 2.8860957622528076  Validation Loss : 59.384443283081055 Val_Reconstruction : 56.5203742980957 Val_KL : 2.8640689849853516\n","Epoch: 1269/5000  Traning Loss: 60.30383539199829  Train_Reconstruction: 57.41168737411499  Train_KL: 2.892147332429886  Validation Loss : 59.28648376464844 Val_Reconstruction : 56.42802429199219 Val_KL : 2.8584593534469604\n","Epoch: 1270/5000  Traning Loss: 60.05293607711792  Train_Reconstruction: 57.164175033569336  Train_KL: 2.8887609243392944  Validation Loss : 58.836177825927734 Val_Reconstruction : 55.972238540649414 Val_KL : 2.8639382123947144\n","Epoch: 1271/5000  Traning Loss: 59.846771240234375  Train_Reconstruction: 56.94985342025757  Train_KL: 2.8969181180000305  Validation Loss : 58.796852111816406 Val_Reconstruction : 55.93673896789551 Val_KL : 2.8601129055023193\n","Epoch: 1272/5000  Traning Loss: 59.96948862075806  Train_Reconstruction: 57.07942581176758  Train_KL: 2.8900628089904785  Validation Loss : 59.30868721008301 Val_Reconstruction : 56.46474838256836 Val_KL : 2.8439401388168335\n","Epoch: 1273/5000  Traning Loss: 60.01168394088745  Train_Reconstruction: 57.1207070350647  Train_KL: 2.8909766376018524  Validation Loss : 59.03187942504883 Val_Reconstruction : 56.16063690185547 Val_KL : 2.8712414503097534\n","Epoch: 1274/5000  Traning Loss: 59.98696804046631  Train_Reconstruction: 57.07228755950928  Train_KL: 2.914681077003479  Validation Loss : 59.358720779418945 Val_Reconstruction : 56.47529220581055 Val_KL : 2.883427381515503\n","Epoch: 1275/5000  Traning Loss: 60.173799991607666  Train_Reconstruction: 57.28295183181763  Train_KL: 2.8908482491970062  Validation Loss : 59.30928611755371 Val_Reconstruction : 56.4640998840332 Val_KL : 2.8451857566833496\n","Epoch: 1276/5000  Traning Loss: 60.70970582962036  Train_Reconstruction: 57.822285175323486  Train_KL: 2.8874209225177765  Validation Loss : 60.11309814453125 Val_Reconstruction : 57.2442626953125 Val_KL : 2.8688358068466187\n","Epoch: 1277/5000  Traning Loss: 60.63697576522827  Train_Reconstruction: 57.731913566589355  Train_KL: 2.9050620198249817  Validation Loss : 59.68305015563965 Val_Reconstruction : 56.83512306213379 Val_KL : 2.847927689552307\n","Epoch: 1278/5000  Traning Loss: 60.36810874938965  Train_Reconstruction: 57.477227210998535  Train_KL: 2.890881270170212  Validation Loss : 59.723487854003906 Val_Reconstruction : 56.87512397766113 Val_KL : 2.848363518714905\n","Epoch: 1279/5000  Traning Loss: 60.17731857299805  Train_Reconstruction: 57.28083086013794  Train_KL: 2.8964876532554626  Validation Loss : 59.24762153625488 Val_Reconstruction : 56.392784118652344 Val_KL : 2.8548386096954346\n","Epoch: 1280/5000  Traning Loss: 60.659533977508545  Train_Reconstruction: 57.76992177963257  Train_KL: 2.8896115720272064  Validation Loss : 60.1422176361084 Val_Reconstruction : 57.29587364196777 Val_KL : 2.8463430404663086\n","Epoch: 1281/5000  Traning Loss: 60.77229309082031  Train_Reconstruction: 57.8824987411499  Train_KL: 2.8897946178913116  Validation Loss : 59.42610549926758 Val_Reconstruction : 56.57311820983887 Val_KL : 2.8529880046844482\n","Epoch: 1282/5000  Traning Loss: 60.40190505981445  Train_Reconstruction: 57.505356311798096  Train_KL: 2.8965490460395813  Validation Loss : 59.86609649658203 Val_Reconstruction : 56.99843788146973 Val_KL : 2.867658019065857\n","Epoch: 1283/5000  Traning Loss: 60.64919900894165  Train_Reconstruction: 57.756349086761475  Train_KL: 2.892850309610367  Validation Loss : 60.18329429626465 Val_Reconstruction : 57.3283576965332 Val_KL : 2.85493803024292\n","Epoch: 1284/5000  Traning Loss: 60.209423542022705  Train_Reconstruction: 57.31550216674805  Train_KL: 2.8939217925071716  Validation Loss : 59.13010787963867 Val_Reconstruction : 56.270498275756836 Val_KL : 2.8596094846725464\n","Epoch: 1285/5000  Traning Loss: 60.00942850112915  Train_Reconstruction: 57.120187282562256  Train_KL: 2.88924178481102  Validation Loss : 59.38083839416504 Val_Reconstruction : 56.536338806152344 Val_KL : 2.8444995880126953\n","Epoch: 1286/5000  Traning Loss: 59.98592662811279  Train_Reconstruction: 57.09472703933716  Train_KL: 2.8911996483802795  Validation Loss : 58.950439453125 Val_Reconstruction : 56.079872131347656 Val_KL : 2.870567560195923\n","Epoch: 1287/5000  Traning Loss: 59.97242450714111  Train_Reconstruction: 57.083484172821045  Train_KL: 2.88894122838974  Validation Loss : 58.99700927734375 Val_Reconstruction : 56.155385971069336 Val_KL : 2.8416234254837036\n","Epoch: 1288/5000  Traning Loss: 59.90866136550903  Train_Reconstruction: 57.02428674697876  Train_KL: 2.8843747973442078  Validation Loss : 58.83822441101074 Val_Reconstruction : 55.9720516204834 Val_KL : 2.8661723136901855\n","Epoch: 1289/5000  Traning Loss: 59.86235761642456  Train_Reconstruction: 56.96415042877197  Train_KL: 2.898207187652588  Validation Loss : 59.00214958190918 Val_Reconstruction : 56.127113342285156 Val_KL : 2.8750370740890503\n","Epoch: 1290/5000  Traning Loss: 59.99653911590576  Train_Reconstruction: 57.08103895187378  Train_KL: 2.915500372648239  Validation Loss : 59.116825103759766 Val_Reconstruction : 56.24637031555176 Val_KL : 2.870454788208008\n","Epoch: 1291/5000  Traning Loss: 59.95361566543579  Train_Reconstruction: 57.06096792221069  Train_KL: 2.892647832632065  Validation Loss : 59.062374114990234 Val_Reconstruction : 56.20160675048828 Val_KL : 2.860769033432007\n","Epoch: 1292/5000  Traning Loss: 60.00072002410889  Train_Reconstruction: 57.099549293518066  Train_KL: 2.9011714160442352  Validation Loss : 59.094970703125 Val_Reconstruction : 56.21516990661621 Val_KL : 2.8798009157180786\n","Epoch: 1293/5000  Traning Loss: 59.97463369369507  Train_Reconstruction: 57.065675258636475  Train_KL: 2.9089578986167908  Validation Loss : 59.06832695007324 Val_Reconstruction : 56.17621612548828 Val_KL : 2.8921114206314087\n","Epoch: 1294/5000  Traning Loss: 60.02484130859375  Train_Reconstruction: 57.11207437515259  Train_KL: 2.9127672910690308  Validation Loss : 58.86313247680664 Val_Reconstruction : 55.98122024536133 Val_KL : 2.881912112236023\n","Epoch: 1295/5000  Traning Loss: 60.12373638153076  Train_Reconstruction: 57.21178722381592  Train_KL: 2.9119493663311005  Validation Loss : 59.15732383728027 Val_Reconstruction : 56.2758674621582 Val_KL : 2.8814576864242554\n","Epoch: 1296/5000  Traning Loss: 59.931865215301514  Train_Reconstruction: 57.02381610870361  Train_KL: 2.908049166202545  Validation Loss : 58.76551055908203 Val_Reconstruction : 55.89336395263672 Val_KL : 2.8721461296081543\n","Epoch: 1297/5000  Traning Loss: 60.00329828262329  Train_Reconstruction: 57.112995624542236  Train_KL: 2.8903031051158905  Validation Loss : 59.43867301940918 Val_Reconstruction : 56.58427810668945 Val_KL : 2.8543955087661743\n","Epoch: 1298/5000  Traning Loss: 60.105499267578125  Train_Reconstruction: 57.21669006347656  Train_KL: 2.888809382915497  Validation Loss : 59.47922134399414 Val_Reconstruction : 56.62471389770508 Val_KL : 2.8545072078704834\n","Epoch: 1299/5000  Traning Loss: 60.18454885482788  Train_Reconstruction: 57.29326581954956  Train_KL: 2.8912830352783203  Validation Loss : 59.021114349365234 Val_Reconstruction : 56.15900421142578 Val_KL : 2.8621108531951904\n","Epoch: 1300/5000  Traning Loss: 59.81421613693237  Train_Reconstruction: 56.9203839302063  Train_KL: 2.893832206726074  Validation Loss : 58.94691848754883 Val_Reconstruction : 56.07776641845703 Val_KL : 2.86915123462677\n","Epoch: 1301/5000  Traning Loss: 60.22583341598511  Train_Reconstruction: 57.32541275024414  Train_KL: 2.900420665740967  Validation Loss : 59.52293014526367 Val_Reconstruction : 56.64072036743164 Val_KL : 2.882210612297058\n","Epoch: 1302/5000  Traning Loss: 60.5484938621521  Train_Reconstruction: 57.64189624786377  Train_KL: 2.906597524881363  Validation Loss : 59.092533111572266 Val_Reconstruction : 56.22488975524902 Val_KL : 2.8676432371139526\n","Epoch: 1303/5000  Traning Loss: 60.27911996841431  Train_Reconstruction: 57.37014722824097  Train_KL: 2.908973067998886  Validation Loss : 59.71518325805664 Val_Reconstruction : 56.85176658630371 Val_KL : 2.8634159564971924\n","Epoch: 1304/5000  Traning Loss: 60.41063833236694  Train_Reconstruction: 57.510159969329834  Train_KL: 2.9004786014556885  Validation Loss : 59.42878532409668 Val_Reconstruction : 56.55606460571289 Val_KL : 2.872719645500183\n","Epoch: 1305/5000  Traning Loss: 60.686060428619385  Train_Reconstruction: 57.780038356781006  Train_KL: 2.906022369861603  Validation Loss : 59.83164978027344 Val_Reconstruction : 56.97597885131836 Val_KL : 2.8556714057922363\n","Epoch: 1306/5000  Traning Loss: 60.25686168670654  Train_Reconstruction: 57.36896991729736  Train_KL: 2.887891858816147  Validation Loss : 58.803293228149414 Val_Reconstruction : 55.94499397277832 Val_KL : 2.8582992553710938\n","Epoch: 1307/5000  Traning Loss: 59.87151050567627  Train_Reconstruction: 56.97279357910156  Train_KL: 2.898716926574707  Validation Loss : 58.85202217102051 Val_Reconstruction : 55.97859001159668 Val_KL : 2.8734320402145386\n","Epoch: 1308/5000  Traning Loss: 59.87619161605835  Train_Reconstruction: 56.98144197463989  Train_KL: 2.894748955965042  Validation Loss : 58.79774284362793 Val_Reconstruction : 55.95352363586426 Val_KL : 2.8442188501358032\n","Epoch: 1309/5000  Traning Loss: 59.91101694107056  Train_Reconstruction: 57.022510051727295  Train_KL: 2.888506442308426  Validation Loss : 58.910728454589844 Val_Reconstruction : 56.037885665893555 Val_KL : 2.872843623161316\n","Epoch: 1310/5000  Traning Loss: 59.93979024887085  Train_Reconstruction: 57.02041721343994  Train_KL: 2.9193734228610992  Validation Loss : 58.79921913146973 Val_Reconstruction : 55.90962791442871 Val_KL : 2.8895914554595947\n","Epoch: 1311/5000  Traning Loss: 59.929855823516846  Train_Reconstruction: 57.03007507324219  Train_KL: 2.899780750274658  Validation Loss : 59.266510009765625 Val_Reconstruction : 56.404354095458984 Val_KL : 2.862155795097351\n","Epoch: 1312/5000  Traning Loss: 60.1980881690979  Train_Reconstruction: 57.30368614196777  Train_KL: 2.89440193772316  Validation Loss : 60.006906509399414 Val_Reconstruction : 57.12482452392578 Val_KL : 2.8820836544036865\n","Epoch: 1313/5000  Traning Loss: 60.216729164123535  Train_Reconstruction: 57.32088613510132  Train_KL: 2.8958433270454407  Validation Loss : 59.454524993896484 Val_Reconstruction : 56.598167419433594 Val_KL : 2.8563575744628906\n","Epoch: 1314/5000  Traning Loss: 60.10754632949829  Train_Reconstruction: 57.20066690444946  Train_KL: 2.9068791270256042  Validation Loss : 59.745750427246094 Val_Reconstruction : 56.8648796081543 Val_KL : 2.880871534347534\n","Epoch: 1315/5000  Traning Loss: 60.193124771118164  Train_Reconstruction: 57.295283794403076  Train_KL: 2.897840917110443  Validation Loss : 59.21713829040527 Val_Reconstruction : 56.36661720275879 Val_KL : 2.8505226373672485\n","Epoch: 1316/5000  Traning Loss: 60.0749077796936  Train_Reconstruction: 57.17885494232178  Train_KL: 2.896053373813629  Validation Loss : 59.31205368041992 Val_Reconstruction : 56.439321517944336 Val_KL : 2.8727318048477173\n","Epoch: 1317/5000  Traning Loss: 60.20963144302368  Train_Reconstruction: 57.31254291534424  Train_KL: 2.897088646888733  Validation Loss : 59.16311454772949 Val_Reconstruction : 56.30709457397461 Val_KL : 2.8560203313827515\n","Epoch: 1318/5000  Traning Loss: 59.913005352020264  Train_Reconstruction: 57.010944843292236  Train_KL: 2.9020602703094482  Validation Loss : 58.90651512145996 Val_Reconstruction : 56.01579475402832 Val_KL : 2.890721082687378\n","Epoch: 1319/5000  Traning Loss: 59.7764253616333  Train_Reconstruction: 56.86298704147339  Train_KL: 2.9134380519390106  Validation Loss : 58.86265182495117 Val_Reconstruction : 55.982330322265625 Val_KL : 2.880320429801941\n","Epoch: 1320/5000  Traning Loss: 59.87288570404053  Train_Reconstruction: 56.96068334579468  Train_KL: 2.912203222513199  Validation Loss : 58.936893463134766 Val_Reconstruction : 56.05122947692871 Val_KL : 2.885662794113159\n","Epoch: 1321/5000  Traning Loss: 60.06966972351074  Train_Reconstruction: 57.170902252197266  Train_KL: 2.898767739534378  Validation Loss : 59.812387466430664 Val_Reconstruction : 56.9554328918457 Val_KL : 2.8569555282592773\n","Epoch: 1322/5000  Traning Loss: 60.04281663894653  Train_Reconstruction: 57.14719009399414  Train_KL: 2.8956266343593597  Validation Loss : 59.126638412475586 Val_Reconstruction : 56.24863815307617 Val_KL : 2.8779993057250977\n","Epoch: 1323/5000  Traning Loss: 60.14662504196167  Train_Reconstruction: 57.23831081390381  Train_KL: 2.9083142280578613  Validation Loss : 59.20871543884277 Val_Reconstruction : 56.346317291259766 Val_KL : 2.862397074699402\n","Epoch: 1324/5000  Traning Loss: 59.97744703292847  Train_Reconstruction: 57.08974599838257  Train_KL: 2.8877011835575104  Validation Loss : 59.122013092041016 Val_Reconstruction : 56.2662353515625 Val_KL : 2.8557777404785156\n","Epoch: 1325/5000  Traning Loss: 59.764230251312256  Train_Reconstruction: 56.86225700378418  Train_KL: 2.9019731879234314  Validation Loss : 58.79591941833496 Val_Reconstruction : 55.92312049865723 Val_KL : 2.872799277305603\n","Epoch: 1326/5000  Traning Loss: 59.89188051223755  Train_Reconstruction: 56.97877550125122  Train_KL: 2.913104921579361  Validation Loss : 59.08273887634277 Val_Reconstruction : 56.19768142700195 Val_KL : 2.8850587606430054\n","Epoch: 1327/5000  Traning Loss: 59.735321044921875  Train_Reconstruction: 56.828001499176025  Train_KL: 2.9073190093040466  Validation Loss : 58.84162521362305 Val_Reconstruction : 55.972909927368164 Val_KL : 2.8687145709991455\n","Epoch: 1328/5000  Traning Loss: 59.63670825958252  Train_Reconstruction: 56.73201608657837  Train_KL: 2.904691904783249  Validation Loss : 58.80509948730469 Val_Reconstruction : 55.93700408935547 Val_KL : 2.8680951595306396\n","Epoch: 1329/5000  Traning Loss: 59.73682355880737  Train_Reconstruction: 56.832711696624756  Train_KL: 2.9041115939617157  Validation Loss : 58.859052658081055 Val_Reconstruction : 55.98517036437988 Val_KL : 2.873881697654724\n","Epoch: 1330/5000  Traning Loss: 59.788901805877686  Train_Reconstruction: 56.88599395751953  Train_KL: 2.902907758951187  Validation Loss : 58.86716842651367 Val_Reconstruction : 55.99716567993164 Val_KL : 2.870002269744873\n","Epoch: 1331/5000  Traning Loss: 60.05586862564087  Train_Reconstruction: 57.16991996765137  Train_KL: 2.885948657989502  Validation Loss : 59.13906669616699 Val_Reconstruction : 56.28630256652832 Val_KL : 2.852764368057251\n","Epoch: 1332/5000  Traning Loss: 60.21568155288696  Train_Reconstruction: 57.325193881988525  Train_KL: 2.890487402677536  Validation Loss : 59.08949661254883 Val_Reconstruction : 56.22690010070801 Val_KL : 2.8625969886779785\n","Epoch: 1333/5000  Traning Loss: 60.15079927444458  Train_Reconstruction: 57.25658369064331  Train_KL: 2.8942155241966248  Validation Loss : 59.314029693603516 Val_Reconstruction : 56.440237045288086 Val_KL : 2.8737915754318237\n","Epoch: 1334/5000  Traning Loss: 60.339608669281006  Train_Reconstruction: 57.42923974990845  Train_KL: 2.9103682935237885  Validation Loss : 59.78560256958008 Val_Reconstruction : 56.89350891113281 Val_KL : 2.8920934200286865\n","Epoch: 1335/5000  Traning Loss: 60.263917446136475  Train_Reconstruction: 57.363365173339844  Train_KL: 2.900552213191986  Validation Loss : 59.21857452392578 Val_Reconstruction : 56.34908103942871 Val_KL : 2.8694945573806763\n","Epoch: 1336/5000  Traning Loss: 59.83762216567993  Train_Reconstruction: 56.94324064254761  Train_KL: 2.8943816423416138  Validation Loss : 58.96967697143555 Val_Reconstruction : 56.10947608947754 Val_KL : 2.8602010011672974\n","Epoch: 1337/5000  Traning Loss: 59.787150382995605  Train_Reconstruction: 56.88816738128662  Train_KL: 2.8989830017089844  Validation Loss : 58.850446701049805 Val_Reconstruction : 55.98092842102051 Val_KL : 2.8695188760757446\n","Epoch: 1338/5000  Traning Loss: 59.864444732666016  Train_Reconstruction: 56.95375299453735  Train_KL: 2.9106920063495636  Validation Loss : 58.97504997253418 Val_Reconstruction : 56.10170555114746 Val_KL : 2.8733439445495605\n","Epoch: 1339/5000  Traning Loss: 59.89534521102905  Train_Reconstruction: 56.98956537246704  Train_KL: 2.905779868364334  Validation Loss : 58.93575859069824 Val_Reconstruction : 56.05585289001465 Val_KL : 2.8799049854278564\n","Epoch: 1340/5000  Traning Loss: 59.835700035095215  Train_Reconstruction: 56.9368314743042  Train_KL: 2.8988689482212067  Validation Loss : 58.69390869140625 Val_Reconstruction : 55.825029373168945 Val_KL : 2.868878126144409\n","Epoch: 1341/5000  Traning Loss: 59.719109535217285  Train_Reconstruction: 56.81490898132324  Train_KL: 2.904200106859207  Validation Loss : 58.78469467163086 Val_Reconstruction : 55.89209747314453 Val_KL : 2.89259672164917\n","Epoch: 1342/5000  Traning Loss: 60.12816333770752  Train_Reconstruction: 57.22350883483887  Train_KL: 2.9046542048454285  Validation Loss : 59.38221740722656 Val_Reconstruction : 56.52081108093262 Val_KL : 2.8614068031311035\n","Epoch: 1343/5000  Traning Loss: 59.93271493911743  Train_Reconstruction: 57.027817726135254  Train_KL: 2.904897391796112  Validation Loss : 59.097042083740234 Val_Reconstruction : 56.22515678405762 Val_KL : 2.8718847036361694\n","Epoch: 1344/5000  Traning Loss: 60.088003635406494  Train_Reconstruction: 57.195191383361816  Train_KL: 2.892812430858612  Validation Loss : 59.205549240112305 Val_Reconstruction : 56.34528350830078 Val_KL : 2.8602654933929443\n","Epoch: 1345/5000  Traning Loss: 60.06422281265259  Train_Reconstruction: 57.16892719268799  Train_KL: 2.895295411348343  Validation Loss : 59.168046951293945 Val_Reconstruction : 56.307512283325195 Val_KL : 2.860534191131592\n","Epoch: 1346/5000  Traning Loss: 59.739840507507324  Train_Reconstruction: 56.84027624130249  Train_KL: 2.8995643854141235  Validation Loss : 59.017311096191406 Val_Reconstruction : 56.15249252319336 Val_KL : 2.8648186922073364\n","Epoch: 1347/5000  Traning Loss: 60.09527254104614  Train_Reconstruction: 57.19268751144409  Train_KL: 2.902584582567215  Validation Loss : 59.51516342163086 Val_Reconstruction : 56.6506404876709 Val_KL : 2.8645224571228027\n","Epoch: 1348/5000  Traning Loss: 60.19365882873535  Train_Reconstruction: 57.292428970336914  Train_KL: 2.901229649782181  Validation Loss : 59.282670974731445 Val_Reconstruction : 56.41900825500488 Val_KL : 2.863664150238037\n","Epoch: 1349/5000  Traning Loss: 60.0361590385437  Train_Reconstruction: 57.141119956970215  Train_KL: 2.8950394690036774  Validation Loss : 58.99331092834473 Val_Reconstruction : 56.129085540771484 Val_KL : 2.864226222038269\n","Epoch: 1350/5000  Traning Loss: 60.24973011016846  Train_Reconstruction: 57.35750913619995  Train_KL: 2.89222115278244  Validation Loss : 59.26987266540527 Val_Reconstruction : 56.40872001647949 Val_KL : 2.861153244972229\n","Epoch: 1351/5000  Traning Loss: 60.14320182800293  Train_Reconstruction: 57.243332386016846  Train_KL: 2.8998696506023407  Validation Loss : 59.192678451538086 Val_Reconstruction : 56.321035385131836 Val_KL : 2.8716424703598022\n","Epoch: 1352/5000  Traning Loss: 60.041064739227295  Train_Reconstruction: 57.14330196380615  Train_KL: 2.897762894630432  Validation Loss : 58.77276802062988 Val_Reconstruction : 55.90701866149902 Val_KL : 2.8657491207122803\n","Epoch: 1353/5000  Traning Loss: 59.97545146942139  Train_Reconstruction: 57.07507610321045  Train_KL: 2.900375157594681  Validation Loss : 59.247955322265625 Val_Reconstruction : 56.378957748413086 Val_KL : 2.868996500968933\n","Epoch: 1354/5000  Traning Loss: 59.96831226348877  Train_Reconstruction: 57.06697130203247  Train_KL: 2.9013409316539764  Validation Loss : 59.16402816772461 Val_Reconstruction : 56.30527687072754 Val_KL : 2.858751654624939\n","Epoch: 1355/5000  Traning Loss: 59.90236759185791  Train_Reconstruction: 56.99250078201294  Train_KL: 2.9098668098449707  Validation Loss : 58.80706024169922 Val_Reconstruction : 55.914554595947266 Val_KL : 2.89250648021698\n","Epoch: 1356/5000  Traning Loss: 59.78520107269287  Train_Reconstruction: 56.882147789001465  Train_KL: 2.903053253889084  Validation Loss : 58.811758041381836 Val_Reconstruction : 55.946372985839844 Val_KL : 2.865384101867676\n","Epoch: 1357/5000  Traning Loss: 59.7426700592041  Train_Reconstruction: 56.84508800506592  Train_KL: 2.897582471370697  Validation Loss : 58.77775764465332 Val_Reconstruction : 55.90712928771973 Val_KL : 2.8706283569335938\n","Epoch: 1358/5000  Traning Loss: 59.79143810272217  Train_Reconstruction: 56.88429069519043  Train_KL: 2.9071474969387054  Validation Loss : 58.83426856994629 Val_Reconstruction : 55.95472526550293 Val_KL : 2.879543900489807\n","Epoch: 1359/5000  Traning Loss: 59.77741813659668  Train_Reconstruction: 56.87627029418945  Train_KL: 2.9011475443840027  Validation Loss : 58.71127510070801 Val_Reconstruction : 55.8394889831543 Val_KL : 2.871786117553711\n","Epoch: 1360/5000  Traning Loss: 59.988473892211914  Train_Reconstruction: 57.08658742904663  Train_KL: 2.901887208223343  Validation Loss : 59.263383865356445 Val_Reconstruction : 56.393672943115234 Val_KL : 2.8697112798690796\n","Epoch: 1361/5000  Traning Loss: 59.830501079559326  Train_Reconstruction: 56.931954860687256  Train_KL: 2.8985454738140106  Validation Loss : 59.083608627319336 Val_Reconstruction : 56.20656967163086 Val_KL : 2.877038836479187\n","Epoch: 1362/5000  Traning Loss: 59.92367219924927  Train_Reconstruction: 57.01004695892334  Train_KL: 2.9136259853839874  Validation Loss : 59.601919174194336 Val_Reconstruction : 56.716758728027344 Val_KL : 2.8851596117019653\n","Epoch: 1363/5000  Traning Loss: 60.83113241195679  Train_Reconstruction: 57.915634632110596  Train_KL: 2.9154978692531586  Validation Loss : 60.27852249145508 Val_Reconstruction : 57.3969669342041 Val_KL : 2.881556510925293\n","Epoch: 1364/5000  Traning Loss: 60.62860679626465  Train_Reconstruction: 57.72688341140747  Train_KL: 2.901723802089691  Validation Loss : 59.21391487121582 Val_Reconstruction : 56.33944511413574 Val_KL : 2.874469041824341\n","Epoch: 1365/5000  Traning Loss: 60.02403116226196  Train_Reconstruction: 57.10836744308472  Train_KL: 2.915664106607437  Validation Loss : 58.81658172607422 Val_Reconstruction : 55.93324661254883 Val_KL : 2.883333444595337\n","Epoch: 1366/5000  Traning Loss: 59.76628255844116  Train_Reconstruction: 56.85932111740112  Train_KL: 2.906961441040039  Validation Loss : 59.1087646484375 Val_Reconstruction : 56.21916961669922 Val_KL : 2.88959538936615\n","Epoch: 1367/5000  Traning Loss: 59.89641046524048  Train_Reconstruction: 56.9971137046814  Train_KL: 2.8992972373962402  Validation Loss : 58.99438667297363 Val_Reconstruction : 56.12747383117676 Val_KL : 2.8669129610061646\n","Epoch: 1368/5000  Traning Loss: 59.820006370544434  Train_Reconstruction: 56.91457653045654  Train_KL: 2.9054293036460876  Validation Loss : 58.54251289367676 Val_Reconstruction : 55.661630630493164 Val_KL : 2.8808823823928833\n","Epoch: 1369/5000  Traning Loss: 59.65819072723389  Train_Reconstruction: 56.73978519439697  Train_KL: 2.9184060394763947  Validation Loss : 58.65823173522949 Val_Reconstruction : 55.76392936706543 Val_KL : 2.8943034410476685\n","Epoch: 1370/5000  Traning Loss: 59.68968343734741  Train_Reconstruction: 56.77890205383301  Train_KL: 2.9107813239097595  Validation Loss : 59.007633209228516 Val_Reconstruction : 56.129886627197266 Val_KL : 2.8777475357055664\n","Epoch: 1371/5000  Traning Loss: 59.77775001525879  Train_Reconstruction: 56.88024091720581  Train_KL: 2.897508829832077  Validation Loss : 59.36696434020996 Val_Reconstruction : 56.4935188293457 Val_KL : 2.873444437980652\n","Epoch: 1372/5000  Traning Loss: 59.93412923812866  Train_Reconstruction: 57.035584449768066  Train_KL: 2.8985446989536285  Validation Loss : 58.9293155670166 Val_Reconstruction : 56.077232360839844 Val_KL : 2.8520843982696533\n","Epoch: 1373/5000  Traning Loss: 60.115304946899414  Train_Reconstruction: 57.22195339202881  Train_KL: 2.8933518528938293  Validation Loss : 59.15747261047363 Val_Reconstruction : 56.279781341552734 Val_KL : 2.8776925802230835\n","Epoch: 1374/5000  Traning Loss: 60.04097604751587  Train_Reconstruction: 57.140371799468994  Train_KL: 2.900604009628296  Validation Loss : 59.153337478637695 Val_Reconstruction : 56.282379150390625 Val_KL : 2.8709572553634644\n","Epoch: 1375/5000  Traning Loss: 59.83917999267578  Train_Reconstruction: 56.93664264678955  Train_KL: 2.9025371074676514  Validation Loss : 58.88047218322754 Val_Reconstruction : 56.01336097717285 Val_KL : 2.8671112060546875\n","Epoch: 1376/5000  Traning Loss: 59.67630195617676  Train_Reconstruction: 56.770949363708496  Train_KL: 2.9053524136543274  Validation Loss : 58.497894287109375 Val_Reconstruction : 55.63655471801758 Val_KL : 2.8613393306732178\n","Epoch: 1377/5000  Traning Loss: 59.83446455001831  Train_Reconstruction: 56.92840242385864  Train_KL: 2.9060624539852142  Validation Loss : 58.603891372680664 Val_Reconstruction : 55.729774475097656 Val_KL : 2.874116063117981\n","Epoch: 1378/5000  Traning Loss: 59.9334077835083  Train_Reconstruction: 57.026023864746094  Train_KL: 2.9073828160762787  Validation Loss : 58.748117446899414 Val_Reconstruction : 55.88621711730957 Val_KL : 2.861899971961975\n","Epoch: 1379/5000  Traning Loss: 59.98072910308838  Train_Reconstruction: 57.07778453826904  Train_KL: 2.9029451310634613  Validation Loss : 58.95712471008301 Val_Reconstruction : 56.07966995239258 Val_KL : 2.8774538040161133\n","Epoch: 1380/5000  Traning Loss: 59.73627853393555  Train_Reconstruction: 56.83243799209595  Train_KL: 2.9038410782814026  Validation Loss : 58.74879455566406 Val_Reconstruction : 55.87261962890625 Val_KL : 2.8761757612228394\n","Epoch: 1381/5000  Traning Loss: 59.924880504608154  Train_Reconstruction: 57.018890380859375  Train_KL: 2.9059896767139435  Validation Loss : 59.35408020019531 Val_Reconstruction : 56.472543716430664 Val_KL : 2.881537079811096\n","Epoch: 1382/5000  Traning Loss: 60.613938331604004  Train_Reconstruction: 57.70574998855591  Train_KL: 2.9081884026527405  Validation Loss : 59.93259048461914 Val_Reconstruction : 57.06801986694336 Val_KL : 2.8645708560943604\n","Epoch: 1383/5000  Traning Loss: 60.59571552276611  Train_Reconstruction: 57.70552349090576  Train_KL: 2.8901918828487396  Validation Loss : 59.30906105041504 Val_Reconstruction : 56.46966361999512 Val_KL : 2.8393977880477905\n","Epoch: 1384/5000  Traning Loss: 60.117520809173584  Train_Reconstruction: 57.22176551818848  Train_KL: 2.8957552015781403  Validation Loss : 59.75831985473633 Val_Reconstruction : 56.90088081359863 Val_KL : 2.8574399948120117\n","Epoch: 1385/5000  Traning Loss: 60.04655408859253  Train_Reconstruction: 57.13913869857788  Train_KL: 2.907415509223938  Validation Loss : 59.06628608703613 Val_Reconstruction : 56.196346282958984 Val_KL : 2.869940161705017\n","Epoch: 1386/5000  Traning Loss: 60.331496238708496  Train_Reconstruction: 57.43583965301514  Train_KL: 2.895657032728195  Validation Loss : 59.52391242980957 Val_Reconstruction : 56.659664154052734 Val_KL : 2.8642479181289673\n","Epoch: 1387/5000  Traning Loss: 60.299870014190674  Train_Reconstruction: 57.40671920776367  Train_KL: 2.8931508660316467  Validation Loss : 59.21720504760742 Val_Reconstruction : 56.34796333312988 Val_KL : 2.8692420721054077\n","Epoch: 1388/5000  Traning Loss: 59.75908088684082  Train_Reconstruction: 56.86117601394653  Train_KL: 2.8979051411151886  Validation Loss : 58.740787506103516 Val_Reconstruction : 55.88587951660156 Val_KL : 2.854907751083374\n","Epoch: 1389/5000  Traning Loss: 59.61085224151611  Train_Reconstruction: 56.71949100494385  Train_KL: 2.8913606703281403  Validation Loss : 58.88446617126465 Val_Reconstruction : 56.027692794799805 Val_KL : 2.8567744493484497\n","Epoch: 1390/5000  Traning Loss: 59.854785442352295  Train_Reconstruction: 56.950846672058105  Train_KL: 2.9039385616779327  Validation Loss : 59.58754920959473 Val_Reconstruction : 56.71478080749512 Val_KL : 2.8727691173553467\n","Epoch: 1391/5000  Traning Loss: 60.08224105834961  Train_Reconstruction: 57.17955923080444  Train_KL: 2.9026817679405212  Validation Loss : 59.221933364868164 Val_Reconstruction : 56.35967826843262 Val_KL : 2.862255811691284\n","Epoch: 1392/5000  Traning Loss: 60.08167314529419  Train_Reconstruction: 57.18091344833374  Train_KL: 2.900759279727936  Validation Loss : 59.60830116271973 Val_Reconstruction : 56.7420768737793 Val_KL : 2.866223692893982\n","Epoch: 1393/5000  Traning Loss: 60.084221839904785  Train_Reconstruction: 57.181042194366455  Train_KL: 2.903179883956909  Validation Loss : 59.49137496948242 Val_Reconstruction : 56.631507873535156 Val_KL : 2.8598662614822388\n","Epoch: 1394/5000  Traning Loss: 60.01621770858765  Train_Reconstruction: 57.10666513442993  Train_KL: 2.9095529913902283  Validation Loss : 59.2009334564209 Val_Reconstruction : 56.32610893249512 Val_KL : 2.874823212623596\n","Epoch: 1395/5000  Traning Loss: 60.01942253112793  Train_Reconstruction: 57.12552356719971  Train_KL: 2.893898665904999  Validation Loss : 59.177921295166016 Val_Reconstruction : 56.32918739318848 Val_KL : 2.848734974861145\n","Epoch: 1396/5000  Traning Loss: 59.90306806564331  Train_Reconstruction: 57.00836229324341  Train_KL: 2.8947059214115143  Validation Loss : 59.103689193725586 Val_Reconstruction : 56.23727989196777 Val_KL : 2.866408944129944\n","Epoch: 1397/5000  Traning Loss: 59.89453172683716  Train_Reconstruction: 56.99042463302612  Train_KL: 2.9041072726249695  Validation Loss : 58.824161529541016 Val_Reconstruction : 55.96298027038574 Val_KL : 2.861180305480957\n","Epoch: 1398/5000  Traning Loss: 59.6898136138916  Train_Reconstruction: 56.793580055236816  Train_KL: 2.896234005689621  Validation Loss : 58.772865295410156 Val_Reconstruction : 55.91812324523926 Val_KL : 2.8547409772872925\n","Epoch: 1399/5000  Traning Loss: 59.75422477722168  Train_Reconstruction: 56.865055084228516  Train_KL: 2.8891695141792297  Validation Loss : 58.962074279785156 Val_Reconstruction : 56.09772491455078 Val_KL : 2.864350438117981\n","Epoch: 1400/5000  Traning Loss: 59.87754726409912  Train_Reconstruction: 56.98739290237427  Train_KL: 2.890154153108597  Validation Loss : 59.073617935180664 Val_Reconstruction : 56.21306228637695 Val_KL : 2.8605560064315796\n","Epoch: 1401/5000  Traning Loss: 59.94884395599365  Train_Reconstruction: 57.046592712402344  Train_KL: 2.9022513031959534  Validation Loss : 59.14516639709473 Val_Reconstruction : 56.27318000793457 Val_KL : 2.871986985206604\n","Epoch: 1402/5000  Traning Loss: 59.80045461654663  Train_Reconstruction: 56.894280433654785  Train_KL: 2.9061743915081024  Validation Loss : 58.75576591491699 Val_Reconstruction : 55.89629364013672 Val_KL : 2.859472632408142\n","Epoch: 1403/5000  Traning Loss: 59.54177808761597  Train_Reconstruction: 56.65246868133545  Train_KL: 2.8893091082572937  Validation Loss : 58.55145072937012 Val_Reconstruction : 55.69038772583008 Val_KL : 2.861062526702881\n","Epoch: 1404/5000  Traning Loss: 59.7451434135437  Train_Reconstruction: 56.8285493850708  Train_KL: 2.916594237089157  Validation Loss : 58.898569107055664 Val_Reconstruction : 56.01077651977539 Val_KL : 2.8877910375595093\n","Epoch: 1405/5000  Traning Loss: 59.71405601501465  Train_Reconstruction: 56.80941820144653  Train_KL: 2.904637634754181  Validation Loss : 58.706830978393555 Val_Reconstruction : 55.835798263549805 Val_KL : 2.871033549308777\n","Epoch: 1406/5000  Traning Loss: 59.652759075164795  Train_Reconstruction: 56.74336814880371  Train_KL: 2.909390926361084  Validation Loss : 58.763671875 Val_Reconstruction : 55.881582260131836 Val_KL : 2.8820894956588745\n","Epoch: 1407/5000  Traning Loss: 59.77679634094238  Train_Reconstruction: 56.86737871170044  Train_KL: 2.909417986869812  Validation Loss : 59.035682678222656 Val_Reconstruction : 56.16390419006348 Val_KL : 2.871777892112732\n","Epoch: 1408/5000  Traning Loss: 59.609525203704834  Train_Reconstruction: 56.7082405090332  Train_KL: 2.9012844562530518  Validation Loss : 58.71877861022949 Val_Reconstruction : 55.8470344543457 Val_KL : 2.8717451095581055\n","Epoch: 1409/5000  Traning Loss: 59.69772672653198  Train_Reconstruction: 56.78464889526367  Train_KL: 2.9130782186985016  Validation Loss : 58.95108604431152 Val_Reconstruction : 56.066322326660156 Val_KL : 2.8847641944885254\n","Epoch: 1410/5000  Traning Loss: 59.68325853347778  Train_Reconstruction: 56.78166723251343  Train_KL: 2.9015911519527435  Validation Loss : 58.705617904663086 Val_Reconstruction : 55.83009147644043 Val_KL : 2.8755266666412354\n","Epoch: 1411/5000  Traning Loss: 59.54064989089966  Train_Reconstruction: 56.635053634643555  Train_KL: 2.905595988035202  Validation Loss : 58.5162296295166 Val_Reconstruction : 55.64949035644531 Val_KL : 2.866737961769104\n","Epoch: 1412/5000  Traning Loss: 59.6413140296936  Train_Reconstruction: 56.74404716491699  Train_KL: 2.89726659655571  Validation Loss : 59.093034744262695 Val_Reconstruction : 56.23386573791504 Val_KL : 2.859169363975525\n","Epoch: 1413/5000  Traning Loss: 60.07225751876831  Train_Reconstruction: 57.169944763183594  Train_KL: 2.902313083410263  Validation Loss : 58.91848945617676 Val_Reconstruction : 56.047019958496094 Val_KL : 2.871470808982849\n","Epoch: 1414/5000  Traning Loss: 59.814937591552734  Train_Reconstruction: 56.91295528411865  Train_KL: 2.901982456445694  Validation Loss : 58.466453552246094 Val_Reconstruction : 55.59936714172363 Val_KL : 2.867085099220276\n","Epoch: 1415/5000  Traning Loss: 59.65417146682739  Train_Reconstruction: 56.75278854370117  Train_KL: 2.90138241648674  Validation Loss : 58.680253982543945 Val_Reconstruction : 55.81845474243164 Val_KL : 2.8618006706237793\n","Epoch: 1416/5000  Traning Loss: 59.78776025772095  Train_Reconstruction: 56.88258934020996  Train_KL: 2.905170828104019  Validation Loss : 58.99124717712402 Val_Reconstruction : 56.11800193786621 Val_KL : 2.8732460737228394\n","Epoch: 1417/5000  Traning Loss: 59.90239334106445  Train_Reconstruction: 56.99368619918823  Train_KL: 2.9087076783180237  Validation Loss : 58.90034484863281 Val_Reconstruction : 56.02967834472656 Val_KL : 2.870666265487671\n","Epoch: 1418/5000  Traning Loss: 60.095139503479004  Train_Reconstruction: 57.1913161277771  Train_KL: 2.9038231670856476  Validation Loss : 59.228424072265625 Val_Reconstruction : 56.34400749206543 Val_KL : 2.884416699409485\n","Epoch: 1419/5000  Traning Loss: 59.834819316864014  Train_Reconstruction: 56.92675161361694  Train_KL: 2.9080678820610046  Validation Loss : 58.730674743652344 Val_Reconstruction : 55.85858154296875 Val_KL : 2.8720918893814087\n","Epoch: 1420/5000  Traning Loss: 59.70524215698242  Train_Reconstruction: 56.7958722114563  Train_KL: 2.90937003493309  Validation Loss : 58.72159957885742 Val_Reconstruction : 55.83438301086426 Val_KL : 2.8872172832489014\n","Epoch: 1421/5000  Traning Loss: 59.65095520019531  Train_Reconstruction: 56.74492645263672  Train_KL: 2.9060281217098236  Validation Loss : 58.621904373168945 Val_Reconstruction : 55.76018142700195 Val_KL : 2.8617221117019653\n","Epoch: 1422/5000  Traning Loss: 59.564274311065674  Train_Reconstruction: 56.674046993255615  Train_KL: 2.8902265429496765  Validation Loss : 58.52828788757324 Val_Reconstruction : 55.67731285095215 Val_KL : 2.850975275039673\n","Epoch: 1423/5000  Traning Loss: 59.538817405700684  Train_Reconstruction: 56.647119998931885  Train_KL: 2.891697585582733  Validation Loss : 58.765708923339844 Val_Reconstruction : 55.89091491699219 Val_KL : 2.8747947216033936\n","Epoch: 1424/5000  Traning Loss: 59.60878801345825  Train_Reconstruction: 56.69327735900879  Train_KL: 2.9155111014842987  Validation Loss : 58.614484786987305 Val_Reconstruction : 55.737571716308594 Val_KL : 2.876913070678711\n","Epoch: 1425/5000  Traning Loss: 59.70651721954346  Train_Reconstruction: 56.79873037338257  Train_KL: 2.9077871441841125  Validation Loss : 58.6920280456543 Val_Reconstruction : 55.807809829711914 Val_KL : 2.8842188119888306\n","Epoch: 1426/5000  Traning Loss: 59.84681415557861  Train_Reconstruction: 56.93556594848633  Train_KL: 2.9112481474876404  Validation Loss : 59.109846115112305 Val_Reconstruction : 56.23135185241699 Val_KL : 2.878493547439575\n","Epoch: 1427/5000  Traning Loss: 60.00390148162842  Train_Reconstruction: 57.09920263290405  Train_KL: 2.9046983420848846  Validation Loss : 59.40781021118164 Val_Reconstruction : 56.53121757507324 Val_KL : 2.8765934705734253\n","Epoch: 1428/5000  Traning Loss: 59.91899061203003  Train_Reconstruction: 57.017394065856934  Train_KL: 2.9015961289405823  Validation Loss : 59.17030715942383 Val_Reconstruction : 56.314035415649414 Val_KL : 2.856271982192993\n","Epoch: 1429/5000  Traning Loss: 59.74113893508911  Train_Reconstruction: 56.84177494049072  Train_KL: 2.899364560842514  Validation Loss : 59.03934860229492 Val_Reconstruction : 56.167036056518555 Val_KL : 2.8723132610321045\n","Epoch: 1430/5000  Traning Loss: 59.798166275024414  Train_Reconstruction: 56.89433431625366  Train_KL: 2.90383243560791  Validation Loss : 58.93839454650879 Val_Reconstruction : 56.0510196685791 Val_KL : 2.8873759508132935\n","Epoch: 1431/5000  Traning Loss: 59.5912880897522  Train_Reconstruction: 56.67818880081177  Train_KL: 2.9130991995334625  Validation Loss : 58.59767150878906 Val_Reconstruction : 55.71955680847168 Val_KL : 2.8781150579452515\n","Epoch: 1432/5000  Traning Loss: 59.501203536987305  Train_Reconstruction: 56.59016036987305  Train_KL: 2.911042958498001  Validation Loss : 58.66198539733887 Val_Reconstruction : 55.78457069396973 Val_KL : 2.877413749694824\n","Epoch: 1433/5000  Traning Loss: 59.50842618942261  Train_Reconstruction: 56.59671592712402  Train_KL: 2.911710411310196  Validation Loss : 58.62962341308594 Val_Reconstruction : 55.74543762207031 Val_KL : 2.8841843605041504\n","Epoch: 1434/5000  Traning Loss: 59.54710006713867  Train_Reconstruction: 56.626588344573975  Train_KL: 2.9205115735530853  Validation Loss : 58.719377517700195 Val_Reconstruction : 55.84185218811035 Val_KL : 2.877525568008423\n","Epoch: 1435/5000  Traning Loss: 59.63029718399048  Train_Reconstruction: 56.72083854675293  Train_KL: 2.909458637237549  Validation Loss : 58.80571937561035 Val_Reconstruction : 55.92641830444336 Val_KL : 2.879300355911255\n","Epoch: 1436/5000  Traning Loss: 59.52505874633789  Train_Reconstruction: 56.63186502456665  Train_KL: 2.893193870782852  Validation Loss : 58.56689262390137 Val_Reconstruction : 55.7121696472168 Val_KL : 2.854721784591675\n","Epoch: 1437/5000  Traning Loss: 59.590559005737305  Train_Reconstruction: 56.7010293006897  Train_KL: 2.889529287815094  Validation Loss : 58.852394104003906 Val_Reconstruction : 55.99542427062988 Val_KL : 2.8569709062576294\n","Epoch: 1438/5000  Traning Loss: 59.647329330444336  Train_Reconstruction: 56.74156951904297  Train_KL: 2.9057593047618866  Validation Loss : 58.55900764465332 Val_Reconstruction : 55.67755126953125 Val_KL : 2.8814574480056763\n","Epoch: 1439/5000  Traning Loss: 59.546791553497314  Train_Reconstruction: 56.633487701416016  Train_KL: 2.913303643465042  Validation Loss : 58.60237693786621 Val_Reconstruction : 55.727970123291016 Val_KL : 2.8744059801101685\n","Epoch: 1440/5000  Traning Loss: 59.74947738647461  Train_Reconstruction: 56.8330192565918  Train_KL: 2.9164575934410095  Validation Loss : 59.22746658325195 Val_Reconstruction : 56.351661682128906 Val_KL : 2.8758044242858887\n","Epoch: 1441/5000  Traning Loss: 59.73949146270752  Train_Reconstruction: 56.83406972885132  Train_KL: 2.905421644449234  Validation Loss : 58.642974853515625 Val_Reconstruction : 55.771305084228516 Val_KL : 2.8716708421707153\n","Epoch: 1442/5000  Traning Loss: 59.77355241775513  Train_Reconstruction: 56.875693798065186  Train_KL: 2.897858500480652  Validation Loss : 58.91431999206543 Val_Reconstruction : 56.04269027709961 Val_KL : 2.8716301918029785\n","Epoch: 1443/5000  Traning Loss: 59.860840797424316  Train_Reconstruction: 56.95543622970581  Train_KL: 2.9054047763347626  Validation Loss : 58.689697265625 Val_Reconstruction : 55.81732177734375 Val_KL : 2.8723748922348022\n","Epoch: 1444/5000  Traning Loss: 60.10901594161987  Train_Reconstruction: 57.201146602630615  Train_KL: 2.907869517803192  Validation Loss : 59.88229751586914 Val_Reconstruction : 57.00839424133301 Val_KL : 2.8739043474197388\n","Epoch: 1445/5000  Traning Loss: 60.20143795013428  Train_Reconstruction: 57.3114218711853  Train_KL: 2.8900162875652313  Validation Loss : 59.16662788391113 Val_Reconstruction : 56.31247901916504 Val_KL : 2.8541488647460938\n","Epoch: 1446/5000  Traning Loss: 59.76669931411743  Train_Reconstruction: 56.86036920547485  Train_KL: 2.9063296616077423  Validation Loss : 59.0198917388916 Val_Reconstruction : 56.13008117675781 Val_KL : 2.8898104429244995\n","Epoch: 1447/5000  Traning Loss: 59.760963439941406  Train_Reconstruction: 56.85236215591431  Train_KL: 2.908601939678192  Validation Loss : 58.645769119262695 Val_Reconstruction : 55.7707405090332 Val_KL : 2.875028610229492\n","Epoch: 1448/5000  Traning Loss: 59.533382415771484  Train_Reconstruction: 56.62521553039551  Train_KL: 2.908167064189911  Validation Loss : 58.65977668762207 Val_Reconstruction : 55.78463363647461 Val_KL : 2.8751437664031982\n","Epoch: 1449/5000  Traning Loss: 59.49255418777466  Train_Reconstruction: 56.586998462677  Train_KL: 2.905555844306946  Validation Loss : 58.84381103515625 Val_Reconstruction : 55.96502113342285 Val_KL : 2.8787901401519775\n","Epoch: 1450/5000  Traning Loss: 59.63408279418945  Train_Reconstruction: 56.73664712905884  Train_KL: 2.897436261177063  Validation Loss : 58.463130950927734 Val_Reconstruction : 55.59872627258301 Val_KL : 2.864404559135437\n","Epoch: 1451/5000  Traning Loss: 59.575077056884766  Train_Reconstruction: 56.67215299606323  Train_KL: 2.902923971414566  Validation Loss : 58.87611961364746 Val_Reconstruction : 56.01392936706543 Val_KL : 2.8621902465820312\n","Epoch: 1452/5000  Traning Loss: 59.73780965805054  Train_Reconstruction: 56.82230186462402  Train_KL: 2.9155080020427704  Validation Loss : 58.96225166320801 Val_Reconstruction : 56.074140548706055 Val_KL : 2.8881096839904785\n","Epoch: 1453/5000  Traning Loss: 59.839824199676514  Train_Reconstruction: 56.919840812683105  Train_KL: 2.919983386993408  Validation Loss : 58.69917297363281 Val_Reconstruction : 55.816864013671875 Val_KL : 2.8823084831237793\n","Epoch: 1454/5000  Traning Loss: 59.7873592376709  Train_Reconstruction: 56.854347705841064  Train_KL: 2.9330121874809265  Validation Loss : 58.707401275634766 Val_Reconstruction : 55.81588363647461 Val_KL : 2.8915181159973145\n","Epoch: 1455/5000  Traning Loss: 59.60018253326416  Train_Reconstruction: 56.68708896636963  Train_KL: 2.913093864917755  Validation Loss : 58.84881591796875 Val_Reconstruction : 55.96704864501953 Val_KL : 2.8817676305770874\n","Epoch: 1456/5000  Traning Loss: 59.718531131744385  Train_Reconstruction: 56.820207595825195  Train_KL: 2.8983235359191895  Validation Loss : 58.77592658996582 Val_Reconstruction : 55.913639068603516 Val_KL : 2.8622876405715942\n","Epoch: 1457/5000  Traning Loss: 59.98097848892212  Train_Reconstruction: 57.07869815826416  Train_KL: 2.9022804498672485  Validation Loss : 59.561628341674805 Val_Reconstruction : 56.691152572631836 Val_KL : 2.8704763650894165\n","Epoch: 1458/5000  Traning Loss: 60.0033655166626  Train_Reconstruction: 57.09538793563843  Train_KL: 2.907977879047394  Validation Loss : 59.285186767578125 Val_Reconstruction : 56.40908432006836 Val_KL : 2.876103639602661\n","Epoch: 1459/5000  Traning Loss: 59.91847276687622  Train_Reconstruction: 57.02120351791382  Train_KL: 2.897269517183304  Validation Loss : 58.8489990234375 Val_Reconstruction : 55.9751091003418 Val_KL : 2.873889923095703\n","Epoch: 1460/5000  Traning Loss: 59.62448263168335  Train_Reconstruction: 56.700669288635254  Train_KL: 2.9238131940364838  Validation Loss : 58.56730651855469 Val_Reconstruction : 55.684722900390625 Val_KL : 2.882584810256958\n","Epoch: 1461/5000  Traning Loss: 59.46559429168701  Train_Reconstruction: 56.57187032699585  Train_KL: 2.8937245309352875  Validation Loss : 58.397239685058594 Val_Reconstruction : 55.54250526428223 Val_KL : 2.8547346591949463\n","Epoch: 1462/5000  Traning Loss: 59.45672035217285  Train_Reconstruction: 56.553656578063965  Train_KL: 2.90306356549263  Validation Loss : 58.52932929992676 Val_Reconstruction : 55.65440368652344 Val_KL : 2.8749263286590576\n","Epoch: 1463/5000  Traning Loss: 59.34660768508911  Train_Reconstruction: 56.43963289260864  Train_KL: 2.906974107027054  Validation Loss : 58.65707206726074 Val_Reconstruction : 55.78297424316406 Val_KL : 2.874098062515259\n","Epoch: 1464/5000  Traning Loss: 59.35669755935669  Train_Reconstruction: 56.45229387283325  Train_KL: 2.9044039249420166  Validation Loss : 58.47717094421387 Val_Reconstruction : 55.59587097167969 Val_KL : 2.8813010454177856\n","Epoch: 1465/5000  Traning Loss: 59.41448163986206  Train_Reconstruction: 56.50998401641846  Train_KL: 2.9044973254203796  Validation Loss : 58.571367263793945 Val_Reconstruction : 55.70393180847168 Val_KL : 2.8674354553222656\n","Epoch: 1466/5000  Traning Loss: 59.57590675354004  Train_Reconstruction: 56.669677734375  Train_KL: 2.9062288105487823  Validation Loss : 58.658620834350586 Val_Reconstruction : 55.78544998168945 Val_KL : 2.873170495033264\n","Epoch: 1467/5000  Traning Loss: 59.59470224380493  Train_Reconstruction: 56.68528604507446  Train_KL: 2.9094164967536926  Validation Loss : 58.51556396484375 Val_Reconstruction : 55.6522159576416 Val_KL : 2.8633477687835693\n","Epoch: 1468/5000  Traning Loss: 59.65136528015137  Train_Reconstruction: 56.74941110610962  Train_KL: 2.9019543528556824  Validation Loss : 58.84636116027832 Val_Reconstruction : 55.976661682128906 Val_KL : 2.8697009086608887\n","Epoch: 1469/5000  Traning Loss: 59.88802909851074  Train_Reconstruction: 56.98439264297485  Train_KL: 2.903636157512665  Validation Loss : 58.923025131225586 Val_Reconstruction : 56.05866050720215 Val_KL : 2.864364266395569\n","Epoch: 1470/5000  Traning Loss: 59.86562967300415  Train_Reconstruction: 56.96535396575928  Train_KL: 2.9002759754657745  Validation Loss : 59.131635665893555 Val_Reconstruction : 56.25976371765137 Val_KL : 2.871870517730713\n","Epoch: 1471/5000  Traning Loss: 59.921706199645996  Train_Reconstruction: 57.01293706893921  Train_KL: 2.908768802881241  Validation Loss : 58.987693786621094 Val_Reconstruction : 56.101566314697266 Val_KL : 2.8861289024353027\n","Epoch: 1472/5000  Traning Loss: 59.470715045928955  Train_Reconstruction: 56.55404996871948  Train_KL: 2.916664808988571  Validation Loss : 58.55643844604492 Val_Reconstruction : 55.674997329711914 Val_KL : 2.881439685821533\n","Epoch: 1473/5000  Traning Loss: 59.4170298576355  Train_Reconstruction: 56.51493787765503  Train_KL: 2.9020926356315613  Validation Loss : 58.74564170837402 Val_Reconstruction : 55.875905990600586 Val_KL : 2.869734764099121\n","Epoch: 1474/5000  Traning Loss: 59.42568778991699  Train_Reconstruction: 56.513248920440674  Train_KL: 2.912438452243805  Validation Loss : 58.56926918029785 Val_Reconstruction : 55.67092514038086 Val_KL : 2.8983429670333862\n","Epoch: 1475/5000  Traning Loss: 59.335607051849365  Train_Reconstruction: 56.419944763183594  Train_KL: 2.9156622886657715  Validation Loss : 58.62951850891113 Val_Reconstruction : 55.75181770324707 Val_KL : 2.877700924873352\n","Epoch: 1476/5000  Traning Loss: 59.391629219055176  Train_Reconstruction: 56.50123357772827  Train_KL: 2.8903958201408386  Validation Loss : 58.54747772216797 Val_Reconstruction : 55.691322326660156 Val_KL : 2.85615611076355\n","Epoch: 1477/5000  Traning Loss: 59.54584360122681  Train_Reconstruction: 56.645272731781006  Train_KL: 2.9005707502365112  Validation Loss : 59.18809509277344 Val_Reconstruction : 56.32247352600098 Val_KL : 2.8656208515167236\n","Epoch: 1478/5000  Traning Loss: 60.03663396835327  Train_Reconstruction: 57.14438343048096  Train_KL: 2.8922504484653473  Validation Loss : 59.44302940368652 Val_Reconstruction : 56.56951141357422 Val_KL : 2.8735191822052\n","Epoch: 1479/5000  Traning Loss: 60.02140188217163  Train_Reconstruction: 57.10900115966797  Train_KL: 2.9124012887477875  Validation Loss : 59.00446128845215 Val_Reconstruction : 56.1181755065918 Val_KL : 2.8862868547439575\n","Epoch: 1480/5000  Traning Loss: 59.66596221923828  Train_Reconstruction: 56.74558448791504  Train_KL: 2.9203774631023407  Validation Loss : 58.651390075683594 Val_Reconstruction : 55.77252960205078 Val_KL : 2.8788599967956543\n","Epoch: 1481/5000  Traning Loss: 59.52695178985596  Train_Reconstruction: 56.619454860687256  Train_KL: 2.9074977040290833  Validation Loss : 58.971397399902344 Val_Reconstruction : 56.087228775024414 Val_KL : 2.8841683864593506\n","Epoch: 1482/5000  Traning Loss: 59.469523429870605  Train_Reconstruction: 56.55982065200806  Train_KL: 2.90970242023468  Validation Loss : 58.48204040527344 Val_Reconstruction : 55.610107421875 Val_KL : 2.8719327449798584\n","Epoch: 1483/5000  Traning Loss: 59.60539960861206  Train_Reconstruction: 56.69795799255371  Train_KL: 2.907441794872284  Validation Loss : 58.84493637084961 Val_Reconstruction : 55.97837448120117 Val_KL : 2.866560697555542\n","Epoch: 1484/5000  Traning Loss: 59.54856586456299  Train_Reconstruction: 56.661749839782715  Train_KL: 2.8868159651756287  Validation Loss : 58.691802978515625 Val_Reconstruction : 55.83115196228027 Val_KL : 2.8606523275375366\n","Epoch: 1485/5000  Traning Loss: 59.532421588897705  Train_Reconstruction: 56.63082027435303  Train_KL: 2.901601880788803  Validation Loss : 58.60917282104492 Val_Reconstruction : 55.724395751953125 Val_KL : 2.8847771883010864\n","Epoch: 1486/5000  Traning Loss: 59.58056354522705  Train_Reconstruction: 56.68120765686035  Train_KL: 2.8993561267852783  Validation Loss : 59.12334442138672 Val_Reconstruction : 56.249006271362305 Val_KL : 2.874338746070862\n","Epoch: 1487/5000  Traning Loss: 59.67077302932739  Train_Reconstruction: 56.76526975631714  Train_KL: 2.9055035412311554  Validation Loss : 58.74796485900879 Val_Reconstruction : 55.86580848693848 Val_KL : 2.8821566104888916\n","Epoch: 1488/5000  Traning Loss: 59.37700033187866  Train_Reconstruction: 56.46446466445923  Train_KL: 2.912536144256592  Validation Loss : 58.6130428314209 Val_Reconstruction : 55.74566650390625 Val_KL : 2.8673757314682007\n","Epoch: 1489/5000  Traning Loss: 59.4789080619812  Train_Reconstruction: 56.56287145614624  Train_KL: 2.916036695241928  Validation Loss : 58.863037109375 Val_Reconstruction : 55.973697662353516 Val_KL : 2.8893386125564575\n","Epoch: 1490/5000  Traning Loss: 59.540064334869385  Train_Reconstruction: 56.637619495391846  Train_KL: 2.9024445712566376  Validation Loss : 58.876752853393555 Val_Reconstruction : 56.01473426818848 Val_KL : 2.8620195388793945\n","Epoch: 1491/5000  Traning Loss: 59.982038497924805  Train_Reconstruction: 57.07151794433594  Train_KL: 2.910520374774933  Validation Loss : 59.09800910949707 Val_Reconstruction : 56.21849250793457 Val_KL : 2.879517078399658\n","Epoch: 1492/5000  Traning Loss: 59.97112989425659  Train_Reconstruction: 57.052181243896484  Train_KL: 2.918948382139206  Validation Loss : 59.11660957336426 Val_Reconstruction : 56.22127342224121 Val_KL : 2.8953356742858887\n","Epoch: 1493/5000  Traning Loss: 59.89653921127319  Train_Reconstruction: 56.978092193603516  Train_KL: 2.9184473752975464  Validation Loss : 59.04986000061035 Val_Reconstruction : 56.1626033782959 Val_KL : 2.887256383895874\n","Epoch: 1494/5000  Traning Loss: 59.78361368179321  Train_Reconstruction: 56.87969493865967  Train_KL: 2.9039189219474792  Validation Loss : 58.70269966125488 Val_Reconstruction : 55.83260917663574 Val_KL : 2.870089888572693\n","Epoch: 1495/5000  Traning Loss: 59.63821029663086  Train_Reconstruction: 56.73606300354004  Train_KL: 2.9021472930908203  Validation Loss : 58.69241142272949 Val_Reconstruction : 55.82461929321289 Val_KL : 2.8677914142608643\n","Epoch: 1496/5000  Traning Loss: 59.376402378082275  Train_Reconstruction: 56.4668025970459  Train_KL: 2.9096002876758575  Validation Loss : 58.416053771972656 Val_Reconstruction : 55.532630920410156 Val_KL : 2.8834221363067627\n","Epoch: 1497/5000  Traning Loss: 59.44619035720825  Train_Reconstruction: 56.53754806518555  Train_KL: 2.9086423218250275  Validation Loss : 58.40281295776367 Val_Reconstruction : 55.53599548339844 Val_KL : 2.8668168783187866\n","Epoch: 1498/5000  Traning Loss: 59.260887145996094  Train_Reconstruction: 56.36514616012573  Train_KL: 2.8957411348819733  Validation Loss : 58.25852012634277 Val_Reconstruction : 55.38945007324219 Val_KL : 2.869070529937744\n","Epoch: 1499/5000  Traning Loss: 59.276729106903076  Train_Reconstruction: 56.35977125167847  Train_KL: 2.9169585704803467  Validation Loss : 58.43022918701172 Val_Reconstruction : 55.541168212890625 Val_KL : 2.8890610933303833\n","Epoch: 1500/5000  Traning Loss: 59.5447039604187  Train_Reconstruction: 56.632429122924805  Train_KL: 2.912274807691574  Validation Loss : 58.83824920654297 Val_Reconstruction : 55.9654598236084 Val_KL : 2.872788906097412\n","Epoch: 1501/5000  Traning Loss: 59.61783456802368  Train_Reconstruction: 56.71067142486572  Train_KL: 2.9071635007858276  Validation Loss : 58.64408302307129 Val_Reconstruction : 55.76041793823242 Val_KL : 2.883665680885315\n","Epoch: 1502/5000  Traning Loss: 59.71067762374878  Train_Reconstruction: 56.795498847961426  Train_KL: 2.9151788651943207  Validation Loss : 58.80386161804199 Val_Reconstruction : 55.93999099731445 Val_KL : 2.8638713359832764\n","Epoch: 1503/5000  Traning Loss: 59.49258613586426  Train_Reconstruction: 56.597187519073486  Train_KL: 2.8953984081745148  Validation Loss : 58.44988822937012 Val_Reconstruction : 55.59501266479492 Val_KL : 2.854875683784485\n","Epoch: 1504/5000  Traning Loss: 59.41194295883179  Train_Reconstruction: 56.513296604156494  Train_KL: 2.8986468613147736  Validation Loss : 58.48820686340332 Val_Reconstruction : 55.62043571472168 Val_KL : 2.8677709102630615\n","Epoch: 1505/5000  Traning Loss: 59.42026424407959  Train_Reconstruction: 56.5076789855957  Train_KL: 2.9125847816467285  Validation Loss : 58.696603775024414 Val_Reconstruction : 55.81381797790527 Val_KL : 2.8827871084213257\n","Epoch: 1506/5000  Traning Loss: 59.42643594741821  Train_Reconstruction: 56.522647857666016  Train_KL: 2.9037880301475525  Validation Loss : 58.522470474243164 Val_Reconstruction : 55.645652770996094 Val_KL : 2.876818060874939\n","Epoch: 1507/5000  Traning Loss: 59.507572174072266  Train_Reconstruction: 56.59387159347534  Train_KL: 2.9137010872364044  Validation Loss : 58.62319374084473 Val_Reconstruction : 55.73947715759277 Val_KL : 2.883716106414795\n","Epoch: 1508/5000  Traning Loss: 59.52188777923584  Train_Reconstruction: 56.61498975753784  Train_KL: 2.906898319721222  Validation Loss : 58.69217109680176 Val_Reconstruction : 55.82763862609863 Val_KL : 2.864532709121704\n","Epoch: 1509/5000  Traning Loss: 59.47696876525879  Train_Reconstruction: 56.58108377456665  Train_KL: 2.8958847522735596  Validation Loss : 58.550872802734375 Val_Reconstruction : 55.68632888793945 Val_KL : 2.8645424842834473\n","Epoch: 1510/5000  Traning Loss: 59.45000505447388  Train_Reconstruction: 56.548255920410156  Train_KL: 2.9017491936683655  Validation Loss : 58.51633262634277 Val_Reconstruction : 55.63867950439453 Val_KL : 2.8776543140411377\n","Epoch: 1511/5000  Traning Loss: 59.83290767669678  Train_Reconstruction: 56.930152893066406  Train_KL: 2.90275502204895  Validation Loss : 58.71257781982422 Val_Reconstruction : 55.83364677429199 Val_KL : 2.8789305686950684\n","Epoch: 1512/5000  Traning Loss: 59.664483070373535  Train_Reconstruction: 56.76204299926758  Train_KL: 2.9024400115013123  Validation Loss : 58.653512954711914 Val_Reconstruction : 55.786102294921875 Val_KL : 2.8674107789993286\n","Epoch: 1513/5000  Traning Loss: 59.558509826660156  Train_Reconstruction: 56.66985607147217  Train_KL: 2.888653814792633  Validation Loss : 58.90397644042969 Val_Reconstruction : 56.0463924407959 Val_KL : 2.8575832843780518\n","Epoch: 1514/5000  Traning Loss: 59.596466064453125  Train_Reconstruction: 56.69117975234985  Train_KL: 2.9052863717079163  Validation Loss : 59.123111724853516 Val_Reconstruction : 56.25258445739746 Val_KL : 2.8705270290374756\n","Epoch: 1515/5000  Traning Loss: 59.78783130645752  Train_Reconstruction: 56.872695446014404  Train_KL: 2.9151361286640167  Validation Loss : 58.97006416320801 Val_Reconstruction : 56.091238021850586 Val_KL : 2.878825068473816\n","Epoch: 1516/5000  Traning Loss: 59.641907691955566  Train_Reconstruction: 56.744460582733154  Train_KL: 2.8974474370479584  Validation Loss : 58.9440975189209 Val_Reconstruction : 56.07735252380371 Val_KL : 2.866745114326477\n","Epoch: 1517/5000  Traning Loss: 59.61449384689331  Train_Reconstruction: 56.70776844024658  Train_KL: 2.906725585460663  Validation Loss : 59.07251739501953 Val_Reconstruction : 56.188323974609375 Val_KL : 2.8841936588287354\n","Epoch: 1518/5000  Traning Loss: 59.53050518035889  Train_Reconstruction: 56.61509132385254  Train_KL: 2.9154141545295715  Validation Loss : 58.6590633392334 Val_Reconstruction : 55.78127479553223 Val_KL : 2.8777884244918823\n","Epoch: 1519/5000  Traning Loss: 59.441935539245605  Train_Reconstruction: 56.54243755340576  Train_KL: 2.89949768781662  Validation Loss : 58.721261978149414 Val_Reconstruction : 55.869497299194336 Val_KL : 2.851765513420105\n","Epoch: 1520/5000  Traning Loss: 59.64926528930664  Train_Reconstruction: 56.748206615448  Train_KL: 2.9010588228702545  Validation Loss : 58.65376281738281 Val_Reconstruction : 55.79179763793945 Val_KL : 2.8619651794433594\n","Epoch: 1521/5000  Traning Loss: 59.520798683166504  Train_Reconstruction: 56.62217378616333  Train_KL: 2.898624926805496  Validation Loss : 58.82124900817871 Val_Reconstruction : 55.964630126953125 Val_KL : 2.8566179275512695\n","Epoch: 1522/5000  Traning Loss: 59.44501543045044  Train_Reconstruction: 56.54957962036133  Train_KL: 2.895436704158783  Validation Loss : 58.79891014099121 Val_Reconstruction : 55.92435264587402 Val_KL : 2.8745580911636353\n","Epoch: 1523/5000  Traning Loss: 59.69994115829468  Train_Reconstruction: 56.78343725204468  Train_KL: 2.9165037870407104  Validation Loss : 58.82525444030762 Val_Reconstruction : 55.940425872802734 Val_KL : 2.884829044342041\n","Epoch: 1524/5000  Traning Loss: 59.509172439575195  Train_Reconstruction: 56.59560537338257  Train_KL: 2.9135671854019165  Validation Loss : 58.55370903015137 Val_Reconstruction : 55.66845703125 Val_KL : 2.8852522373199463\n","Epoch: 1525/5000  Traning Loss: 59.52205514907837  Train_Reconstruction: 56.5995135307312  Train_KL: 2.9225421249866486  Validation Loss : 58.51632118225098 Val_Reconstruction : 55.633907318115234 Val_KL : 2.882413387298584\n","Epoch: 1526/5000  Traning Loss: 59.78387928009033  Train_Reconstruction: 56.87855529785156  Train_KL: 2.9053245186805725  Validation Loss : 58.631900787353516 Val_Reconstruction : 55.759042739868164 Val_KL : 2.8728588819503784\n","Epoch: 1527/5000  Traning Loss: 59.72263050079346  Train_Reconstruction: 56.82484769821167  Train_KL: 2.8977826833724976  Validation Loss : 58.64934539794922 Val_Reconstruction : 55.7746639251709 Val_KL : 2.874680280685425\n","Epoch: 1528/5000  Traning Loss: 59.325077056884766  Train_Reconstruction: 56.40580177307129  Train_KL: 2.9192749857902527  Validation Loss : 58.70916557312012 Val_Reconstruction : 55.823930740356445 Val_KL : 2.885235905647278\n","Epoch: 1529/5000  Traning Loss: 59.38588857650757  Train_Reconstruction: 56.47282552719116  Train_KL: 2.9130629897117615  Validation Loss : 58.79835319519043 Val_Reconstruction : 55.91677474975586 Val_KL : 2.881579041481018\n","Epoch: 1530/5000  Traning Loss: 59.563902378082275  Train_Reconstruction: 56.645673751831055  Train_KL: 2.9182284772396088  Validation Loss : 58.88303756713867 Val_Reconstruction : 55.9985466003418 Val_KL : 2.8844913244247437\n","Epoch: 1531/5000  Traning Loss: 59.82015323638916  Train_Reconstruction: 56.902331829071045  Train_KL: 2.9178218841552734  Validation Loss : 59.15329360961914 Val_Reconstruction : 56.27048873901367 Val_KL : 2.8828046321868896\n","Epoch: 1532/5000  Traning Loss: 59.645668506622314  Train_Reconstruction: 56.74424600601196  Train_KL: 2.90142223238945  Validation Loss : 58.48061180114746 Val_Reconstruction : 55.610158920288086 Val_KL : 2.8704530000686646\n","Epoch: 1533/5000  Traning Loss: 59.35839509963989  Train_Reconstruction: 56.44831466674805  Train_KL: 2.9100802540779114  Validation Loss : 58.691646575927734 Val_Reconstruction : 55.81435012817383 Val_KL : 2.87729549407959\n","Epoch: 1534/5000  Traning Loss: 59.52479648590088  Train_Reconstruction: 56.62008762359619  Train_KL: 2.904709130525589  Validation Loss : 58.98194122314453 Val_Reconstruction : 56.11617851257324 Val_KL : 2.8657625913619995\n","Epoch: 1535/5000  Traning Loss: 59.41779136657715  Train_Reconstruction: 56.51678943634033  Train_KL: 2.9010017216205597  Validation Loss : 58.80978012084961 Val_Reconstruction : 55.93741416931152 Val_KL : 2.8723663091659546\n","Epoch: 1536/5000  Traning Loss: 59.57858419418335  Train_Reconstruction: 56.67730951309204  Train_KL: 2.901274234056473  Validation Loss : 58.85684776306152 Val_Reconstruction : 55.98513984680176 Val_KL : 2.871707320213318\n","Epoch: 1537/5000  Traning Loss: 59.697197914123535  Train_Reconstruction: 56.78851079940796  Train_KL: 2.9086870551109314  Validation Loss : 58.771841049194336 Val_Reconstruction : 55.896833419799805 Val_KL : 2.8750075101852417\n","Epoch: 1538/5000  Traning Loss: 59.358388900756836  Train_Reconstruction: 56.45119571685791  Train_KL: 2.9071937203407288  Validation Loss : 58.35970687866211 Val_Reconstruction : 55.480918884277344 Val_KL : 2.8787879943847656\n","Epoch: 1539/5000  Traning Loss: 59.60057306289673  Train_Reconstruction: 56.688908100128174  Train_KL: 2.911664664745331  Validation Loss : 58.578725814819336 Val_Reconstruction : 55.70784950256348 Val_KL : 2.8708776235580444\n","Epoch: 1540/5000  Traning Loss: 59.51343584060669  Train_Reconstruction: 56.603477001190186  Train_KL: 2.90995854139328  Validation Loss : 58.656028747558594 Val_Reconstruction : 55.76388740539551 Val_KL : 2.8921416997909546\n","Epoch: 1541/5000  Traning Loss: 59.656829833984375  Train_Reconstruction: 56.748711585998535  Train_KL: 2.9081180095672607  Validation Loss : 58.860002517700195 Val_Reconstruction : 55.99442100524902 Val_KL : 2.8655805587768555\n","Epoch: 1542/5000  Traning Loss: 59.73120641708374  Train_Reconstruction: 56.82772350311279  Train_KL: 2.9034828543663025  Validation Loss : 58.7036247253418 Val_Reconstruction : 55.835506439208984 Val_KL : 2.868118405342102\n","Epoch: 1543/5000  Traning Loss: 59.72689151763916  Train_Reconstruction: 56.83093452453613  Train_KL: 2.895957112312317  Validation Loss : 58.56509780883789 Val_Reconstruction : 55.697601318359375 Val_KL : 2.8674970865249634\n","Epoch: 1544/5000  Traning Loss: 59.74168014526367  Train_Reconstruction: 56.83256006240845  Train_KL: 2.9091197848320007  Validation Loss : 58.57296562194824 Val_Reconstruction : 55.69534873962402 Val_KL : 2.8776159286499023\n","Epoch: 1545/5000  Traning Loss: 59.68767690658569  Train_Reconstruction: 56.78854417800903  Train_KL: 2.8991324603557587  Validation Loss : 58.70953559875488 Val_Reconstruction : 55.85795211791992 Val_KL : 2.851583480834961\n","Epoch: 1546/5000  Traning Loss: 59.52828598022461  Train_Reconstruction: 56.63669967651367  Train_KL: 2.8915866911411285  Validation Loss : 58.65417289733887 Val_Reconstruction : 55.79630661010742 Val_KL : 2.8578648567199707\n","Epoch: 1547/5000  Traning Loss: 59.85562705993652  Train_Reconstruction: 56.954673767089844  Train_KL: 2.9009536802768707  Validation Loss : 58.72684288024902 Val_Reconstruction : 55.85382270812988 Val_KL : 2.8730205297470093\n","Epoch: 1548/5000  Traning Loss: 59.59178829193115  Train_Reconstruction: 56.686222076416016  Train_KL: 2.9055661261081696  Validation Loss : 58.48831367492676 Val_Reconstruction : 55.60332489013672 Val_KL : 2.8849871158599854\n","Epoch: 1549/5000  Traning Loss: 59.431145668029785  Train_Reconstruction: 56.51646661758423  Train_KL: 2.914679229259491  Validation Loss : 58.570552825927734 Val_Reconstruction : 55.703189849853516 Val_KL : 2.86736261844635\n","Epoch: 1550/5000  Traning Loss: 59.42070484161377  Train_Reconstruction: 56.50846767425537  Train_KL: 2.9122370183467865  Validation Loss : 59.07313346862793 Val_Reconstruction : 56.19462776184082 Val_KL : 2.8785054683685303\n","Epoch: 1551/5000  Traning Loss: 59.483556270599365  Train_Reconstruction: 56.57989740371704  Train_KL: 2.9036591053009033  Validation Loss : 58.61783027648926 Val_Reconstruction : 55.75029945373535 Val_KL : 2.8675320148468018\n","Epoch: 1552/5000  Traning Loss: 59.28173208236694  Train_Reconstruction: 56.375258922576904  Train_KL: 2.9064731001853943  Validation Loss : 58.4097900390625 Val_Reconstruction : 55.528350830078125 Val_KL : 2.8814390897750854\n","Epoch: 1553/5000  Traning Loss: 59.28658151626587  Train_Reconstruction: 56.37909126281738  Train_KL: 2.9074901044368744  Validation Loss : 58.49738311767578 Val_Reconstruction : 55.622785568237305 Val_KL : 2.874597191810608\n","Epoch: 1554/5000  Traning Loss: 59.368486404418945  Train_Reconstruction: 56.464698791503906  Train_KL: 2.903787672519684  Validation Loss : 58.68130302429199 Val_Reconstruction : 55.8108024597168 Val_KL : 2.870501756668091\n","Epoch: 1555/5000  Traning Loss: 59.53461694717407  Train_Reconstruction: 56.62105417251587  Train_KL: 2.913562625646591  Validation Loss : 58.80360221862793 Val_Reconstruction : 55.93571090698242 Val_KL : 2.867889881134033\n","Epoch: 1556/5000  Traning Loss: 59.8039608001709  Train_Reconstruction: 56.888421058654785  Train_KL: 2.915540039539337  Validation Loss : 59.880300521850586 Val_Reconstruction : 56.9913330078125 Val_KL : 2.888967275619507\n","Epoch: 1557/5000  Traning Loss: 59.55682849884033  Train_Reconstruction: 56.646835803985596  Train_KL: 2.909992516040802  Validation Loss : 58.68775177001953 Val_Reconstruction : 55.8070068359375 Val_KL : 2.8807454109191895\n","Epoch: 1558/5000  Traning Loss: 59.60189390182495  Train_Reconstruction: 56.703840255737305  Train_KL: 2.898054301738739  Validation Loss : 58.8438777923584 Val_Reconstruction : 55.99000549316406 Val_KL : 2.8538728952407837\n","Epoch: 1559/5000  Traning Loss: 59.65414810180664  Train_Reconstruction: 56.74705123901367  Train_KL: 2.9070968627929688  Validation Loss : 58.74336242675781 Val_Reconstruction : 55.8689022064209 Val_KL : 2.8744595050811768\n","Epoch: 1560/5000  Traning Loss: 59.632261753082275  Train_Reconstruction: 56.7316632270813  Train_KL: 2.900598406791687  Validation Loss : 58.97467803955078 Val_Reconstruction : 56.099836349487305 Val_KL : 2.874841570854187\n","Epoch: 1561/5000  Traning Loss: 59.5499472618103  Train_Reconstruction: 56.64048957824707  Train_KL: 2.9094579219818115  Validation Loss : 59.156005859375 Val_Reconstruction : 56.26816940307617 Val_KL : 2.8878347873687744\n","Epoch: 1562/5000  Traning Loss: 59.64809322357178  Train_Reconstruction: 56.72989082336426  Train_KL: 2.918202131986618  Validation Loss : 58.76416206359863 Val_Reconstruction : 55.884403228759766 Val_KL : 2.8797597885131836\n","Epoch: 1563/5000  Traning Loss: 60.11399221420288  Train_Reconstruction: 57.21989393234253  Train_KL: 2.8940981924533844  Validation Loss : 59.687076568603516 Val_Reconstruction : 56.82844161987305 Val_KL : 2.858633875846863\n","Epoch: 1564/5000  Traning Loss: 60.09883117675781  Train_Reconstruction: 57.19263029098511  Train_KL: 2.906200259923935  Validation Loss : 58.76709175109863 Val_Reconstruction : 55.887136459350586 Val_KL : 2.8799549341201782\n","Epoch: 1565/5000  Traning Loss: 59.51265239715576  Train_Reconstruction: 56.60695171356201  Train_KL: 2.9057006537914276  Validation Loss : 58.456743240356445 Val_Reconstruction : 55.587928771972656 Val_KL : 2.868814706802368\n","Epoch: 1566/5000  Traning Loss: 59.24854803085327  Train_Reconstruction: 56.34154653549194  Train_KL: 2.907001316547394  Validation Loss : 58.45729064941406 Val_Reconstruction : 55.570343017578125 Val_KL : 2.886946439743042\n","Epoch: 1567/5000  Traning Loss: 59.24917984008789  Train_Reconstruction: 56.334842681884766  Train_KL: 2.914337247610092  Validation Loss : 58.17652893066406 Val_Reconstruction : 55.30216979980469 Val_KL : 2.8743603229522705\n","Epoch: 1568/5000  Traning Loss: 59.373334884643555  Train_Reconstruction: 56.45809745788574  Train_KL: 2.9152378737926483  Validation Loss : 58.48510551452637 Val_Reconstruction : 55.599374771118164 Val_KL : 2.885730504989624\n","Epoch: 1569/5000  Traning Loss: 59.28950834274292  Train_Reconstruction: 56.37163782119751  Train_KL: 2.917870342731476  Validation Loss : 58.467708587646484 Val_Reconstruction : 55.58949661254883 Val_KL : 2.8782131671905518\n","Epoch: 1570/5000  Traning Loss: 59.50753211975098  Train_Reconstruction: 56.60375356674194  Train_KL: 2.9037781953811646  Validation Loss : 59.07342338562012 Val_Reconstruction : 56.19954490661621 Val_KL : 2.873879313468933\n","Epoch: 1571/5000  Traning Loss: 59.53997755050659  Train_Reconstruction: 56.64077663421631  Train_KL: 2.899201065301895  Validation Loss : 58.73130416870117 Val_Reconstruction : 55.86791229248047 Val_KL : 2.8633922338485718\n","Epoch: 1572/5000  Traning Loss: 59.460153579711914  Train_Reconstruction: 56.56266736984253  Train_KL: 2.897486627101898  Validation Loss : 58.70566177368164 Val_Reconstruction : 55.845401763916016 Val_KL : 2.8602607250213623\n","Epoch: 1573/5000  Traning Loss: 59.549776554107666  Train_Reconstruction: 56.651280879974365  Train_KL: 2.8984957933425903  Validation Loss : 58.83088493347168 Val_Reconstruction : 55.96011543273926 Val_KL : 2.8707683086395264\n","Epoch: 1574/5000  Traning Loss: 59.22307538986206  Train_Reconstruction: 56.314828395843506  Train_KL: 2.9082471132278442  Validation Loss : 58.41107368469238 Val_Reconstruction : 55.52230453491211 Val_KL : 2.888769507408142\n","Epoch: 1575/5000  Traning Loss: 59.17954206466675  Train_Reconstruction: 56.26952505111694  Train_KL: 2.910016745328903  Validation Loss : 58.35643196105957 Val_Reconstruction : 55.487749099731445 Val_KL : 2.8686814308166504\n","Epoch: 1576/5000  Traning Loss: 59.27099609375  Train_Reconstruction: 56.35728979110718  Train_KL: 2.9137062728405  Validation Loss : 58.512969970703125 Val_Reconstruction : 55.63603973388672 Val_KL : 2.8769301176071167\n","Epoch: 1577/5000  Traning Loss: 59.392826557159424  Train_Reconstruction: 56.4874382019043  Train_KL: 2.9053884744644165  Validation Loss : 58.777963638305664 Val_Reconstruction : 55.89865684509277 Val_KL : 2.8793071508407593\n","Epoch: 1578/5000  Traning Loss: 59.358405113220215  Train_Reconstruction: 56.45006704330444  Train_KL: 2.9083390831947327  Validation Loss : 58.33544921875 Val_Reconstruction : 55.45577430725098 Val_KL : 2.8796746730804443\n","Epoch: 1579/5000  Traning Loss: 59.25788116455078  Train_Reconstruction: 56.34822750091553  Train_KL: 2.90965336561203  Validation Loss : 58.643646240234375 Val_Reconstruction : 55.75962448120117 Val_KL : 2.884023070335388\n","Epoch: 1580/5000  Traning Loss: 59.26790237426758  Train_Reconstruction: 56.35741424560547  Train_KL: 2.910488039255142  Validation Loss : 58.507734298706055 Val_Reconstruction : 55.616363525390625 Val_KL : 2.8913716077804565\n","Epoch: 1581/5000  Traning Loss: 59.53030967712402  Train_Reconstruction: 56.60968494415283  Train_KL: 2.9206246435642242  Validation Loss : 58.936767578125 Val_Reconstruction : 56.053863525390625 Val_KL : 2.8829030990600586\n","Epoch: 1582/5000  Traning Loss: 59.60377359390259  Train_Reconstruction: 56.702810287475586  Train_KL: 2.90096378326416  Validation Loss : 58.69548797607422 Val_Reconstruction : 55.83750915527344 Val_KL : 2.857977509498596\n","Epoch: 1583/5000  Traning Loss: 59.54643440246582  Train_Reconstruction: 56.639389514923096  Train_KL: 2.90704482793808  Validation Loss : 58.925092697143555 Val_Reconstruction : 56.04508972167969 Val_KL : 2.880002021789551\n","Epoch: 1584/5000  Traning Loss: 60.25940418243408  Train_Reconstruction: 57.3351469039917  Train_KL: 2.9242571890354156  Validation Loss : 59.959938049316406 Val_Reconstruction : 57.07791709899902 Val_KL : 2.882020950317383\n","Epoch: 1585/5000  Traning Loss: 60.06012487411499  Train_Reconstruction: 57.15117692947388  Train_KL: 2.908947616815567  Validation Loss : 59.04864311218262 Val_Reconstruction : 56.18067932128906 Val_KL : 2.8679641485214233\n","Epoch: 1586/5000  Traning Loss: 59.666958808898926  Train_Reconstruction: 56.760770320892334  Train_KL: 2.9061883091926575  Validation Loss : 58.61964797973633 Val_Reconstruction : 55.74469184875488 Val_KL : 2.874955177307129\n","Epoch: 1587/5000  Traning Loss: 59.64698076248169  Train_Reconstruction: 56.74514722824097  Train_KL: 2.9018338918685913  Validation Loss : 58.49772834777832 Val_Reconstruction : 55.62615203857422 Val_KL : 2.8715755939483643\n","Epoch: 1588/5000  Traning Loss: 59.565059661865234  Train_Reconstruction: 56.65836763381958  Train_KL: 2.906692296266556  Validation Loss : 58.74678993225098 Val_Reconstruction : 55.86957931518555 Val_KL : 2.8772106170654297\n","Epoch: 1589/5000  Traning Loss: 59.44138145446777  Train_Reconstruction: 56.53248167037964  Train_KL: 2.908899426460266  Validation Loss : 58.44259834289551 Val_Reconstruction : 55.56868362426758 Val_KL : 2.8739150762557983\n","Epoch: 1590/5000  Traning Loss: 59.3481969833374  Train_Reconstruction: 56.432979583740234  Train_KL: 2.915217846632004  Validation Loss : 58.57125663757324 Val_Reconstruction : 55.69419479370117 Val_KL : 2.877060651779175\n","Epoch: 1591/5000  Traning Loss: 59.50753355026245  Train_Reconstruction: 56.5908145904541  Train_KL: 2.9167189598083496  Validation Loss : 58.98059272766113 Val_Reconstruction : 56.09510040283203 Val_KL : 2.885492444038391\n","Epoch: 1592/5000  Traning Loss: 59.890066146850586  Train_Reconstruction: 56.967806339263916  Train_KL: 2.9222597181797028  Validation Loss : 58.89983558654785 Val_Reconstruction : 56.010833740234375 Val_KL : 2.889001250267029\n","Epoch: 1593/5000  Traning Loss: 59.742977142333984  Train_Reconstruction: 56.81974744796753  Train_KL: 2.9232295155525208  Validation Loss : 58.5185604095459 Val_Reconstruction : 55.62531089782715 Val_KL : 2.89324951171875\n","Epoch: 1594/5000  Traning Loss: 59.45399188995361  Train_Reconstruction: 56.54029560089111  Train_KL: 2.9136964678764343  Validation Loss : 58.61427879333496 Val_Reconstruction : 55.73820495605469 Val_KL : 2.876074194908142\n","Epoch: 1595/5000  Traning Loss: 59.62868309020996  Train_Reconstruction: 56.7250657081604  Train_KL: 2.90361687541008  Validation Loss : 58.91971206665039 Val_Reconstruction : 56.04641914367676 Val_KL : 2.8732922077178955\n","Epoch: 1596/5000  Traning Loss: 59.615429401397705  Train_Reconstruction: 56.71504735946655  Train_KL: 2.9003822207450867  Validation Loss : 58.56583023071289 Val_Reconstruction : 55.694034576416016 Val_KL : 2.8717955350875854\n","Epoch: 1597/5000  Traning Loss: 59.49942636489868  Train_Reconstruction: 56.590566635131836  Train_KL: 2.908859521150589  Validation Loss : 58.57431221008301 Val_Reconstruction : 55.69467544555664 Val_KL : 2.879637122154236\n","Epoch: 1598/5000  Traning Loss: 59.61017179489136  Train_Reconstruction: 56.69855213165283  Train_KL: 2.9116199612617493  Validation Loss : 59.064292907714844 Val_Reconstruction : 56.18991279602051 Val_KL : 2.874380588531494\n","Epoch: 1599/5000  Traning Loss: 59.852012634277344  Train_Reconstruction: 56.9450626373291  Train_KL: 2.9069497883319855  Validation Loss : 59.42031478881836 Val_Reconstruction : 56.55682945251465 Val_KL : 2.863485336303711\n","Epoch: 1600/5000  Traning Loss: 59.629287242889404  Train_Reconstruction: 56.7182731628418  Train_KL: 2.9110137820243835  Validation Loss : 58.63842582702637 Val_Reconstruction : 55.760684967041016 Val_KL : 2.8777413368225098\n","Epoch: 1601/5000  Traning Loss: 59.3944411277771  Train_Reconstruction: 56.49149036407471  Train_KL: 2.902950942516327  Validation Loss : 58.80046272277832 Val_Reconstruction : 55.93435287475586 Val_KL : 2.866108775138855\n","Epoch: 1602/5000  Traning Loss: 59.554275035858154  Train_Reconstruction: 56.65373086929321  Train_KL: 2.9005443453788757  Validation Loss : 58.712961196899414 Val_Reconstruction : 55.831787109375 Val_KL : 2.8811748027801514\n","Epoch: 1603/5000  Traning Loss: 59.53061628341675  Train_Reconstruction: 56.62032651901245  Train_KL: 2.910289466381073  Validation Loss : 58.34051513671875 Val_Reconstruction : 55.46511459350586 Val_KL : 2.8754003047943115\n","Epoch: 1604/5000  Traning Loss: 59.58642292022705  Train_Reconstruction: 56.68233633041382  Train_KL: 2.904086172580719  Validation Loss : 58.917062759399414 Val_Reconstruction : 56.045040130615234 Val_KL : 2.87202250957489\n","Epoch: 1605/5000  Traning Loss: 59.49609661102295  Train_Reconstruction: 56.59740972518921  Train_KL: 2.898686796426773  Validation Loss : 58.63035774230957 Val_Reconstruction : 55.77103042602539 Val_KL : 2.85932719707489\n","Epoch: 1606/5000  Traning Loss: 59.509631633758545  Train_Reconstruction: 56.60548162460327  Train_KL: 2.904150038957596  Validation Loss : 58.81093978881836 Val_Reconstruction : 55.935638427734375 Val_KL : 2.8753002882003784\n","Epoch: 1607/5000  Traning Loss: 59.44973611831665  Train_Reconstruction: 56.53862380981445  Train_KL: 2.911112278699875  Validation Loss : 58.78577995300293 Val_Reconstruction : 55.90386962890625 Val_KL : 2.881909132003784\n","Epoch: 1608/5000  Traning Loss: 59.70451354980469  Train_Reconstruction: 56.800240993499756  Train_KL: 2.90427303314209  Validation Loss : 58.62791061401367 Val_Reconstruction : 55.76020812988281 Val_KL : 2.867702841758728\n","Epoch: 1609/5000  Traning Loss: 59.333868980407715  Train_Reconstruction: 56.42641305923462  Train_KL: 2.9074558913707733  Validation Loss : 58.46124458312988 Val_Reconstruction : 55.58440971374512 Val_KL : 2.876833915710449\n","Epoch: 1610/5000  Traning Loss: 59.33776330947876  Train_Reconstruction: 56.42493295669556  Train_KL: 2.9128308296203613  Validation Loss : 58.529802322387695 Val_Reconstruction : 55.653154373168945 Val_KL : 2.8766478300094604\n","Epoch: 1611/5000  Traning Loss: 59.37868404388428  Train_Reconstruction: 56.4786434173584  Train_KL: 2.9000405073165894  Validation Loss : 58.50212860107422 Val_Reconstruction : 55.640995025634766 Val_KL : 2.8611336946487427\n","Epoch: 1612/5000  Traning Loss: 59.26931619644165  Train_Reconstruction: 56.36825084686279  Train_KL: 2.9010647535324097  Validation Loss : 58.372934341430664 Val_Reconstruction : 55.5014591217041 Val_KL : 2.8714747428894043\n","Epoch: 1613/5000  Traning Loss: 59.2667441368103  Train_Reconstruction: 56.362637519836426  Train_KL: 2.90410652756691  Validation Loss : 58.311702728271484 Val_Reconstruction : 55.43589210510254 Val_KL : 2.8758106231689453\n","Epoch: 1614/5000  Traning Loss: 59.36376333236694  Train_Reconstruction: 56.46281623840332  Train_KL: 2.900947332382202  Validation Loss : 58.73773193359375 Val_Reconstruction : 55.86216163635254 Val_KL : 2.8755704164505005\n","Epoch: 1615/5000  Traning Loss: 59.30041980743408  Train_Reconstruction: 56.39280986785889  Train_KL: 2.9076099693775177  Validation Loss : 58.70778465270996 Val_Reconstruction : 55.83633804321289 Val_KL : 2.8714451789855957\n","Epoch: 1616/5000  Traning Loss: 59.43638467788696  Train_Reconstruction: 56.521653175354004  Train_KL: 2.9147315323352814  Validation Loss : 59.160953521728516 Val_Reconstruction : 56.26032066345215 Val_KL : 2.900634527206421\n","Epoch: 1617/5000  Traning Loss: 59.484437465667725  Train_Reconstruction: 56.56630516052246  Train_KL: 2.9181333482265472  Validation Loss : 58.63052749633789 Val_Reconstruction : 55.75617027282715 Val_KL : 2.8743577003479004\n","Epoch: 1618/5000  Traning Loss: 59.35464429855347  Train_Reconstruction: 56.43918466567993  Train_KL: 2.9154591858386993  Validation Loss : 58.40705871582031 Val_Reconstruction : 55.52424621582031 Val_KL : 2.8828125\n","Epoch: 1619/5000  Traning Loss: 59.204450607299805  Train_Reconstruction: 56.293684005737305  Train_KL: 2.9107666611671448  Validation Loss : 58.5647087097168 Val_Reconstruction : 55.697622299194336 Val_KL : 2.867086887359619\n","Epoch: 1620/5000  Traning Loss: 59.28830432891846  Train_Reconstruction: 56.38454294204712  Train_KL: 2.903761386871338  Validation Loss : 58.486572265625 Val_Reconstruction : 55.61865425109863 Val_KL : 2.867918014526367\n","Epoch: 1621/5000  Traning Loss: 59.192001819610596  Train_Reconstruction: 56.286744117736816  Train_KL: 2.905257761478424  Validation Loss : 58.290931701660156 Val_Reconstruction : 55.41034698486328 Val_KL : 2.880583167076111\n","Epoch: 1622/5000  Traning Loss: 59.345794677734375  Train_Reconstruction: 56.42829513549805  Train_KL: 2.9174999594688416  Validation Loss : 58.105478286743164 Val_Reconstruction : 55.224365234375 Val_KL : 2.881113052368164\n","Epoch: 1623/5000  Traning Loss: 59.3706169128418  Train_Reconstruction: 56.461018085479736  Train_KL: 2.9095986783504486  Validation Loss : 58.44339179992676 Val_Reconstruction : 55.5748233795166 Val_KL : 2.8685686588287354\n","Epoch: 1624/5000  Traning Loss: 59.25472068786621  Train_Reconstruction: 56.34740447998047  Train_KL: 2.9073159396648407  Validation Loss : 58.72479057312012 Val_Reconstruction : 55.851205825805664 Val_KL : 2.8735839128494263\n","Epoch: 1625/5000  Traning Loss: 59.18719482421875  Train_Reconstruction: 56.28642749786377  Train_KL: 2.900767147541046  Validation Loss : 58.39120864868164 Val_Reconstruction : 55.524309158325195 Val_KL : 2.8668997287750244\n","Epoch: 1626/5000  Traning Loss: 59.133878231048584  Train_Reconstruction: 56.23751783370972  Train_KL: 2.8963601887226105  Validation Loss : 58.503814697265625 Val_Reconstruction : 55.639076232910156 Val_KL : 2.864739775657654\n","Epoch: 1627/5000  Traning Loss: 59.332343101501465  Train_Reconstruction: 56.42384386062622  Train_KL: 2.9084985554218292  Validation Loss : 58.43901824951172 Val_Reconstruction : 55.559030532836914 Val_KL : 2.8799866437911987\n","Epoch: 1628/5000  Traning Loss: 59.18592309951782  Train_Reconstruction: 56.284321784973145  Train_KL: 2.901601195335388  Validation Loss : 58.27194786071777 Val_Reconstruction : 55.412227630615234 Val_KL : 2.859720230102539\n","Epoch: 1629/5000  Traning Loss: 59.171308517456055  Train_Reconstruction: 56.26868724822998  Train_KL: 2.9026213586330414  Validation Loss : 58.330562591552734 Val_Reconstruction : 55.454036712646484 Val_KL : 2.876526355743408\n","Epoch: 1630/5000  Traning Loss: 59.24209451675415  Train_Reconstruction: 56.33217811584473  Train_KL: 2.9099166691303253  Validation Loss : 58.49154853820801 Val_Reconstruction : 55.627607345581055 Val_KL : 2.8639419078826904\n","Epoch: 1631/5000  Traning Loss: 59.469144344329834  Train_Reconstruction: 56.56201791763306  Train_KL: 2.907125949859619  Validation Loss : 58.83617401123047 Val_Reconstruction : 55.95829200744629 Val_KL : 2.877882957458496\n","Epoch: 1632/5000  Traning Loss: 59.169861793518066  Train_Reconstruction: 56.24413871765137  Train_KL: 2.925723046064377  Validation Loss : 58.31972122192383 Val_Reconstruction : 55.420827865600586 Val_KL : 2.8988932371139526\n","Epoch: 1633/5000  Traning Loss: 59.154534339904785  Train_Reconstruction: 56.23712635040283  Train_KL: 2.917408436536789  Validation Loss : 58.03663635253906 Val_Reconstruction : 55.151601791381836 Val_KL : 2.8850350379943848\n","Epoch: 1634/5000  Traning Loss: 59.259655475616455  Train_Reconstruction: 56.343565464019775  Train_KL: 2.916090279817581  Validation Loss : 58.53861999511719 Val_Reconstruction : 55.655839920043945 Val_KL : 2.8827805519104004\n","Epoch: 1635/5000  Traning Loss: 59.27992820739746  Train_Reconstruction: 56.36442041397095  Train_KL: 2.9155077636241913  Validation Loss : 58.066123962402344 Val_Reconstruction : 55.19152641296387 Val_KL : 2.8745983839035034\n","Epoch: 1636/5000  Traning Loss: 59.352397441864014  Train_Reconstruction: 56.44042444229126  Train_KL: 2.9119728803634644  Validation Loss : 58.24327850341797 Val_Reconstruction : 55.361968994140625 Val_KL : 2.881309151649475\n","Epoch: 1637/5000  Traning Loss: 59.22854709625244  Train_Reconstruction: 56.322185039520264  Train_KL: 2.9063618779182434  Validation Loss : 58.14807891845703 Val_Reconstruction : 55.27810859680176 Val_KL : 2.8699716329574585\n","Epoch: 1638/5000  Traning Loss: 59.08368730545044  Train_Reconstruction: 56.1663384437561  Train_KL: 2.9173487722873688  Validation Loss : 58.312782287597656 Val_Reconstruction : 55.427955627441406 Val_KL : 2.884827971458435\n","Epoch: 1639/5000  Traning Loss: 59.01773691177368  Train_Reconstruction: 56.10139322280884  Train_KL: 2.9163438379764557  Validation Loss : 58.239776611328125 Val_Reconstruction : 55.351619720458984 Val_KL : 2.8881577253341675\n","Epoch: 1640/5000  Traning Loss: 59.16000318527222  Train_Reconstruction: 56.24581575393677  Train_KL: 2.914186865091324  Validation Loss : 58.561790466308594 Val_Reconstruction : 55.68617820739746 Val_KL : 2.875611662864685\n","Epoch: 1641/5000  Traning Loss: 59.10114336013794  Train_Reconstruction: 56.19895362854004  Train_KL: 2.9021899700164795  Validation Loss : 58.30778503417969 Val_Reconstruction : 55.43180465698242 Val_KL : 2.875980257987976\n","Epoch: 1642/5000  Traning Loss: 59.32159900665283  Train_Reconstruction: 56.416893005371094  Train_KL: 2.9047066271305084  Validation Loss : 58.48160934448242 Val_Reconstruction : 55.612884521484375 Val_KL : 2.8687247037887573\n","Epoch: 1643/5000  Traning Loss: 59.250821113586426  Train_Reconstruction: 56.35154724121094  Train_KL: 2.899273246526718  Validation Loss : 58.462196350097656 Val_Reconstruction : 55.59442329406738 Val_KL : 2.8677737712860107\n","Epoch: 1644/5000  Traning Loss: 59.10895347595215  Train_Reconstruction: 56.207133769989014  Train_KL: 2.901819109916687  Validation Loss : 58.09693717956543 Val_Reconstruction : 55.21984672546387 Val_KL : 2.8770912885665894\n","Epoch: 1645/5000  Traning Loss: 58.952922344207764  Train_Reconstruction: 56.043410301208496  Train_KL: 2.9095121026039124  Validation Loss : 58.103830337524414 Val_Reconstruction : 55.21195030212402 Val_KL : 2.8918793201446533\n","Epoch: 1646/5000  Traning Loss: 59.06630897521973  Train_Reconstruction: 56.14988374710083  Train_KL: 2.9164256155490875  Validation Loss : 58.091835021972656 Val_Reconstruction : 55.21274185180664 Val_KL : 2.87909197807312\n","Epoch: 1647/5000  Traning Loss: 59.34363842010498  Train_Reconstruction: 56.44117498397827  Train_KL: 2.9024641513824463  Validation Loss : 58.537330627441406 Val_Reconstruction : 55.67060852050781 Val_KL : 2.8667222261428833\n","Epoch: 1648/5000  Traning Loss: 59.9896502494812  Train_Reconstruction: 57.078420639038086  Train_KL: 2.911229521036148  Validation Loss : 58.78836441040039 Val_Reconstruction : 55.90490531921387 Val_KL : 2.883458733558655\n","Epoch: 1649/5000  Traning Loss: 59.486183166503906  Train_Reconstruction: 56.57150840759277  Train_KL: 2.9146751165390015  Validation Loss : 58.59343719482422 Val_Reconstruction : 55.71762657165527 Val_KL : 2.8758116960525513\n","Epoch: 1650/5000  Traning Loss: 59.262507915496826  Train_Reconstruction: 56.36044692993164  Train_KL: 2.9020603597164154  Validation Loss : 58.47720718383789 Val_Reconstruction : 55.60960578918457 Val_KL : 2.8676005601882935\n","Epoch: 1651/5000  Traning Loss: 59.21921968460083  Train_Reconstruction: 56.314220905303955  Train_KL: 2.9049999117851257  Validation Loss : 58.7413444519043 Val_Reconstruction : 55.86341857910156 Val_KL : 2.877925157546997\n","Epoch: 1652/5000  Traning Loss: 59.37363052368164  Train_Reconstruction: 56.45540809631348  Train_KL: 2.9182223081588745  Validation Loss : 58.33187294006348 Val_Reconstruction : 55.44327735900879 Val_KL : 2.88859486579895\n","Epoch: 1653/5000  Traning Loss: 59.38782501220703  Train_Reconstruction: 56.47284269332886  Train_KL: 2.9149812757968903  Validation Loss : 58.403228759765625 Val_Reconstruction : 55.52058410644531 Val_KL : 2.882644534111023\n","Epoch: 1654/5000  Traning Loss: 59.113746643066406  Train_Reconstruction: 56.21004915237427  Train_KL: 2.9036976993083954  Validation Loss : 58.177001953125 Val_Reconstruction : 55.305694580078125 Val_KL : 2.8713090419769287\n","Epoch: 1655/5000  Traning Loss: 59.04030466079712  Train_Reconstruction: 56.13693952560425  Train_KL: 2.9033644795417786  Validation Loss : 58.39284324645996 Val_Reconstruction : 55.52916145324707 Val_KL : 2.8636817932128906\n","Epoch: 1656/5000  Traning Loss: 59.299004554748535  Train_Reconstruction: 56.39036321640015  Train_KL: 2.9086409509181976  Validation Loss : 58.265581130981445 Val_Reconstruction : 55.37420654296875 Val_KL : 2.8913748264312744\n","Epoch: 1657/5000  Traning Loss: 59.34725761413574  Train_Reconstruction: 56.41740703582764  Train_KL: 2.9298499822616577  Validation Loss : 58.24782943725586 Val_Reconstruction : 55.35136032104492 Val_KL : 2.8964691162109375\n","Epoch: 1658/5000  Traning Loss: 58.98684787750244  Train_Reconstruction: 56.081050872802734  Train_KL: 2.90579754114151  Validation Loss : 58.09619331359863 Val_Reconstruction : 55.22620964050293 Val_KL : 2.8699841499328613\n","Epoch: 1659/5000  Traning Loss: 58.89348745346069  Train_Reconstruction: 55.9820294380188  Train_KL: 2.911457747220993  Validation Loss : 57.95805740356445 Val_Reconstruction : 55.073307037353516 Val_KL : 2.884750247001648\n","Epoch: 1660/5000  Traning Loss: 59.03769111633301  Train_Reconstruction: 56.11699676513672  Train_KL: 2.9206944406032562  Validation Loss : 58.74028778076172 Val_Reconstruction : 55.85402870178223 Val_KL : 2.8862595558166504\n","Epoch: 1661/5000  Traning Loss: 59.35676336288452  Train_Reconstruction: 56.43666410446167  Train_KL: 2.920099824666977  Validation Loss : 58.514583587646484 Val_Reconstruction : 55.630754470825195 Val_KL : 2.8838300704956055\n","Epoch: 1662/5000  Traning Loss: 59.162246227264404  Train_Reconstruction: 56.245107650756836  Train_KL: 2.9171378314495087  Validation Loss : 58.13750648498535 Val_Reconstruction : 55.258270263671875 Val_KL : 2.8792366981506348\n","Epoch: 1663/5000  Traning Loss: 59.26192331314087  Train_Reconstruction: 56.33810615539551  Train_KL: 2.923816978931427  Validation Loss : 58.75170707702637 Val_Reconstruction : 55.858062744140625 Val_KL : 2.8936450481414795\n","Epoch: 1664/5000  Traning Loss: 59.41771841049194  Train_Reconstruction: 56.502498149871826  Train_KL: 2.9152206778526306  Validation Loss : 59.097402572631836 Val_Reconstruction : 56.212846755981445 Val_KL : 2.8845572471618652\n","Epoch: 1665/5000  Traning Loss: 59.75112009048462  Train_Reconstruction: 56.835002422332764  Train_KL: 2.916117489337921  Validation Loss : 58.91459083557129 Val_Reconstruction : 56.023942947387695 Val_KL : 2.890648126602173\n","Epoch: 1666/5000  Traning Loss: 59.64037847518921  Train_Reconstruction: 56.72108602523804  Train_KL: 2.9192931056022644  Validation Loss : 58.510169982910156 Val_Reconstruction : 55.639373779296875 Val_KL : 2.8707951307296753\n","Epoch: 1667/5000  Traning Loss: 59.37330436706543  Train_Reconstruction: 56.46438026428223  Train_KL: 2.908924490213394  Validation Loss : 58.27863693237305 Val_Reconstruction : 55.402442932128906 Val_KL : 2.876194477081299\n","Epoch: 1668/5000  Traning Loss: 59.43443822860718  Train_Reconstruction: 56.529712200164795  Train_KL: 2.904726028442383  Validation Loss : 58.76047897338867 Val_Reconstruction : 55.89581108093262 Val_KL : 2.8646684885025024\n","Epoch: 1669/5000  Traning Loss: 59.22238206863403  Train_Reconstruction: 56.31681299209595  Train_KL: 2.90556937456131  Validation Loss : 58.45170021057129 Val_Reconstruction : 55.56686592102051 Val_KL : 2.8848326206207275\n","Epoch: 1670/5000  Traning Loss: 59.14385223388672  Train_Reconstruction: 56.23483848571777  Train_KL: 2.9090138375759125  Validation Loss : 58.215139389038086 Val_Reconstruction : 55.34356498718262 Val_KL : 2.871574282646179\n","Epoch: 1671/5000  Traning Loss: 59.28844213485718  Train_Reconstruction: 56.3692889213562  Train_KL: 2.9191536009311676  Validation Loss : 58.280094146728516 Val_Reconstruction : 55.40245819091797 Val_KL : 2.8776363134384155\n","Epoch: 1672/5000  Traning Loss: 59.3801326751709  Train_Reconstruction: 56.464680194854736  Train_KL: 2.9154523611068726  Validation Loss : 58.625043869018555 Val_Reconstruction : 55.751970291137695 Val_KL : 2.8730729818344116\n","Epoch: 1673/5000  Traning Loss: 59.479894161224365  Train_Reconstruction: 56.579848289489746  Train_KL: 2.900045335292816  Validation Loss : 58.77195167541504 Val_Reconstruction : 55.90920448303223 Val_KL : 2.8627469539642334\n","Epoch: 1674/5000  Traning Loss: 59.66148853302002  Train_Reconstruction: 56.75108861923218  Train_KL: 2.9103997945785522  Validation Loss : 58.89792060852051 Val_Reconstruction : 56.02227592468262 Val_KL : 2.875644564628601\n","Epoch: 1675/5000  Traning Loss: 59.56085824966431  Train_Reconstruction: 56.658631324768066  Train_KL: 2.902226537466049  Validation Loss : 58.727460861206055 Val_Reconstruction : 55.8612003326416 Val_KL : 2.8662617206573486\n","Epoch: 1676/5000  Traning Loss: 59.36926031112671  Train_Reconstruction: 56.45781993865967  Train_KL: 2.9114406406879425  Validation Loss : 58.62226486206055 Val_Reconstruction : 55.73861503601074 Val_KL : 2.8836508989334106\n","Epoch: 1677/5000  Traning Loss: 59.18987798690796  Train_Reconstruction: 56.27035999298096  Train_KL: 2.919517368078232  Validation Loss : 58.12228584289551 Val_Reconstruction : 55.24701690673828 Val_KL : 2.875268340110779\n","Epoch: 1678/5000  Traning Loss: 59.27615451812744  Train_Reconstruction: 56.3679404258728  Train_KL: 2.9082140922546387  Validation Loss : 58.59395408630371 Val_Reconstruction : 55.71615409851074 Val_KL : 2.8777990341186523\n","Epoch: 1679/5000  Traning Loss: 58.960246562957764  Train_Reconstruction: 56.04956579208374  Train_KL: 2.910681575536728  Validation Loss : 58.00450325012207 Val_Reconstruction : 55.1261100769043 Val_KL : 2.8783938884735107\n","Epoch: 1680/5000  Traning Loss: 59.13988637924194  Train_Reconstruction: 56.23366975784302  Train_KL: 2.906217038631439  Validation Loss : 58.16008186340332 Val_Reconstruction : 55.27518653869629 Val_KL : 2.884893774986267\n","Epoch: 1681/5000  Traning Loss: 58.97427177429199  Train_Reconstruction: 56.06194019317627  Train_KL: 2.9123319685459137  Validation Loss : 58.11877250671387 Val_Reconstruction : 55.24032211303711 Val_KL : 2.8784514665603638\n","Epoch: 1682/5000  Traning Loss: 59.027995586395264  Train_Reconstruction: 56.11714029312134  Train_KL: 2.9108552038669586  Validation Loss : 58.189369201660156 Val_Reconstruction : 55.31855392456055 Val_KL : 2.8708159923553467\n","Epoch: 1683/5000  Traning Loss: 59.09720754623413  Train_Reconstruction: 56.1872878074646  Train_KL: 2.9099200665950775  Validation Loss : 58.388044357299805 Val_Reconstruction : 55.51469039916992 Val_KL : 2.8733550310134888\n","Epoch: 1684/5000  Traning Loss: 59.14925479888916  Train_Reconstruction: 56.237794399261475  Train_KL: 2.911460220813751  Validation Loss : 58.185720443725586 Val_Reconstruction : 55.30912399291992 Val_KL : 2.876595973968506\n","Epoch: 1685/5000  Traning Loss: 59.27457284927368  Train_Reconstruction: 56.36928176879883  Train_KL: 2.9052914083004  Validation Loss : 58.213212966918945 Val_Reconstruction : 55.349721908569336 Val_KL : 2.8634908199310303\n","Epoch: 1686/5000  Traning Loss: 59.08846569061279  Train_Reconstruction: 56.18499517440796  Train_KL: 2.9034700989723206  Validation Loss : 58.30052947998047 Val_Reconstruction : 55.42228126525879 Val_KL : 2.8782492876052856\n","Epoch: 1687/5000  Traning Loss: 59.481046199798584  Train_Reconstruction: 56.56691360473633  Train_KL: 2.914132744073868  Validation Loss : 58.68950271606445 Val_Reconstruction : 55.80624961853027 Val_KL : 2.883253335952759\n","Epoch: 1688/5000  Traning Loss: 59.27817106246948  Train_Reconstruction: 56.36388158798218  Train_KL: 2.914289891719818  Validation Loss : 58.374711990356445 Val_Reconstruction : 55.48746109008789 Val_KL : 2.8872514963150024\n","Epoch: 1689/5000  Traning Loss: 59.33356046676636  Train_Reconstruction: 56.43054533004761  Train_KL: 2.903015524148941  Validation Loss : 58.65862846374512 Val_Reconstruction : 55.780099868774414 Val_KL : 2.8785289525985718\n","Epoch: 1690/5000  Traning Loss: 59.464850425720215  Train_Reconstruction: 56.55205488204956  Train_KL: 2.9127961099147797  Validation Loss : 58.53738021850586 Val_Reconstruction : 55.65754318237305 Val_KL : 2.8798378705978394\n","Epoch: 1691/5000  Traning Loss: 59.21629476547241  Train_Reconstruction: 56.301122188568115  Train_KL: 2.915173441171646  Validation Loss : 58.17946243286133 Val_Reconstruction : 55.30185317993164 Val_KL : 2.877610445022583\n","Epoch: 1692/5000  Traning Loss: 58.84604549407959  Train_Reconstruction: 55.94354581832886  Train_KL: 2.9024993181228638  Validation Loss : 57.95743179321289 Val_Reconstruction : 55.09328842163086 Val_KL : 2.8641443252563477\n","Epoch: 1693/5000  Traning Loss: 58.840402126312256  Train_Reconstruction: 55.92848205566406  Train_KL: 2.911919802427292  Validation Loss : 58.14497375488281 Val_Reconstruction : 55.252702713012695 Val_KL : 2.892272114753723\n","Epoch: 1694/5000  Traning Loss: 59.126455783843994  Train_Reconstruction: 56.206809520721436  Train_KL: 2.9196461141109467  Validation Loss : 58.46931266784668 Val_Reconstruction : 55.5733757019043 Val_KL : 2.8959360122680664\n","Epoch: 1695/5000  Traning Loss: 59.185412883758545  Train_Reconstruction: 56.2646050453186  Train_KL: 2.9208077788352966  Validation Loss : 58.26609420776367 Val_Reconstruction : 55.37553787231445 Val_KL : 2.8905571699142456\n","Epoch: 1696/5000  Traning Loss: 59.014368534088135  Train_Reconstruction: 56.09847450256348  Train_KL: 2.915893852710724  Validation Loss : 58.19858360290527 Val_Reconstruction : 55.316078186035156 Val_KL : 2.8825048208236694\n","Epoch: 1697/5000  Traning Loss: 59.49439287185669  Train_Reconstruction: 56.584468841552734  Train_KL: 2.909924656152725  Validation Loss : 59.41397666931152 Val_Reconstruction : 56.548112869262695 Val_KL : 2.8658641576766968\n","Epoch: 1698/5000  Traning Loss: 59.50937509536743  Train_Reconstruction: 56.59355354309082  Train_KL: 2.9158211052417755  Validation Loss : 58.634117126464844 Val_Reconstruction : 55.747047424316406 Val_KL : 2.887068510055542\n","Epoch: 1699/5000  Traning Loss: 59.19043827056885  Train_Reconstruction: 56.27971839904785  Train_KL: 2.910719484090805  Validation Loss : 58.399269104003906 Val_Reconstruction : 55.526817321777344 Val_KL : 2.872451901435852\n","Epoch: 1700/5000  Traning Loss: 59.27707052230835  Train_Reconstruction: 56.36336040496826  Train_KL: 2.9137103259563446  Validation Loss : 58.348440170288086 Val_Reconstruction : 55.46105766296387 Val_KL : 2.8873833417892456\n","Epoch: 1701/5000  Traning Loss: 59.115111351013184  Train_Reconstruction: 56.20135021209717  Train_KL: 2.913760542869568  Validation Loss : 58.21112251281738 Val_Reconstruction : 55.34221649169922 Val_KL : 2.8689069747924805\n","Epoch: 1702/5000  Traning Loss: 58.980525970458984  Train_Reconstruction: 56.06921625137329  Train_KL: 2.911310166120529  Validation Loss : 58.24256134033203 Val_Reconstruction : 55.35527420043945 Val_KL : 2.887286901473999\n","Epoch: 1703/5000  Traning Loss: 59.22835731506348  Train_Reconstruction: 56.31005239486694  Train_KL: 2.9183050394058228  Validation Loss : 58.48258399963379 Val_Reconstruction : 55.61350059509277 Val_KL : 2.869084358215332\n","Epoch: 1704/5000  Traning Loss: 59.20458507537842  Train_Reconstruction: 56.2967414855957  Train_KL: 2.9078434109687805  Validation Loss : 58.230289459228516 Val_Reconstruction : 55.33941078186035 Val_KL : 2.890878915786743\n","Epoch: 1705/5000  Traning Loss: 59.12790393829346  Train_Reconstruction: 56.21221113204956  Train_KL: 2.9156925678253174  Validation Loss : 58.22111129760742 Val_Reconstruction : 55.329620361328125 Val_KL : 2.8914917707443237\n","Epoch: 1706/5000  Traning Loss: 59.03910160064697  Train_Reconstruction: 56.122543811798096  Train_KL: 2.9165583550930023  Validation Loss : 58.231285095214844 Val_Reconstruction : 55.3420295715332 Val_KL : 2.8892558813095093\n","Epoch: 1707/5000  Traning Loss: 58.939714431762695  Train_Reconstruction: 56.025728702545166  Train_KL: 2.913985788822174  Validation Loss : 58.15939140319824 Val_Reconstruction : 55.27305793762207 Val_KL : 2.886334180831909\n","Epoch: 1708/5000  Traning Loss: 59.19596767425537  Train_Reconstruction: 56.28547811508179  Train_KL: 2.910489320755005  Validation Loss : 58.54104423522949 Val_Reconstruction : 55.658138275146484 Val_KL : 2.8829060792922974\n","Epoch: 1709/5000  Traning Loss: 59.26385736465454  Train_Reconstruction: 56.35407257080078  Train_KL: 2.9097852408885956  Validation Loss : 58.43238830566406 Val_Reconstruction : 55.56732368469238 Val_KL : 2.865064263343811\n","Epoch: 1710/5000  Traning Loss: 59.305543422698975  Train_Reconstruction: 56.39804935455322  Train_KL: 2.90749454498291  Validation Loss : 58.62854194641113 Val_Reconstruction : 55.7537727355957 Val_KL : 2.8747689723968506\n","Epoch: 1711/5000  Traning Loss: 59.39106225967407  Train_Reconstruction: 56.47872829437256  Train_KL: 2.912333130836487  Validation Loss : 58.53788375854492 Val_Reconstruction : 55.66021537780762 Val_KL : 2.877668023109436\n","Epoch: 1712/5000  Traning Loss: 59.63639211654663  Train_Reconstruction: 56.71494960784912  Train_KL: 2.921442061662674  Validation Loss : 58.49273109436035 Val_Reconstruction : 55.60657501220703 Val_KL : 2.8861559629440308\n","Epoch: 1713/5000  Traning Loss: 59.271986961364746  Train_Reconstruction: 56.35476875305176  Train_KL: 2.9172178208827972  Validation Loss : 58.59772872924805 Val_Reconstruction : 55.715559005737305 Val_KL : 2.882169723510742\n","Epoch: 1714/5000  Traning Loss: 59.262773513793945  Train_Reconstruction: 56.35744285583496  Train_KL: 2.905331254005432  Validation Loss : 58.51570129394531 Val_Reconstruction : 55.63895034790039 Val_KL : 2.8767507076263428\n","Epoch: 1715/5000  Traning Loss: 59.34496259689331  Train_Reconstruction: 56.43042039871216  Train_KL: 2.914542406797409  Validation Loss : 58.38593864440918 Val_Reconstruction : 55.50730895996094 Val_KL : 2.8786293268203735\n","Epoch: 1716/5000  Traning Loss: 59.229387283325195  Train_Reconstruction: 56.3146595954895  Train_KL: 2.914727658033371  Validation Loss : 58.35965347290039 Val_Reconstruction : 55.47626495361328 Val_KL : 2.883388876914978\n","Epoch: 1717/5000  Traning Loss: 59.41991186141968  Train_Reconstruction: 56.50495529174805  Train_KL: 2.9149565994739532  Validation Loss : 58.26422119140625 Val_Reconstruction : 55.37843132019043 Val_KL : 2.8857903480529785\n","Epoch: 1718/5000  Traning Loss: 59.02756214141846  Train_Reconstruction: 56.11839771270752  Train_KL: 2.909164637327194  Validation Loss : 58.19028854370117 Val_Reconstruction : 55.304948806762695 Val_KL : 2.885340690612793\n","Epoch: 1719/5000  Traning Loss: 58.87337350845337  Train_Reconstruction: 55.947752952575684  Train_KL: 2.925620049238205  Validation Loss : 58.15834426879883 Val_Reconstruction : 55.26740074157715 Val_KL : 2.8909428119659424\n","Epoch: 1720/5000  Traning Loss: 58.92234802246094  Train_Reconstruction: 56.01171541213989  Train_KL: 2.9106323421001434  Validation Loss : 58.062137603759766 Val_Reconstruction : 55.190345764160156 Val_KL : 2.871791958808899\n","Epoch: 1721/5000  Traning Loss: 59.242526054382324  Train_Reconstruction: 56.327430725097656  Train_KL: 2.915095627307892  Validation Loss : 58.33692169189453 Val_Reconstruction : 55.45486831665039 Val_KL : 2.882053852081299\n","Epoch: 1722/5000  Traning Loss: 59.21374464035034  Train_Reconstruction: 56.30064821243286  Train_KL: 2.9130965173244476  Validation Loss : 58.16845703125 Val_Reconstruction : 55.288414001464844 Val_KL : 2.88004207611084\n","Epoch: 1723/5000  Traning Loss: 59.19177961349487  Train_Reconstruction: 56.286250591278076  Train_KL: 2.9055290818214417  Validation Loss : 58.172719955444336 Val_Reconstruction : 55.30019569396973 Val_KL : 2.8725244998931885\n","Epoch: 1724/5000  Traning Loss: 58.945462703704834  Train_Reconstruction: 56.04148292541504  Train_KL: 2.9039789736270905  Validation Loss : 58.15988540649414 Val_Reconstruction : 55.29096221923828 Val_KL : 2.8689229488372803\n","Epoch: 1725/5000  Traning Loss: 59.03344202041626  Train_Reconstruction: 56.12159872055054  Train_KL: 2.911843240261078  Validation Loss : 58.218820571899414 Val_Reconstruction : 55.34390830993652 Val_KL : 2.874911904335022\n","Epoch: 1726/5000  Traning Loss: 59.26488637924194  Train_Reconstruction: 56.35492134094238  Train_KL: 2.9099651873111725  Validation Loss : 58.16107749938965 Val_Reconstruction : 55.29316711425781 Val_KL : 2.867910385131836\n","Epoch: 1727/5000  Traning Loss: 59.06538009643555  Train_Reconstruction: 56.15564203262329  Train_KL: 2.9097386300563812  Validation Loss : 58.198503494262695 Val_Reconstruction : 55.30998229980469 Val_KL : 2.8885198831558228\n","Epoch: 1728/5000  Traning Loss: 59.05940341949463  Train_Reconstruction: 56.13111972808838  Train_KL: 2.928283929824829  Validation Loss : 58.27989196777344 Val_Reconstruction : 55.37242889404297 Val_KL : 2.9074631929397583\n","Epoch: 1729/5000  Traning Loss: 59.265106201171875  Train_Reconstruction: 56.33221101760864  Train_KL: 2.9328954815864563  Validation Loss : 58.57575988769531 Val_Reconstruction : 55.68713569641113 Val_KL : 2.8886255025863647\n","Epoch: 1730/5000  Traning Loss: 59.36583709716797  Train_Reconstruction: 56.44239044189453  Train_KL: 2.9234467446804047  Validation Loss : 58.69856071472168 Val_Reconstruction : 55.822566986083984 Val_KL : 2.875992774963379\n","Epoch: 1731/5000  Traning Loss: 59.62364339828491  Train_Reconstruction: 56.70732593536377  Train_KL: 2.9163172245025635  Validation Loss : 58.9615421295166 Val_Reconstruction : 56.07789421081543 Val_KL : 2.883647084236145\n","Epoch: 1732/5000  Traning Loss: 59.308922290802  Train_Reconstruction: 56.38878917694092  Train_KL: 2.9201328456401825  Validation Loss : 58.57442092895508 Val_Reconstruction : 55.68860054016113 Val_KL : 2.8858211040496826\n","Epoch: 1733/5000  Traning Loss: 59.13043403625488  Train_Reconstruction: 56.20867443084717  Train_KL: 2.9217602014541626  Validation Loss : 58.367361068725586 Val_Reconstruction : 55.48014259338379 Val_KL : 2.887217879295349\n","Epoch: 1734/5000  Traning Loss: 59.09100151062012  Train_Reconstruction: 56.17143678665161  Train_KL: 2.9195646345615387  Validation Loss : 58.27747917175293 Val_Reconstruction : 55.39310836791992 Val_KL : 2.8843706846237183\n","Epoch: 1735/5000  Traning Loss: 58.948415756225586  Train_Reconstruction: 56.03513431549072  Train_KL: 2.913281500339508  Validation Loss : 58.3692626953125 Val_Reconstruction : 55.48118591308594 Val_KL : 2.8880770206451416\n","Epoch: 1736/5000  Traning Loss: 58.9476113319397  Train_Reconstruction: 56.0332670211792  Train_KL: 2.91434383392334  Validation Loss : 58.4191780090332 Val_Reconstruction : 55.53312301635742 Val_KL : 2.886054277420044\n","Epoch: 1737/5000  Traning Loss: 58.98470067977905  Train_Reconstruction: 56.074485301971436  Train_KL: 2.91021528840065  Validation Loss : 58.274770736694336 Val_Reconstruction : 55.384212493896484 Val_KL : 2.8905577659606934\n","Epoch: 1738/5000  Traning Loss: 59.00509738922119  Train_Reconstruction: 56.09268760681152  Train_KL: 2.9124101102352142  Validation Loss : 58.03786849975586 Val_Reconstruction : 55.15939903259277 Val_KL : 2.8784682750701904\n","Epoch: 1739/5000  Traning Loss: 59.34848213195801  Train_Reconstruction: 56.43379259109497  Train_KL: 2.914689838886261  Validation Loss : 58.6802978515625 Val_Reconstruction : 55.79867935180664 Val_KL : 2.881618618965149\n","Epoch: 1740/5000  Traning Loss: 60.24078130722046  Train_Reconstruction: 57.32740259170532  Train_KL: 2.913378804922104  Validation Loss : 59.55182647705078 Val_Reconstruction : 56.67429733276367 Val_KL : 2.8775296211242676\n","Epoch: 1741/5000  Traning Loss: 59.689924240112305  Train_Reconstruction: 56.78987741470337  Train_KL: 2.9000465869903564  Validation Loss : 58.4672737121582 Val_Reconstruction : 55.60106086730957 Val_KL : 2.866213321685791\n","Epoch: 1742/5000  Traning Loss: 59.115020751953125  Train_Reconstruction: 56.20199108123779  Train_KL: 2.9130295515060425  Validation Loss : 58.268232345581055 Val_Reconstruction : 55.38469886779785 Val_KL : 2.8835328817367554\n","Epoch: 1743/5000  Traning Loss: 59.23146629333496  Train_Reconstruction: 56.31096601486206  Train_KL: 2.9205001294612885  Validation Loss : 58.849191665649414 Val_Reconstruction : 55.9630012512207 Val_KL : 2.8861894607543945\n","Epoch: 1744/5000  Traning Loss: 59.07173824310303  Train_Reconstruction: 56.155569553375244  Train_KL: 2.9161684215068817  Validation Loss : 58.2556209564209 Val_Reconstruction : 55.37253952026367 Val_KL : 2.8830827474594116\n","Epoch: 1745/5000  Traning Loss: 58.8755464553833  Train_Reconstruction: 55.957308292388916  Train_KL: 2.918238043785095  Validation Loss : 57.84903907775879 Val_Reconstruction : 54.95546913146973 Val_KL : 2.8935693502426147\n","Epoch: 1746/5000  Traning Loss: 58.95236682891846  Train_Reconstruction: 56.038740158081055  Train_KL: 2.913626343011856  Validation Loss : 57.878347396850586 Val_Reconstruction : 54.99971580505371 Val_KL : 2.8786303997039795\n","Epoch: 1747/5000  Traning Loss: 58.83583736419678  Train_Reconstruction: 55.928945541381836  Train_KL: 2.9068911969661713  Validation Loss : 58.00076484680176 Val_Reconstruction : 55.116811752319336 Val_KL : 2.8839523792266846\n","Epoch: 1748/5000  Traning Loss: 58.98269844055176  Train_Reconstruction: 56.056363582611084  Train_KL: 2.926334798336029  Validation Loss : 58.23535346984863 Val_Reconstruction : 55.347665786743164 Val_KL : 2.8876885175704956\n","Epoch: 1749/5000  Traning Loss: 58.91707897186279  Train_Reconstruction: 55.99989366531372  Train_KL: 2.917185515165329  Validation Loss : 58.19064140319824 Val_Reconstruction : 55.304948806762695 Val_KL : 2.885691285133362\n","Epoch: 1750/5000  Traning Loss: 58.97981929779053  Train_Reconstruction: 56.0595326423645  Train_KL: 2.920286536216736  Validation Loss : 58.141496658325195 Val_Reconstruction : 55.250736236572266 Val_KL : 2.8907586336135864\n","Epoch: 1751/5000  Traning Loss: 59.063026428222656  Train_Reconstruction: 56.15043067932129  Train_KL: 2.9125960171222687  Validation Loss : 58.4665470123291 Val_Reconstruction : 55.57485389709473 Val_KL : 2.8916937112808228\n","Epoch: 1752/5000  Traning Loss: 59.16902017593384  Train_Reconstruction: 56.2451868057251  Train_KL: 2.923833876848221  Validation Loss : 58.301382064819336 Val_Reconstruction : 55.4062385559082 Val_KL : 2.8951445817947388\n","Epoch: 1753/5000  Traning Loss: 58.99163103103638  Train_Reconstruction: 56.0688853263855  Train_KL: 2.922746568918228  Validation Loss : 58.16830253601074 Val_Reconstruction : 55.2775993347168 Val_KL : 2.890703797340393\n","Epoch: 1754/5000  Traning Loss: 59.218369007110596  Train_Reconstruction: 56.30519342422485  Train_KL: 2.913175731897354  Validation Loss : 58.1612434387207 Val_Reconstruction : 55.28085136413574 Val_KL : 2.8803908824920654\n","Epoch: 1755/5000  Traning Loss: 59.11058950424194  Train_Reconstruction: 56.18548393249512  Train_KL: 2.9251056611537933  Validation Loss : 58.400583267211914 Val_Reconstruction : 55.4904670715332 Val_KL : 2.9101157188415527\n","Epoch: 1756/5000  Traning Loss: 59.16030025482178  Train_Reconstruction: 56.23272657394409  Train_KL: 2.927573025226593  Validation Loss : 58.15329360961914 Val_Reconstruction : 55.26806449890137 Val_KL : 2.8852304220199585\n","Epoch: 1757/5000  Traning Loss: 58.98065137863159  Train_Reconstruction: 56.0639853477478  Train_KL: 2.9166658520698547  Validation Loss : 58.432207107543945 Val_Reconstruction : 55.53624153137207 Val_KL : 2.895965814590454\n","Epoch: 1758/5000  Traning Loss: 59.19633388519287  Train_Reconstruction: 56.27233648300171  Train_KL: 2.9239975214004517  Validation Loss : 58.37383842468262 Val_Reconstruction : 55.48248100280762 Val_KL : 2.8913573026657104\n","Epoch: 1759/5000  Traning Loss: 59.23744249343872  Train_Reconstruction: 56.32031488418579  Train_KL: 2.9171285927295685  Validation Loss : 58.138362884521484 Val_Reconstruction : 55.23196220397949 Val_KL : 2.9063998460769653\n","Epoch: 1760/5000  Traning Loss: 58.975682735443115  Train_Reconstruction: 56.051703453063965  Train_KL: 2.9239790737628937  Validation Loss : 58.36420249938965 Val_Reconstruction : 55.479366302490234 Val_KL : 2.8848369121551514\n","Epoch: 1761/5000  Traning Loss: 59.316129207611084  Train_Reconstruction: 56.40287160873413  Train_KL: 2.9132579267024994  Validation Loss : 58.27180862426758 Val_Reconstruction : 55.389726638793945 Val_KL : 2.882082223892212\n","Epoch: 1762/5000  Traning Loss: 59.415149211883545  Train_Reconstruction: 56.491793155670166  Train_KL: 2.9233557283878326  Validation Loss : 58.38640785217285 Val_Reconstruction : 55.49795341491699 Val_KL : 2.888455033302307\n","Epoch: 1763/5000  Traning Loss: 59.01933145523071  Train_Reconstruction: 56.10301971435547  Train_KL: 2.9163118302822113  Validation Loss : 58.395843505859375 Val_Reconstruction : 55.51521682739258 Val_KL : 2.8806254863739014\n","Epoch: 1764/5000  Traning Loss: 58.96753787994385  Train_Reconstruction: 56.05047607421875  Train_KL: 2.9170613884925842  Validation Loss : 57.992156982421875 Val_Reconstruction : 55.10665702819824 Val_KL : 2.885499119758606\n","Epoch: 1765/5000  Traning Loss: 58.77977895736694  Train_Reconstruction: 55.86349010467529  Train_KL: 2.916288137435913  Validation Loss : 58.01142120361328 Val_Reconstruction : 55.13308143615723 Val_KL : 2.8783395290374756\n","Epoch: 1766/5000  Traning Loss: 58.985748291015625  Train_Reconstruction: 56.073044300079346  Train_KL: 2.912704288959503  Validation Loss : 58.55764961242676 Val_Reconstruction : 55.672332763671875 Val_KL : 2.885315775871277\n","Epoch: 1767/5000  Traning Loss: 58.94758319854736  Train_Reconstruction: 56.02586793899536  Train_KL: 2.9217160046100616  Validation Loss : 58.13033103942871 Val_Reconstruction : 55.25020408630371 Val_KL : 2.880125880241394\n","Epoch: 1768/5000  Traning Loss: 58.830986976623535  Train_Reconstruction: 55.91961860656738  Train_KL: 2.91136834025383  Validation Loss : 58.096553802490234 Val_Reconstruction : 55.21952247619629 Val_KL : 2.877030372619629\n","Epoch: 1769/5000  Traning Loss: 59.02523946762085  Train_Reconstruction: 56.11455059051514  Train_KL: 2.9106884002685547  Validation Loss : 58.12971878051758 Val_Reconstruction : 55.25147247314453 Val_KL : 2.8782453536987305\n","Epoch: 1770/5000  Traning Loss: 59.297372341156006  Train_Reconstruction: 56.38586950302124  Train_KL: 2.911502480506897  Validation Loss : 59.084754943847656 Val_Reconstruction : 56.19753646850586 Val_KL : 2.887218713760376\n","Epoch: 1771/5000  Traning Loss: 59.465500831604004  Train_Reconstruction: 56.54766035079956  Train_KL: 2.9178402721881866  Validation Loss : 59.14202880859375 Val_Reconstruction : 56.25808525085449 Val_KL : 2.883942723274231\n","Epoch: 1772/5000  Traning Loss: 59.331263065338135  Train_Reconstruction: 56.4191312789917  Train_KL: 2.912132054567337  Validation Loss : 58.81473350524902 Val_Reconstruction : 55.93171691894531 Val_KL : 2.883016586303711\n","Epoch: 1773/5000  Traning Loss: 59.20714235305786  Train_Reconstruction: 56.29202461242676  Train_KL: 2.915117174386978  Validation Loss : 58.24111366271973 Val_Reconstruction : 55.361310958862305 Val_KL : 2.8798030614852905\n","Epoch: 1774/5000  Traning Loss: 58.95075750350952  Train_Reconstruction: 56.03923988342285  Train_KL: 2.911517858505249  Validation Loss : 58.12843894958496 Val_Reconstruction : 55.25588607788086 Val_KL : 2.872554302215576\n","Epoch: 1775/5000  Traning Loss: 59.0697455406189  Train_Reconstruction: 56.16543197631836  Train_KL: 2.904314249753952  Validation Loss : 58.343671798706055 Val_Reconstruction : 55.47084617614746 Val_KL : 2.872825026512146\n","Epoch: 1776/5000  Traning Loss: 59.521668910980225  Train_Reconstruction: 56.611170291900635  Train_KL: 2.9104986786842346  Validation Loss : 58.48506164550781 Val_Reconstruction : 55.58628463745117 Val_KL : 2.898777484893799\n","Epoch: 1777/5000  Traning Loss: 59.60274791717529  Train_Reconstruction: 56.691136837005615  Train_KL: 2.9116105437278748  Validation Loss : 59.02742958068848 Val_Reconstruction : 56.163692474365234 Val_KL : 2.863736391067505\n","Epoch: 1778/5000  Traning Loss: 59.15735673904419  Train_Reconstruction: 56.2502064704895  Train_KL: 2.9071505665779114  Validation Loss : 58.331302642822266 Val_Reconstruction : 55.45256996154785 Val_KL : 2.8787328004837036\n","Epoch: 1779/5000  Traning Loss: 59.0664176940918  Train_Reconstruction: 56.14828968048096  Train_KL: 2.9181283712387085  Validation Loss : 58.758867263793945 Val_Reconstruction : 55.86993980407715 Val_KL : 2.88892662525177\n","Epoch: 1780/5000  Traning Loss: 58.887813091278076  Train_Reconstruction: 55.9670991897583  Train_KL: 2.9207142293453217  Validation Loss : 58.07989311218262 Val_Reconstruction : 55.20647048950195 Val_KL : 2.8734235763549805\n","Epoch: 1781/5000  Traning Loss: 58.96155834197998  Train_Reconstruction: 56.04293489456177  Train_KL: 2.918623834848404  Validation Loss : 58.54450225830078 Val_Reconstruction : 55.6617374420166 Val_KL : 2.882763624191284\n","Epoch: 1782/5000  Traning Loss: 59.10429668426514  Train_Reconstruction: 56.18820762634277  Train_KL: 2.9160895943641663  Validation Loss : 58.25602340698242 Val_Reconstruction : 55.367692947387695 Val_KL : 2.888329029083252\n","Epoch: 1783/5000  Traning Loss: 59.146085262298584  Train_Reconstruction: 56.23218584060669  Train_KL: 2.913899928331375  Validation Loss : 58.96560287475586 Val_Reconstruction : 56.088605880737305 Val_KL : 2.876996636390686\n","Epoch: 1784/5000  Traning Loss: 59.40651798248291  Train_Reconstruction: 56.49915838241577  Train_KL: 2.9073596596717834  Validation Loss : 58.979352951049805 Val_Reconstruction : 56.09698677062988 Val_KL : 2.8823657035827637\n","Epoch: 1785/5000  Traning Loss: 59.4258074760437  Train_Reconstruction: 56.51045227050781  Train_KL: 2.9153550267219543  Validation Loss : 58.75959587097168 Val_Reconstruction : 55.87482452392578 Val_KL : 2.884771227836609\n","Epoch: 1786/5000  Traning Loss: 59.31299877166748  Train_Reconstruction: 56.3907585144043  Train_KL: 2.922240138053894  Validation Loss : 58.08989715576172 Val_Reconstruction : 55.203908920288086 Val_KL : 2.8859875202178955\n","Epoch: 1787/5000  Traning Loss: 58.937912940979004  Train_Reconstruction: 56.03396558761597  Train_KL: 2.9039470851421356  Validation Loss : 58.147090911865234 Val_Reconstruction : 55.28523826599121 Val_KL : 2.86185359954834\n","Epoch: 1788/5000  Traning Loss: 58.71344614028931  Train_Reconstruction: 55.807433128356934  Train_KL: 2.9060138165950775  Validation Loss : 57.821969985961914 Val_Reconstruction : 54.94696807861328 Val_KL : 2.8750011920928955\n","Epoch: 1789/5000  Traning Loss: 58.855567932128906  Train_Reconstruction: 55.93976640701294  Train_KL: 2.9158019423484802  Validation Loss : 58.105167388916016 Val_Reconstruction : 55.22595405578613 Val_KL : 2.879212498664856\n","Epoch: 1790/5000  Traning Loss: 58.97175216674805  Train_Reconstruction: 56.05969285964966  Train_KL: 2.9120600521564484  Validation Loss : 58.045366287231445 Val_Reconstruction : 55.16383743286133 Val_KL : 2.8815287351608276\n","Epoch: 1791/5000  Traning Loss: 58.962913036346436  Train_Reconstruction: 56.048237323760986  Train_KL: 2.9146753549575806  Validation Loss : 58.327091217041016 Val_Reconstruction : 55.43715476989746 Val_KL : 2.8899368047714233\n","Epoch: 1792/5000  Traning Loss: 59.189762115478516  Train_Reconstruction: 56.268325328826904  Train_KL: 2.921435981988907  Validation Loss : 58.78922080993652 Val_Reconstruction : 55.891963958740234 Val_KL : 2.897257685661316\n","Epoch: 1793/5000  Traning Loss: 59.485591888427734  Train_Reconstruction: 56.577433586120605  Train_KL: 2.9081583619117737  Validation Loss : 58.77863311767578 Val_Reconstruction : 55.898475646972656 Val_KL : 2.880157470703125\n","Epoch: 1794/5000  Traning Loss: 59.39804553985596  Train_Reconstruction: 56.48115158081055  Train_KL: 2.9168937504291534  Validation Loss : 58.48587226867676 Val_Reconstruction : 55.59819221496582 Val_KL : 2.8876794576644897\n","Epoch: 1795/5000  Traning Loss: 59.12043619155884  Train_Reconstruction: 56.199281215667725  Train_KL: 2.9211551547050476  Validation Loss : 58.299692153930664 Val_Reconstruction : 55.40314865112305 Val_KL : 2.896543860435486\n","Epoch: 1796/5000  Traning Loss: 59.10874366760254  Train_Reconstruction: 56.191336154937744  Train_KL: 2.917407751083374  Validation Loss : 58.23295593261719 Val_Reconstruction : 55.34754943847656 Val_KL : 2.885406732559204\n","Epoch: 1797/5000  Traning Loss: 59.17723751068115  Train_Reconstruction: 56.26547861099243  Train_KL: 2.9117591083049774  Validation Loss : 58.19027137756348 Val_Reconstruction : 55.31768226623535 Val_KL : 2.8725889921188354\n","Epoch: 1798/5000  Traning Loss: 58.94818115234375  Train_Reconstruction: 56.030893325805664  Train_KL: 2.917287200689316  Validation Loss : 58.255685806274414 Val_Reconstruction : 55.36615562438965 Val_KL : 2.889531135559082\n","Epoch: 1799/5000  Traning Loss: 59.08758592605591  Train_Reconstruction: 56.16663360595703  Train_KL: 2.9209526777267456  Validation Loss : 58.414072036743164 Val_Reconstruction : 55.52926254272461 Val_KL : 2.8848108053207397\n","Epoch: 1800/5000  Traning Loss: 58.95968246459961  Train_Reconstruction: 56.04312610626221  Train_KL: 2.9165558218955994  Validation Loss : 58.277198791503906 Val_Reconstruction : 55.39207458496094 Val_KL : 2.8851237297058105\n","Epoch: 1801/5000  Traning Loss: 59.072265625  Train_Reconstruction: 56.15601634979248  Train_KL: 2.916249305009842  Validation Loss : 58.20111274719238 Val_Reconstruction : 55.310922622680664 Val_KL : 2.8901907205581665\n","Epoch: 1802/5000  Traning Loss: 59.214895725250244  Train_Reconstruction: 56.29850769042969  Train_KL: 2.9163873195648193  Validation Loss : 58.65023994445801 Val_Reconstruction : 55.75507354736328 Val_KL : 2.8951674699783325\n","Epoch: 1803/5000  Traning Loss: 59.265713691711426  Train_Reconstruction: 56.34002161026001  Train_KL: 2.925692528486252  Validation Loss : 58.55789756774902 Val_Reconstruction : 55.6586799621582 Val_KL : 2.8992165327072144\n","Epoch: 1804/5000  Traning Loss: 59.02099847793579  Train_Reconstruction: 56.09974765777588  Train_KL: 2.921250730752945  Validation Loss : 58.25999069213867 Val_Reconstruction : 55.37998008728027 Val_KL : 2.880009889602661\n","Epoch: 1805/5000  Traning Loss: 58.88716459274292  Train_Reconstruction: 55.98236417770386  Train_KL: 2.904800772666931  Validation Loss : 58.0973014831543 Val_Reconstruction : 55.217519760131836 Val_KL : 2.879782199859619\n","Epoch: 1806/5000  Traning Loss: 59.002302169799805  Train_Reconstruction: 56.085843086242676  Train_KL: 2.916458636522293  Validation Loss : 58.31515121459961 Val_Reconstruction : 55.41581726074219 Val_KL : 2.8993335962295532\n","Epoch: 1807/5000  Traning Loss: 59.025967597961426  Train_Reconstruction: 56.10349130630493  Train_KL: 2.9224760830402374  Validation Loss : 58.391212463378906 Val_Reconstruction : 55.50796890258789 Val_KL : 2.883244514465332\n","Epoch: 1808/5000  Traning Loss: 58.81783199310303  Train_Reconstruction: 55.90319490432739  Train_KL: 2.9146366715431213  Validation Loss : 58.11221504211426 Val_Reconstruction : 55.23663139343262 Val_KL : 2.875583291053772\n","Epoch: 1809/5000  Traning Loss: 58.91649341583252  Train_Reconstruction: 55.99557876586914  Train_KL: 2.9209147691726685  Validation Loss : 58.21875762939453 Val_Reconstruction : 55.3177375793457 Val_KL : 2.9010190963745117\n","Epoch: 1810/5000  Traning Loss: 58.94723415374756  Train_Reconstruction: 56.02694320678711  Train_KL: 2.920290619134903  Validation Loss : 58.123653411865234 Val_Reconstruction : 55.23604393005371 Val_KL : 2.8876097202301025\n","Epoch: 1811/5000  Traning Loss: 58.894068241119385  Train_Reconstruction: 55.97710418701172  Train_KL: 2.916964143514633  Validation Loss : 58.10725212097168 Val_Reconstruction : 55.21482276916504 Val_KL : 2.892428755760193\n","Epoch: 1812/5000  Traning Loss: 59.15025758743286  Train_Reconstruction: 56.23074722290039  Train_KL: 2.919510394334793  Validation Loss : 58.19227409362793 Val_Reconstruction : 55.30445861816406 Val_KL : 2.8878164291381836\n","Epoch: 1813/5000  Traning Loss: 58.843369483947754  Train_Reconstruction: 55.93918991088867  Train_KL: 2.9041795134544373  Validation Loss : 57.960391998291016 Val_Reconstruction : 55.09168815612793 Val_KL : 2.868704319000244\n","Epoch: 1814/5000  Traning Loss: 58.834768772125244  Train_Reconstruction: 55.934903621673584  Train_KL: 2.8998651206493378  Validation Loss : 58.09769058227539 Val_Reconstruction : 55.22639846801758 Val_KL : 2.8712925910949707\n","Epoch: 1815/5000  Traning Loss: 58.874342918395996  Train_Reconstruction: 55.96220636367798  Train_KL: 2.9121372997760773  Validation Loss : 58.06372261047363 Val_Reconstruction : 55.18083572387695 Val_KL : 2.882887601852417\n","Epoch: 1816/5000  Traning Loss: 58.76267910003662  Train_Reconstruction: 55.85744094848633  Train_KL: 2.905238062143326  Validation Loss : 58.20902061462402 Val_Reconstruction : 55.33512878417969 Val_KL : 2.8738925457000732\n","Epoch: 1817/5000  Traning Loss: 59.04882764816284  Train_Reconstruction: 56.131714820861816  Train_KL: 2.917113274335861  Validation Loss : 58.385013580322266 Val_Reconstruction : 55.49919891357422 Val_KL : 2.8858134746551514\n","Epoch: 1818/5000  Traning Loss: 58.87346839904785  Train_Reconstruction: 55.961575984954834  Train_KL: 2.911892682313919  Validation Loss : 57.876203536987305 Val_Reconstruction : 54.997528076171875 Val_KL : 2.8786745071411133\n","Epoch: 1819/5000  Traning Loss: 58.883816719055176  Train_Reconstruction: 55.96336078643799  Train_KL: 2.920455753803253  Validation Loss : 57.93733024597168 Val_Reconstruction : 55.0312557220459 Val_KL : 2.9060744047164917\n","Epoch: 1820/5000  Traning Loss: 58.93819856643677  Train_Reconstruction: 56.01495027542114  Train_KL: 2.9232473969459534  Validation Loss : 58.101491928100586 Val_Reconstruction : 55.21820068359375 Val_KL : 2.8832907676696777\n","Epoch: 1821/5000  Traning Loss: 58.80145072937012  Train_Reconstruction: 55.89289712905884  Train_KL: 2.9085540771484375  Validation Loss : 57.7779655456543 Val_Reconstruction : 54.89794921875 Val_KL : 2.88001549243927\n","Epoch: 1822/5000  Traning Loss: 58.890177726745605  Train_Reconstruction: 55.97179174423218  Train_KL: 2.918385624885559  Validation Loss : 58.53679656982422 Val_Reconstruction : 55.62894058227539 Val_KL : 2.9078562259674072\n","Epoch: 1823/5000  Traning Loss: 59.0371527671814  Train_Reconstruction: 56.10011625289917  Train_KL: 2.9370361864566803  Validation Loss : 58.32876205444336 Val_Reconstruction : 55.42867469787598 Val_KL : 2.9000868797302246\n","Epoch: 1824/5000  Traning Loss: 59.365249156951904  Train_Reconstruction: 56.453369140625  Train_KL: 2.911879748106003  Validation Loss : 58.6138801574707 Val_Reconstruction : 55.74964141845703 Val_KL : 2.864238739013672\n","Epoch: 1825/5000  Traning Loss: 59.47479009628296  Train_Reconstruction: 56.55272960662842  Train_KL: 2.9220604598522186  Validation Loss : 58.634626388549805 Val_Reconstruction : 55.72526931762695 Val_KL : 2.909356474876404\n","Epoch: 1826/5000  Traning Loss: 59.114707946777344  Train_Reconstruction: 56.189223766326904  Train_KL: 2.9254834949970245  Validation Loss : 58.34491157531738 Val_Reconstruction : 55.45132064819336 Val_KL : 2.893590211868286\n","Epoch: 1827/5000  Traning Loss: 59.019269943237305  Train_Reconstruction: 56.10817050933838  Train_KL: 2.9110991954803467  Validation Loss : 57.99609375 Val_Reconstruction : 55.1190128326416 Val_KL : 2.8770813941955566\n","Epoch: 1828/5000  Traning Loss: 59.10737419128418  Train_Reconstruction: 56.194151878356934  Train_KL: 2.9132224321365356  Validation Loss : 58.44570350646973 Val_Reconstruction : 55.559518814086914 Val_KL : 2.8861844539642334\n","Epoch: 1829/5000  Traning Loss: 59.29868173599243  Train_Reconstruction: 56.37667942047119  Train_KL: 2.9220023453235626  Validation Loss : 58.380008697509766 Val_Reconstruction : 55.496116638183594 Val_KL : 2.883892297744751\n","Epoch: 1830/5000  Traning Loss: 59.126118183135986  Train_Reconstruction: 56.20777988433838  Train_KL: 2.9183378517627716  Validation Loss : 58.59099769592285 Val_Reconstruction : 55.68196678161621 Val_KL : 2.9090312719345093\n","Epoch: 1831/5000  Traning Loss: 59.06765842437744  Train_Reconstruction: 56.152705669403076  Train_KL: 2.914952874183655  Validation Loss : 58.245025634765625 Val_Reconstruction : 55.370941162109375 Val_KL : 2.87408447265625\n","Epoch: 1832/5000  Traning Loss: 59.44105243682861  Train_Reconstruction: 56.54613733291626  Train_KL: 2.8949146568775177  Validation Loss : 58.497676849365234 Val_Reconstruction : 55.63654327392578 Val_KL : 2.8611340522766113\n","Epoch: 1833/5000  Traning Loss: 59.14812755584717  Train_Reconstruction: 56.24843072891235  Train_KL: 2.8996968269348145  Validation Loss : 58.55433464050293 Val_Reconstruction : 55.68858337402344 Val_KL : 2.865752339363098\n","Epoch: 1834/5000  Traning Loss: 59.06571102142334  Train_Reconstruction: 56.16550254821777  Train_KL: 2.900207996368408  Validation Loss : 58.157718658447266 Val_Reconstruction : 55.29348373413086 Val_KL : 2.8642354011535645\n","Epoch: 1835/5000  Traning Loss: 59.01438045501709  Train_Reconstruction: 56.10996150970459  Train_KL: 2.9044190049171448  Validation Loss : 58.19993209838867 Val_Reconstruction : 55.33173370361328 Val_KL : 2.8681976795196533\n","Epoch: 1836/5000  Traning Loss: 59.03539705276489  Train_Reconstruction: 56.12436246871948  Train_KL: 2.911034017801285  Validation Loss : 58.178428649902344 Val_Reconstruction : 55.29568290710449 Val_KL : 2.882745623588562\n","Epoch: 1837/5000  Traning Loss: 58.91431474685669  Train_Reconstruction: 55.9932861328125  Train_KL: 2.92102912068367  Validation Loss : 58.35428428649902 Val_Reconstruction : 55.466773986816406 Val_KL : 2.8875101804733276\n","Epoch: 1838/5000  Traning Loss: 59.26274108886719  Train_Reconstruction: 56.342612743377686  Train_KL: 2.9201283156871796  Validation Loss : 58.4177360534668 Val_Reconstruction : 55.532697677612305 Val_KL : 2.8850380182266235\n","Epoch: 1839/5000  Traning Loss: 59.273279666900635  Train_Reconstruction: 56.35702896118164  Train_KL: 2.9162500202655792  Validation Loss : 58.36509132385254 Val_Reconstruction : 55.47661781311035 Val_KL : 2.8884748220443726\n","Epoch: 1840/5000  Traning Loss: 58.86677169799805  Train_Reconstruction: 55.945072650909424  Train_KL: 2.9216990172863007  Validation Loss : 57.98081588745117 Val_Reconstruction : 55.08625793457031 Val_KL : 2.8945573568344116\n","Epoch: 1841/5000  Traning Loss: 58.67241048812866  Train_Reconstruction: 55.743990421295166  Train_KL: 2.928419679403305  Validation Loss : 57.90725326538086 Val_Reconstruction : 55.014488220214844 Val_KL : 2.892765164375305\n","Epoch: 1842/5000  Traning Loss: 58.65924882888794  Train_Reconstruction: 55.74746894836426  Train_KL: 2.911780506372452  Validation Loss : 57.85391426086426 Val_Reconstruction : 54.98378562927246 Val_KL : 2.870129704475403\n","Epoch: 1843/5000  Traning Loss: 58.982943534851074  Train_Reconstruction: 56.071550369262695  Train_KL: 2.911393314599991  Validation Loss : 58.9171257019043 Val_Reconstruction : 56.0278205871582 Val_KL : 2.8893059492111206\n","Epoch: 1844/5000  Traning Loss: 59.47276735305786  Train_Reconstruction: 56.5525279045105  Train_KL: 2.9202393293380737  Validation Loss : 59.214834213256836 Val_Reconstruction : 56.31453323364258 Val_KL : 2.900299549102783\n","Epoch: 1845/5000  Traning Loss: 59.415048122406006  Train_Reconstruction: 56.50530529022217  Train_KL: 2.909741997718811  Validation Loss : 58.552846908569336 Val_Reconstruction : 55.68705749511719 Val_KL : 2.865790367126465\n","Epoch: 1846/5000  Traning Loss: 59.39699363708496  Train_Reconstruction: 56.494760036468506  Train_KL: 2.9022336602211  Validation Loss : 58.44203186035156 Val_Reconstruction : 55.56438064575195 Val_KL : 2.8776509761810303\n","Epoch: 1847/5000  Traning Loss: 59.190757751464844  Train_Reconstruction: 56.26984357833862  Train_KL: 2.920914202928543  Validation Loss : 57.88513374328613 Val_Reconstruction : 55.00731086730957 Val_KL : 2.8778215646743774\n","Epoch: 1848/5000  Traning Loss: 58.75470972061157  Train_Reconstruction: 55.84409284591675  Train_KL: 2.9106167256832123  Validation Loss : 57.75371170043945 Val_Reconstruction : 54.871469497680664 Val_KL : 2.882243514060974\n","Epoch: 1849/5000  Traning Loss: 58.81991243362427  Train_Reconstruction: 55.90985441207886  Train_KL: 2.9100588858127594  Validation Loss : 58.106544494628906 Val_Reconstruction : 55.2327880859375 Val_KL : 2.8737573623657227\n","Epoch: 1850/5000  Traning Loss: 58.86680555343628  Train_Reconstruction: 55.95620250701904  Train_KL: 2.9106028378009796  Validation Loss : 57.94184875488281 Val_Reconstruction : 55.06971740722656 Val_KL : 2.8721309900283813\n","Epoch: 1851/5000  Traning Loss: 58.776158809661865  Train_Reconstruction: 55.86372661590576  Train_KL: 2.912432312965393  Validation Loss : 57.98122787475586 Val_Reconstruction : 55.10292625427246 Val_KL : 2.8783018589019775\n","Epoch: 1852/5000  Traning Loss: 58.94587230682373  Train_Reconstruction: 56.040401458740234  Train_KL: 2.9054707884788513  Validation Loss : 58.16854286193848 Val_Reconstruction : 55.29815101623535 Val_KL : 2.8703925609588623\n","Epoch: 1853/5000  Traning Loss: 59.065526485443115  Train_Reconstruction: 56.15848779678345  Train_KL: 2.907038629055023  Validation Loss : 58.602294921875 Val_Reconstruction : 55.71629333496094 Val_KL : 2.886000871658325\n","Epoch: 1854/5000  Traning Loss: 59.181684494018555  Train_Reconstruction: 56.274807929992676  Train_KL: 2.9068763256073  Validation Loss : 58.501169204711914 Val_Reconstruction : 55.62876510620117 Val_KL : 2.8724039793014526\n","Epoch: 1855/5000  Traning Loss: 59.25781536102295  Train_Reconstruction: 56.35224533081055  Train_KL: 2.905570238828659  Validation Loss : 58.21923637390137 Val_Reconstruction : 55.3532829284668 Val_KL : 2.8659528493881226\n","Epoch: 1856/5000  Traning Loss: 58.97344160079956  Train_Reconstruction: 56.06461811065674  Train_KL: 2.908823162317276  Validation Loss : 58.02206611633301 Val_Reconstruction : 55.1419677734375 Val_KL : 2.880098819732666\n","Epoch: 1857/5000  Traning Loss: 58.80581998825073  Train_Reconstruction: 55.88755130767822  Train_KL: 2.918268620967865  Validation Loss : 58.130807876586914 Val_Reconstruction : 55.242109298706055 Val_KL : 2.888700008392334\n","Epoch: 1858/5000  Traning Loss: 58.731199741363525  Train_Reconstruction: 55.808473110198975  Train_KL: 2.9227262437343597  Validation Loss : 57.797607421875 Val_Reconstruction : 54.89753723144531 Val_KL : 2.900070548057556\n","Epoch: 1859/5000  Traning Loss: 58.702454566955566  Train_Reconstruction: 55.7780499458313  Train_KL: 2.924404740333557  Validation Loss : 58.058597564697266 Val_Reconstruction : 55.176551818847656 Val_KL : 2.882045865058899\n","Epoch: 1860/5000  Traning Loss: 59.08201360702515  Train_Reconstruction: 56.166977882385254  Train_KL: 2.9150360226631165  Validation Loss : 58.65456581115723 Val_Reconstruction : 55.76291084289551 Val_KL : 2.8916531801223755\n","Epoch: 1861/5000  Traning Loss: 59.25821018218994  Train_Reconstruction: 56.3299674987793  Train_KL: 2.9282430708408356  Validation Loss : 58.67684364318848 Val_Reconstruction : 55.78367805480957 Val_KL : 2.8931663036346436\n","Epoch: 1862/5000  Traning Loss: 59.18729305267334  Train_Reconstruction: 56.26621770858765  Train_KL: 2.9210761189460754  Validation Loss : 58.67597007751465 Val_Reconstruction : 55.79088020324707 Val_KL : 2.885090708732605\n","Epoch: 1863/5000  Traning Loss: 59.32043790817261  Train_Reconstruction: 56.410237312316895  Train_KL: 2.9102006554603577  Validation Loss : 58.62968635559082 Val_Reconstruction : 55.743764877319336 Val_KL : 2.8859214782714844\n","Epoch: 1864/5000  Traning Loss: 59.467610359191895  Train_Reconstruction: 56.53153324127197  Train_KL: 2.9360770285129547  Validation Loss : 58.69162178039551 Val_Reconstruction : 55.77603530883789 Val_KL : 2.9155877828598022\n","Epoch: 1865/5000  Traning Loss: 59.61787414550781  Train_Reconstruction: 56.70024871826172  Train_KL: 2.9176252484321594  Validation Loss : 58.74327278137207 Val_Reconstruction : 55.86529541015625 Val_KL : 2.877977728843689\n","Epoch: 1866/5000  Traning Loss: 59.70013427734375  Train_Reconstruction: 56.79252576828003  Train_KL: 2.907608985900879  Validation Loss : 58.56234169006348 Val_Reconstruction : 55.67489242553711 Val_KL : 2.887447953224182\n","Epoch: 1867/5000  Traning Loss: 59.476091384887695  Train_Reconstruction: 56.56064796447754  Train_KL: 2.9154436588287354  Validation Loss : 59.024606704711914 Val_Reconstruction : 56.13800239562988 Val_KL : 2.8866041898727417\n","Epoch: 1868/5000  Traning Loss: 58.95307731628418  Train_Reconstruction: 56.037193775177  Train_KL: 2.915884494781494  Validation Loss : 58.147186279296875 Val_Reconstruction : 55.24917411804199 Val_KL : 2.8980129957199097\n","Epoch: 1869/5000  Traning Loss: 58.63917303085327  Train_Reconstruction: 55.711244106292725  Train_KL: 2.927929162979126  Validation Loss : 57.97392272949219 Val_Reconstruction : 55.0763053894043 Val_KL : 2.89761745929718\n","Epoch: 1870/5000  Traning Loss: 58.563538551330566  Train_Reconstruction: 55.655720710754395  Train_KL: 2.9078181982040405  Validation Loss : 57.82586097717285 Val_Reconstruction : 54.95008087158203 Val_KL : 2.8757801055908203\n","Epoch: 1871/5000  Traning Loss: 58.694703102111816  Train_Reconstruction: 55.77151918411255  Train_KL: 2.923184335231781  Validation Loss : 58.07636070251465 Val_Reconstruction : 55.181766510009766 Val_KL : 2.8945940732955933\n","Epoch: 1872/5000  Traning Loss: 58.97473335266113  Train_Reconstruction: 56.04665470123291  Train_KL: 2.9280786216259003  Validation Loss : 58.16111946105957 Val_Reconstruction : 55.25951385498047 Val_KL : 2.9016045331954956\n","Epoch: 1873/5000  Traning Loss: 59.05078411102295  Train_Reconstruction: 56.12839937210083  Train_KL: 2.9223845303058624  Validation Loss : 58.904062271118164 Val_Reconstruction : 56.0181770324707 Val_KL : 2.885884404182434\n","Epoch: 1874/5000  Traning Loss: 59.07423448562622  Train_Reconstruction: 56.160130977630615  Train_KL: 2.914103925228119  Validation Loss : 58.26021385192871 Val_Reconstruction : 55.388959884643555 Val_KL : 2.8712528944015503\n","Epoch: 1875/5000  Traning Loss: 58.89107894897461  Train_Reconstruction: 55.9830379486084  Train_KL: 2.9080416560173035  Validation Loss : 57.86984634399414 Val_Reconstruction : 55.0028076171875 Val_KL : 2.867038369178772\n","Epoch: 1876/5000  Traning Loss: 58.819164752960205  Train_Reconstruction: 55.90682363510132  Train_KL: 2.912341296672821  Validation Loss : 57.9648380279541 Val_Reconstruction : 55.07293128967285 Val_KL : 2.8919079303741455\n","Epoch: 1877/5000  Traning Loss: 58.66482973098755  Train_Reconstruction: 55.74633073806763  Train_KL: 2.918499380350113  Validation Loss : 57.97080612182617 Val_Reconstruction : 55.089385986328125 Val_KL : 2.881420612335205\n","Epoch: 1878/5000  Traning Loss: 58.732450008392334  Train_Reconstruction: 55.82319116592407  Train_KL: 2.909257858991623  Validation Loss : 57.902774810791016 Val_Reconstruction : 55.022701263427734 Val_KL : 2.880073308944702\n","Epoch: 1879/5000  Traning Loss: 59.07641839981079  Train_Reconstruction: 56.16052055358887  Train_KL: 2.91589817404747  Validation Loss : 58.15626907348633 Val_Reconstruction : 55.26498603820801 Val_KL : 2.891282558441162\n","Epoch: 1880/5000  Traning Loss: 58.900678634643555  Train_Reconstruction: 55.97616481781006  Train_KL: 2.924514204263687  Validation Loss : 57.8797721862793 Val_Reconstruction : 54.983299255371094 Val_KL : 2.896473526954651\n","Epoch: 1881/5000  Traning Loss: 58.61137771606445  Train_Reconstruction: 55.70305252075195  Train_KL: 2.9083252549171448  Validation Loss : 57.75429153442383 Val_Reconstruction : 54.88338851928711 Val_KL : 2.870903730392456\n","Epoch: 1882/5000  Traning Loss: 58.74603319168091  Train_Reconstruction: 55.83558416366577  Train_KL: 2.910448282957077  Validation Loss : 58.02675247192383 Val_Reconstruction : 55.146244049072266 Val_KL : 2.880508303642273\n","Epoch: 1883/5000  Traning Loss: 59.03195333480835  Train_Reconstruction: 56.106757164001465  Train_KL: 2.925196409225464  Validation Loss : 58.088640213012695 Val_Reconstruction : 55.1971492767334 Val_KL : 2.891491413116455\n","Epoch: 1884/5000  Traning Loss: 59.013214111328125  Train_Reconstruction: 56.09058904647827  Train_KL: 2.9226246178150177  Validation Loss : 58.310577392578125 Val_Reconstruction : 55.42287063598633 Val_KL : 2.8877062797546387\n","Epoch: 1885/5000  Traning Loss: 59.05298089981079  Train_Reconstruction: 56.139729022979736  Train_KL: 2.9132520854473114  Validation Loss : 58.225236892700195 Val_Reconstruction : 55.34046936035156 Val_KL : 2.884766936302185\n","Epoch: 1886/5000  Traning Loss: 58.99276161193848  Train_Reconstruction: 56.083972454071045  Train_KL: 2.9087894558906555  Validation Loss : 58.23251914978027 Val_Reconstruction : 55.353981018066406 Val_KL : 2.8785382509231567\n","Epoch: 1887/5000  Traning Loss: 58.68424081802368  Train_Reconstruction: 55.76804065704346  Train_KL: 2.9162007570266724  Validation Loss : 57.88165473937988 Val_Reconstruction : 54.98670959472656 Val_KL : 2.8949440717697144\n","Epoch: 1888/5000  Traning Loss: 58.536577224731445  Train_Reconstruction: 55.62586688995361  Train_KL: 2.9107098281383514  Validation Loss : 57.82607460021973 Val_Reconstruction : 54.95798873901367 Val_KL : 2.868085265159607\n","Epoch: 1889/5000  Traning Loss: 58.776079177856445  Train_Reconstruction: 55.88073492050171  Train_KL: 2.8953436613082886  Validation Loss : 57.908512115478516 Val_Reconstruction : 55.033565521240234 Val_KL : 2.874946355819702\n","Epoch: 1890/5000  Traning Loss: 58.84538745880127  Train_Reconstruction: 55.92605495452881  Train_KL: 2.9193325340747833  Validation Loss : 57.92767143249512 Val_Reconstruction : 55.03146934509277 Val_KL : 2.8962029218673706\n","Epoch: 1891/5000  Traning Loss: 58.82618808746338  Train_Reconstruction: 55.9007511138916  Train_KL: 2.9254363775253296  Validation Loss : 57.733327865600586 Val_Reconstruction : 54.83950996398926 Val_KL : 2.893817663192749\n","Epoch: 1892/5000  Traning Loss: 58.60041618347168  Train_Reconstruction: 55.67496728897095  Train_KL: 2.9254485964775085  Validation Loss : 57.73916244506836 Val_Reconstruction : 54.841840744018555 Val_KL : 2.897320508956909\n","Epoch: 1893/5000  Traning Loss: 58.610294342041016  Train_Reconstruction: 55.68446636199951  Train_KL: 2.925828456878662  Validation Loss : 57.86713790893555 Val_Reconstruction : 54.97403144836426 Val_KL : 2.8931058645248413\n","Epoch: 1894/5000  Traning Loss: 58.78717613220215  Train_Reconstruction: 55.86743497848511  Train_KL: 2.919740855693817  Validation Loss : 58.03604698181152 Val_Reconstruction : 55.148685455322266 Val_KL : 2.8873625993728638\n","Epoch: 1895/5000  Traning Loss: 58.759482860565186  Train_Reconstruction: 55.83915424346924  Train_KL: 2.9203287959098816  Validation Loss : 57.82868003845215 Val_Reconstruction : 54.93514633178711 Val_KL : 2.893533229827881\n","Epoch: 1896/5000  Traning Loss: 58.839056968688965  Train_Reconstruction: 55.92015314102173  Train_KL: 2.918903946876526  Validation Loss : 58.33659553527832 Val_Reconstruction : 55.455284118652344 Val_KL : 2.881311297416687\n","Epoch: 1897/5000  Traning Loss: 59.35233783721924  Train_Reconstruction: 56.437297344207764  Train_KL: 2.915040612220764  Validation Loss : 58.55881690979004 Val_Reconstruction : 55.6821346282959 Val_KL : 2.8766822814941406\n","Epoch: 1898/5000  Traning Loss: 59.19413137435913  Train_Reconstruction: 56.27771234512329  Train_KL: 2.916419565677643  Validation Loss : 58.60457992553711 Val_Reconstruction : 55.71846961975098 Val_KL : 2.886110782623291\n","Epoch: 1899/5000  Traning Loss: 59.24211597442627  Train_Reconstruction: 56.329959869384766  Train_KL: 2.9121563136577606  Validation Loss : 58.34539794921875 Val_Reconstruction : 55.46345520019531 Val_KL : 2.881943702697754\n","Epoch: 1900/5000  Traning Loss: 59.14134502410889  Train_Reconstruction: 56.22471237182617  Train_KL: 2.9166325628757477  Validation Loss : 58.71657180786133 Val_Reconstruction : 55.822364807128906 Val_KL : 2.894208312034607\n","Epoch: 1901/5000  Traning Loss: 59.18644618988037  Train_Reconstruction: 56.259525299072266  Train_KL: 2.926921308040619  Validation Loss : 59.05777168273926 Val_Reconstruction : 56.16651725769043 Val_KL : 2.8912546634674072\n","Epoch: 1902/5000  Traning Loss: 59.24621868133545  Train_Reconstruction: 56.33222770690918  Train_KL: 2.913991302251816  Validation Loss : 58.645965576171875 Val_Reconstruction : 55.77386283874512 Val_KL : 2.8721020221710205\n","Epoch: 1903/5000  Traning Loss: 59.08740186691284  Train_Reconstruction: 56.175575733184814  Train_KL: 2.911826640367508  Validation Loss : 58.98604774475098 Val_Reconstruction : 56.09442329406738 Val_KL : 2.891623616218567\n","Epoch: 1904/5000  Traning Loss: 59.15977096557617  Train_Reconstruction: 56.23606491088867  Train_KL: 2.9237054586410522  Validation Loss : 58.08490562438965 Val_Reconstruction : 55.184635162353516 Val_KL : 2.9002718925476074\n","Epoch: 1905/5000  Traning Loss: 58.82613468170166  Train_Reconstruction: 55.913676738739014  Train_KL: 2.912457823753357  Validation Loss : 57.943180084228516 Val_Reconstruction : 55.073347091674805 Val_KL : 2.8698337078094482\n","Epoch: 1906/5000  Traning Loss: 58.60725688934326  Train_Reconstruction: 55.700583934783936  Train_KL: 2.9066728949546814  Validation Loss : 57.71490669250488 Val_Reconstruction : 54.836416244506836 Val_KL : 2.8784900903701782\n","Epoch: 1907/5000  Traning Loss: 58.68667554855347  Train_Reconstruction: 55.77872610092163  Train_KL: 2.907949447631836  Validation Loss : 58.10152244567871 Val_Reconstruction : 55.21830749511719 Val_KL : 2.8832149505615234\n","Epoch: 1908/5000  Traning Loss: 58.60302972793579  Train_Reconstruction: 55.687289237976074  Train_KL: 2.915740728378296  Validation Loss : 57.77238464355469 Val_Reconstruction : 54.88145065307617 Val_KL : 2.890934109687805\n","Epoch: 1909/5000  Traning Loss: 58.630040645599365  Train_Reconstruction: 55.71314859390259  Train_KL: 2.916891396045685  Validation Loss : 58.12357139587402 Val_Reconstruction : 55.23690223693848 Val_KL : 2.8866692781448364\n","Epoch: 1910/5000  Traning Loss: 58.70701313018799  Train_Reconstruction: 55.79353189468384  Train_KL: 2.913480907678604  Validation Loss : 57.73348617553711 Val_Reconstruction : 54.84588623046875 Val_KL : 2.8875999450683594\n","Epoch: 1911/5000  Traning Loss: 58.736427783966064  Train_Reconstruction: 55.81817674636841  Train_KL: 2.9182510673999786  Validation Loss : 57.80343818664551 Val_Reconstruction : 54.91555404663086 Val_KL : 2.8878846168518066\n","Epoch: 1912/5000  Traning Loss: 58.904727935791016  Train_Reconstruction: 55.98376703262329  Train_KL: 2.9209598302841187  Validation Loss : 58.496829986572266 Val_Reconstruction : 55.611419677734375 Val_KL : 2.8854100704193115\n","Epoch: 1913/5000  Traning Loss: 59.195892333984375  Train_Reconstruction: 56.27904939651489  Train_KL: 2.9168421626091003  Validation Loss : 58.3322696685791 Val_Reconstruction : 55.453338623046875 Val_KL : 2.878931164741516\n","Epoch: 1914/5000  Traning Loss: 58.90371608734131  Train_Reconstruction: 55.97612810134888  Train_KL: 2.9275885224342346  Validation Loss : 58.1527156829834 Val_Reconstruction : 55.24226188659668 Val_KL : 2.91045343875885\n","Epoch: 1915/5000  Traning Loss: 59.05835485458374  Train_Reconstruction: 56.12958574295044  Train_KL: 2.928769290447235  Validation Loss : 58.679683685302734 Val_Reconstruction : 55.79597091674805 Val_KL : 2.8837130069732666\n","Epoch: 1916/5000  Traning Loss: 59.001962184906006  Train_Reconstruction: 56.09637403488159  Train_KL: 2.9055885076522827  Validation Loss : 58.067880630493164 Val_Reconstruction : 55.18956184387207 Val_KL : 2.8783185482025146\n","Epoch: 1917/5000  Traning Loss: 59.3618597984314  Train_Reconstruction: 56.442628383636475  Train_KL: 2.9192317128181458  Validation Loss : 58.48275566101074 Val_Reconstruction : 55.5882453918457 Val_KL : 2.89451003074646\n","Epoch: 1918/5000  Traning Loss: 58.944326400756836  Train_Reconstruction: 56.02689981460571  Train_KL: 2.9174268543720245  Validation Loss : 58.0607852935791 Val_Reconstruction : 55.17839813232422 Val_KL : 2.882388472557068\n","Epoch: 1919/5000  Traning Loss: 58.83959627151489  Train_Reconstruction: 55.92102003097534  Train_KL: 2.9185761511325836  Validation Loss : 57.915191650390625 Val_Reconstruction : 55.02116394042969 Val_KL : 2.8940277099609375\n","Epoch: 1920/5000  Traning Loss: 58.75779628753662  Train_Reconstruction: 55.84002685546875  Train_KL: 2.9177700877189636  Validation Loss : 57.90599822998047 Val_Reconstruction : 55.015201568603516 Val_KL : 2.890796422958374\n","Epoch: 1921/5000  Traning Loss: 58.884103775024414  Train_Reconstruction: 55.96568536758423  Train_KL: 2.918418735265732  Validation Loss : 58.5020637512207 Val_Reconstruction : 55.6109561920166 Val_KL : 2.8911091089248657\n","Epoch: 1922/5000  Traning Loss: 58.90871000289917  Train_Reconstruction: 55.992043018341064  Train_KL: 2.91666641831398  Validation Loss : 58.32217216491699 Val_Reconstruction : 55.43085479736328 Val_KL : 2.891318202018738\n","Epoch: 1923/5000  Traning Loss: 58.850889682769775  Train_Reconstruction: 55.923519134521484  Train_KL: 2.927370101213455  Validation Loss : 58.065486907958984 Val_Reconstruction : 55.16267967224121 Val_KL : 2.9028072357177734\n","Epoch: 1924/5000  Traning Loss: 58.70539474487305  Train_Reconstruction: 55.77600812911987  Train_KL: 2.9293867647647858  Validation Loss : 57.80817794799805 Val_Reconstruction : 54.91484069824219 Val_KL : 2.89333713054657\n","Epoch: 1925/5000  Traning Loss: 58.940735816955566  Train_Reconstruction: 56.021960735321045  Train_KL: 2.918774902820587  Validation Loss : 58.18338966369629 Val_Reconstruction : 55.29076957702637 Val_KL : 2.892621159553528\n","Epoch: 1926/5000  Traning Loss: 58.91279411315918  Train_Reconstruction: 55.993972301483154  Train_KL: 2.9188219904899597  Validation Loss : 57.93843460083008 Val_Reconstruction : 55.04299736022949 Val_KL : 2.895436644554138\n","Epoch: 1927/5000  Traning Loss: 58.8411169052124  Train_Reconstruction: 55.92935276031494  Train_KL: 2.9117645025253296  Validation Loss : 57.808767318725586 Val_Reconstruction : 54.92385292053223 Val_KL : 2.8849146366119385\n","Epoch: 1928/5000  Traning Loss: 58.80382490158081  Train_Reconstruction: 55.88837385177612  Train_KL: 2.9154507219791412  Validation Loss : 57.763410568237305 Val_Reconstruction : 54.86444854736328 Val_KL : 2.898961305618286\n","Epoch: 1929/5000  Traning Loss: 58.89081287384033  Train_Reconstruction: 55.96860599517822  Train_KL: 2.9222066402435303  Validation Loss : 58.090904235839844 Val_Reconstruction : 55.20138740539551 Val_KL : 2.889517903327942\n","Epoch: 1930/5000  Traning Loss: 59.14106750488281  Train_Reconstruction: 56.22300958633423  Train_KL: 2.91805762052536  Validation Loss : 58.55100059509277 Val_Reconstruction : 55.67060089111328 Val_KL : 2.8803991079330444\n","Epoch: 1931/5000  Traning Loss: 58.994404315948486  Train_Reconstruction: 56.08028268814087  Train_KL: 2.9141213595867157  Validation Loss : 58.16744422912598 Val_Reconstruction : 55.28857612609863 Val_KL : 2.878869414329529\n","Epoch: 1932/5000  Traning Loss: 58.97275114059448  Train_Reconstruction: 56.04822874069214  Train_KL: 2.9245219230651855  Validation Loss : 57.98721694946289 Val_Reconstruction : 55.0958251953125 Val_KL : 2.891392469406128\n","Epoch: 1933/5000  Traning Loss: 58.826438903808594  Train_Reconstruction: 55.908122539520264  Train_KL: 2.9183164834976196  Validation Loss : 57.66326332092285 Val_Reconstruction : 54.77493667602539 Val_KL : 2.8883273601531982\n","Epoch: 1934/5000  Traning Loss: 58.660839557647705  Train_Reconstruction: 55.74313926696777  Train_KL: 2.9177001416683197  Validation Loss : 57.888553619384766 Val_Reconstruction : 54.998212814331055 Val_KL : 2.8903409242630005\n","Epoch: 1935/5000  Traning Loss: 58.54572820663452  Train_Reconstruction: 55.61807441711426  Train_KL: 2.9276535511016846  Validation Loss : 57.92359733581543 Val_Reconstruction : 55.026912689208984 Val_KL : 2.8966835737228394\n","Epoch: 1936/5000  Traning Loss: 58.794628620147705  Train_Reconstruction: 55.86690807342529  Train_KL: 2.9277215003967285  Validation Loss : 58.16992378234863 Val_Reconstruction : 55.26747512817383 Val_KL : 2.9024477005004883\n","Epoch: 1937/5000  Traning Loss: 59.161025524139404  Train_Reconstruction: 56.23985433578491  Train_KL: 2.921171694993973  Validation Loss : 58.363433837890625 Val_Reconstruction : 55.47588920593262 Val_KL : 2.8875447511672974\n","Epoch: 1938/5000  Traning Loss: 58.83933687210083  Train_Reconstruction: 55.91888475418091  Train_KL: 2.920452982187271  Validation Loss : 57.92364311218262 Val_Reconstruction : 55.03409767150879 Val_KL : 2.889546036720276\n","Epoch: 1939/5000  Traning Loss: 58.79527235031128  Train_Reconstruction: 55.86911487579346  Train_KL: 2.9261575043201447  Validation Loss : 58.53881072998047 Val_Reconstruction : 55.63255500793457 Val_KL : 2.9062557220458984\n","Epoch: 1940/5000  Traning Loss: 58.83768081665039  Train_Reconstruction: 55.912996768951416  Train_KL: 2.9246844053268433  Validation Loss : 57.92851638793945 Val_Reconstruction : 55.03688430786133 Val_KL : 2.8916313648223877\n","Epoch: 1941/5000  Traning Loss: 58.824517250061035  Train_Reconstruction: 55.892887115478516  Train_KL: 2.931630253791809  Validation Loss : 58.2220516204834 Val_Reconstruction : 55.32006645202637 Val_KL : 2.9019861221313477\n","Epoch: 1942/5000  Traning Loss: 58.80203437805176  Train_Reconstruction: 55.875977993011475  Train_KL: 2.9260567724704742  Validation Loss : 58.126604080200195 Val_Reconstruction : 55.242767333984375 Val_KL : 2.8838361501693726\n","Epoch: 1943/5000  Traning Loss: 58.950236797332764  Train_Reconstruction: 56.024882316589355  Train_KL: 2.9253546595573425  Validation Loss : 58.30472373962402 Val_Reconstruction : 55.403133392333984 Val_KL : 2.9015899896621704\n","Epoch: 1944/5000  Traning Loss: 59.12473392486572  Train_Reconstruction: 56.197875022888184  Train_KL: 2.926857888698578  Validation Loss : 58.22833061218262 Val_Reconstruction : 55.33221435546875 Val_KL : 2.8961150646209717\n","Epoch: 1945/5000  Traning Loss: 59.06957721710205  Train_Reconstruction: 56.143239974975586  Train_KL: 2.9263369739055634  Validation Loss : 58.11337089538574 Val_Reconstruction : 55.203521728515625 Val_KL : 2.909849524497986\n","Epoch: 1946/5000  Traning Loss: 58.96788740158081  Train_Reconstruction: 56.03784894943237  Train_KL: 2.9300383925437927  Validation Loss : 57.90047645568848 Val_Reconstruction : 55.01444435119629 Val_KL : 2.8860318660736084\n","Epoch: 1947/5000  Traning Loss: 59.05880308151245  Train_Reconstruction: 56.1444878578186  Train_KL: 2.914314955472946  Validation Loss : 58.187644958496094 Val_Reconstruction : 55.29348564147949 Val_KL : 2.8941593170166016\n","Epoch: 1948/5000  Traning Loss: 59.290727615356445  Train_Reconstruction: 56.363298416137695  Train_KL: 2.927429497241974  Validation Loss : 58.589271545410156 Val_Reconstruction : 55.69701957702637 Val_KL : 2.8922524452209473\n","Epoch: 1949/5000  Traning Loss: 58.9263014793396  Train_Reconstruction: 56.00211048126221  Train_KL: 2.924191653728485  Validation Loss : 57.952369689941406 Val_Reconstruction : 55.05998992919922 Val_KL : 2.8923795223236084\n","Epoch: 1950/5000  Traning Loss: 58.691425800323486  Train_Reconstruction: 55.77141332626343  Train_KL: 2.9200126230716705  Validation Loss : 57.78611946105957 Val_Reconstruction : 54.90694618225098 Val_KL : 2.8791736364364624\n","Epoch: 1951/5000  Traning Loss: 58.873167991638184  Train_Reconstruction: 55.959203243255615  Train_KL: 2.913964420557022  Validation Loss : 57.75882148742676 Val_Reconstruction : 54.87009239196777 Val_KL : 2.8887298107147217\n","Epoch: 1952/5000  Traning Loss: 58.676151275634766  Train_Reconstruction: 55.75659799575806  Train_KL: 2.9195530116558075  Validation Loss : 57.61493682861328 Val_Reconstruction : 54.71989440917969 Val_KL : 2.895042896270752\n","Epoch: 1953/5000  Traning Loss: 58.602262020111084  Train_Reconstruction: 55.67753791809082  Train_KL: 2.924723982810974  Validation Loss : 57.75956916809082 Val_Reconstruction : 54.86740684509277 Val_KL : 2.8921608924865723\n","Epoch: 1954/5000  Traning Loss: 58.92672061920166  Train_Reconstruction: 55.99868440628052  Train_KL: 2.9280361235141754  Validation Loss : 58.038368225097656 Val_Reconstruction : 55.14568901062012 Val_KL : 2.8926806449890137\n","Epoch: 1955/5000  Traning Loss: 58.90007305145264  Train_Reconstruction: 55.984416484832764  Train_KL: 2.9156573712825775  Validation Loss : 57.658836364746094 Val_Reconstruction : 54.77216148376465 Val_KL : 2.886675238609314\n","Epoch: 1956/5000  Traning Loss: 58.611462116241455  Train_Reconstruction: 55.69371747970581  Train_KL: 2.9177449345588684  Validation Loss : 57.87795639038086 Val_Reconstruction : 54.994829177856445 Val_KL : 2.883128046989441\n","Epoch: 1957/5000  Traning Loss: 58.641419410705566  Train_Reconstruction: 55.72774410247803  Train_KL: 2.9136751294136047  Validation Loss : 57.90166091918945 Val_Reconstruction : 55.01956748962402 Val_KL : 2.882094979286194\n","Epoch: 1958/5000  Traning Loss: 58.6022162437439  Train_Reconstruction: 55.68421649932861  Train_KL: 2.917999893426895  Validation Loss : 57.71864891052246 Val_Reconstruction : 54.833513259887695 Val_KL : 2.8851360082626343\n","Epoch: 1959/5000  Traning Loss: 58.5668420791626  Train_Reconstruction: 55.62757062911987  Train_KL: 2.9392715990543365  Validation Loss : 57.76076698303223 Val_Reconstruction : 54.85956954956055 Val_KL : 2.901196002960205\n","Epoch: 1960/5000  Traning Loss: 58.63788366317749  Train_Reconstruction: 55.714460372924805  Train_KL: 2.923423409461975  Validation Loss : 57.9289436340332 Val_Reconstruction : 55.05813980102539 Val_KL : 2.8708040714263916\n","Epoch: 1961/5000  Traning Loss: 58.766629219055176  Train_Reconstruction: 55.858383655548096  Train_KL: 2.9082455337047577  Validation Loss : 57.861549377441406 Val_Reconstruction : 54.99197196960449 Val_KL : 2.869578242301941\n","Epoch: 1962/5000  Traning Loss: 58.682137966156006  Train_Reconstruction: 55.773465156555176  Train_KL: 2.9086727797985077  Validation Loss : 57.786155700683594 Val_Reconstruction : 54.91699028015137 Val_KL : 2.8691658973693848\n","Epoch: 1963/5000  Traning Loss: 58.577195167541504  Train_Reconstruction: 55.66615295410156  Train_KL: 2.9110428988933563  Validation Loss : 57.631385803222656 Val_Reconstruction : 54.74554634094238 Val_KL : 2.8858394622802734\n","Epoch: 1964/5000  Traning Loss: 58.506086349487305  Train_Reconstruction: 55.587878704071045  Train_KL: 2.9182077646255493  Validation Loss : 57.5549373626709 Val_Reconstruction : 54.68246841430664 Val_KL : 2.872468590736389\n","Epoch: 1965/5000  Traning Loss: 58.366393089294434  Train_Reconstruction: 55.44611120223999  Train_KL: 2.9202821254730225  Validation Loss : 57.60373497009277 Val_Reconstruction : 54.71563148498535 Val_KL : 2.8881030082702637\n","Epoch: 1966/5000  Traning Loss: 59.02062129974365  Train_Reconstruction: 56.09617376327515  Train_KL: 2.9244468808174133  Validation Loss : 58.83004379272461 Val_Reconstruction : 55.929988861083984 Val_KL : 2.90005362033844\n","Epoch: 1967/5000  Traning Loss: 59.605785846710205  Train_Reconstruction: 56.686030864715576  Train_KL: 2.919754534959793  Validation Loss : 58.72719955444336 Val_Reconstruction : 55.83845138549805 Val_KL : 2.888747811317444\n","Epoch: 1968/5000  Traning Loss: 59.32358169555664  Train_Reconstruction: 56.401482582092285  Train_KL: 2.9220989048480988  Validation Loss : 58.614707946777344 Val_Reconstruction : 55.73236274719238 Val_KL : 2.8823453187942505\n","Epoch: 1969/5000  Traning Loss: 58.86017608642578  Train_Reconstruction: 55.944568157196045  Train_KL: 2.915608525276184  Validation Loss : 57.9665412902832 Val_Reconstruction : 55.08029365539551 Val_KL : 2.8862475156784058\n","Epoch: 1970/5000  Traning Loss: 58.99847984313965  Train_Reconstruction: 56.07856512069702  Train_KL: 2.9199143648147583  Validation Loss : 58.061262130737305 Val_Reconstruction : 55.17109298706055 Val_KL : 2.890169858932495\n","Epoch: 1971/5000  Traning Loss: 58.71866798400879  Train_Reconstruction: 55.79816961288452  Train_KL: 2.9204979836940765  Validation Loss : 58.168107986450195 Val_Reconstruction : 55.288082122802734 Val_KL : 2.8800251483917236\n","Epoch: 1972/5000  Traning Loss: 58.64946126937866  Train_Reconstruction: 55.73178291320801  Train_KL: 2.91767817735672  Validation Loss : 58.128971099853516 Val_Reconstruction : 55.244720458984375 Val_KL : 2.8842498064041138\n","Epoch: 1973/5000  Traning Loss: 58.777974128723145  Train_Reconstruction: 55.84380388259888  Train_KL: 2.9341698586940765  Validation Loss : 57.88700485229492 Val_Reconstruction : 54.99052619934082 Val_KL : 2.89647901058197\n","Epoch: 1974/5000  Traning Loss: 58.617894649505615  Train_Reconstruction: 55.70122575759888  Train_KL: 2.9166689813137054  Validation Loss : 57.73944664001465 Val_Reconstruction : 54.85520935058594 Val_KL : 2.88423752784729\n","Epoch: 1975/5000  Traning Loss: 58.587928771972656  Train_Reconstruction: 55.670183181762695  Train_KL: 2.9177457690238953  Validation Loss : 57.928707122802734 Val_Reconstruction : 55.03116989135742 Val_KL : 2.8975377082824707\n","Epoch: 1976/5000  Traning Loss: 58.71226358413696  Train_Reconstruction: 55.78681135177612  Train_KL: 2.925452381372452  Validation Loss : 57.796255111694336 Val_Reconstruction : 54.89689254760742 Val_KL : 2.8993618488311768\n","Epoch: 1977/5000  Traning Loss: 58.72562885284424  Train_Reconstruction: 55.804255962371826  Train_KL: 2.9213730096817017  Validation Loss : 58.114023208618164 Val_Reconstruction : 55.230607986450195 Val_KL : 2.8834153413772583\n","Epoch: 1978/5000  Traning Loss: 58.83996868133545  Train_Reconstruction: 55.9269061088562  Train_KL: 2.9130627512931824  Validation Loss : 57.66455841064453 Val_Reconstruction : 54.78610610961914 Val_KL : 2.878452777862549\n","Epoch: 1979/5000  Traning Loss: 58.56313896179199  Train_Reconstruction: 55.64473342895508  Train_KL: 2.9184051156044006  Validation Loss : 57.81806564331055 Val_Reconstruction : 54.93389892578125 Val_KL : 2.8841673135757446\n","Epoch: 1980/5000  Traning Loss: 58.851747035980225  Train_Reconstruction: 55.93068742752075  Train_KL: 2.9210597574710846  Validation Loss : 58.284915924072266 Val_Reconstruction : 55.395273208618164 Val_KL : 2.889643669128418\n","Epoch: 1981/5000  Traning Loss: 59.01064729690552  Train_Reconstruction: 56.09666204452515  Train_KL: 2.9139846563339233  Validation Loss : 57.80059623718262 Val_Reconstruction : 54.92717933654785 Val_KL : 2.8734169006347656\n","Epoch: 1982/5000  Traning Loss: 58.755518436431885  Train_Reconstruction: 55.84081029891968  Train_KL: 2.91470929980278  Validation Loss : 58.157127380371094 Val_Reconstruction : 55.26474952697754 Val_KL : 2.892378330230713\n","Epoch: 1983/5000  Traning Loss: 58.66746139526367  Train_Reconstruction: 55.754250049591064  Train_KL: 2.9132113456726074  Validation Loss : 57.93918418884277 Val_Reconstruction : 55.054887771606445 Val_KL : 2.884296178817749\n","Epoch: 1984/5000  Traning Loss: 58.6774377822876  Train_Reconstruction: 55.756131649017334  Train_KL: 2.9213058948516846  Validation Loss : 57.79971694946289 Val_Reconstruction : 54.90330123901367 Val_KL : 2.8964145183563232\n","Epoch: 1985/5000  Traning Loss: 58.543517112731934  Train_Reconstruction: 55.61547613143921  Train_KL: 2.9280408024787903  Validation Loss : 57.76956367492676 Val_Reconstruction : 54.85868263244629 Val_KL : 2.910881519317627\n","Epoch: 1986/5000  Traning Loss: 58.7659330368042  Train_Reconstruction: 55.83515739440918  Train_KL: 2.9307758510112762  Validation Loss : 57.930973052978516 Val_Reconstruction : 55.0350227355957 Val_KL : 2.8959509134292603\n","Epoch: 1987/5000  Traning Loss: 58.96665668487549  Train_Reconstruction: 56.05122089385986  Train_KL: 2.9154359102249146  Validation Loss : 58.30832481384277 Val_Reconstruction : 55.423471450805664 Val_KL : 2.884853482246399\n","Epoch: 1988/5000  Traning Loss: 59.03172492980957  Train_Reconstruction: 56.10734176635742  Train_KL: 2.9243834018707275  Validation Loss : 57.93851089477539 Val_Reconstruction : 55.04303550720215 Val_KL : 2.895475745201111\n","Epoch: 1989/5000  Traning Loss: 58.876338958740234  Train_Reconstruction: 55.94502782821655  Train_KL: 2.9313111305236816  Validation Loss : 57.8466682434082 Val_Reconstruction : 54.94673538208008 Val_KL : 2.899933934211731\n","Epoch: 1990/5000  Traning Loss: 58.60812282562256  Train_Reconstruction: 55.68079662322998  Train_KL: 2.927326738834381  Validation Loss : 57.90806198120117 Val_Reconstruction : 55.01460266113281 Val_KL : 2.8934600353240967\n","Epoch: 1991/5000  Traning Loss: 58.601848125457764  Train_Reconstruction: 55.68283748626709  Train_KL: 2.9190106093883514  Validation Loss : 57.79450988769531 Val_Reconstruction : 54.90517807006836 Val_KL : 2.889331579208374\n","Epoch: 1992/5000  Traning Loss: 58.79037094116211  Train_Reconstruction: 55.8645133972168  Train_KL: 2.925857126712799  Validation Loss : 57.87949180603027 Val_Reconstruction : 54.98373222351074 Val_KL : 2.895761251449585\n","Epoch: 1993/5000  Traning Loss: 59.098812103271484  Train_Reconstruction: 56.169397830963135  Train_KL: 2.929413288831711  Validation Loss : 57.78234672546387 Val_Reconstruction : 54.88900947570801 Val_KL : 2.8933372497558594\n","Epoch: 1994/5000  Traning Loss: 58.84705352783203  Train_Reconstruction: 55.918607234954834  Train_KL: 2.9284461438655853  Validation Loss : 57.91921043395996 Val_Reconstruction : 55.016845703125 Val_KL : 2.9023654460906982\n","Epoch: 1995/5000  Traning Loss: 58.82168483734131  Train_Reconstruction: 55.88740634918213  Train_KL: 2.93427836894989  Validation Loss : 58.04307174682617 Val_Reconstruction : 55.13486671447754 Val_KL : 2.908205032348633\n","Epoch: 1996/5000  Traning Loss: 58.850903034210205  Train_Reconstruction: 55.918912410736084  Train_KL: 2.9319911003112793  Validation Loss : 58.25177001953125 Val_Reconstruction : 55.33791923522949 Val_KL : 2.9138498306274414\n","Epoch: 1997/5000  Traning Loss: 58.88993215560913  Train_Reconstruction: 55.95594787597656  Train_KL: 2.933984011411667  Validation Loss : 58.10664939880371 Val_Reconstruction : 55.20264434814453 Val_KL : 2.90400493144989\n","Epoch: 1998/5000  Traning Loss: 58.96873950958252  Train_Reconstruction: 56.040162086486816  Train_KL: 2.928576946258545  Validation Loss : 58.31032943725586 Val_Reconstruction : 55.42045593261719 Val_KL : 2.8898738622665405\n","Epoch: 1999/5000  Traning Loss: 59.53775072097778  Train_Reconstruction: 56.612671852111816  Train_KL: 2.9250783026218414  Validation Loss : 58.36781311035156 Val_Reconstruction : 55.47461128234863 Val_KL : 2.8932018280029297\n","Epoch: 2000/5000  Traning Loss: 59.281094551086426  Train_Reconstruction: 56.35441827774048  Train_KL: 2.926676243543625  Validation Loss : 57.88928031921387 Val_Reconstruction : 54.99994468688965 Val_KL : 2.889335870742798\n","Epoch: 2001/5000  Traning Loss: 58.97886848449707  Train_Reconstruction: 56.06212854385376  Train_KL: 2.9167400598526  Validation Loss : 58.06246566772461 Val_Reconstruction : 55.18302917480469 Val_KL : 2.8794370889663696\n","Epoch: 2002/5000  Traning Loss: 58.82996463775635  Train_Reconstruction: 55.90965127944946  Train_KL: 2.9203134775161743  Validation Loss : 58.02724838256836 Val_Reconstruction : 55.13395118713379 Val_KL : 2.893296003341675\n","Epoch: 2003/5000  Traning Loss: 58.974730014801025  Train_Reconstruction: 56.046634674072266  Train_KL: 2.928095579147339  Validation Loss : 58.272993087768555 Val_Reconstruction : 55.375478744506836 Val_KL : 2.897514224052429\n","Epoch: 2004/5000  Traning Loss: 59.129958152770996  Train_Reconstruction: 56.1984658241272  Train_KL: 2.931492656469345  Validation Loss : 58.45652389526367 Val_Reconstruction : 55.54536056518555 Val_KL : 2.911164164543152\n","Epoch: 2005/5000  Traning Loss: 58.758190631866455  Train_Reconstruction: 55.83617353439331  Train_KL: 2.922016888856888  Validation Loss : 57.85118293762207 Val_Reconstruction : 54.97588348388672 Val_KL : 2.875298857688904\n","Epoch: 2006/5000  Traning Loss: 58.7166051864624  Train_Reconstruction: 55.79523468017578  Train_KL: 2.921370208263397  Validation Loss : 57.70763397216797 Val_Reconstruction : 54.81338310241699 Val_KL : 2.8942500352859497\n","Epoch: 2007/5000  Traning Loss: 58.71234083175659  Train_Reconstruction: 55.77606534957886  Train_KL: 2.9362754225730896  Validation Loss : 57.80371284484863 Val_Reconstruction : 54.90299415588379 Val_KL : 2.9007190465927124\n","Epoch: 2008/5000  Traning Loss: 58.674684047698975  Train_Reconstruction: 55.73670244216919  Train_KL: 2.9379820823669434  Validation Loss : 57.89762306213379 Val_Reconstruction : 54.9858283996582 Val_KL : 2.911794662475586\n","Epoch: 2009/5000  Traning Loss: 58.758312702178955  Train_Reconstruction: 55.8322491645813  Train_KL: 2.926063597202301  Validation Loss : 58.09297561645508 Val_Reconstruction : 55.20321273803711 Val_KL : 2.8897626399993896\n","Epoch: 2010/5000  Traning Loss: 58.89596128463745  Train_Reconstruction: 55.977842807769775  Train_KL: 2.9181185960769653  Validation Loss : 57.96393394470215 Val_Reconstruction : 55.07608604431152 Val_KL : 2.8878477811813354\n","Epoch: 2011/5000  Traning Loss: 58.96404266357422  Train_Reconstruction: 56.041626930236816  Train_KL: 2.9224153459072113  Validation Loss : 58.26481246948242 Val_Reconstruction : 55.36981391906738 Val_KL : 2.8949981927871704\n","Epoch: 2012/5000  Traning Loss: 58.821298599243164  Train_Reconstruction: 55.90847301483154  Train_KL: 2.912826031446457  Validation Loss : 58.0976619720459 Val_Reconstruction : 55.22462463378906 Val_KL : 2.8730372190475464\n","Epoch: 2013/5000  Traning Loss: 59.03078031539917  Train_Reconstruction: 56.114296436309814  Train_KL: 2.916484296321869  Validation Loss : 58.34272003173828 Val_Reconstruction : 55.444448471069336 Val_KL : 2.898271679878235\n","Epoch: 2014/5000  Traning Loss: 58.826003551483154  Train_Reconstruction: 55.90113830566406  Train_KL: 2.9248653054237366  Validation Loss : 57.94611358642578 Val_Reconstruction : 55.04682922363281 Val_KL : 2.899284601211548\n","Epoch: 2015/5000  Traning Loss: 58.764118671417236  Train_Reconstruction: 55.83654737472534  Train_KL: 2.9275710582733154  Validation Loss : 57.93602180480957 Val_Reconstruction : 55.04023361206055 Val_KL : 2.895788311958313\n","Epoch: 2016/5000  Traning Loss: 58.916983127593994  Train_Reconstruction: 55.9939866065979  Train_KL: 2.922996401786804  Validation Loss : 58.00801467895508 Val_Reconstruction : 55.1196403503418 Val_KL : 2.8883742094039917\n","Epoch: 2017/5000  Traning Loss: 58.90304613113403  Train_Reconstruction: 55.97844362258911  Train_KL: 2.9246025383472443  Validation Loss : 58.20268249511719 Val_Reconstruction : 55.29894256591797 Val_KL : 2.9037389755249023\n","Epoch: 2018/5000  Traning Loss: 58.91166830062866  Train_Reconstruction: 55.98774766921997  Train_KL: 2.9239203333854675  Validation Loss : 58.149580001831055 Val_Reconstruction : 55.241750717163086 Val_KL : 2.907831072807312\n","Epoch: 2019/5000  Traning Loss: 58.68941116333008  Train_Reconstruction: 55.76410102844238  Train_KL: 2.9253106713294983  Validation Loss : 57.87688446044922 Val_Reconstruction : 54.98859977722168 Val_KL : 2.8882851600646973\n","Epoch: 2020/5000  Traning Loss: 58.55355644226074  Train_Reconstruction: 55.632282733917236  Train_KL: 2.9212736189365387  Validation Loss : 57.80516815185547 Val_Reconstruction : 54.91540336608887 Val_KL : 2.8897647857666016\n","Epoch: 2021/5000  Traning Loss: 58.4390754699707  Train_Reconstruction: 55.52490425109863  Train_KL: 2.9141708612442017  Validation Loss : 57.580657958984375 Val_Reconstruction : 54.69329833984375 Val_KL : 2.887359857559204\n","Epoch: 2022/5000  Traning Loss: 58.483402252197266  Train_Reconstruction: 55.558610916137695  Train_KL: 2.9247907102108  Validation Loss : 57.98149490356445 Val_Reconstruction : 55.085060119628906 Val_KL : 2.8964351415634155\n","Epoch: 2023/5000  Traning Loss: 58.6644082069397  Train_Reconstruction: 55.74212598800659  Train_KL: 2.9222824573516846  Validation Loss : 57.88059997558594 Val_Reconstruction : 54.99631690979004 Val_KL : 2.8842833042144775\n","Epoch: 2024/5000  Traning Loss: 58.874051094055176  Train_Reconstruction: 55.95315217971802  Train_KL: 2.920898824930191  Validation Loss : 57.95023536682129 Val_Reconstruction : 55.0600700378418 Val_KL : 2.890165328979492\n","Epoch: 2025/5000  Traning Loss: 58.41152906417847  Train_Reconstruction: 55.484211444854736  Train_KL: 2.927317351102829  Validation Loss : 57.5374870300293 Val_Reconstruction : 54.64667320251465 Val_KL : 2.89081346988678\n","Epoch: 2026/5000  Traning Loss: 58.496216773986816  Train_Reconstruction: 55.56803560256958  Train_KL: 2.9281804263591766  Validation Loss : 57.94411659240723 Val_Reconstruction : 55.04450225830078 Val_KL : 2.899614095687866\n","Epoch: 2027/5000  Traning Loss: 58.556312561035156  Train_Reconstruction: 55.626816749572754  Train_KL: 2.9294958114624023  Validation Loss : 57.82375144958496 Val_Reconstruction : 54.93472480773926 Val_KL : 2.8890268802642822\n","Epoch: 2028/5000  Traning Loss: 58.51612663269043  Train_Reconstruction: 55.60176420211792  Train_KL: 2.9143622517585754  Validation Loss : 57.912527084350586 Val_Reconstruction : 55.0261116027832 Val_KL : 2.8864163160324097\n","Epoch: 2029/5000  Traning Loss: 58.54977464675903  Train_Reconstruction: 55.62926197052002  Train_KL: 2.920511931180954  Validation Loss : 58.02901649475098 Val_Reconstruction : 55.129920959472656 Val_KL : 2.8990960121154785\n","Epoch: 2030/5000  Traning Loss: 58.54205656051636  Train_Reconstruction: 55.61010122299194  Train_KL: 2.9319556951522827  Validation Loss : 57.65983963012695 Val_Reconstruction : 54.76674842834473 Val_KL : 2.893089771270752\n","Epoch: 2031/5000  Traning Loss: 58.4919810295105  Train_Reconstruction: 55.56165266036987  Train_KL: 2.9303280413150787  Validation Loss : 57.75821495056152 Val_Reconstruction : 54.855966567993164 Val_KL : 2.902249336242676\n","Epoch: 2032/5000  Traning Loss: 58.40795373916626  Train_Reconstruction: 55.48112440109253  Train_KL: 2.9268298745155334  Validation Loss : 57.65275955200195 Val_Reconstruction : 54.764657974243164 Val_KL : 2.8881008625030518\n","Epoch: 2033/5000  Traning Loss: 58.91124439239502  Train_Reconstruction: 56.0082426071167  Train_KL: 2.903001695871353  Validation Loss : 58.25284767150879 Val_Reconstruction : 55.37665939331055 Val_KL : 2.8761874437332153\n","Epoch: 2034/5000  Traning Loss: 59.050814151763916  Train_Reconstruction: 56.134902477264404  Train_KL: 2.915912479162216  Validation Loss : 57.94110107421875 Val_Reconstruction : 55.04176330566406 Val_KL : 2.8993375301361084\n","Epoch: 2035/5000  Traning Loss: 58.709054470062256  Train_Reconstruction: 55.78663969039917  Train_KL: 2.9224146008491516  Validation Loss : 58.28349685668945 Val_Reconstruction : 55.39042091369629 Val_KL : 2.893074870109558\n","Epoch: 2036/5000  Traning Loss: 58.748108863830566  Train_Reconstruction: 55.81989669799805  Train_KL: 2.9282117187976837  Validation Loss : 58.03822708129883 Val_Reconstruction : 55.150489807128906 Val_KL : 2.8877370357513428\n","Epoch: 2037/5000  Traning Loss: 58.66246271133423  Train_Reconstruction: 55.737168312072754  Train_KL: 2.925294905900955  Validation Loss : 58.05179977416992 Val_Reconstruction : 55.142005920410156 Val_KL : 2.909793734550476\n","Epoch: 2038/5000  Traning Loss: 58.57512712478638  Train_Reconstruction: 55.63879632949829  Train_KL: 2.936330407857895  Validation Loss : 57.72446060180664 Val_Reconstruction : 54.81768989562988 Val_KL : 2.906770706176758\n","Epoch: 2039/5000  Traning Loss: 58.782413482666016  Train_Reconstruction: 55.852312088012695  Train_KL: 2.93010213971138  Validation Loss : 57.93988609313965 Val_Reconstruction : 55.04186248779297 Val_KL : 2.898023843765259\n","Epoch: 2040/5000  Traning Loss: 59.141141414642334  Train_Reconstruction: 56.20810270309448  Train_KL: 2.933038592338562  Validation Loss : 58.223501205444336 Val_Reconstruction : 55.31804084777832 Val_KL : 2.9054611921310425\n","Epoch: 2041/5000  Traning Loss: 59.075623512268066  Train_Reconstruction: 56.1447172164917  Train_KL: 2.930906444787979  Validation Loss : 58.20815658569336 Val_Reconstruction : 55.326805114746094 Val_KL : 2.881351113319397\n","Epoch: 2042/5000  Traning Loss: 59.00011491775513  Train_Reconstruction: 56.08399677276611  Train_KL: 2.9161179661750793  Validation Loss : 58.51444625854492 Val_Reconstruction : 55.62396240234375 Val_KL : 2.890483856201172\n","Epoch: 2043/5000  Traning Loss: 58.72564458847046  Train_Reconstruction: 55.801547050476074  Train_KL: 2.924097716808319  Validation Loss : 57.70810890197754 Val_Reconstruction : 54.810237884521484 Val_KL : 2.89787220954895\n","Epoch: 2044/5000  Traning Loss: 58.71570539474487  Train_Reconstruction: 55.79124736785889  Train_KL: 2.9244576394557953  Validation Loss : 57.78628730773926 Val_Reconstruction : 54.900516510009766 Val_KL : 2.88577139377594\n","Epoch: 2045/5000  Traning Loss: 58.466612339019775  Train_Reconstruction: 55.550289154052734  Train_KL: 2.916323721408844  Validation Loss : 57.6082706451416 Val_Reconstruction : 54.72141456604004 Val_KL : 2.886855363845825\n","Epoch: 2046/5000  Traning Loss: 58.32483530044556  Train_Reconstruction: 55.40562963485718  Train_KL: 2.919205605983734  Validation Loss : 57.45277786254883 Val_Reconstruction : 54.56528091430664 Val_KL : 2.887496590614319\n","Epoch: 2047/5000  Traning Loss: 58.25580072402954  Train_Reconstruction: 55.34351062774658  Train_KL: 2.9122898280620575  Validation Loss : 57.63315391540527 Val_Reconstruction : 54.755001068115234 Val_KL : 2.8781524896621704\n","Epoch: 2048/5000  Traning Loss: 58.40295648574829  Train_Reconstruction: 55.475733280181885  Train_KL: 2.9272229373455048  Validation Loss : 57.736711502075195 Val_Reconstruction : 54.844581604003906 Val_KL : 2.892130732536316\n","Epoch: 2049/5000  Traning Loss: 58.51088333129883  Train_Reconstruction: 55.58490800857544  Train_KL: 2.9259756803512573  Validation Loss : 57.71377944946289 Val_Reconstruction : 54.824209213256836 Val_KL : 2.889571189880371\n","Epoch: 2050/5000  Traning Loss: 58.870532512664795  Train_Reconstruction: 55.955543994903564  Train_KL: 2.9149889647960663  Validation Loss : 58.21299743652344 Val_Reconstruction : 55.34047889709473 Val_KL : 2.87251877784729\n","Epoch: 2051/5000  Traning Loss: 58.84315299987793  Train_Reconstruction: 55.92707443237305  Train_KL: 2.9160784780979156  Validation Loss : 58.29236030578613 Val_Reconstruction : 55.419795989990234 Val_KL : 2.8725645542144775\n","Epoch: 2052/5000  Traning Loss: 58.70060062408447  Train_Reconstruction: 55.78807210922241  Train_KL: 2.9125280380249023  Validation Loss : 57.86179161071777 Val_Reconstruction : 54.98022651672363 Val_KL : 2.8815656900405884\n","Epoch: 2053/5000  Traning Loss: 58.667720794677734  Train_Reconstruction: 55.74840068817139  Train_KL: 2.9193205535411835  Validation Loss : 57.99844932556152 Val_Reconstruction : 55.108633041381836 Val_KL : 2.8898171186447144\n","Epoch: 2054/5000  Traning Loss: 58.475972175598145  Train_Reconstruction: 55.554579734802246  Train_KL: 2.9213922321796417  Validation Loss : 57.64646339416504 Val_Reconstruction : 54.76411819458008 Val_KL : 2.882344603538513\n","Epoch: 2055/5000  Traning Loss: 58.694448947906494  Train_Reconstruction: 55.7763614654541  Train_KL: 2.9180869460105896  Validation Loss : 58.030473709106445 Val_Reconstruction : 55.14484405517578 Val_KL : 2.8856281042099\n","Epoch: 2056/5000  Traning Loss: 58.674771308898926  Train_Reconstruction: 55.74933099746704  Train_KL: 2.9254404604434967  Validation Loss : 57.689544677734375 Val_Reconstruction : 54.8053092956543 Val_KL : 2.8842345476150513\n","Epoch: 2057/5000  Traning Loss: 58.60419845581055  Train_Reconstruction: 55.687286376953125  Train_KL: 2.9169120490550995  Validation Loss : 57.947505950927734 Val_Reconstruction : 55.058950424194336 Val_KL : 2.88855516910553\n","Epoch: 2058/5000  Traning Loss: 58.594014167785645  Train_Reconstruction: 55.66614532470703  Train_KL: 2.927868276834488  Validation Loss : 58.07339859008789 Val_Reconstruction : 55.16849708557129 Val_KL : 2.9049015045166016\n","Epoch: 2059/5000  Traning Loss: 58.61394023895264  Train_Reconstruction: 55.67790985107422  Train_KL: 2.9360302686691284  Validation Loss : 57.95220375061035 Val_Reconstruction : 55.04833793640137 Val_KL : 2.9038658142089844\n","Epoch: 2060/5000  Traning Loss: 58.57496976852417  Train_Reconstruction: 55.645731925964355  Train_KL: 2.9292378425598145  Validation Loss : 58.043731689453125 Val_Reconstruction : 55.14063835144043 Val_KL : 2.9030929803848267\n","Epoch: 2061/5000  Traning Loss: 58.789823055267334  Train_Reconstruction: 55.85632276535034  Train_KL: 2.933500051498413  Validation Loss : 58.82083511352539 Val_Reconstruction : 55.91809844970703 Val_KL : 2.90273654460907\n","Epoch: 2062/5000  Traning Loss: 58.7781867980957  Train_Reconstruction: 55.8515682220459  Train_KL: 2.9266184270381927  Validation Loss : 57.42913627624512 Val_Reconstruction : 54.53778266906738 Val_KL : 2.8913527727127075\n","Epoch: 2063/5000  Traning Loss: 58.53980016708374  Train_Reconstruction: 55.61821699142456  Train_KL: 2.92158380150795  Validation Loss : 57.69207954406738 Val_Reconstruction : 54.800315856933594 Val_KL : 2.891762614250183\n","Epoch: 2064/5000  Traning Loss: 58.43557786941528  Train_Reconstruction: 55.51847696304321  Train_KL: 2.9171008467674255  Validation Loss : 57.905099868774414 Val_Reconstruction : 55.0203742980957 Val_KL : 2.884726047515869\n","Epoch: 2065/5000  Traning Loss: 58.55113172531128  Train_Reconstruction: 55.63968229293823  Train_KL: 2.911449283361435  Validation Loss : 57.950260162353516 Val_Reconstruction : 55.05821990966797 Val_KL : 2.8920388221740723\n","Epoch: 2066/5000  Traning Loss: 58.54544115066528  Train_Reconstruction: 55.621896743774414  Train_KL: 2.9235441982746124  Validation Loss : 57.7637825012207 Val_Reconstruction : 54.87050437927246 Val_KL : 2.8932788372039795\n","Epoch: 2067/5000  Traning Loss: 58.45720815658569  Train_Reconstruction: 55.53727340698242  Train_KL: 2.9199343621730804  Validation Loss : 57.65213966369629 Val_Reconstruction : 54.769426345825195 Val_KL : 2.8827123641967773\n","Epoch: 2068/5000  Traning Loss: 58.57006216049194  Train_Reconstruction: 55.65067768096924  Train_KL: 2.919384866952896  Validation Loss : 57.781036376953125 Val_Reconstruction : 54.89004707336426 Val_KL : 2.8909889459609985\n","Epoch: 2069/5000  Traning Loss: 58.5391149520874  Train_Reconstruction: 55.618815898895264  Train_KL: 2.9202988147735596  Validation Loss : 58.08136177062988 Val_Reconstruction : 55.20229911804199 Val_KL : 2.879064202308655\n","Epoch: 2070/5000  Traning Loss: 58.56316566467285  Train_Reconstruction: 55.649248123168945  Train_KL: 2.9139181673526764  Validation Loss : 57.69976234436035 Val_Reconstruction : 54.82033348083496 Val_KL : 2.8794283866882324\n","Epoch: 2071/5000  Traning Loss: 58.61759614944458  Train_Reconstruction: 55.69721698760986  Train_KL: 2.9203793704509735  Validation Loss : 57.90243339538574 Val_Reconstruction : 55.01401710510254 Val_KL : 2.888415575027466\n","Epoch: 2072/5000  Traning Loss: 58.6731481552124  Train_Reconstruction: 55.76099729537964  Train_KL: 2.912150502204895  Validation Loss : 57.76337242126465 Val_Reconstruction : 54.87055778503418 Val_KL : 2.8928143978118896\n","Epoch: 2073/5000  Traning Loss: 58.67616128921509  Train_Reconstruction: 55.75493335723877  Train_KL: 2.9212273955345154  Validation Loss : 58.08563041687012 Val_Reconstruction : 55.198896408081055 Val_KL : 2.886733651161194\n","Epoch: 2074/5000  Traning Loss: 58.700987339019775  Train_Reconstruction: 55.7792010307312  Train_KL: 2.921786218881607  Validation Loss : 58.40156173706055 Val_Reconstruction : 55.5124397277832 Val_KL : 2.8891220092773438\n","Epoch: 2075/5000  Traning Loss: 59.05499315261841  Train_Reconstruction: 56.127660274505615  Train_KL: 2.9273330867290497  Validation Loss : 58.36848831176758 Val_Reconstruction : 55.47873115539551 Val_KL : 2.8897567987442017\n","Epoch: 2076/5000  Traning Loss: 59.393298625946045  Train_Reconstruction: 56.485026836395264  Train_KL: 2.9082721769809723  Validation Loss : 58.16992378234863 Val_Reconstruction : 55.27881622314453 Val_KL : 2.8911086320877075\n","Epoch: 2077/5000  Traning Loss: 58.94670009613037  Train_Reconstruction: 56.02743101119995  Train_KL: 2.9192700684070587  Validation Loss : 57.88127136230469 Val_Reconstruction : 54.991106033325195 Val_KL : 2.890164852142334\n","Epoch: 2078/5000  Traning Loss: 58.56760931015015  Train_Reconstruction: 55.645113945007324  Train_KL: 2.9224950969219208  Validation Loss : 57.729736328125 Val_Reconstruction : 54.848331451416016 Val_KL : 2.881405830383301\n","Epoch: 2079/5000  Traning Loss: 58.405203342437744  Train_Reconstruction: 55.49411678314209  Train_KL: 2.911087393760681  Validation Loss : 57.60595512390137 Val_Reconstruction : 54.72376823425293 Val_KL : 2.8821868896484375\n","Epoch: 2080/5000  Traning Loss: 58.528780460357666  Train_Reconstruction: 55.610076904296875  Train_KL: 2.918703317642212  Validation Loss : 57.87266159057617 Val_Reconstruction : 54.98299789428711 Val_KL : 2.8896634578704834\n","Epoch: 2081/5000  Traning Loss: 58.618568897247314  Train_Reconstruction: 55.694746017456055  Train_KL: 2.923822432756424  Validation Loss : 58.074350357055664 Val_Reconstruction : 55.20028114318848 Val_KL : 2.874069571495056\n","Epoch: 2082/5000  Traning Loss: 58.47760009765625  Train_Reconstruction: 55.55658578872681  Train_KL: 2.921014726161957  Validation Loss : 57.85906791687012 Val_Reconstruction : 54.97245788574219 Val_KL : 2.88660991191864\n","Epoch: 2083/5000  Traning Loss: 58.558730602264404  Train_Reconstruction: 55.63823699951172  Train_KL: 2.9204936921596527  Validation Loss : 57.96479606628418 Val_Reconstruction : 55.07466697692871 Val_KL : 2.8901305198669434\n","Epoch: 2084/5000  Traning Loss: 58.47424507141113  Train_Reconstruction: 55.55259370803833  Train_KL: 2.9216508269309998  Validation Loss : 57.597782135009766 Val_Reconstruction : 54.70719909667969 Val_KL : 2.8905829191207886\n","Epoch: 2085/5000  Traning Loss: 58.61912298202515  Train_Reconstruction: 55.69715356826782  Train_KL: 2.9219689071178436  Validation Loss : 57.87170600891113 Val_Reconstruction : 54.98463249206543 Val_KL : 2.8870723247528076\n","Epoch: 2086/5000  Traning Loss: 58.64859914779663  Train_Reconstruction: 55.724194049835205  Train_KL: 2.924404799938202  Validation Loss : 57.785024642944336 Val_Reconstruction : 54.902143478393555 Val_KL : 2.8828811645507812\n","Epoch: 2087/5000  Traning Loss: 58.508007526397705  Train_Reconstruction: 55.5912389755249  Train_KL: 2.9167677760124207  Validation Loss : 57.98976707458496 Val_Reconstruction : 55.09851837158203 Val_KL : 2.89124858379364\n","Epoch: 2088/5000  Traning Loss: 58.58527183532715  Train_Reconstruction: 55.66361045837402  Train_KL: 2.9216612577438354  Validation Loss : 57.7819709777832 Val_Reconstruction : 54.89854049682617 Val_KL : 2.883431553840637\n","Epoch: 2089/5000  Traning Loss: 58.58316516876221  Train_Reconstruction: 55.66288661956787  Train_KL: 2.920277953147888  Validation Loss : 57.79959487915039 Val_Reconstruction : 54.90844917297363 Val_KL : 2.8911460638046265\n","Epoch: 2090/5000  Traning Loss: 58.516905784606934  Train_Reconstruction: 55.59782028198242  Train_KL: 2.9190854132175446  Validation Loss : 57.990325927734375 Val_Reconstruction : 55.098093032836914 Val_KL : 2.8922334909439087\n","Epoch: 2091/5000  Traning Loss: 58.43093729019165  Train_Reconstruction: 55.501832008361816  Train_KL: 2.9291054904460907  Validation Loss : 57.443763732910156 Val_Reconstruction : 54.55260467529297 Val_KL : 2.8911592960357666\n","Epoch: 2092/5000  Traning Loss: 58.597660541534424  Train_Reconstruction: 55.67622470855713  Train_KL: 2.921435922384262  Validation Loss : 57.98859405517578 Val_Reconstruction : 55.10744094848633 Val_KL : 2.8811527490615845\n","Epoch: 2093/5000  Traning Loss: 58.6228461265564  Train_Reconstruction: 55.710543155670166  Train_KL: 2.912303328514099  Validation Loss : 57.630680084228516 Val_Reconstruction : 54.744192123413086 Val_KL : 2.886487364768982\n","Epoch: 2094/5000  Traning Loss: 58.72353219985962  Train_Reconstruction: 55.788349628448486  Train_KL: 2.935182511806488  Validation Loss : 58.437530517578125 Val_Reconstruction : 55.53500556945801 Val_KL : 2.902523159980774\n","Epoch: 2095/5000  Traning Loss: 58.862802505493164  Train_Reconstruction: 55.939136028289795  Train_KL: 2.9236668050289154  Validation Loss : 58.44005012512207 Val_Reconstruction : 55.55184555053711 Val_KL : 2.888204574584961\n","Epoch: 2096/5000  Traning Loss: 58.64104413986206  Train_Reconstruction: 55.72181415557861  Train_KL: 2.9192296266555786  Validation Loss : 57.55087089538574 Val_Reconstruction : 54.651275634765625 Val_KL : 2.8995946645736694\n","Epoch: 2097/5000  Traning Loss: 58.48906135559082  Train_Reconstruction: 55.561150550842285  Train_KL: 2.9279103577136993  Validation Loss : 57.8842830657959 Val_Reconstruction : 54.996673583984375 Val_KL : 2.8876086473464966\n","Epoch: 2098/5000  Traning Loss: 58.80791425704956  Train_Reconstruction: 55.894776821136475  Train_KL: 2.913137912750244  Validation Loss : 58.329254150390625 Val_Reconstruction : 55.45675468444824 Val_KL : 2.8724981546401978\n","Epoch: 2099/5000  Traning Loss: 58.853638648986816  Train_Reconstruction: 55.94214868545532  Train_KL: 2.9114892780780792  Validation Loss : 57.98952293395996 Val_Reconstruction : 55.10624694824219 Val_KL : 2.883275032043457\n","Epoch: 2100/5000  Traning Loss: 58.71423578262329  Train_Reconstruction: 55.78634166717529  Train_KL: 2.9278945922851562  Validation Loss : 57.92393684387207 Val_Reconstruction : 55.01869773864746 Val_KL : 2.9052395820617676\n","Epoch: 2101/5000  Traning Loss: 58.68104887008667  Train_Reconstruction: 55.75739574432373  Train_KL: 2.9236528873443604  Validation Loss : 57.84602355957031 Val_Reconstruction : 54.96082878112793 Val_KL : 2.8851948976516724\n","Epoch: 2102/5000  Traning Loss: 58.36351156234741  Train_Reconstruction: 55.44059753417969  Train_KL: 2.922913759946823  Validation Loss : 57.6219596862793 Val_Reconstruction : 54.73251533508301 Val_KL : 2.8894450664520264\n","Epoch: 2103/5000  Traning Loss: 58.38554573059082  Train_Reconstruction: 55.46717929840088  Train_KL: 2.918366312980652  Validation Loss : 57.623544692993164 Val_Reconstruction : 54.73984718322754 Val_KL : 2.8836973905563354\n","Epoch: 2104/5000  Traning Loss: 58.353374004364014  Train_Reconstruction: 55.43048858642578  Train_KL: 2.922885090112686  Validation Loss : 57.85229301452637 Val_Reconstruction : 54.957210540771484 Val_KL : 2.8950823545455933\n","Epoch: 2105/5000  Traning Loss: 58.36813735961914  Train_Reconstruction: 55.44856023788452  Train_KL: 2.919577181339264  Validation Loss : 57.64789390563965 Val_Reconstruction : 54.762773513793945 Val_KL : 2.8851191997528076\n","Epoch: 2106/5000  Traning Loss: 58.63330841064453  Train_Reconstruction: 55.70835494995117  Train_KL: 2.9249534010887146  Validation Loss : 57.90695571899414 Val_Reconstruction : 55.0137825012207 Val_KL : 2.8931732177734375\n","Epoch: 2107/5000  Traning Loss: 58.88353252410889  Train_Reconstruction: 55.95837926864624  Train_KL: 2.925152838230133  Validation Loss : 57.915117263793945 Val_Reconstruction : 55.03037452697754 Val_KL : 2.8847427368164062\n","Epoch: 2108/5000  Traning Loss: 58.59518766403198  Train_Reconstruction: 55.67210102081299  Train_KL: 2.9230869710445404  Validation Loss : 57.62784957885742 Val_Reconstruction : 54.73141670227051 Val_KL : 2.8964333534240723\n","Epoch: 2109/5000  Traning Loss: 58.42846155166626  Train_Reconstruction: 55.50831174850464  Train_KL: 2.920149564743042  Validation Loss : 57.62246894836426 Val_Reconstruction : 54.73959541320801 Val_KL : 2.8828749656677246\n","Epoch: 2110/5000  Traning Loss: 58.40593099594116  Train_Reconstruction: 55.49302339553833  Train_KL: 2.9129069447517395  Validation Loss : 57.8290901184082 Val_Reconstruction : 54.94341850280762 Val_KL : 2.8856719732284546\n","Epoch: 2111/5000  Traning Loss: 58.912068367004395  Train_Reconstruction: 55.98940658569336  Train_KL: 2.9226617217063904  Validation Loss : 58.38628387451172 Val_Reconstruction : 55.485206604003906 Val_KL : 2.901076078414917\n","Epoch: 2112/5000  Traning Loss: 58.816458225250244  Train_Reconstruction: 55.89620780944824  Train_KL: 2.920250564813614  Validation Loss : 57.7172737121582 Val_Reconstruction : 54.83961486816406 Val_KL : 2.8776583671569824\n","Epoch: 2113/5000  Traning Loss: 58.41091585159302  Train_Reconstruction: 55.49664258956909  Train_KL: 2.914273828268051  Validation Loss : 57.398277282714844 Val_Reconstruction : 54.51504898071289 Val_KL : 2.8832271099090576\n","Epoch: 2114/5000  Traning Loss: 58.301162242889404  Train_Reconstruction: 55.38365030288696  Train_KL: 2.9175115823745728  Validation Loss : 57.64574432373047 Val_Reconstruction : 54.75994110107422 Val_KL : 2.8858039379119873\n","Epoch: 2115/5000  Traning Loss: 58.35166692733765  Train_Reconstruction: 55.431564807891846  Train_KL: 2.920102447271347  Validation Loss : 57.46480751037598 Val_Reconstruction : 54.57703971862793 Val_KL : 2.8877673149108887\n","Epoch: 2116/5000  Traning Loss: 58.26353693008423  Train_Reconstruction: 55.339481830596924  Train_KL: 2.9240545332431793  Validation Loss : 57.49496650695801 Val_Reconstruction : 54.60232353210449 Val_KL : 2.892643094062805\n","Epoch: 2117/5000  Traning Loss: 58.52558374404907  Train_Reconstruction: 55.59993124008179  Train_KL: 2.925652414560318  Validation Loss : 58.29502868652344 Val_Reconstruction : 55.408912658691406 Val_KL : 2.886117100715637\n","Epoch: 2118/5000  Traning Loss: 58.65573453903198  Train_Reconstruction: 55.72971820831299  Train_KL: 2.9260161221027374  Validation Loss : 57.62673377990723 Val_Reconstruction : 54.73032188415527 Val_KL : 2.8964104652404785\n","Epoch: 2119/5000  Traning Loss: 58.36376476287842  Train_Reconstruction: 55.436644554138184  Train_KL: 2.927120327949524  Validation Loss : 57.604244232177734 Val_Reconstruction : 54.712873458862305 Val_KL : 2.891369938850403\n","Epoch: 2120/5000  Traning Loss: 58.5185341835022  Train_Reconstruction: 55.588462829589844  Train_KL: 2.930071383714676  Validation Loss : 57.980281829833984 Val_Reconstruction : 55.08530235290527 Val_KL : 2.894979238510132\n","Epoch: 2121/5000  Traning Loss: 58.73384141921997  Train_Reconstruction: 55.81342792510986  Train_KL: 2.920413553714752  Validation Loss : 57.985050201416016 Val_Reconstruction : 55.091529846191406 Val_KL : 2.8935219049453735\n","Epoch: 2122/5000  Traning Loss: 58.4353609085083  Train_Reconstruction: 55.50490713119507  Train_KL: 2.930453509092331  Validation Loss : 57.80370330810547 Val_Reconstruction : 54.901044845581055 Val_KL : 2.9026583433151245\n","Epoch: 2123/5000  Traning Loss: 59.02952194213867  Train_Reconstruction: 56.10019636154175  Train_KL: 2.929325520992279  Validation Loss : 58.3428840637207 Val_Reconstruction : 55.45656776428223 Val_KL : 2.886315941810608\n","Epoch: 2124/5000  Traning Loss: 58.51855993270874  Train_Reconstruction: 55.603251457214355  Train_KL: 2.9153080880641937  Validation Loss : 57.83079528808594 Val_Reconstruction : 54.94589424133301 Val_KL : 2.8849014043807983\n","Epoch: 2125/5000  Traning Loss: 58.193727016448975  Train_Reconstruction: 55.26323127746582  Train_KL: 2.9304955303668976  Validation Loss : 57.48325157165527 Val_Reconstruction : 54.58426475524902 Val_KL : 2.8989880084991455\n","Epoch: 2126/5000  Traning Loss: 58.401938915252686  Train_Reconstruction: 55.48519277572632  Train_KL: 2.916745811700821  Validation Loss : 57.72772789001465 Val_Reconstruction : 54.84272003173828 Val_KL : 2.885007858276367\n","Epoch: 2127/5000  Traning Loss: 59.09406280517578  Train_Reconstruction: 56.16756200790405  Train_KL: 2.926500618457794  Validation Loss : 58.35365295410156 Val_Reconstruction : 55.453866958618164 Val_KL : 2.8997853994369507\n","Epoch: 2128/5000  Traning Loss: 59.01913642883301  Train_Reconstruction: 56.096923828125  Train_KL: 2.9222125113010406  Validation Loss : 57.87063026428223 Val_Reconstruction : 54.99296760559082 Val_KL : 2.877662777900696\n","Epoch: 2129/5000  Traning Loss: 58.62330675125122  Train_Reconstruction: 55.70631122589111  Train_KL: 2.9169946908950806  Validation Loss : 57.67783546447754 Val_Reconstruction : 54.79127502441406 Val_KL : 2.8865612745285034\n","Epoch: 2130/5000  Traning Loss: 58.63880205154419  Train_Reconstruction: 55.721604347229004  Train_KL: 2.917197346687317  Validation Loss : 57.7637939453125 Val_Reconstruction : 54.873695373535156 Val_KL : 2.890099048614502\n","Epoch: 2131/5000  Traning Loss: 58.9273157119751  Train_Reconstruction: 55.9976019859314  Train_KL: 2.9297134280204773  Validation Loss : 58.26577377319336 Val_Reconstruction : 55.371089935302734 Val_KL : 2.894683003425598\n","Epoch: 2132/5000  Traning Loss: 59.05449199676514  Train_Reconstruction: 56.141751289367676  Train_KL: 2.9127411246299744  Validation Loss : 58.167396545410156 Val_Reconstruction : 55.289995193481445 Val_KL : 2.877401351928711\n","Epoch: 2133/5000  Traning Loss: 58.526219844818115  Train_Reconstruction: 55.60395860671997  Train_KL: 2.922261267900467  Validation Loss : 57.80855751037598 Val_Reconstruction : 54.90201187133789 Val_KL : 2.90654718875885\n","Epoch: 2134/5000  Traning Loss: 58.50161790847778  Train_Reconstruction: 55.57772350311279  Train_KL: 2.9238941371440887  Validation Loss : 57.58388328552246 Val_Reconstruction : 54.68938636779785 Val_KL : 2.894497513771057\n","Epoch: 2135/5000  Traning Loss: 58.80379581451416  Train_Reconstruction: 55.88490009307861  Train_KL: 2.918895959854126  Validation Loss : 57.473812103271484 Val_Reconstruction : 54.579816818237305 Val_KL : 2.8939945697784424\n","Epoch: 2136/5000  Traning Loss: 58.50504970550537  Train_Reconstruction: 55.590861797332764  Train_KL: 2.914188504219055  Validation Loss : 57.62858963012695 Val_Reconstruction : 54.7486686706543 Val_KL : 2.879921317100525\n","Epoch: 2137/5000  Traning Loss: 58.59999418258667  Train_Reconstruction: 55.678741455078125  Train_KL: 2.9212528467178345  Validation Loss : 58.02633285522461 Val_Reconstruction : 55.12483024597168 Val_KL : 2.901502847671509\n","Epoch: 2138/5000  Traning Loss: 59.215442180633545  Train_Reconstruction: 56.29764986038208  Train_KL: 2.9177917540073395  Validation Loss : 58.02884292602539 Val_Reconstruction : 55.135318756103516 Val_KL : 2.8935248851776123\n","Epoch: 2139/5000  Traning Loss: 59.296499729156494  Train_Reconstruction: 56.37850522994995  Train_KL: 2.91799458861351  Validation Loss : 57.797794342041016 Val_Reconstruction : 54.901336669921875 Val_KL : 2.896458625793457\n","Epoch: 2140/5000  Traning Loss: 59.00137376785278  Train_Reconstruction: 56.08353519439697  Train_KL: 2.917838007211685  Validation Loss : 57.91213607788086 Val_Reconstruction : 55.02045249938965 Val_KL : 2.8916826248168945\n","Epoch: 2141/5000  Traning Loss: 59.09151792526245  Train_Reconstruction: 56.17461538314819  Train_KL: 2.9169023036956787  Validation Loss : 58.40640830993652 Val_Reconstruction : 55.513790130615234 Val_KL : 2.8926196098327637\n","Epoch: 2142/5000  Traning Loss: 58.91890811920166  Train_Reconstruction: 55.99427604675293  Train_KL: 2.9246322214603424  Validation Loss : 58.186567306518555 Val_Reconstruction : 55.29283142089844 Val_KL : 2.8937355279922485\n","Epoch: 2143/5000  Traning Loss: 58.9107027053833  Train_Reconstruction: 55.989622592926025  Train_KL: 2.921079635620117  Validation Loss : 58.09990310668945 Val_Reconstruction : 55.21278381347656 Val_KL : 2.8871185779571533\n","Epoch: 2144/5000  Traning Loss: 58.69389057159424  Train_Reconstruction: 55.77310657501221  Train_KL: 2.9207838773727417  Validation Loss : 58.125450134277344 Val_Reconstruction : 55.22970199584961 Val_KL : 2.8957492113113403\n","Epoch: 2145/5000  Traning Loss: 58.74870538711548  Train_Reconstruction: 55.81152105331421  Train_KL: 2.9371840059757233  Validation Loss : 57.90217208862305 Val_Reconstruction : 54.990779876708984 Val_KL : 2.911392331123352\n","Epoch: 2146/5000  Traning Loss: 59.10319900512695  Train_Reconstruction: 56.167858600616455  Train_KL: 2.9353411495685577  Validation Loss : 58.508384704589844 Val_Reconstruction : 55.60976982116699 Val_KL : 2.8986151218414307\n","Epoch: 2147/5000  Traning Loss: 58.7825493812561  Train_Reconstruction: 55.854158878326416  Train_KL: 2.928390681743622  Validation Loss : 57.87968635559082 Val_Reconstruction : 54.98042106628418 Val_KL : 2.899265766143799\n","Epoch: 2148/5000  Traning Loss: 58.53475856781006  Train_Reconstruction: 55.615872383117676  Train_KL: 2.91888627409935  Validation Loss : 57.77125358581543 Val_Reconstruction : 54.88919258117676 Val_KL : 2.8820592164993286\n","Epoch: 2149/5000  Traning Loss: 58.617621421813965  Train_Reconstruction: 55.70373773574829  Train_KL: 2.9138838946819305  Validation Loss : 58.31632423400879 Val_Reconstruction : 55.43033409118652 Val_KL : 2.8859915733337402\n","Epoch: 2150/5000  Traning Loss: 58.66244840621948  Train_Reconstruction: 55.74819087982178  Train_KL: 2.9142574965953827  Validation Loss : 57.830366134643555 Val_Reconstruction : 54.944719314575195 Val_KL : 2.885647416114807\n","Epoch: 2151/5000  Traning Loss: 58.56367635726929  Train_Reconstruction: 55.63791608810425  Train_KL: 2.9257606267929077  Validation Loss : 57.66909599304199 Val_Reconstruction : 54.77163505554199 Val_KL : 2.8974621295928955\n","Epoch: 2152/5000  Traning Loss: 58.276801109313965  Train_Reconstruction: 55.356396198272705  Train_KL: 2.920405000448227  Validation Loss : 57.824310302734375 Val_Reconstruction : 54.93895149230957 Val_KL : 2.885358691215515\n","Epoch: 2153/5000  Traning Loss: 58.419968605041504  Train_Reconstruction: 55.50155591964722  Train_KL: 2.918412685394287  Validation Loss : 57.75689506530762 Val_Reconstruction : 54.86760330200195 Val_KL : 2.889293074607849\n","Epoch: 2154/5000  Traning Loss: 58.38007402420044  Train_Reconstruction: 55.45912456512451  Train_KL: 2.920948773622513  Validation Loss : 57.59538650512695 Val_Reconstruction : 54.707040786743164 Val_KL : 2.8883461952209473\n","Epoch: 2155/5000  Traning Loss: 58.36048412322998  Train_Reconstruction: 55.43901205062866  Train_KL: 2.921471804380417  Validation Loss : 57.73847961425781 Val_Reconstruction : 54.853620529174805 Val_KL : 2.8848589658737183\n","Epoch: 2156/5000  Traning Loss: 58.26368188858032  Train_Reconstruction: 55.34229278564453  Train_KL: 2.921389728784561  Validation Loss : 57.52131652832031 Val_Reconstruction : 54.61711311340332 Val_KL : 2.90420401096344\n","Epoch: 2157/5000  Traning Loss: 58.28169822692871  Train_Reconstruction: 55.3590612411499  Train_KL: 2.9226366877555847  Validation Loss : 57.525978088378906 Val_Reconstruction : 54.64448356628418 Val_KL : 2.8814953565597534\n","Epoch: 2158/5000  Traning Loss: 58.25957441329956  Train_Reconstruction: 55.344624519348145  Train_KL: 2.914949983358383  Validation Loss : 57.624589920043945 Val_Reconstruction : 54.736501693725586 Val_KL : 2.888089179992676\n","Epoch: 2159/5000  Traning Loss: 58.52593469619751  Train_Reconstruction: 55.60148811340332  Train_KL: 2.9244464635849  Validation Loss : 58.08027648925781 Val_Reconstruction : 55.19648551940918 Val_KL : 2.883789896965027\n","Epoch: 2160/5000  Traning Loss: 58.490806102752686  Train_Reconstruction: 55.58193063735962  Train_KL: 2.908875524997711  Validation Loss : 57.68149375915527 Val_Reconstruction : 54.80659294128418 Val_KL : 2.8749016523361206\n","Epoch: 2161/5000  Traning Loss: 58.6346549987793  Train_Reconstruction: 55.711445331573486  Train_KL: 2.9232094287872314  Validation Loss : 58.15913200378418 Val_Reconstruction : 55.27562713623047 Val_KL : 2.8835054636001587\n","Epoch: 2162/5000  Traning Loss: 58.44593286514282  Train_Reconstruction: 55.515177726745605  Train_KL: 2.9307545721530914  Validation Loss : 57.592796325683594 Val_Reconstruction : 54.70001411437988 Val_KL : 2.8927828073501587\n","Epoch: 2163/5000  Traning Loss: 58.449684143066406  Train_Reconstruction: 55.528122425079346  Train_KL: 2.921561598777771  Validation Loss : 57.79423522949219 Val_Reconstruction : 54.905738830566406 Val_KL : 2.88849675655365\n","Epoch: 2164/5000  Traning Loss: 58.55781698226929  Train_Reconstruction: 55.6483268737793  Train_KL: 2.9094898402690887  Validation Loss : 58.000436782836914 Val_Reconstruction : 55.13008499145508 Val_KL : 2.8703523874282837\n","Epoch: 2165/5000  Traning Loss: 59.01932764053345  Train_Reconstruction: 56.1029691696167  Train_KL: 2.9163593351840973  Validation Loss : 58.66793251037598 Val_Reconstruction : 55.77029609680176 Val_KL : 2.8976372480392456\n","Epoch: 2166/5000  Traning Loss: 59.05281972885132  Train_Reconstruction: 56.12347221374512  Train_KL: 2.9293473064899445  Validation Loss : 58.31403732299805 Val_Reconstruction : 55.41850280761719 Val_KL : 2.8955352306365967\n","Epoch: 2167/5000  Traning Loss: 58.811569690704346  Train_Reconstruction: 55.893943309783936  Train_KL: 2.9176260828971863  Validation Loss : 58.147335052490234 Val_Reconstruction : 55.26138877868652 Val_KL : 2.8859457969665527\n","Epoch: 2168/5000  Traning Loss: 58.44489622116089  Train_Reconstruction: 55.52591896057129  Train_KL: 2.9189769625663757  Validation Loss : 57.6085090637207 Val_Reconstruction : 54.71727180480957 Val_KL : 2.8912363052368164\n","Epoch: 2169/5000  Traning Loss: 58.21496391296387  Train_Reconstruction: 55.29515361785889  Train_KL: 2.9198097586631775  Validation Loss : 57.51971244812012 Val_Reconstruction : 54.61800765991211 Val_KL : 2.9017058610916138\n","Epoch: 2170/5000  Traning Loss: 58.43720579147339  Train_Reconstruction: 55.49909830093384  Train_KL: 2.938108265399933  Validation Loss : 58.64847755432129 Val_Reconstruction : 55.734933853149414 Val_KL : 2.9135431051254272\n","Epoch: 2171/5000  Traning Loss: 58.71117830276489  Train_Reconstruction: 55.78736352920532  Train_KL: 2.923815429210663  Validation Loss : 57.99251174926758 Val_Reconstruction : 55.10879707336426 Val_KL : 2.883714199066162\n","Epoch: 2172/5000  Traning Loss: 58.61074495315552  Train_Reconstruction: 55.695826053619385  Train_KL: 2.9149193167686462  Validation Loss : 57.936195373535156 Val_Reconstruction : 55.054847717285156 Val_KL : 2.8813475370407104\n","Epoch: 2173/5000  Traning Loss: 58.56840467453003  Train_Reconstruction: 55.64869832992554  Train_KL: 2.919706642627716  Validation Loss : 57.92100524902344 Val_Reconstruction : 55.02458572387695 Val_KL : 2.896419882774353\n","Epoch: 2174/5000  Traning Loss: 58.67346954345703  Train_Reconstruction: 55.74739742279053  Train_KL: 2.926071673631668  Validation Loss : 57.880319595336914 Val_Reconstruction : 54.99595069885254 Val_KL : 2.8843681812286377\n","Epoch: 2175/5000  Traning Loss: 58.66493272781372  Train_Reconstruction: 55.739351749420166  Train_KL: 2.925580769777298  Validation Loss : 58.00725555419922 Val_Reconstruction : 55.10717582702637 Val_KL : 2.9000794887542725\n","Epoch: 2176/5000  Traning Loss: 58.74509143829346  Train_Reconstruction: 55.81563425064087  Train_KL: 2.929457366466522  Validation Loss : 58.22329902648926 Val_Reconstruction : 55.32363700866699 Val_KL : 2.8996613025665283\n","Epoch: 2177/5000  Traning Loss: 58.91934680938721  Train_Reconstruction: 55.98505210876465  Train_KL: 2.934294641017914  Validation Loss : 58.212148666381836 Val_Reconstruction : 55.31144142150879 Val_KL : 2.9007071256637573\n","Epoch: 2178/5000  Traning Loss: 58.52691650390625  Train_Reconstruction: 55.60119438171387  Train_KL: 2.9257217347621918  Validation Loss : 57.45236587524414 Val_Reconstruction : 54.569801330566406 Val_KL : 2.882564902305603\n","Epoch: 2179/5000  Traning Loss: 58.286622047424316  Train_Reconstruction: 55.364389419555664  Train_KL: 2.922232359647751  Validation Loss : 57.73799514770508 Val_Reconstruction : 54.82457733154297 Val_KL : 2.9134174585342407\n","Epoch: 2180/5000  Traning Loss: 58.232125759124756  Train_Reconstruction: 55.30753803253174  Train_KL: 2.9245878756046295  Validation Loss : 57.49857711791992 Val_Reconstruction : 54.60495948791504 Val_KL : 2.8936175107955933\n","Epoch: 2181/5000  Traning Loss: 58.232972621917725  Train_Reconstruction: 55.302364349365234  Train_KL: 2.930608332157135  Validation Loss : 57.33774185180664 Val_Reconstruction : 54.42774963378906 Val_KL : 2.909993290901184\n","Epoch: 2182/5000  Traning Loss: 58.33595561981201  Train_Reconstruction: 55.39451217651367  Train_KL: 2.9414438903331757  Validation Loss : 57.516090393066406 Val_Reconstruction : 54.61209678649902 Val_KL : 2.903992533683777\n","Epoch: 2183/5000  Traning Loss: 58.469605922698975  Train_Reconstruction: 55.538405418395996  Train_KL: 2.9312004148960114  Validation Loss : 57.41598320007324 Val_Reconstruction : 54.510541915893555 Val_KL : 2.9054412841796875\n","Epoch: 2184/5000  Traning Loss: 58.18503761291504  Train_Reconstruction: 55.24797010421753  Train_KL: 2.937068372964859  Validation Loss : 57.45236778259277 Val_Reconstruction : 54.53445625305176 Val_KL : 2.917912483215332\n","Epoch: 2185/5000  Traning Loss: 58.1790075302124  Train_Reconstruction: 55.2463960647583  Train_KL: 2.932611882686615  Validation Loss : 57.50539970397949 Val_Reconstruction : 54.6182804107666 Val_KL : 2.887117624282837\n","Epoch: 2186/5000  Traning Loss: 58.38024950027466  Train_Reconstruction: 55.463725090026855  Train_KL: 2.91652449965477  Validation Loss : 57.947357177734375 Val_Reconstruction : 55.05615234375 Val_KL : 2.891205668449402\n","Epoch: 2187/5000  Traning Loss: 58.58298349380493  Train_Reconstruction: 55.657838344573975  Train_KL: 2.9251458644866943  Validation Loss : 57.73516654968262 Val_Reconstruction : 54.84355926513672 Val_KL : 2.8916068077087402\n","Epoch: 2188/5000  Traning Loss: 58.718544006347656  Train_Reconstruction: 55.79290294647217  Train_KL: 2.925640970468521  Validation Loss : 57.97366523742676 Val_Reconstruction : 55.098384857177734 Val_KL : 2.875279664993286\n","Epoch: 2189/5000  Traning Loss: 58.72384166717529  Train_Reconstruction: 55.812068462371826  Train_KL: 2.911773055791855  Validation Loss : 58.002315521240234 Val_Reconstruction : 55.11878967285156 Val_KL : 2.8835262060165405\n","Epoch: 2190/5000  Traning Loss: 58.807236194610596  Train_Reconstruction: 55.89412784576416  Train_KL: 2.9131082892417908  Validation Loss : 58.15238380432129 Val_Reconstruction : 55.27029037475586 Val_KL : 2.8820924758911133\n","Epoch: 2191/5000  Traning Loss: 59.02722406387329  Train_Reconstruction: 56.11556625366211  Train_KL: 2.911657840013504  Validation Loss : 58.327226638793945 Val_Reconstruction : 55.43706130981445 Val_KL : 2.8901666402816772\n","Epoch: 2192/5000  Traning Loss: 58.85986328125  Train_Reconstruction: 55.93915605545044  Train_KL: 2.920707046985626  Validation Loss : 57.92645263671875 Val_Reconstruction : 55.04185485839844 Val_KL : 2.8845980167388916\n","Epoch: 2193/5000  Traning Loss: 58.667065143585205  Train_Reconstruction: 55.75291156768799  Train_KL: 2.914154440164566  Validation Loss : 57.7288875579834 Val_Reconstruction : 54.852617263793945 Val_KL : 2.876268982887268\n","Epoch: 2194/5000  Traning Loss: 58.627644062042236  Train_Reconstruction: 55.70885419845581  Train_KL: 2.918790340423584  Validation Loss : 58.131309509277344 Val_Reconstruction : 55.23641014099121 Val_KL : 2.894898295402527\n","Epoch: 2195/5000  Traning Loss: 58.89318513870239  Train_Reconstruction: 55.96902322769165  Train_KL: 2.9241620302200317  Validation Loss : 57.924644470214844 Val_Reconstruction : 55.02503204345703 Val_KL : 2.8996130228042603\n","Epoch: 2196/5000  Traning Loss: 58.48382520675659  Train_Reconstruction: 55.55634927749634  Train_KL: 2.9274754226207733  Validation Loss : 57.700496673583984 Val_Reconstruction : 54.79607582092285 Val_KL : 2.9044206142425537\n","Epoch: 2197/5000  Traning Loss: 58.32500886917114  Train_Reconstruction: 55.400524616241455  Train_KL: 2.924484133720398  Validation Loss : 57.73692512512207 Val_Reconstruction : 54.85586929321289 Val_KL : 2.8810555934906006\n","Epoch: 2198/5000  Traning Loss: 58.47560405731201  Train_Reconstruction: 55.559767723083496  Train_KL: 2.9158362448215485  Validation Loss : 57.993865966796875 Val_Reconstruction : 55.10714340209961 Val_KL : 2.8867223262786865\n","Epoch: 2199/5000  Traning Loss: 58.76060914993286  Train_Reconstruction: 55.83958578109741  Train_KL: 2.9210232496261597  Validation Loss : 58.249881744384766 Val_Reconstruction : 55.36113929748535 Val_KL : 2.88874351978302\n","Epoch: 2200/5000  Traning Loss: 58.66155242919922  Train_Reconstruction: 55.74405479431152  Train_KL: 2.9174975156784058  Validation Loss : 57.74591255187988 Val_Reconstruction : 54.85511779785156 Val_KL : 2.890795111656189\n","Epoch: 2201/5000  Traning Loss: 58.47218990325928  Train_Reconstruction: 55.54918622970581  Train_KL: 2.923004001379013  Validation Loss : 57.96786880493164 Val_Reconstruction : 55.07729148864746 Val_KL : 2.8905766010284424\n","Epoch: 2202/5000  Traning Loss: 58.535380840301514  Train_Reconstruction: 55.61837911605835  Train_KL: 2.917001783847809  Validation Loss : 57.81003379821777 Val_Reconstruction : 54.92382049560547 Val_KL : 2.886211633682251\n","Epoch: 2203/5000  Traning Loss: 58.533090114593506  Train_Reconstruction: 55.61859941482544  Train_KL: 2.9144909977912903  Validation Loss : 57.62855529785156 Val_Reconstruction : 54.74415969848633 Val_KL : 2.8843955993652344\n","Epoch: 2204/5000  Traning Loss: 58.34567356109619  Train_Reconstruction: 55.42781591415405  Train_KL: 2.917858064174652  Validation Loss : 57.4985294342041 Val_Reconstruction : 54.60825157165527 Val_KL : 2.890277862548828\n","Epoch: 2205/5000  Traning Loss: 58.236032009124756  Train_Reconstruction: 55.31672716140747  Train_KL: 2.919304221868515  Validation Loss : 57.67363929748535 Val_Reconstruction : 54.775774002075195 Val_KL : 2.8978649377822876\n","Epoch: 2206/5000  Traning Loss: 58.293973445892334  Train_Reconstruction: 55.368813037872314  Train_KL: 2.925161600112915  Validation Loss : 57.56266784667969 Val_Reconstruction : 54.662946701049805 Val_KL : 2.8997199535369873\n","Epoch: 2207/5000  Traning Loss: 58.42300462722778  Train_Reconstruction: 55.49567365646362  Train_KL: 2.927331328392029  Validation Loss : 57.60191535949707 Val_Reconstruction : 54.7055606842041 Val_KL : 2.8963543176651\n","Epoch: 2208/5000  Traning Loss: 58.34388780593872  Train_Reconstruction: 55.41935682296753  Train_KL: 2.9245306253433228  Validation Loss : 57.81666564941406 Val_Reconstruction : 54.92308235168457 Val_KL : 2.8935829401016235\n","Epoch: 2209/5000  Traning Loss: 58.50820875167847  Train_Reconstruction: 55.5884165763855  Train_KL: 2.9197925329208374  Validation Loss : 57.603092193603516 Val_Reconstruction : 54.71243667602539 Val_KL : 2.8906545639038086\n","Epoch: 2210/5000  Traning Loss: 58.199320793151855  Train_Reconstruction: 55.27631664276123  Train_KL: 2.9230037331581116  Validation Loss : 57.48104667663574 Val_Reconstruction : 54.58920478820801 Val_KL : 2.8918426036834717\n","Epoch: 2211/5000  Traning Loss: 58.082462310791016  Train_Reconstruction: 55.16324710845947  Train_KL: 2.91921466588974  Validation Loss : 57.41635513305664 Val_Reconstruction : 54.5261173248291 Val_KL : 2.890236735343933\n","Epoch: 2212/5000  Traning Loss: 58.315916538238525  Train_Reconstruction: 55.38181257247925  Train_KL: 2.9341035187244415  Validation Loss : 57.941986083984375 Val_Reconstruction : 55.032575607299805 Val_KL : 2.9094107151031494\n","Epoch: 2213/5000  Traning Loss: 58.571375370025635  Train_Reconstruction: 55.642831802368164  Train_KL: 2.928543657064438  Validation Loss : 58.11590003967285 Val_Reconstruction : 55.22570610046387 Val_KL : 2.890194535255432\n","Epoch: 2214/5000  Traning Loss: 58.51513624191284  Train_Reconstruction: 55.58755350112915  Train_KL: 2.927582234144211  Validation Loss : 57.93186569213867 Val_Reconstruction : 55.038373947143555 Val_KL : 2.893491268157959\n","Epoch: 2215/5000  Traning Loss: 58.301681995391846  Train_Reconstruction: 55.372079849243164  Train_KL: 2.9296022057533264  Validation Loss : 57.6021842956543 Val_Reconstruction : 54.70804786682129 Val_KL : 2.8941365480422974\n","Epoch: 2216/5000  Traning Loss: 58.27785110473633  Train_Reconstruction: 55.35015869140625  Train_KL: 2.927692174911499  Validation Loss : 57.40670394897461 Val_Reconstruction : 54.5153694152832 Val_KL : 2.89133358001709\n","Epoch: 2217/5000  Traning Loss: 58.34598779678345  Train_Reconstruction: 55.41987228393555  Train_KL: 2.9261148273944855  Validation Loss : 57.55828285217285 Val_Reconstruction : 54.65441703796387 Val_KL : 2.903865933418274\n","Epoch: 2218/5000  Traning Loss: 58.460880756378174  Train_Reconstruction: 55.5250186920166  Train_KL: 2.9358619451522827  Validation Loss : 57.61383628845215 Val_Reconstruction : 54.7037239074707 Val_KL : 2.91011118888855\n","Epoch: 2219/5000  Traning Loss: 58.347883224487305  Train_Reconstruction: 55.426419258117676  Train_KL: 2.921463519334793  Validation Loss : 57.69515800476074 Val_Reconstruction : 54.80188751220703 Val_KL : 2.893270969390869\n","Epoch: 2220/5000  Traning Loss: 58.49333620071411  Train_Reconstruction: 55.57048845291138  Train_KL: 2.922847181558609  Validation Loss : 57.91710090637207 Val_Reconstruction : 55.019365310668945 Val_KL : 2.8977354764938354\n","Epoch: 2221/5000  Traning Loss: 58.31955146789551  Train_Reconstruction: 55.39320230484009  Train_KL: 2.9263490736484528  Validation Loss : 57.58086395263672 Val_Reconstruction : 54.6854248046875 Val_KL : 2.895439028739929\n","Epoch: 2222/5000  Traning Loss: 58.31718826293945  Train_Reconstruction: 55.38674020767212  Train_KL: 2.9304483234882355  Validation Loss : 57.78202819824219 Val_Reconstruction : 54.88379669189453 Val_KL : 2.8982319831848145\n","Epoch: 2223/5000  Traning Loss: 58.59897232055664  Train_Reconstruction: 55.67599964141846  Train_KL: 2.9229726791381836  Validation Loss : 58.017906188964844 Val_Reconstruction : 55.140459060668945 Val_KL : 2.87744677066803\n","Epoch: 2224/5000  Traning Loss: 58.56360626220703  Train_Reconstruction: 55.649147033691406  Train_KL: 2.9144591987133026  Validation Loss : 57.82149696350098 Val_Reconstruction : 54.936906814575195 Val_KL : 2.884589433670044\n","Epoch: 2225/5000  Traning Loss: 58.39348840713501  Train_Reconstruction: 55.475642681121826  Train_KL: 2.9178450405597687  Validation Loss : 57.60953903198242 Val_Reconstruction : 54.72393035888672 Val_KL : 2.8856085538864136\n","Epoch: 2226/5000  Traning Loss: 58.063400745391846  Train_Reconstruction: 55.14831495285034  Train_KL: 2.915086090564728  Validation Loss : 57.21383285522461 Val_Reconstruction : 54.33120918273926 Val_KL : 2.8826229572296143\n","Epoch: 2227/5000  Traning Loss: 58.198177337646484  Train_Reconstruction: 55.27968120574951  Train_KL: 2.918496459722519  Validation Loss : 57.84103775024414 Val_Reconstruction : 54.95530891418457 Val_KL : 2.885729193687439\n","Epoch: 2228/5000  Traning Loss: 58.51496601104736  Train_Reconstruction: 55.600258350372314  Train_KL: 2.914707660675049  Validation Loss : 57.78705978393555 Val_Reconstruction : 54.90041732788086 Val_KL : 2.8866426944732666\n","Epoch: 2229/5000  Traning Loss: 58.5048885345459  Train_Reconstruction: 55.58379602432251  Train_KL: 2.921092927455902  Validation Loss : 57.619741439819336 Val_Reconstruction : 54.722991943359375 Val_KL : 2.8967493772506714\n","Epoch: 2230/5000  Traning Loss: 58.6306266784668  Train_Reconstruction: 55.70709943771362  Train_KL: 2.9235276579856873  Validation Loss : 57.80086326599121 Val_Reconstruction : 54.91279411315918 Val_KL : 2.888068199157715\n","Epoch: 2231/5000  Traning Loss: 58.56801891326904  Train_Reconstruction: 55.6461877822876  Train_KL: 2.921830803155899  Validation Loss : 57.83369827270508 Val_Reconstruction : 54.93544578552246 Val_KL : 2.898252844810486\n","Epoch: 2232/5000  Traning Loss: 58.96741724014282  Train_Reconstruction: 56.046470165252686  Train_KL: 2.9209476709365845  Validation Loss : 57.92234802246094 Val_Reconstruction : 55.03418159484863 Val_KL : 2.888166308403015\n","Epoch: 2233/5000  Traning Loss: 58.737285137176514  Train_Reconstruction: 55.806002616882324  Train_KL: 2.9312828183174133  Validation Loss : 57.929983139038086 Val_Reconstruction : 55.029130935668945 Val_KL : 2.9008527994155884\n","Epoch: 2234/5000  Traning Loss: 58.6545352935791  Train_Reconstruction: 55.71889543533325  Train_KL: 2.935639977455139  Validation Loss : 58.4601993560791 Val_Reconstruction : 55.56277656555176 Val_KL : 2.8974242210388184\n","Epoch: 2235/5000  Traning Loss: 59.199729919433594  Train_Reconstruction: 56.27701711654663  Train_KL: 2.9227135479450226  Validation Loss : 58.33039665222168 Val_Reconstruction : 55.44194984436035 Val_KL : 2.8884470462799072\n","Epoch: 2236/5000  Traning Loss: 58.81568002700806  Train_Reconstruction: 55.889968395233154  Train_KL: 2.925711512565613  Validation Loss : 58.05038642883301 Val_Reconstruction : 55.159446716308594 Val_KL : 2.8909398317337036\n","Epoch: 2237/5000  Traning Loss: 58.590373516082764  Train_Reconstruction: 55.664711475372314  Train_KL: 2.9256615936756134  Validation Loss : 58.03323554992676 Val_Reconstruction : 55.13235855102539 Val_KL : 2.9008772373199463\n","Epoch: 2238/5000  Traning Loss: 58.29198932647705  Train_Reconstruction: 55.36793041229248  Train_KL: 2.924058884382248  Validation Loss : 57.3670654296875 Val_Reconstruction : 54.4804573059082 Val_KL : 2.8866089582443237\n","Epoch: 2239/5000  Traning Loss: 58.159504890441895  Train_Reconstruction: 55.2324800491333  Train_KL: 2.927025228738785  Validation Loss : 57.48226547241211 Val_Reconstruction : 54.584896087646484 Val_KL : 2.8973695039749146\n","Epoch: 2240/5000  Traning Loss: 58.107022285461426  Train_Reconstruction: 55.18109655380249  Train_KL: 2.925926089286804  Validation Loss : 57.643165588378906 Val_Reconstruction : 54.751304626464844 Val_KL : 2.8918614387512207\n","Epoch: 2241/5000  Traning Loss: 58.31066560745239  Train_Reconstruction: 55.3878607749939  Train_KL: 2.9228046238422394  Validation Loss : 57.82830047607422 Val_Reconstruction : 54.931697845458984 Val_KL : 2.8966033458709717\n","Epoch: 2242/5000  Traning Loss: 58.33694934844971  Train_Reconstruction: 55.41281270980835  Train_KL: 2.924136698246002  Validation Loss : 57.88262939453125 Val_Reconstruction : 54.99355888366699 Val_KL : 2.8890695571899414\n","Epoch: 2243/5000  Traning Loss: 58.479493141174316  Train_Reconstruction: 55.55660820007324  Train_KL: 2.922884166240692  Validation Loss : 57.806209564208984 Val_Reconstruction : 54.91015815734863 Val_KL : 2.8960516452789307\n","Epoch: 2244/5000  Traning Loss: 58.68460416793823  Train_Reconstruction: 55.754568576812744  Train_KL: 2.9300354719161987  Validation Loss : 57.819271087646484 Val_Reconstruction : 54.92898750305176 Val_KL : 2.890283942222595\n","Epoch: 2245/5000  Traning Loss: 58.74272680282593  Train_Reconstruction: 55.8127760887146  Train_KL: 2.9299505949020386  Validation Loss : 58.05941581726074 Val_Reconstruction : 55.16855049133301 Val_KL : 2.8908655643463135\n","Epoch: 2246/5000  Traning Loss: 58.93441152572632  Train_Reconstruction: 56.0166802406311  Train_KL: 2.9177311658859253  Validation Loss : 58.267343521118164 Val_Reconstruction : 55.37464141845703 Val_KL : 2.8927019834518433\n","Epoch: 2247/5000  Traning Loss: 58.64326763153076  Train_Reconstruction: 55.716434478759766  Train_KL: 2.9268330335617065  Validation Loss : 57.72698211669922 Val_Reconstruction : 54.831172943115234 Val_KL : 2.8958094120025635\n","Epoch: 2248/5000  Traning Loss: 58.52586841583252  Train_Reconstruction: 55.60238981246948  Train_KL: 2.9234791100025177  Validation Loss : 57.88406562805176 Val_Reconstruction : 54.99860954284668 Val_KL : 2.8854565620422363\n","Epoch: 2249/5000  Traning Loss: 58.40043878555298  Train_Reconstruction: 55.47960138320923  Train_KL: 2.92083740234375  Validation Loss : 57.96528244018555 Val_Reconstruction : 55.067237854003906 Val_KL : 2.898044228553772\n","Epoch: 2250/5000  Traning Loss: 58.37859535217285  Train_Reconstruction: 55.451820850372314  Train_KL: 2.926774889230728  Validation Loss : 57.53863525390625 Val_Reconstruction : 54.65921401977539 Val_KL : 2.87942111492157\n","Epoch: 2251/5000  Traning Loss: 58.208794593811035  Train_Reconstruction: 55.302274227142334  Train_KL: 2.9065200090408325  Validation Loss : 57.46672248840332 Val_Reconstruction : 54.58238983154297 Val_KL : 2.8843311071395874\n","Epoch: 2252/5000  Traning Loss: 58.136006355285645  Train_Reconstruction: 55.20169496536255  Train_KL: 2.934311091899872  Validation Loss : 57.57353210449219 Val_Reconstruction : 54.663291931152344 Val_KL : 2.910240650177002\n","Epoch: 2253/5000  Traning Loss: 58.47184658050537  Train_Reconstruction: 55.52677583694458  Train_KL: 2.945070743560791  Validation Loss : 58.05108070373535 Val_Reconstruction : 55.12605667114258 Val_KL : 2.9250231981277466\n","Epoch: 2254/5000  Traning Loss: 58.551833152770996  Train_Reconstruction: 55.614882469177246  Train_KL: 2.936950534582138  Validation Loss : 57.83681678771973 Val_Reconstruction : 54.941877365112305 Val_KL : 2.8949379920959473\n","Epoch: 2255/5000  Traning Loss: 58.374821186065674  Train_Reconstruction: 55.45466947555542  Train_KL: 2.9201522171497345  Validation Loss : 57.4342041015625 Val_Reconstruction : 54.53721046447754 Val_KL : 2.8969926834106445\n","Epoch: 2256/5000  Traning Loss: 58.12766361236572  Train_Reconstruction: 55.19818830490112  Train_KL: 2.929475486278534  Validation Loss : 57.47117614746094 Val_Reconstruction : 54.57053184509277 Val_KL : 2.9006435871124268\n","Epoch: 2257/5000  Traning Loss: 58.15280055999756  Train_Reconstruction: 55.22678852081299  Train_KL: 2.9260123670101166  Validation Loss : 57.618221282958984 Val_Reconstruction : 54.73917770385742 Val_KL : 2.8790438175201416\n","Epoch: 2258/5000  Traning Loss: 58.191293716430664  Train_Reconstruction: 55.279038429260254  Train_KL: 2.9122549295425415  Validation Loss : 57.56002616882324 Val_Reconstruction : 54.67690658569336 Val_KL : 2.883120894432068\n","Epoch: 2259/5000  Traning Loss: 58.52816343307495  Train_Reconstruction: 55.60266065597534  Train_KL: 2.9255023896694183  Validation Loss : 57.768205642700195 Val_Reconstruction : 54.86220169067383 Val_KL : 2.906004309654236\n","Epoch: 2260/5000  Traning Loss: 58.354079246520996  Train_Reconstruction: 55.43087148666382  Train_KL: 2.9232075214385986  Validation Loss : 57.500783920288086 Val_Reconstruction : 54.606082916259766 Val_KL : 2.8947014808654785\n","Epoch: 2261/5000  Traning Loss: 58.482611656188965  Train_Reconstruction: 55.55961084365845  Train_KL: 2.923001080751419  Validation Loss : 57.64767074584961 Val_Reconstruction : 54.74831199645996 Val_KL : 2.899358868598938\n","Epoch: 2262/5000  Traning Loss: 58.27152109146118  Train_Reconstruction: 55.34372520446777  Train_KL: 2.927795499563217  Validation Loss : 57.56752967834473 Val_Reconstruction : 54.66080665588379 Val_KL : 2.9067232608795166\n","Epoch: 2263/5000  Traning Loss: 58.205469608306885  Train_Reconstruction: 55.28326892852783  Train_KL: 2.9222013354301453  Validation Loss : 57.56945037841797 Val_Reconstruction : 54.672773361206055 Val_KL : 2.8966771364212036\n","Epoch: 2264/5000  Traning Loss: 58.35586500167847  Train_Reconstruction: 55.43081045150757  Train_KL: 2.9250544905662537  Validation Loss : 57.70767021179199 Val_Reconstruction : 54.81204032897949 Val_KL : 2.8956292867660522\n","Epoch: 2265/5000  Traning Loss: 58.543060302734375  Train_Reconstruction: 55.61574745178223  Train_KL: 2.9273127615451813  Validation Loss : 57.96135330200195 Val_Reconstruction : 55.05093574523926 Val_KL : 2.910417675971985\n","Epoch: 2266/5000  Traning Loss: 58.700801372528076  Train_Reconstruction: 55.767520904541016  Train_KL: 2.9332810640335083  Validation Loss : 57.71334457397461 Val_Reconstruction : 54.81219673156738 Val_KL : 2.901147961616516\n","Epoch: 2267/5000  Traning Loss: 58.64623975753784  Train_Reconstruction: 55.71666622161865  Train_KL: 2.9295730590820312  Validation Loss : 57.931427001953125 Val_Reconstruction : 55.035316467285156 Val_KL : 2.896110415458679\n","Epoch: 2268/5000  Traning Loss: 58.65799903869629  Train_Reconstruction: 55.739015102386475  Train_KL: 2.9189837276935577  Validation Loss : 57.88232612609863 Val_Reconstruction : 54.99498176574707 Val_KL : 2.887344717979431\n","Epoch: 2269/5000  Traning Loss: 58.63856840133667  Train_Reconstruction: 55.719908237457275  Train_KL: 2.9186604619026184  Validation Loss : 58.05580520629883 Val_Reconstruction : 55.158966064453125 Val_KL : 2.8968398571014404\n","Epoch: 2270/5000  Traning Loss: 58.90598440170288  Train_Reconstruction: 55.97809362411499  Train_KL: 2.927890866994858  Validation Loss : 58.23791313171387 Val_Reconstruction : 55.34281539916992 Val_KL : 2.8950964212417603\n","Epoch: 2271/5000  Traning Loss: 59.03017711639404  Train_Reconstruction: 56.11198329925537  Train_KL: 2.91819429397583  Validation Loss : 58.07177543640137 Val_Reconstruction : 55.16437339782715 Val_KL : 2.907402276992798\n","Epoch: 2272/5000  Traning Loss: 58.691569328308105  Train_Reconstruction: 55.7493953704834  Train_KL: 2.9421730041503906  Validation Loss : 57.788808822631836 Val_Reconstruction : 54.87834548950195 Val_KL : 2.9104636907577515\n","Epoch: 2273/5000  Traning Loss: 58.634746074676514  Train_Reconstruction: 55.707136154174805  Train_KL: 2.927609384059906  Validation Loss : 57.72426986694336 Val_Reconstruction : 54.82046699523926 Val_KL : 2.9038028717041016\n","Epoch: 2274/5000  Traning Loss: 58.497788429260254  Train_Reconstruction: 55.56591844558716  Train_KL: 2.9318700432777405  Validation Loss : 57.65563774108887 Val_Reconstruction : 54.75543975830078 Val_KL : 2.9001989364624023\n","Epoch: 2275/5000  Traning Loss: 58.45174551010132  Train_Reconstruction: 55.52956676483154  Train_KL: 2.9221793115139008  Validation Loss : 58.12692451477051 Val_Reconstruction : 55.22723197937012 Val_KL : 2.8996928930282593\n","Epoch: 2276/5000  Traning Loss: 58.59432506561279  Train_Reconstruction: 55.667415618896484  Train_KL: 2.9269092977046967  Validation Loss : 58.11500930786133 Val_Reconstruction : 55.222543716430664 Val_KL : 2.8924646377563477\n","Epoch: 2277/5000  Traning Loss: 58.70123243331909  Train_Reconstruction: 55.7795295715332  Train_KL: 2.921702414751053  Validation Loss : 57.84461212158203 Val_Reconstruction : 54.952049255371094 Val_KL : 2.8925631046295166\n","Epoch: 2278/5000  Traning Loss: 58.326091289520264  Train_Reconstruction: 55.405099391937256  Train_KL: 2.9209916591644287  Validation Loss : 57.57204246520996 Val_Reconstruction : 54.68467903137207 Val_KL : 2.887362480163574\n","Epoch: 2279/5000  Traning Loss: 58.300171852111816  Train_Reconstruction: 55.37325048446655  Train_KL: 2.9269215762615204  Validation Loss : 57.672563552856445 Val_Reconstruction : 54.77341651916504 Val_KL : 2.899148106575012\n","Epoch: 2280/5000  Traning Loss: 58.385101318359375  Train_Reconstruction: 55.46197986602783  Train_KL: 2.92312154173851  Validation Loss : 57.65832328796387 Val_Reconstruction : 54.77016258239746 Val_KL : 2.8881620168685913\n","Epoch: 2281/5000  Traning Loss: 58.343960762023926  Train_Reconstruction: 55.42626476287842  Train_KL: 2.9176963567733765  Validation Loss : 57.40134048461914 Val_Reconstruction : 54.50990104675293 Val_KL : 2.891439199447632\n","Epoch: 2282/5000  Traning Loss: 58.083975315093994  Train_Reconstruction: 55.1525559425354  Train_KL: 2.931418776512146  Validation Loss : 57.4794864654541 Val_Reconstruction : 54.57786178588867 Val_KL : 2.901624321937561\n","Epoch: 2283/5000  Traning Loss: 58.276090145111084  Train_Reconstruction: 55.35212564468384  Train_KL: 2.9239646792411804  Validation Loss : 58.03828430175781 Val_Reconstruction : 55.15113830566406 Val_KL : 2.887145757675171\n","Epoch: 2284/5000  Traning Loss: 58.525432109832764  Train_Reconstruction: 55.598297119140625  Train_KL: 2.9271352887153625  Validation Loss : 57.90040397644043 Val_Reconstruction : 54.99746131896973 Val_KL : 2.9029418230056763\n","Epoch: 2285/5000  Traning Loss: 58.65110635757446  Train_Reconstruction: 55.71494436264038  Train_KL: 2.9361618757247925  Validation Loss : 58.084110260009766 Val_Reconstruction : 55.17588996887207 Val_KL : 2.9082207679748535\n","Epoch: 2286/5000  Traning Loss: 58.468441009521484  Train_Reconstruction: 55.54121446609497  Train_KL: 2.927226275205612  Validation Loss : 57.910396575927734 Val_Reconstruction : 55.016883850097656 Val_KL : 2.8935130834579468\n","Epoch: 2287/5000  Traning Loss: 58.17869567871094  Train_Reconstruction: 55.25652742385864  Train_KL: 2.9221689105033875  Validation Loss : 57.515146255493164 Val_Reconstruction : 54.61394119262695 Val_KL : 2.9012056589126587\n","Epoch: 2288/5000  Traning Loss: 58.1398983001709  Train_Reconstruction: 55.21818733215332  Train_KL: 2.921711027622223  Validation Loss : 57.482303619384766 Val_Reconstruction : 54.596168518066406 Val_KL : 2.886136770248413\n","Epoch: 2289/5000  Traning Loss: 58.14491319656372  Train_Reconstruction: 55.22718667984009  Train_KL: 2.917726218700409  Validation Loss : 57.57490921020508 Val_Reconstruction : 54.68344688415527 Val_KL : 2.8914623260498047\n","Epoch: 2290/5000  Traning Loss: 58.24314498901367  Train_Reconstruction: 55.318511962890625  Train_KL: 2.9246326982975006  Validation Loss : 57.77098846435547 Val_Reconstruction : 54.88094520568848 Val_KL : 2.890043020248413\n","Epoch: 2291/5000  Traning Loss: 58.49052858352661  Train_Reconstruction: 55.57190179824829  Train_KL: 2.9186273515224457  Validation Loss : 58.09000015258789 Val_Reconstruction : 55.19915580749512 Val_KL : 2.8908441066741943\n","Epoch: 2292/5000  Traning Loss: 58.40414237976074  Train_Reconstruction: 55.47402000427246  Train_KL: 2.930121958255768  Validation Loss : 57.888601303100586 Val_Reconstruction : 54.98176574707031 Val_KL : 2.9068360328674316\n","Epoch: 2293/5000  Traning Loss: 58.34868240356445  Train_Reconstruction: 55.41830348968506  Train_KL: 2.9303786754608154  Validation Loss : 57.66263771057129 Val_Reconstruction : 54.77256965637207 Val_KL : 2.8900688886642456\n","Epoch: 2294/5000  Traning Loss: 58.441850662231445  Train_Reconstruction: 55.52255344390869  Train_KL: 2.919296979904175  Validation Loss : 57.74330520629883 Val_Reconstruction : 54.85887145996094 Val_KL : 2.8844326734542847\n","Epoch: 2295/5000  Traning Loss: 58.215152740478516  Train_Reconstruction: 55.301530838012695  Train_KL: 2.91362127661705  Validation Loss : 57.59953308105469 Val_Reconstruction : 54.71257019042969 Val_KL : 2.886963725090027\n","Epoch: 2296/5000  Traning Loss: 58.13391733169556  Train_Reconstruction: 55.215590476989746  Train_KL: 2.9183273911476135  Validation Loss : 57.5646858215332 Val_Reconstruction : 54.675323486328125 Val_KL : 2.889362931251526\n","Epoch: 2297/5000  Traning Loss: 58.17602729797363  Train_Reconstruction: 55.247934341430664  Train_KL: 2.928092271089554  Validation Loss : 57.358449935913086 Val_Reconstruction : 54.46405029296875 Val_KL : 2.8944002389907837\n","Epoch: 2298/5000  Traning Loss: 58.14180088043213  Train_Reconstruction: 55.221988677978516  Train_KL: 2.9198114573955536  Validation Loss : 57.42866897583008 Val_Reconstruction : 54.52900314331055 Val_KL : 2.8996652364730835\n","Epoch: 2299/5000  Traning Loss: 58.36206388473511  Train_Reconstruction: 55.43732929229736  Train_KL: 2.9247341454029083  Validation Loss : 57.824729919433594 Val_Reconstruction : 54.931392669677734 Val_KL : 2.893337607383728\n","Epoch: 2300/5000  Traning Loss: 58.60525894165039  Train_Reconstruction: 55.68447399139404  Train_KL: 2.9207844734191895  Validation Loss : 58.087602615356445 Val_Reconstruction : 55.20345687866211 Val_KL : 2.8841474056243896\n","Epoch: 2301/5000  Traning Loss: 58.43128490447998  Train_Reconstruction: 55.50779914855957  Train_KL: 2.9234853982925415  Validation Loss : 57.5627384185791 Val_Reconstruction : 54.659881591796875 Val_KL : 2.902857184410095\n","Epoch: 2302/5000  Traning Loss: 58.317148208618164  Train_Reconstruction: 55.39108896255493  Train_KL: 2.926058977842331  Validation Loss : 57.420013427734375 Val_Reconstruction : 54.52858543395996 Val_KL : 2.8914263248443604\n","Epoch: 2303/5000  Traning Loss: 58.656803607940674  Train_Reconstruction: 55.734628200531006  Train_KL: 2.9221743047237396  Validation Loss : 58.307600021362305 Val_Reconstruction : 55.41591453552246 Val_KL : 2.8916866779327393\n","Epoch: 2304/5000  Traning Loss: 58.804466247558594  Train_Reconstruction: 55.883259773254395  Train_KL: 2.921206533908844  Validation Loss : 58.0720157623291 Val_Reconstruction : 55.18220138549805 Val_KL : 2.8898143768310547\n","Epoch: 2305/5000  Traning Loss: 58.68651103973389  Train_Reconstruction: 55.766685485839844  Train_KL: 2.919825464487076  Validation Loss : 57.9943733215332 Val_Reconstruction : 55.09994125366211 Val_KL : 2.8944324254989624\n","Epoch: 2306/5000  Traning Loss: 58.46512842178345  Train_Reconstruction: 55.54421901702881  Train_KL: 2.920909970998764  Validation Loss : 57.60991096496582 Val_Reconstruction : 54.71649742126465 Val_KL : 2.8934152126312256\n","Epoch: 2307/5000  Traning Loss: 58.3333306312561  Train_Reconstruction: 55.4101767539978  Train_KL: 2.9231540262699127  Validation Loss : 57.55846405029297 Val_Reconstruction : 54.669559478759766 Val_KL : 2.888905882835388\n","Epoch: 2308/5000  Traning Loss: 58.25939893722534  Train_Reconstruction: 55.334341049194336  Train_KL: 2.9250580072402954  Validation Loss : 57.56017303466797 Val_Reconstruction : 54.66067886352539 Val_KL : 2.8994940519332886\n","Epoch: 2309/5000  Traning Loss: 58.53788089752197  Train_Reconstruction: 55.611135482788086  Train_KL: 2.9267452359199524  Validation Loss : 57.86863708496094 Val_Reconstruction : 54.9762020111084 Val_KL : 2.892435669898987\n","Epoch: 2310/5000  Traning Loss: 58.342966079711914  Train_Reconstruction: 55.42466592788696  Train_KL: 2.9182996451854706  Validation Loss : 57.73869514465332 Val_Reconstruction : 54.846988677978516 Val_KL : 2.8917065858840942\n","Epoch: 2311/5000  Traning Loss: 58.333895206451416  Train_Reconstruction: 55.403305530548096  Train_KL: 2.9305901527404785  Validation Loss : 57.622554779052734 Val_Reconstruction : 54.716182708740234 Val_KL : 2.906371831893921\n","Epoch: 2312/5000  Traning Loss: 58.326857566833496  Train_Reconstruction: 55.38687753677368  Train_KL: 2.9399802684783936  Validation Loss : 57.669681549072266 Val_Reconstruction : 54.76433753967285 Val_KL : 2.905344605445862\n","Epoch: 2313/5000  Traning Loss: 58.11677885055542  Train_Reconstruction: 55.195274353027344  Train_KL: 2.9215048253536224  Validation Loss : 57.282548904418945 Val_Reconstruction : 54.39401626586914 Val_KL : 2.8885343074798584\n","Epoch: 2314/5000  Traning Loss: 58.0723934173584  Train_Reconstruction: 55.144423961639404  Train_KL: 2.9279689490795135  Validation Loss : 57.436058044433594 Val_Reconstruction : 54.539663314819336 Val_KL : 2.89639413356781\n","Epoch: 2315/5000  Traning Loss: 58.55121946334839  Train_Reconstruction: 55.62604236602783  Train_KL: 2.9251781702041626  Validation Loss : 57.59977340698242 Val_Reconstruction : 54.708194732666016 Val_KL : 2.8915786743164062\n","Epoch: 2316/5000  Traning Loss: 58.27935457229614  Train_Reconstruction: 55.35075283050537  Train_KL: 2.9286012649536133  Validation Loss : 57.728416442871094 Val_Reconstruction : 54.8236198425293 Val_KL : 2.904795527458191\n","Epoch: 2317/5000  Traning Loss: 58.224228858947754  Train_Reconstruction: 55.289390563964844  Train_KL: 2.9348381757736206  Validation Loss : 57.4578857421875 Val_Reconstruction : 54.55174255371094 Val_KL : 2.906143546104431\n","Epoch: 2318/5000  Traning Loss: 58.21965980529785  Train_Reconstruction: 55.2791805267334  Train_KL: 2.9404795169830322  Validation Loss : 57.49398231506348 Val_Reconstruction : 54.583417892456055 Val_KL : 2.910565137863159\n","Epoch: 2319/5000  Traning Loss: 58.20247840881348  Train_Reconstruction: 55.268296241760254  Train_KL: 2.934181809425354  Validation Loss : 57.69059753417969 Val_Reconstruction : 54.79473686218262 Val_KL : 2.8958600759506226\n","Epoch: 2320/5000  Traning Loss: 58.11235237121582  Train_Reconstruction: 55.19429588317871  Train_KL: 2.918056219816208  Validation Loss : 57.324777603149414 Val_Reconstruction : 54.440406799316406 Val_KL : 2.8843716382980347\n","Epoch: 2321/5000  Traning Loss: 58.002636432647705  Train_Reconstruction: 55.081146240234375  Train_KL: 2.9214893579483032  Validation Loss : 57.13216972351074 Val_Reconstruction : 54.22821617126465 Val_KL : 2.9039525985717773\n","Epoch: 2322/5000  Traning Loss: 57.973533630371094  Train_Reconstruction: 55.03712844848633  Train_KL: 2.936405748128891  Validation Loss : 57.48640441894531 Val_Reconstruction : 54.593013763427734 Val_KL : 2.8933913707733154\n","Epoch: 2323/5000  Traning Loss: 57.96332883834839  Train_Reconstruction: 55.04036283493042  Train_KL: 2.9229655861854553  Validation Loss : 57.43121337890625 Val_Reconstruction : 54.529794692993164 Val_KL : 2.901418089866638\n","Epoch: 2324/5000  Traning Loss: 58.11491298675537  Train_Reconstruction: 55.195420265197754  Train_KL: 2.9194922149181366  Validation Loss : 57.473318099975586 Val_Reconstruction : 54.595211029052734 Val_KL : 2.878105878829956\n","Epoch: 2325/5000  Traning Loss: 58.26431846618652  Train_Reconstruction: 55.33792066574097  Train_KL: 2.926398128271103  Validation Loss : 57.52567672729492 Val_Reconstruction : 54.62532615661621 Val_KL : 2.900352120399475\n","Epoch: 2326/5000  Traning Loss: 58.137282848358154  Train_Reconstruction: 55.200472354888916  Train_KL: 2.936810255050659  Validation Loss : 57.745561599731445 Val_Reconstruction : 54.847002029418945 Val_KL : 2.898560404777527\n","Epoch: 2327/5000  Traning Loss: 58.21215295791626  Train_Reconstruction: 55.28470182418823  Train_KL: 2.9274512231349945  Validation Loss : 57.372962951660156 Val_Reconstruction : 54.47373580932617 Val_KL : 2.8992257118225098\n","Epoch: 2328/5000  Traning Loss: 57.97130346298218  Train_Reconstruction: 55.03556203842163  Train_KL: 2.9357414841651917  Validation Loss : 57.24271774291992 Val_Reconstruction : 54.34078407287598 Val_KL : 2.9019333124160767\n","Epoch: 2329/5000  Traning Loss: 58.08993673324585  Train_Reconstruction: 55.16086483001709  Train_KL: 2.9290715754032135  Validation Loss : 57.659427642822266 Val_Reconstruction : 54.773033142089844 Val_KL : 2.8863953351974487\n","Epoch: 2330/5000  Traning Loss: 58.44424819946289  Train_Reconstruction: 55.52607011795044  Train_KL: 2.9181788861751556  Validation Loss : 57.747453689575195 Val_Reconstruction : 54.85993576049805 Val_KL : 2.887517809867859\n","Epoch: 2331/5000  Traning Loss: 58.10099935531616  Train_Reconstruction: 55.17917442321777  Train_KL: 2.9218249022960663  Validation Loss : 57.415517807006836 Val_Reconstruction : 54.5283317565918 Val_KL : 2.8871846199035645\n","Epoch: 2332/5000  Traning Loss: 58.00455284118652  Train_Reconstruction: 55.085439682006836  Train_KL: 2.919113129377365  Validation Loss : 57.27956199645996 Val_Reconstruction : 54.3898811340332 Val_KL : 2.889681816101074\n","Epoch: 2333/5000  Traning Loss: 58.04289674758911  Train_Reconstruction: 55.11906814575195  Train_KL: 2.9238288402557373  Validation Loss : 57.5159969329834 Val_Reconstruction : 54.61547088623047 Val_KL : 2.9005268812179565\n","Epoch: 2334/5000  Traning Loss: 58.092238426208496  Train_Reconstruction: 55.16804027557373  Train_KL: 2.9241981506347656  Validation Loss : 57.48703575134277 Val_Reconstruction : 54.5921688079834 Val_KL : 2.8948670625686646\n","Epoch: 2335/5000  Traning Loss: 58.13161039352417  Train_Reconstruction: 55.207627296447754  Train_KL: 2.923983186483383  Validation Loss : 57.8021125793457 Val_Reconstruction : 54.90744590759277 Val_KL : 2.894666910171509\n","Epoch: 2336/5000  Traning Loss: 58.70305681228638  Train_Reconstruction: 55.76584196090698  Train_KL: 2.937214732170105  Validation Loss : 57.537546157836914 Val_Reconstruction : 54.620479583740234 Val_KL : 2.917065978050232\n","Epoch: 2337/5000  Traning Loss: 58.28820037841797  Train_Reconstruction: 55.35480308532715  Train_KL: 2.933396875858307  Validation Loss : 57.29975128173828 Val_Reconstruction : 54.400068283081055 Val_KL : 2.899683952331543\n","Epoch: 2338/5000  Traning Loss: 58.13974046707153  Train_Reconstruction: 55.22153091430664  Train_KL: 2.9182099103927612  Validation Loss : 57.62175941467285 Val_Reconstruction : 54.724016189575195 Val_KL : 2.8977437019348145\n","Epoch: 2339/5000  Traning Loss: 58.112515926361084  Train_Reconstruction: 55.176934242248535  Train_KL: 2.935581475496292  Validation Loss : 57.70205307006836 Val_Reconstruction : 54.780242919921875 Val_KL : 2.9218097925186157\n","Epoch: 2340/5000  Traning Loss: 58.501619815826416  Train_Reconstruction: 55.56580305099487  Train_KL: 2.935817211866379  Validation Loss : 57.89256286621094 Val_Reconstruction : 54.99599266052246 Val_KL : 2.896570324897766\n","Epoch: 2341/5000  Traning Loss: 58.57666015625  Train_Reconstruction: 55.64901351928711  Train_KL: 2.9276463091373444  Validation Loss : 57.89748954772949 Val_Reconstruction : 55.01095390319824 Val_KL : 2.8865357637405396\n","Epoch: 2342/5000  Traning Loss: 58.36945295333862  Train_Reconstruction: 55.44699954986572  Train_KL: 2.9224533140659332  Validation Loss : 57.76104545593262 Val_Reconstruction : 54.87513732910156 Val_KL : 2.8859091997146606\n","Epoch: 2343/5000  Traning Loss: 58.263508796691895  Train_Reconstruction: 55.33606576919556  Train_KL: 2.927443206310272  Validation Loss : 57.61747932434082 Val_Reconstruction : 54.703086853027344 Val_KL : 2.914392828941345\n","Epoch: 2344/5000  Traning Loss: 58.28765106201172  Train_Reconstruction: 55.36105728149414  Train_KL: 2.926594078540802  Validation Loss : 57.68931007385254 Val_Reconstruction : 54.80048942565918 Val_KL : 2.888821005821228\n","Epoch: 2345/5000  Traning Loss: 58.22059154510498  Train_Reconstruction: 55.29654264450073  Train_KL: 2.924048811197281  Validation Loss : 57.49286079406738 Val_Reconstruction : 54.600704193115234 Val_KL : 2.8921581506729126\n","Epoch: 2346/5000  Traning Loss: 58.087573528289795  Train_Reconstruction: 55.163665771484375  Train_KL: 2.923907518386841  Validation Loss : 57.525957107543945 Val_Reconstruction : 54.629356384277344 Val_KL : 2.8966017961502075\n","Epoch: 2347/5000  Traning Loss: 58.420087814331055  Train_Reconstruction: 55.491177558898926  Train_KL: 2.928910344839096  Validation Loss : 57.760080337524414 Val_Reconstruction : 54.86163139343262 Val_KL : 2.8984479904174805\n","Epoch: 2348/5000  Traning Loss: 58.533602237701416  Train_Reconstruction: 55.621450901031494  Train_KL: 2.912151336669922  Validation Loss : 57.753379821777344 Val_Reconstruction : 54.871788024902344 Val_KL : 2.881590723991394\n","Epoch: 2349/5000  Traning Loss: 58.14738416671753  Train_Reconstruction: 55.228291511535645  Train_KL: 2.919092446565628  Validation Loss : 57.541521072387695 Val_Reconstruction : 54.63355255126953 Val_KL : 2.907969117164612\n","Epoch: 2350/5000  Traning Loss: 58.12724494934082  Train_Reconstruction: 55.18298673629761  Train_KL: 2.944258540868759  Validation Loss : 57.485023498535156 Val_Reconstruction : 54.57839393615723 Val_KL : 2.9066290855407715\n","Epoch: 2351/5000  Traning Loss: 58.39230823516846  Train_Reconstruction: 55.47257423400879  Train_KL: 2.9197345077991486  Validation Loss : 57.64091873168945 Val_Reconstruction : 54.7615909576416 Val_KL : 2.8793277740478516\n","Epoch: 2352/5000  Traning Loss: 58.28249645233154  Train_Reconstruction: 55.36364221572876  Train_KL: 2.918854594230652  Validation Loss : 57.52138328552246 Val_Reconstruction : 54.606435775756836 Val_KL : 2.914946436882019\n","Epoch: 2353/5000  Traning Loss: 58.130255699157715  Train_Reconstruction: 55.186349868774414  Train_KL: 2.943905144929886  Validation Loss : 57.60236740112305 Val_Reconstruction : 54.69143104553223 Val_KL : 2.9109362363815308\n","Epoch: 2354/5000  Traning Loss: 58.571327209472656  Train_Reconstruction: 55.6435604095459  Train_KL: 2.9277670085430145  Validation Loss : 57.70370292663574 Val_Reconstruction : 54.820146560668945 Val_KL : 2.8835549354553223\n","Epoch: 2355/5000  Traning Loss: 58.52816438674927  Train_Reconstruction: 55.62413311004639  Train_KL: 2.9040311574935913  Validation Loss : 58.067636489868164 Val_Reconstruction : 55.19547462463379 Val_KL : 2.8721617460250854\n","Epoch: 2356/5000  Traning Loss: 58.278586864471436  Train_Reconstruction: 55.34697341918945  Train_KL: 2.9316138923168182  Validation Loss : 57.51463508605957 Val_Reconstruction : 54.61594009399414 Val_KL : 2.8986945152282715\n","Epoch: 2357/5000  Traning Loss: 58.05631113052368  Train_Reconstruction: 55.12667894363403  Train_KL: 2.929632157087326  Validation Loss : 57.241878509521484 Val_Reconstruction : 54.35329246520996 Val_KL : 2.888586640357971\n","Epoch: 2358/5000  Traning Loss: 57.98328638076782  Train_Reconstruction: 55.062012672424316  Train_KL: 2.9212737679481506  Validation Loss : 57.52449035644531 Val_Reconstruction : 54.628347396850586 Val_KL : 2.8961434364318848\n","Epoch: 2359/5000  Traning Loss: 58.19416332244873  Train_Reconstruction: 55.26729726791382  Train_KL: 2.9268656969070435  Validation Loss : 57.825395584106445 Val_Reconstruction : 54.92548370361328 Val_KL : 2.8999104499816895\n","Epoch: 2360/5000  Traning Loss: 58.41065692901611  Train_Reconstruction: 55.476855754852295  Train_KL: 2.9338008761405945  Validation Loss : 57.9796085357666 Val_Reconstruction : 55.069345474243164 Val_KL : 2.910263419151306\n","Epoch: 2361/5000  Traning Loss: 58.22362518310547  Train_Reconstruction: 55.29295635223389  Train_KL: 2.9306695461273193  Validation Loss : 57.247854232788086 Val_Reconstruction : 54.36004066467285 Val_KL : 2.887814521789551\n","Epoch: 2362/5000  Traning Loss: 58.11863040924072  Train_Reconstruction: 55.195730686187744  Train_KL: 2.9228990972042084  Validation Loss : 57.33945083618164 Val_Reconstruction : 54.44103813171387 Val_KL : 2.898413062095642\n","Epoch: 2363/5000  Traning Loss: 58.03276348114014  Train_Reconstruction: 55.09517765045166  Train_KL: 2.937585949897766  Validation Loss : 57.463003158569336 Val_Reconstruction : 54.557640075683594 Val_KL : 2.9053635597229004\n","Epoch: 2364/5000  Traning Loss: 58.16617202758789  Train_Reconstruction: 55.229496002197266  Train_KL: 2.936675935983658  Validation Loss : 57.812625885009766 Val_Reconstruction : 54.90952682495117 Val_KL : 2.9030991792678833\n","Epoch: 2365/5000  Traning Loss: 58.523841857910156  Train_Reconstruction: 55.57332706451416  Train_KL: 2.950515240430832  Validation Loss : 57.94075584411621 Val_Reconstruction : 55.03203773498535 Val_KL : 2.90871798992157\n","Epoch: 2366/5000  Traning Loss: 58.26773262023926  Train_Reconstruction: 55.34424114227295  Train_KL: 2.923492193222046  Validation Loss : 57.440664291381836 Val_Reconstruction : 54.555320739746094 Val_KL : 2.885342240333557\n","Epoch: 2367/5000  Traning Loss: 58.20410346984863  Train_Reconstruction: 55.283037185668945  Train_KL: 2.921065777540207  Validation Loss : 57.82594680786133 Val_Reconstruction : 54.919851303100586 Val_KL : 2.9060949087142944\n","Epoch: 2368/5000  Traning Loss: 58.198840618133545  Train_Reconstruction: 55.27438831329346  Train_KL: 2.924452781677246  Validation Loss : 57.88198471069336 Val_Reconstruction : 54.98699760437012 Val_KL : 2.8949873447418213\n","Epoch: 2369/5000  Traning Loss: 58.19748878479004  Train_Reconstruction: 55.2740478515625  Train_KL: 2.9234416782855988  Validation Loss : 57.3627815246582 Val_Reconstruction : 54.480560302734375 Val_KL : 2.8822203874588013\n","Epoch: 2370/5000  Traning Loss: 57.95572900772095  Train_Reconstruction: 55.03904676437378  Train_KL: 2.9166823029518127  Validation Loss : 57.19100761413574 Val_Reconstruction : 54.30594062805176 Val_KL : 2.8850659132003784\n","Epoch: 2371/5000  Traning Loss: 58.03946399688721  Train_Reconstruction: 55.1137261390686  Train_KL: 2.925737828016281  Validation Loss : 57.26340103149414 Val_Reconstruction : 54.36327934265137 Val_KL : 2.9001216888427734\n","Epoch: 2372/5000  Traning Loss: 57.991426944732666  Train_Reconstruction: 55.069716453552246  Train_KL: 2.921710640192032  Validation Loss : 57.361398696899414 Val_Reconstruction : 54.480987548828125 Val_KL : 2.880410671234131\n","Epoch: 2373/5000  Traning Loss: 57.99527406692505  Train_Reconstruction: 55.07965135574341  Train_KL: 2.9156233072280884  Validation Loss : 57.39835548400879 Val_Reconstruction : 54.51546669006348 Val_KL : 2.882887125015259\n","Epoch: 2374/5000  Traning Loss: 58.15433311462402  Train_Reconstruction: 55.22996759414673  Train_KL: 2.9243657290935516  Validation Loss : 57.651384353637695 Val_Reconstruction : 54.74820137023926 Val_KL : 2.9031832218170166\n","Epoch: 2375/5000  Traning Loss: 58.253966331481934  Train_Reconstruction: 55.319908142089844  Train_KL: 2.93405881524086  Validation Loss : 57.58737564086914 Val_Reconstruction : 54.69510078430176 Val_KL : 2.8922735452651978\n","Epoch: 2376/5000  Traning Loss: 58.31977033615112  Train_Reconstruction: 55.3893928527832  Train_KL: 2.930377811193466  Validation Loss : 57.712135314941406 Val_Reconstruction : 54.813791275024414 Val_KL : 2.898343563079834\n","Epoch: 2377/5000  Traning Loss: 58.308815002441406  Train_Reconstruction: 55.37951183319092  Train_KL: 2.9293031692504883  Validation Loss : 57.50222587585449 Val_Reconstruction : 54.603271484375 Val_KL : 2.898955225944519\n","Epoch: 2378/5000  Traning Loss: 58.321351051330566  Train_Reconstruction: 55.40995740890503  Train_KL: 2.9113933742046356  Validation Loss : 57.64035224914551 Val_Reconstruction : 54.76070022583008 Val_KL : 2.879651188850403\n","Epoch: 2379/5000  Traning Loss: 58.17700910568237  Train_Reconstruction: 55.263161182403564  Train_KL: 2.9138472974300385  Validation Loss : 57.66642951965332 Val_Reconstruction : 54.77934646606445 Val_KL : 2.887081503868103\n","Epoch: 2380/5000  Traning Loss: 58.506561279296875  Train_Reconstruction: 55.59105587005615  Train_KL: 2.9155057668685913  Validation Loss : 58.186824798583984 Val_Reconstruction : 55.2990837097168 Val_KL : 2.887741208076477\n","Epoch: 2381/5000  Traning Loss: 58.54050922393799  Train_Reconstruction: 55.621076583862305  Train_KL: 2.919432431459427  Validation Loss : 57.45880699157715 Val_Reconstruction : 54.558820724487305 Val_KL : 2.899985909461975\n","Epoch: 2382/5000  Traning Loss: 58.13846492767334  Train_Reconstruction: 55.22119379043579  Train_KL: 2.9172712564468384  Validation Loss : 57.31935119628906 Val_Reconstruction : 54.43440628051758 Val_KL : 2.884944438934326\n","Epoch: 2383/5000  Traning Loss: 58.01005411148071  Train_Reconstruction: 55.097238540649414  Train_KL: 2.912816047668457  Validation Loss : 57.28135871887207 Val_Reconstruction : 54.3958740234375 Val_KL : 2.8854833841323853\n","Epoch: 2384/5000  Traning Loss: 58.09704065322876  Train_Reconstruction: 55.17198085784912  Train_KL: 2.925059527158737  Validation Loss : 57.49928855895996 Val_Reconstruction : 54.5959587097168 Val_KL : 2.903329610824585\n","Epoch: 2385/5000  Traning Loss: 58.118070125579834  Train_Reconstruction: 55.18958520889282  Train_KL: 2.9284844994544983  Validation Loss : 57.268693923950195 Val_Reconstruction : 54.376914978027344 Val_KL : 2.8917778730392456\n","Epoch: 2386/5000  Traning Loss: 57.95757484436035  Train_Reconstruction: 55.03579139709473  Train_KL: 2.921783536672592  Validation Loss : 57.18990898132324 Val_Reconstruction : 54.29940223693848 Val_KL : 2.890507698059082\n","Epoch: 2387/5000  Traning Loss: 57.96010494232178  Train_Reconstruction: 55.034576416015625  Train_KL: 2.9255286157131195  Validation Loss : 57.24812698364258 Val_Reconstruction : 54.349151611328125 Val_KL : 2.8989744186401367\n","Epoch: 2388/5000  Traning Loss: 58.078168869018555  Train_Reconstruction: 55.14144563674927  Train_KL: 2.936723053455353  Validation Loss : 57.55459976196289 Val_Reconstruction : 54.65632438659668 Val_KL : 2.898275852203369\n","Epoch: 2389/5000  Traning Loss: 58.352601051330566  Train_Reconstruction: 55.42493772506714  Train_KL: 2.927663415670395  Validation Loss : 58.31447219848633 Val_Reconstruction : 55.42714881896973 Val_KL : 2.8873226642608643\n","Epoch: 2390/5000  Traning Loss: 58.67357110977173  Train_Reconstruction: 55.74205827713013  Train_KL: 2.931512326002121  Validation Loss : 57.83225059509277 Val_Reconstruction : 54.929588317871094 Val_KL : 2.90266215801239\n","Epoch: 2391/5000  Traning Loss: 58.51518630981445  Train_Reconstruction: 55.58713674545288  Train_KL: 2.9280497431755066  Validation Loss : 57.85470199584961 Val_Reconstruction : 54.944528579711914 Val_KL : 2.9101723432540894\n","Epoch: 2392/5000  Traning Loss: 58.38810729980469  Train_Reconstruction: 55.461833000183105  Train_KL: 2.92627415060997  Validation Loss : 57.39493751525879 Val_Reconstruction : 54.50496482849121 Val_KL : 2.889973521232605\n","Epoch: 2393/5000  Traning Loss: 58.050015926361084  Train_Reconstruction: 55.13181781768799  Train_KL: 2.9181986153125763  Validation Loss : 57.35009765625 Val_Reconstruction : 54.458900451660156 Val_KL : 2.8911972045898438\n","Epoch: 2394/5000  Traning Loss: 58.05014133453369  Train_Reconstruction: 55.125144958496094  Train_KL: 2.9249955117702484  Validation Loss : 57.43813514709473 Val_Reconstruction : 54.53713035583496 Val_KL : 2.9010039567947388\n","Epoch: 2395/5000  Traning Loss: 58.108006954193115  Train_Reconstruction: 55.177274227142334  Train_KL: 2.9307331144809723  Validation Loss : 57.58666229248047 Val_Reconstruction : 54.68509292602539 Val_KL : 2.9015697240829468\n","Epoch: 2396/5000  Traning Loss: 58.2520055770874  Train_Reconstruction: 55.33162546157837  Train_KL: 2.9203803539276123  Validation Loss : 57.61225700378418 Val_Reconstruction : 54.716299057006836 Val_KL : 2.895957589149475\n","Epoch: 2397/5000  Traning Loss: 58.71951198577881  Train_Reconstruction: 55.78483533859253  Train_KL: 2.9346763491630554  Validation Loss : 57.95945167541504 Val_Reconstruction : 55.042293548583984 Val_KL : 2.9171568155288696\n","Epoch: 2398/5000  Traning Loss: 58.44656324386597  Train_Reconstruction: 55.5062894821167  Train_KL: 2.940273493528366  Validation Loss : 57.38618850708008 Val_Reconstruction : 54.478525161743164 Val_KL : 2.9076632261276245\n","Epoch: 2399/5000  Traning Loss: 58.22801065444946  Train_Reconstruction: 55.29393720626831  Train_KL: 2.9340730905532837  Validation Loss : 57.58733558654785 Val_Reconstruction : 54.683319091796875 Val_KL : 2.9040175676345825\n","Epoch: 2400/5000  Traning Loss: 58.33635234832764  Train_Reconstruction: 55.40219831466675  Train_KL: 2.9341546297073364  Validation Loss : 57.66495895385742 Val_Reconstruction : 54.75614547729492 Val_KL : 2.9088131189346313\n","Epoch: 2401/5000  Traning Loss: 58.54124641418457  Train_Reconstruction: 55.61690425872803  Train_KL: 2.924342304468155  Validation Loss : 57.95783615112305 Val_Reconstruction : 55.0621395111084 Val_KL : 2.8956973552703857\n","Epoch: 2402/5000  Traning Loss: 58.624300479888916  Train_Reconstruction: 55.697232723236084  Train_KL: 2.9270675778388977  Validation Loss : 57.904733657836914 Val_Reconstruction : 55.003461837768555 Val_KL : 2.9012712240219116\n","Epoch: 2403/5000  Traning Loss: 58.41811275482178  Train_Reconstruction: 55.487051010131836  Train_KL: 2.9310625195503235  Validation Loss : 57.6136531829834 Val_Reconstruction : 54.716569900512695 Val_KL : 2.897083044052124\n","Epoch: 2404/5000  Traning Loss: 58.240192890167236  Train_Reconstruction: 55.30982065200806  Train_KL: 2.9303719997406006  Validation Loss : 57.60855293273926 Val_Reconstruction : 54.71397399902344 Val_KL : 2.8945791721343994\n","Epoch: 2405/5000  Traning Loss: 58.05385398864746  Train_Reconstruction: 55.12583017349243  Train_KL: 2.9280239641666412  Validation Loss : 57.5960636138916 Val_Reconstruction : 54.69681358337402 Val_KL : 2.8992515802383423\n","Epoch: 2406/5000  Traning Loss: 58.19752883911133  Train_Reconstruction: 55.27359485626221  Train_KL: 2.9239342212677  Validation Loss : 57.71446418762207 Val_Reconstruction : 54.83309364318848 Val_KL : 2.8813703060150146\n","Epoch: 2407/5000  Traning Loss: 58.2242751121521  Train_Reconstruction: 55.303950786590576  Train_KL: 2.9203248620033264  Validation Loss : 57.3094425201416 Val_Reconstruction : 54.41955375671387 Val_KL : 2.8898898363113403\n","Epoch: 2408/5000  Traning Loss: 58.344876289367676  Train_Reconstruction: 55.424909591674805  Train_KL: 2.9199664294719696  Validation Loss : 57.745832443237305 Val_Reconstruction : 54.84590530395508 Val_KL : 2.8999264240264893\n","Epoch: 2409/5000  Traning Loss: 58.49102210998535  Train_Reconstruction: 55.5567193031311  Train_KL: 2.9343034625053406  Validation Loss : 58.00104522705078 Val_Reconstruction : 55.09599494934082 Val_KL : 2.9050506353378296\n","Epoch: 2410/5000  Traning Loss: 58.45684766769409  Train_Reconstruction: 55.52959585189819  Train_KL: 2.9272517263889313  Validation Loss : 57.518362045288086 Val_Reconstruction : 54.633413314819336 Val_KL : 2.8849483728408813\n","Epoch: 2411/5000  Traning Loss: 58.33020067214966  Train_Reconstruction: 55.4170126914978  Train_KL: 2.91318741440773  Validation Loss : 57.69862174987793 Val_Reconstruction : 54.82342338562012 Val_KL : 2.875198245048523\n","Epoch: 2412/5000  Traning Loss: 58.59501838684082  Train_Reconstruction: 55.686028480529785  Train_KL: 2.9089903831481934  Validation Loss : 57.76543617248535 Val_Reconstruction : 54.883771896362305 Val_KL : 2.8816652297973633\n","Epoch: 2413/5000  Traning Loss: 58.35539197921753  Train_Reconstruction: 55.42850303649902  Train_KL: 2.9268889725208282  Validation Loss : 57.804405212402344 Val_Reconstruction : 54.900413513183594 Val_KL : 2.9039909839630127\n","Epoch: 2414/5000  Traning Loss: 58.42271709442139  Train_Reconstruction: 55.49323558807373  Train_KL: 2.929481565952301  Validation Loss : 57.444217681884766 Val_Reconstruction : 54.555192947387695 Val_KL : 2.889024257659912\n","Epoch: 2415/5000  Traning Loss: 58.5427622795105  Train_Reconstruction: 55.61201620101929  Train_KL: 2.930745244026184  Validation Loss : 57.66538429260254 Val_Reconstruction : 54.75981521606445 Val_KL : 2.905569076538086\n","Epoch: 2416/5000  Traning Loss: 58.38948392868042  Train_Reconstruction: 55.45894479751587  Train_KL: 2.930539608001709  Validation Loss : 57.67202568054199 Val_Reconstruction : 54.77864074707031 Val_KL : 2.8933852910995483\n","Epoch: 2417/5000  Traning Loss: 58.587568283081055  Train_Reconstruction: 55.6575870513916  Train_KL: 2.929981052875519  Validation Loss : 57.713409423828125 Val_Reconstruction : 54.82608413696289 Val_KL : 2.8873252868652344\n","Epoch: 2418/5000  Traning Loss: 58.64056205749512  Train_Reconstruction: 55.71425199508667  Train_KL: 2.9263105988502502  Validation Loss : 57.9411735534668 Val_Reconstruction : 55.04085731506348 Val_KL : 2.9003171920776367\n","Epoch: 2419/5000  Traning Loss: 58.20423603057861  Train_Reconstruction: 55.26823616027832  Train_KL: 2.9359999299049377  Validation Loss : 57.465166091918945 Val_Reconstruction : 54.5591926574707 Val_KL : 2.9059725999832153\n","Epoch: 2420/5000  Traning Loss: 58.036672592163086  Train_Reconstruction: 55.10798978805542  Train_KL: 2.928682506084442  Validation Loss : 57.238149642944336 Val_Reconstruction : 54.35123062133789 Val_KL : 2.8869179487228394\n","Epoch: 2421/5000  Traning Loss: 58.43131399154663  Train_Reconstruction: 55.50228929519653  Train_KL: 2.929024577140808  Validation Loss : 58.098453521728516 Val_Reconstruction : 55.193891525268555 Val_KL : 2.9045623540878296\n","Epoch: 2422/5000  Traning Loss: 58.59379053115845  Train_Reconstruction: 55.664453983306885  Train_KL: 2.929336369037628  Validation Loss : 57.683088302612305 Val_Reconstruction : 54.783796310424805 Val_KL : 2.899292469024658\n","Epoch: 2423/5000  Traning Loss: 58.55210590362549  Train_Reconstruction: 55.62017822265625  Train_KL: 2.931927889585495  Validation Loss : 57.50621223449707 Val_Reconstruction : 54.60970115661621 Val_KL : 2.8965110778808594\n","Epoch: 2424/5000  Traning Loss: 58.47487497329712  Train_Reconstruction: 55.542542457580566  Train_KL: 2.9323331713676453  Validation Loss : 57.75645065307617 Val_Reconstruction : 54.85519218444824 Val_KL : 2.9012584686279297\n","Epoch: 2425/5000  Traning Loss: 58.253254890441895  Train_Reconstruction: 55.32061004638672  Train_KL: 2.932644248008728  Validation Loss : 57.519622802734375 Val_Reconstruction : 54.630794525146484 Val_KL : 2.8888288736343384\n","Epoch: 2426/5000  Traning Loss: 58.020755767822266  Train_Reconstruction: 55.11014270782471  Train_KL: 2.9106130599975586  Validation Loss : 57.274641036987305 Val_Reconstruction : 54.38494300842285 Val_KL : 2.8896981477737427\n","Epoch: 2427/5000  Traning Loss: 58.059433460235596  Train_Reconstruction: 55.140554904937744  Train_KL: 2.9188788533210754  Validation Loss : 57.37282180786133 Val_Reconstruction : 54.480979919433594 Val_KL : 2.891842246055603\n","Epoch: 2428/5000  Traning Loss: 57.95132350921631  Train_Reconstruction: 55.035067558288574  Train_KL: 2.9162554144859314  Validation Loss : 57.370683670043945 Val_Reconstruction : 54.49909782409668 Val_KL : 2.8715862035751343\n","Epoch: 2429/5000  Traning Loss: 58.066158294677734  Train_Reconstruction: 55.1524600982666  Train_KL: 2.913697749376297  Validation Loss : 57.56810188293457 Val_Reconstruction : 54.68040657043457 Val_KL : 2.887696146965027\n","Epoch: 2430/5000  Traning Loss: 58.44897508621216  Train_Reconstruction: 55.52486753463745  Train_KL: 2.924107313156128  Validation Loss : 57.93664741516113 Val_Reconstruction : 55.048553466796875 Val_KL : 2.8880926370620728\n","Epoch: 2431/5000  Traning Loss: 58.96802806854248  Train_Reconstruction: 56.038541316986084  Train_KL: 2.9294869899749756  Validation Loss : 58.3891487121582 Val_Reconstruction : 55.49697685241699 Val_KL : 2.8921735286712646\n","Epoch: 2432/5000  Traning Loss: 58.55823850631714  Train_Reconstruction: 55.63640785217285  Train_KL: 2.92183119058609  Validation Loss : 57.700748443603516 Val_Reconstruction : 54.810997009277344 Val_KL : 2.8897511959075928\n","Epoch: 2433/5000  Traning Loss: 58.343172550201416  Train_Reconstruction: 55.41715621948242  Train_KL: 2.9260169863700867  Validation Loss : 57.91208457946777 Val_Reconstruction : 55.01962661743164 Val_KL : 2.892456531524658\n","Epoch: 2434/5000  Traning Loss: 58.37332057952881  Train_Reconstruction: 55.450979232788086  Train_KL: 2.9223413169384003  Validation Loss : 57.37208557128906 Val_Reconstruction : 54.48688507080078 Val_KL : 2.885199785232544\n","Epoch: 2435/5000  Traning Loss: 58.06324481964111  Train_Reconstruction: 55.13787889480591  Train_KL: 2.925367057323456  Validation Loss : 57.470659255981445 Val_Reconstruction : 54.571645736694336 Val_KL : 2.899014115333557\n","Epoch: 2436/5000  Traning Loss: 57.923723220825195  Train_Reconstruction: 54.98614311218262  Train_KL: 2.9375804662704468  Validation Loss : 57.29022407531738 Val_Reconstruction : 54.381675720214844 Val_KL : 2.9085477590560913\n","Epoch: 2437/5000  Traning Loss: 58.061816692352295  Train_Reconstruction: 55.125150203704834  Train_KL: 2.9366664588451385  Validation Loss : 57.43837356567383 Val_Reconstruction : 54.531728744506836 Val_KL : 2.906644105911255\n","Epoch: 2438/5000  Traning Loss: 58.008384704589844  Train_Reconstruction: 55.07495069503784  Train_KL: 2.9334347248077393  Validation Loss : 57.273956298828125 Val_Reconstruction : 54.380279541015625 Val_KL : 2.8936755657196045\n","Epoch: 2439/5000  Traning Loss: 57.98602771759033  Train_Reconstruction: 55.065722942352295  Train_KL: 2.9203041791915894  Validation Loss : 57.3729305267334 Val_Reconstruction : 54.491350173950195 Val_KL : 2.8815795183181763\n","Epoch: 2440/5000  Traning Loss: 57.88589143753052  Train_Reconstruction: 54.962674617767334  Train_KL: 2.923216611146927  Validation Loss : 57.221120834350586 Val_Reconstruction : 54.3251895904541 Val_KL : 2.895931363105774\n","Epoch: 2441/5000  Traning Loss: 58.067748069763184  Train_Reconstruction: 55.138442516326904  Train_KL: 2.9293060302734375  Validation Loss : 57.754600524902344 Val_Reconstruction : 54.855438232421875 Val_KL : 2.8991628885269165\n","Epoch: 2442/5000  Traning Loss: 58.2718563079834  Train_Reconstruction: 55.34183931350708  Train_KL: 2.930016666650772  Validation Loss : 57.54875183105469 Val_Reconstruction : 54.655784606933594 Val_KL : 2.8929672241210938\n","Epoch: 2443/5000  Traning Loss: 58.71029472351074  Train_Reconstruction: 55.781578540802  Train_KL: 2.9287159144878387  Validation Loss : 58.29898643493652 Val_Reconstruction : 55.38821792602539 Val_KL : 2.910768985748291\n","Epoch: 2444/5000  Traning Loss: 58.54619550704956  Train_Reconstruction: 55.60428810119629  Train_KL: 2.941907614469528  Validation Loss : 57.54998016357422 Val_Reconstruction : 54.65150833129883 Val_KL : 2.898471713066101\n","Epoch: 2445/5000  Traning Loss: 58.32962465286255  Train_Reconstruction: 55.41742706298828  Train_KL: 2.9121974408626556  Validation Loss : 57.98712348937988 Val_Reconstruction : 55.09522819519043 Val_KL : 2.8918968439102173\n","Epoch: 2446/5000  Traning Loss: 58.464041233062744  Train_Reconstruction: 55.52898645401001  Train_KL: 2.9350551664829254  Validation Loss : 57.92548370361328 Val_Reconstruction : 55.017818450927734 Val_KL : 2.9076645374298096\n","Epoch: 2447/5000  Traning Loss: 58.50436305999756  Train_Reconstruction: 55.568039417266846  Train_KL: 2.9363231658935547  Validation Loss : 58.01314926147461 Val_Reconstruction : 55.12094497680664 Val_KL : 2.892202854156494\n","Epoch: 2448/5000  Traning Loss: 58.20207452774048  Train_Reconstruction: 55.27818965911865  Train_KL: 2.923884868621826  Validation Loss : 57.38749885559082 Val_Reconstruction : 54.49168395996094 Val_KL : 2.8958131074905396\n","Epoch: 2449/5000  Traning Loss: 58.18555164337158  Train_Reconstruction: 55.26201343536377  Train_KL: 2.923538565635681  Validation Loss : 57.8851261138916 Val_Reconstruction : 54.99405860900879 Val_KL : 2.8910677433013916\n","Epoch: 2450/5000  Traning Loss: 58.48064374923706  Train_Reconstruction: 55.562087059020996  Train_KL: 2.9185569286346436  Validation Loss : 57.75856590270996 Val_Reconstruction : 54.87093544006348 Val_KL : 2.8876307010650635\n","Epoch: 2451/5000  Traning Loss: 58.106436252593994  Train_Reconstruction: 55.185115337371826  Train_KL: 2.921320825815201  Validation Loss : 57.17111587524414 Val_Reconstruction : 54.290178298950195 Val_KL : 2.8809372186660767\n","Epoch: 2452/5000  Traning Loss: 58.35156297683716  Train_Reconstruction: 55.441669940948486  Train_KL: 2.9098925292491913  Validation Loss : 57.96746635437012 Val_Reconstruction : 55.091081619262695 Val_KL : 2.876384973526001\n","Epoch: 2453/5000  Traning Loss: 58.840542793273926  Train_Reconstruction: 55.91651105880737  Train_KL: 2.9240314960479736  Validation Loss : 58.61414909362793 Val_Reconstruction : 55.70195198059082 Val_KL : 2.9121971130371094\n","Epoch: 2454/5000  Traning Loss: 58.4441237449646  Train_Reconstruction: 55.5101637840271  Train_KL: 2.933960258960724  Validation Loss : 57.50411033630371 Val_Reconstruction : 54.601301193237305 Val_KL : 2.9028090238571167\n","Epoch: 2455/5000  Traning Loss: 58.202274322509766  Train_Reconstruction: 55.283621311187744  Train_KL: 2.918652832508087  Validation Loss : 57.47816276550293 Val_Reconstruction : 54.59359550476074 Val_KL : 2.8845659494400024\n","Epoch: 2456/5000  Traning Loss: 58.223474979400635  Train_Reconstruction: 55.304590702056885  Train_KL: 2.918883889913559  Validation Loss : 57.672889709472656 Val_Reconstruction : 54.76845359802246 Val_KL : 2.904435873031616\n","Epoch: 2457/5000  Traning Loss: 57.978983879089355  Train_Reconstruction: 55.049153327941895  Train_KL: 2.929830312728882  Validation Loss : 57.08121109008789 Val_Reconstruction : 54.18576240539551 Val_KL : 2.895449161529541\n","Epoch: 2458/5000  Traning Loss: 57.8333306312561  Train_Reconstruction: 54.913294315338135  Train_KL: 2.9200364351272583  Validation Loss : 57.12487983703613 Val_Reconstruction : 54.23872947692871 Val_KL : 2.8861513137817383\n","Epoch: 2459/5000  Traning Loss: 58.01385831832886  Train_Reconstruction: 55.09923553466797  Train_KL: 2.9146227538585663  Validation Loss : 57.44686698913574 Val_Reconstruction : 54.57208251953125 Val_KL : 2.874782681465149\n","Epoch: 2460/5000  Traning Loss: 58.06716012954712  Train_Reconstruction: 55.154194355010986  Train_KL: 2.912966102361679  Validation Loss : 57.68242073059082 Val_Reconstruction : 54.791290283203125 Val_KL : 2.891130805015564\n","Epoch: 2461/5000  Traning Loss: 58.04301166534424  Train_Reconstruction: 55.11341857910156  Train_KL: 2.9295931458473206  Validation Loss : 57.585968017578125 Val_Reconstruction : 54.68037223815918 Val_KL : 2.905595541000366\n","Epoch: 2462/5000  Traning Loss: 58.17362880706787  Train_Reconstruction: 55.2379355430603  Train_KL: 2.935693711042404  Validation Loss : 57.5506706237793 Val_Reconstruction : 54.64445495605469 Val_KL : 2.9062150716781616\n","Epoch: 2463/5000  Traning Loss: 58.35160732269287  Train_Reconstruction: 55.423569679260254  Train_KL: 2.9280375242233276  Validation Loss : 57.83188056945801 Val_Reconstruction : 54.935768127441406 Val_KL : 2.8961119651794434\n","Epoch: 2464/5000  Traning Loss: 58.377079486846924  Train_Reconstruction: 55.454888343811035  Train_KL: 2.9221910536289215  Validation Loss : 57.82471466064453 Val_Reconstruction : 54.92932891845703 Val_KL : 2.8953853845596313\n","Epoch: 2465/5000  Traning Loss: 58.443936347961426  Train_Reconstruction: 55.50806951522827  Train_KL: 2.935866892337799  Validation Loss : 57.94223213195801 Val_Reconstruction : 55.03084945678711 Val_KL : 2.911383867263794\n","Epoch: 2466/5000  Traning Loss: 58.06609392166138  Train_Reconstruction: 55.126718521118164  Train_KL: 2.9393754601478577  Validation Loss : 57.318599700927734 Val_Reconstruction : 54.4126091003418 Val_KL : 2.9059901237487793\n","Epoch: 2467/5000  Traning Loss: 58.052568435668945  Train_Reconstruction: 55.11823034286499  Train_KL: 2.934338331222534  Validation Loss : 57.64787673950195 Val_Reconstruction : 54.74333572387695 Val_KL : 2.9045422077178955\n","Epoch: 2468/5000  Traning Loss: 58.10222387313843  Train_Reconstruction: 55.16826248168945  Train_KL: 2.933961182832718  Validation Loss : 57.254194259643555 Val_Reconstruction : 54.34540367126465 Val_KL : 2.908790707588196\n","Epoch: 2469/5000  Traning Loss: 57.94268846511841  Train_Reconstruction: 55.0137152671814  Train_KL: 2.9289727210998535  Validation Loss : 57.28931999206543 Val_Reconstruction : 54.388383865356445 Val_KL : 2.900936484336853\n","Epoch: 2470/5000  Traning Loss: 58.09593057632446  Train_Reconstruction: 55.16631078720093  Train_KL: 2.9296195209026337  Validation Loss : 57.82368087768555 Val_Reconstruction : 54.926055908203125 Val_KL : 2.897623896598816\n","Epoch: 2471/5000  Traning Loss: 59.31483507156372  Train_Reconstruction: 56.39206647872925  Train_KL: 2.922769010066986  Validation Loss : 59.1092643737793 Val_Reconstruction : 56.225826263427734 Val_KL : 2.8834372758865356\n","Epoch: 2472/5000  Traning Loss: 59.121094703674316  Train_Reconstruction: 56.19819164276123  Train_KL: 2.922903299331665  Validation Loss : 57.99201583862305 Val_Reconstruction : 55.09456825256348 Val_KL : 2.8974461555480957\n","Epoch: 2473/5000  Traning Loss: 58.54323434829712  Train_Reconstruction: 55.61331796646118  Train_KL: 2.9299161434173584  Validation Loss : 57.86308288574219 Val_Reconstruction : 54.96965408325195 Val_KL : 2.8934284448623657\n","Epoch: 2474/5000  Traning Loss: 58.28450393676758  Train_Reconstruction: 55.35795783996582  Train_KL: 2.926545947790146  Validation Loss : 58.07682228088379 Val_Reconstruction : 55.18379211425781 Val_KL : 2.8930307626724243\n","Epoch: 2475/5000  Traning Loss: 58.79410791397095  Train_Reconstruction: 55.864567279815674  Train_KL: 2.9295410811901093  Validation Loss : 58.69980430603027 Val_Reconstruction : 55.8061637878418 Val_KL : 2.8936392068862915\n","Epoch: 2476/5000  Traning Loss: 58.44495916366577  Train_Reconstruction: 55.520434856414795  Train_KL: 2.924524575471878  Validation Loss : 57.7415714263916 Val_Reconstruction : 54.85083198547363 Val_KL : 2.890740156173706\n","Epoch: 2477/5000  Traning Loss: 58.11833429336548  Train_Reconstruction: 55.19690132141113  Train_KL: 2.921432614326477  Validation Loss : 57.76902961730957 Val_Reconstruction : 54.875993728637695 Val_KL : 2.8930352926254272\n","Epoch: 2478/5000  Traning Loss: 58.26199722290039  Train_Reconstruction: 55.327574253082275  Train_KL: 2.9344233870506287  Validation Loss : 57.650596618652344 Val_Reconstruction : 54.746347427368164 Val_KL : 2.9042489528656006\n","Epoch: 2479/5000  Traning Loss: 58.05337047576904  Train_Reconstruction: 55.11555767059326  Train_KL: 2.937812924385071  Validation Loss : 57.25485801696777 Val_Reconstruction : 54.3530158996582 Val_KL : 2.9018425941467285\n","Epoch: 2480/5000  Traning Loss: 57.927650451660156  Train_Reconstruction: 54.99858903884888  Train_KL: 2.9290615618228912  Validation Loss : 57.39339637756348 Val_Reconstruction : 54.50503921508789 Val_KL : 2.8883562088012695\n","Epoch: 2481/5000  Traning Loss: 58.210084438323975  Train_Reconstruction: 55.29060173034668  Train_KL: 2.9194829761981964  Validation Loss : 57.77878761291504 Val_Reconstruction : 54.899410247802734 Val_KL : 2.8793764114379883\n","Epoch: 2482/5000  Traning Loss: 58.25652456283569  Train_Reconstruction: 55.335779666900635  Train_KL: 2.920745551586151  Validation Loss : 57.18504524230957 Val_Reconstruction : 54.28439140319824 Val_KL : 2.9006545543670654\n","Epoch: 2483/5000  Traning Loss: 57.856696128845215  Train_Reconstruction: 54.92236328125  Train_KL: 2.9343324303627014  Validation Loss : 57.154001235961914 Val_Reconstruction : 54.25139617919922 Val_KL : 2.9026049375534058\n","Epoch: 2484/5000  Traning Loss: 58.10307025909424  Train_Reconstruction: 55.17676591873169  Train_KL: 2.92630398273468  Validation Loss : 57.49570274353027 Val_Reconstruction : 54.59868049621582 Val_KL : 2.8970236778259277\n","Epoch: 2485/5000  Traning Loss: 58.214998722076416  Train_Reconstruction: 55.28089380264282  Train_KL: 2.934105396270752  Validation Loss : 57.61462593078613 Val_Reconstruction : 54.699663162231445 Val_KL : 2.914961814880371\n","Epoch: 2486/5000  Traning Loss: 58.43916130065918  Train_Reconstruction: 55.49911451339722  Train_KL: 2.9400469064712524  Validation Loss : 57.67209434509277 Val_Reconstruction : 54.76917266845703 Val_KL : 2.9029210805892944\n","Epoch: 2487/5000  Traning Loss: 58.652644634246826  Train_Reconstruction: 55.73213863372803  Train_KL: 2.9205061197280884  Validation Loss : 58.10270118713379 Val_Reconstruction : 55.21546936035156 Val_KL : 2.887231469154358\n","Epoch: 2488/5000  Traning Loss: 58.76200342178345  Train_Reconstruction: 55.838276863098145  Train_KL: 2.9237270951271057  Validation Loss : 58.72646522521973 Val_Reconstruction : 55.83739471435547 Val_KL : 2.8890693187713623\n","Epoch: 2489/5000  Traning Loss: 59.03233051300049  Train_Reconstruction: 56.10944080352783  Train_KL: 2.9228895604610443  Validation Loss : 58.31127166748047 Val_Reconstruction : 55.40938377380371 Val_KL : 2.9018869400024414\n","Epoch: 2490/5000  Traning Loss: 58.48141527175903  Train_Reconstruction: 55.559354305267334  Train_KL: 2.9220605194568634  Validation Loss : 57.81329345703125 Val_Reconstruction : 54.929609298706055 Val_KL : 2.883684277534485\n","Epoch: 2491/5000  Traning Loss: 58.28636837005615  Train_Reconstruction: 55.36720895767212  Train_KL: 2.9191591441631317  Validation Loss : 57.55619812011719 Val_Reconstruction : 54.65432357788086 Val_KL : 2.9018744230270386\n","Epoch: 2492/5000  Traning Loss: 58.069275856018066  Train_Reconstruction: 55.129061698913574  Train_KL: 2.940214067697525  Validation Loss : 57.44215202331543 Val_Reconstruction : 54.53475761413574 Val_KL : 2.9073948860168457\n","Epoch: 2493/5000  Traning Loss: 57.989912033081055  Train_Reconstruction: 55.05918598175049  Train_KL: 2.93072646856308  Validation Loss : 57.44519233703613 Val_Reconstruction : 54.547149658203125 Val_KL : 2.8980427980422974\n","Epoch: 2494/5000  Traning Loss: 58.10025596618652  Train_Reconstruction: 55.16807508468628  Train_KL: 2.932181656360626  Validation Loss : 57.53969764709473 Val_Reconstruction : 54.621788024902344 Val_KL : 2.917909860610962\n","Epoch: 2495/5000  Traning Loss: 58.009140491485596  Train_Reconstruction: 55.07965803146362  Train_KL: 2.9294824600219727  Validation Loss : 57.19276809692383 Val_Reconstruction : 54.29292106628418 Val_KL : 2.8998464345932007\n","Epoch: 2496/5000  Traning Loss: 57.87318134307861  Train_Reconstruction: 54.943148136138916  Train_KL: 2.9300330877304077  Validation Loss : 57.11455154418945 Val_Reconstruction : 54.21989822387695 Val_KL : 2.894654631614685\n","Epoch: 2497/5000  Traning Loss: 58.07312536239624  Train_Reconstruction: 55.13826322555542  Train_KL: 2.934861958026886  Validation Loss : 57.53653144836426 Val_Reconstruction : 54.63602828979492 Val_KL : 2.9005037546157837\n","Epoch: 2498/5000  Traning Loss: 58.34321069717407  Train_Reconstruction: 55.411738872528076  Train_KL: 2.9314723908901215  Validation Loss : 57.621538162231445 Val_Reconstruction : 54.71862030029297 Val_KL : 2.902917504310608\n","Epoch: 2499/5000  Traning Loss: 58.244739055633545  Train_Reconstruction: 55.323119163513184  Train_KL: 2.9216195046901703  Validation Loss : 57.69733238220215 Val_Reconstruction : 54.79770851135254 Val_KL : 2.8996236324310303\n","Epoch: 2500/5000  Traning Loss: 58.48625707626343  Train_Reconstruction: 55.55718946456909  Train_KL: 2.929067373275757  Validation Loss : 57.80684280395508 Val_Reconstruction : 54.906843185424805 Val_KL : 2.8999987840652466\n","Epoch: 2501/5000  Traning Loss: 58.13517618179321  Train_Reconstruction: 55.20397615432739  Train_KL: 2.9312005043029785  Validation Loss : 57.22361183166504 Val_Reconstruction : 54.32746124267578 Val_KL : 2.8961496353149414\n","Epoch: 2502/5000  Traning Loss: 57.89564275741577  Train_Reconstruction: 54.971176624298096  Train_KL: 2.9244657158851624  Validation Loss : 57.373722076416016 Val_Reconstruction : 54.47346878051758 Val_KL : 2.900254487991333\n","Epoch: 2503/5000  Traning Loss: 58.04592514038086  Train_Reconstruction: 55.10764408111572  Train_KL: 2.938281625509262  Validation Loss : 57.12752723693848 Val_Reconstruction : 54.22149467468262 Val_KL : 2.9060330390930176\n","Epoch: 2504/5000  Traning Loss: 58.14057922363281  Train_Reconstruction: 55.21409034729004  Train_KL: 2.926488518714905  Validation Loss : 57.6114387512207 Val_Reconstruction : 54.726844787597656 Val_KL : 2.8845934867858887\n","Epoch: 2505/5000  Traning Loss: 57.775572776794434  Train_Reconstruction: 54.8498911857605  Train_KL: 2.9256808757781982  Validation Loss : 57.08607292175293 Val_Reconstruction : 54.183658599853516 Val_KL : 2.9024136066436768\n","Epoch: 2506/5000  Traning Loss: 58.08558416366577  Train_Reconstruction: 55.1554069519043  Train_KL: 2.9301772117614746  Validation Loss : 57.4134578704834 Val_Reconstruction : 54.506879806518555 Val_KL : 2.906578540802002\n","Epoch: 2507/5000  Traning Loss: 58.26180171966553  Train_Reconstruction: 55.3272180557251  Train_KL: 2.9345829784870148  Validation Loss : 57.84519577026367 Val_Reconstruction : 54.93525314331055 Val_KL : 2.9099422693252563\n","Epoch: 2508/5000  Traning Loss: 57.974159717559814  Train_Reconstruction: 55.046130657196045  Train_KL: 2.9280286133289337  Validation Loss : 57.171836853027344 Val_Reconstruction : 54.27118492126465 Val_KL : 2.9006519317626953\n","Epoch: 2509/5000  Traning Loss: 57.93139123916626  Train_Reconstruction: 55.00057888031006  Train_KL: 2.930813044309616  Validation Loss : 57.44954490661621 Val_Reconstruction : 54.54207801818848 Val_KL : 2.9074666500091553\n","Epoch: 2510/5000  Traning Loss: 58.01431751251221  Train_Reconstruction: 55.08176136016846  Train_KL: 2.932556688785553  Validation Loss : 57.23149108886719 Val_Reconstruction : 54.336997985839844 Val_KL : 2.894493341445923\n","Epoch: 2511/5000  Traning Loss: 57.80872583389282  Train_Reconstruction: 54.875768184661865  Train_KL: 2.9329579770565033  Validation Loss : 56.98176383972168 Val_Reconstruction : 54.07766151428223 Val_KL : 2.904103636741638\n","Epoch: 2512/5000  Traning Loss: 57.75686597824097  Train_Reconstruction: 54.81727886199951  Train_KL: 2.9395872056484222  Validation Loss : 57.16666603088379 Val_Reconstruction : 54.25628662109375 Val_KL : 2.910378098487854\n","Epoch: 2513/5000  Traning Loss: 57.75750684738159  Train_Reconstruction: 54.82061958312988  Train_KL: 2.936886966228485  Validation Loss : 56.9644889831543 Val_Reconstruction : 54.06257438659668 Val_KL : 2.901914119720459\n","Epoch: 2514/5000  Traning Loss: 57.7766056060791  Train_Reconstruction: 54.84667778015137  Train_KL: 2.9299286901950836  Validation Loss : 57.26106262207031 Val_Reconstruction : 54.363481521606445 Val_KL : 2.8975799083709717\n","Epoch: 2515/5000  Traning Loss: 58.091956615448  Train_Reconstruction: 55.15733480453491  Train_KL: 2.9346215426921844  Validation Loss : 57.217573165893555 Val_Reconstruction : 54.308244705200195 Val_KL : 2.9093292951583862\n","Epoch: 2516/5000  Traning Loss: 58.27338981628418  Train_Reconstruction: 55.33973455429077  Train_KL: 2.9336551427841187  Validation Loss : 57.59645462036133 Val_Reconstruction : 54.70088195800781 Val_KL : 2.8955743312835693\n","Epoch: 2517/5000  Traning Loss: 58.33628225326538  Train_Reconstruction: 55.4066686630249  Train_KL: 2.929613709449768  Validation Loss : 57.66658973693848 Val_Reconstruction : 54.77130699157715 Val_KL : 2.895283341407776\n","Epoch: 2518/5000  Traning Loss: 58.24221324920654  Train_Reconstruction: 55.31723976135254  Train_KL: 2.924974262714386  Validation Loss : 57.33556365966797 Val_Reconstruction : 54.4355411529541 Val_KL : 2.9000226259231567\n","Epoch: 2519/5000  Traning Loss: 58.33940553665161  Train_Reconstruction: 55.40897035598755  Train_KL: 2.9304347038269043  Validation Loss : 57.87935447692871 Val_Reconstruction : 54.97817420959473 Val_KL : 2.9011800289154053\n","Epoch: 2520/5000  Traning Loss: 58.50991630554199  Train_Reconstruction: 55.57454013824463  Train_KL: 2.935376077890396  Validation Loss : 58.025794982910156 Val_Reconstruction : 55.10920715332031 Val_KL : 2.916588068008423\n","Epoch: 2521/5000  Traning Loss: 58.30987501144409  Train_Reconstruction: 55.37349605560303  Train_KL: 2.9363786578178406  Validation Loss : 57.55938720703125 Val_Reconstruction : 54.655967712402344 Val_KL : 2.9034206867218018\n","Epoch: 2522/5000  Traning Loss: 57.929219245910645  Train_Reconstruction: 55.00048828125  Train_KL: 2.928731292486191  Validation Loss : 57.10358428955078 Val_Reconstruction : 54.2056884765625 Val_KL : 2.897896647453308\n","Epoch: 2523/5000  Traning Loss: 57.712135314941406  Train_Reconstruction: 54.782501220703125  Train_KL: 2.9296334385871887  Validation Loss : 57.00598907470703 Val_Reconstruction : 54.10023307800293 Val_KL : 2.905755639076233\n","Epoch: 2524/5000  Traning Loss: 57.63292217254639  Train_Reconstruction: 54.6912202835083  Train_KL: 2.9417017698287964  Validation Loss : 57.133235931396484 Val_Reconstruction : 54.22381019592285 Val_KL : 2.9094263315200806\n","Epoch: 2525/5000  Traning Loss: 57.716376304626465  Train_Reconstruction: 54.784286975860596  Train_KL: 2.9320895671844482  Validation Loss : 57.222415924072266 Val_Reconstruction : 54.325645446777344 Val_KL : 2.8967705965042114\n","Epoch: 2526/5000  Traning Loss: 57.814899921417236  Train_Reconstruction: 54.88223934173584  Train_KL: 2.9326608180999756  Validation Loss : 57.221086502075195 Val_Reconstruction : 54.32289505004883 Val_KL : 2.898191809654236\n","Epoch: 2527/5000  Traning Loss: 57.823503494262695  Train_Reconstruction: 54.89944505691528  Train_KL: 2.924057900905609  Validation Loss : 57.082624435424805 Val_Reconstruction : 54.191162109375 Val_KL : 2.8914616107940674\n","Epoch: 2528/5000  Traning Loss: 57.89087247848511  Train_Reconstruction: 54.96724224090576  Train_KL: 2.9236303865909576  Validation Loss : 57.32256889343262 Val_Reconstruction : 54.42393112182617 Val_KL : 2.8986376523971558\n","Epoch: 2529/5000  Traning Loss: 57.91182470321655  Train_Reconstruction: 54.97920751571655  Train_KL: 2.9326168298721313  Validation Loss : 57.34937858581543 Val_Reconstruction : 54.452619552612305 Val_KL : 2.8967586755752563\n","Epoch: 2530/5000  Traning Loss: 57.923123359680176  Train_Reconstruction: 55.00100564956665  Train_KL: 2.9221176207065582  Validation Loss : 57.34497833251953 Val_Reconstruction : 54.465721130371094 Val_KL : 2.8792574405670166\n","Epoch: 2531/5000  Traning Loss: 57.922457695007324  Train_Reconstruction: 54.99961519241333  Train_KL: 2.9228423833847046  Validation Loss : 57.31990623474121 Val_Reconstruction : 54.42318916320801 Val_KL : 2.8967182636260986\n","Epoch: 2532/5000  Traning Loss: 58.106478691101074  Train_Reconstruction: 55.17276191711426  Train_KL: 2.9337167739868164  Validation Loss : 57.75537109375 Val_Reconstruction : 54.85464859008789 Val_KL : 2.900722026824951\n","Epoch: 2533/5000  Traning Loss: 58.37098026275635  Train_Reconstruction: 55.444159507751465  Train_KL: 2.9268210530281067  Validation Loss : 57.86669921875 Val_Reconstruction : 54.96491050720215 Val_KL : 2.90178906917572\n","Epoch: 2534/5000  Traning Loss: 58.254714012145996  Train_Reconstruction: 55.31805372238159  Train_KL: 2.936660498380661  Validation Loss : 57.48701477050781 Val_Reconstruction : 54.57783317565918 Val_KL : 2.9091813564300537\n","Epoch: 2535/5000  Traning Loss: 58.08227014541626  Train_Reconstruction: 55.14306306838989  Train_KL: 2.9392073154449463  Validation Loss : 57.25681495666504 Val_Reconstruction : 54.35460662841797 Val_KL : 2.9022088050842285\n","Epoch: 2536/5000  Traning Loss: 57.99927520751953  Train_Reconstruction: 55.06605005264282  Train_KL: 2.933225393295288  Validation Loss : 57.335554122924805 Val_Reconstruction : 54.43275260925293 Val_KL : 2.9028013944625854\n","Epoch: 2537/5000  Traning Loss: 58.49465799331665  Train_Reconstruction: 55.56026077270508  Train_KL: 2.9343970715999603  Validation Loss : 57.804840087890625 Val_Reconstruction : 54.90717887878418 Val_KL : 2.8976610898971558\n","Epoch: 2538/5000  Traning Loss: 58.73672008514404  Train_Reconstruction: 55.80329656600952  Train_KL: 2.9334235191345215  Validation Loss : 57.95814323425293 Val_Reconstruction : 55.05954933166504 Val_KL : 2.8985952138900757\n","Epoch: 2539/5000  Traning Loss: 58.40282201766968  Train_Reconstruction: 55.46867656707764  Train_KL: 2.9341447949409485  Validation Loss : 58.073740005493164 Val_Reconstruction : 55.17381477355957 Val_KL : 2.8999266624450684\n","Epoch: 2540/5000  Traning Loss: 58.042521476745605  Train_Reconstruction: 55.11712837219238  Train_KL: 2.92539319396019  Validation Loss : 57.33512306213379 Val_Reconstruction : 54.44319725036621 Val_KL : 2.8919273614883423\n","Epoch: 2541/5000  Traning Loss: 57.87118625640869  Train_Reconstruction: 54.945281982421875  Train_KL: 2.925903469324112  Validation Loss : 57.134178161621094 Val_Reconstruction : 54.23872184753418 Val_KL : 2.8954575061798096\n","Epoch: 2542/5000  Traning Loss: 58.03258800506592  Train_Reconstruction: 55.10728645324707  Train_KL: 2.925302267074585  Validation Loss : 57.50935363769531 Val_Reconstruction : 54.61211967468262 Val_KL : 2.897234320640564\n","Epoch: 2543/5000  Traning Loss: 57.97377014160156  Train_Reconstruction: 55.04819297790527  Train_KL: 2.9255769848823547  Validation Loss : 57.068382263183594 Val_Reconstruction : 54.17752456665039 Val_KL : 2.8908573389053345\n","Epoch: 2544/5000  Traning Loss: 58.02351903915405  Train_Reconstruction: 55.109169006347656  Train_KL: 2.9143491089344025  Validation Loss : 57.5958251953125 Val_Reconstruction : 54.71224403381348 Val_KL : 2.8835811614990234\n","Epoch: 2545/5000  Traning Loss: 58.5193133354187  Train_Reconstruction: 55.59792518615723  Train_KL: 2.9213882386684418  Validation Loss : 58.30588912963867 Val_Reconstruction : 55.40203094482422 Val_KL : 2.9038572311401367\n","Epoch: 2546/5000  Traning Loss: 58.68148231506348  Train_Reconstruction: 55.75791120529175  Train_KL: 2.9235711097717285  Validation Loss : 58.67873573303223 Val_Reconstruction : 55.7869873046875 Val_KL : 2.8917486667633057\n","Epoch: 2547/5000  Traning Loss: 58.743491649627686  Train_Reconstruction: 55.834744930267334  Train_KL: 2.908746838569641  Validation Loss : 58.26014709472656 Val_Reconstruction : 55.37228012084961 Val_KL : 2.8878666162490845\n","Epoch: 2548/5000  Traning Loss: 58.80121564865112  Train_Reconstruction: 55.87588024139404  Train_KL: 2.9253358244895935  Validation Loss : 58.214744567871094 Val_Reconstruction : 55.322166442871094 Val_KL : 2.892578601837158\n","Epoch: 2549/5000  Traning Loss: 58.47674798965454  Train_Reconstruction: 55.542988300323486  Train_KL: 2.9337596595287323  Validation Loss : 57.63210105895996 Val_Reconstruction : 54.73835563659668 Val_KL : 2.893744111061096\n","Epoch: 2550/5000  Traning Loss: 58.08853197097778  Train_Reconstruction: 55.166279315948486  Train_KL: 2.922252506017685  Validation Loss : 57.616689682006836 Val_Reconstruction : 54.73091697692871 Val_KL : 2.885772943496704\n","Epoch: 2551/5000  Traning Loss: 57.856107234954834  Train_Reconstruction: 54.93199014663696  Train_KL: 2.924116790294647  Validation Loss : 57.38522911071777 Val_Reconstruction : 54.500267028808594 Val_KL : 2.8849631547927856\n","Epoch: 2552/5000  Traning Loss: 58.02529048919678  Train_Reconstruction: 55.1116886138916  Train_KL: 2.9136023223400116  Validation Loss : 57.40279579162598 Val_Reconstruction : 54.50699043273926 Val_KL : 2.8958061933517456\n","Epoch: 2553/5000  Traning Loss: 58.118072509765625  Train_Reconstruction: 55.18406820297241  Train_KL: 2.9340041875839233  Validation Loss : 57.315317153930664 Val_Reconstruction : 54.41581916809082 Val_KL : 2.899496912956238\n","Epoch: 2554/5000  Traning Loss: 57.92290496826172  Train_Reconstruction: 55.00111198425293  Train_KL: 2.9217928647994995  Validation Loss : 57.38317108154297 Val_Reconstruction : 54.5064640045166 Val_KL : 2.8767086267471313\n","Epoch: 2555/5000  Traning Loss: 57.84147834777832  Train_Reconstruction: 54.92725992202759  Train_KL: 2.914218157529831  Validation Loss : 57.03712844848633 Val_Reconstruction : 54.14797782897949 Val_KL : 2.8891501426696777\n","Epoch: 2556/5000  Traning Loss: 58.0510516166687  Train_Reconstruction: 55.12552499771118  Train_KL: 2.925526887178421  Validation Loss : 57.7042293548584 Val_Reconstruction : 54.798004150390625 Val_KL : 2.906224489212036\n","Epoch: 2557/5000  Traning Loss: 58.306885719299316  Train_Reconstruction: 55.38179969787598  Train_KL: 2.925086259841919  Validation Loss : 58.081119537353516 Val_Reconstruction : 55.18153953552246 Val_KL : 2.8995790481567383\n","Epoch: 2558/5000  Traning Loss: 58.23500728607178  Train_Reconstruction: 55.301124572753906  Train_KL: 2.933882027864456  Validation Loss : 57.84762954711914 Val_Reconstruction : 54.93866539001465 Val_KL : 2.9089646339416504\n","Epoch: 2559/5000  Traning Loss: 58.303651332855225  Train_Reconstruction: 55.37440252304077  Train_KL: 2.9292488992214203  Validation Loss : 57.49602508544922 Val_Reconstruction : 54.59584426879883 Val_KL : 2.9001821279525757\n","Epoch: 2560/5000  Traning Loss: 57.90658473968506  Train_Reconstruction: 54.97179079055786  Train_KL: 2.9347927570343018  Validation Loss : 56.99262237548828 Val_Reconstruction : 54.08035850524902 Val_KL : 2.9122629165649414\n","Epoch: 2561/5000  Traning Loss: 58.04290294647217  Train_Reconstruction: 55.10624170303345  Train_KL: 2.9366616010665894  Validation Loss : 57.526798248291016 Val_Reconstruction : 54.614768981933594 Val_KL : 2.912028431892395\n","Epoch: 2562/5000  Traning Loss: 58.14563512802124  Train_Reconstruction: 55.20847177505493  Train_KL: 2.937163531780243  Validation Loss : 57.69303512573242 Val_Reconstruction : 54.79083061218262 Val_KL : 2.902203917503357\n","Epoch: 2563/5000  Traning Loss: 58.216309547424316  Train_Reconstruction: 55.29099369049072  Train_KL: 2.92531618475914  Validation Loss : 57.39823341369629 Val_Reconstruction : 54.50844383239746 Val_KL : 2.8897892236709595\n","Epoch: 2564/5000  Traning Loss: 57.81077861785889  Train_Reconstruction: 54.889904499053955  Train_KL: 2.920873522758484  Validation Loss : 57.124473571777344 Val_Reconstruction : 54.2348518371582 Val_KL : 2.889622449874878\n","Epoch: 2565/5000  Traning Loss: 57.747408866882324  Train_Reconstruction: 54.82468032836914  Train_KL: 2.9227287471294403  Validation Loss : 57.26441764831543 Val_Reconstruction : 54.37252426147461 Val_KL : 2.891893148422241\n","Epoch: 2566/5000  Traning Loss: 58.01579666137695  Train_Reconstruction: 55.09405326843262  Train_KL: 2.9217430651187897  Validation Loss : 57.519880294799805 Val_Reconstruction : 54.62466239929199 Val_KL : 2.8952181339263916\n","Epoch: 2567/5000  Traning Loss: 57.99405765533447  Train_Reconstruction: 55.06916856765747  Train_KL: 2.92488893866539  Validation Loss : 57.26425743103027 Val_Reconstruction : 54.367454528808594 Val_KL : 2.896803855895996\n","Epoch: 2568/5000  Traning Loss: 57.91216850280762  Train_Reconstruction: 54.98179578781128  Train_KL: 2.930373042821884  Validation Loss : 57.27869415283203 Val_Reconstruction : 54.37726974487305 Val_KL : 2.9014244079589844\n","Epoch: 2569/5000  Traning Loss: 57.82349252700806  Train_Reconstruction: 54.8914008140564  Train_KL: 2.9320918321609497  Validation Loss : 57.14055824279785 Val_Reconstruction : 54.246625900268555 Val_KL : 2.893931746482849\n","Epoch: 2570/5000  Traning Loss: 58.191245555877686  Train_Reconstruction: 55.25906467437744  Train_KL: 2.932180732488632  Validation Loss : 57.97929000854492 Val_Reconstruction : 55.08799362182617 Val_KL : 2.8912962675094604\n","Epoch: 2571/5000  Traning Loss: 58.12372636795044  Train_Reconstruction: 55.19687223434448  Train_KL: 2.9268541634082794  Validation Loss : 57.2750244140625 Val_Reconstruction : 54.38893127441406 Val_KL : 2.8860939741134644\n","Epoch: 2572/5000  Traning Loss: 57.839977741241455  Train_Reconstruction: 54.918076038360596  Train_KL: 2.921902120113373  Validation Loss : 57.23470115661621 Val_Reconstruction : 54.34141731262207 Val_KL : 2.8932846784591675\n","Epoch: 2573/5000  Traning Loss: 57.912424087524414  Train_Reconstruction: 54.99140787124634  Train_KL: 2.9210162460803986  Validation Loss : 57.35599327087402 Val_Reconstruction : 54.474924087524414 Val_KL : 2.8810689449310303\n","Epoch: 2574/5000  Traning Loss: 58.060631275177  Train_Reconstruction: 55.14585256576538  Train_KL: 2.914779305458069  Validation Loss : 57.2801570892334 Val_Reconstruction : 54.38583183288574 Val_KL : 2.8943246603012085\n","Epoch: 2575/5000  Traning Loss: 57.869643211364746  Train_Reconstruction: 54.94637107849121  Train_KL: 2.9232717156410217  Validation Loss : 57.077972412109375 Val_Reconstruction : 54.17827606201172 Val_KL : 2.899696111679077\n","Epoch: 2576/5000  Traning Loss: 57.90527963638306  Train_Reconstruction: 54.97909688949585  Train_KL: 2.9261827170848846  Validation Loss : 57.21928596496582 Val_Reconstruction : 54.32853698730469 Val_KL : 2.8907488584518433\n","Epoch: 2577/5000  Traning Loss: 57.874035358428955  Train_Reconstruction: 54.950772762298584  Train_KL: 2.9232624173164368  Validation Loss : 57.13970184326172 Val_Reconstruction : 54.2506046295166 Val_KL : 2.8890973329544067\n","Epoch: 2578/5000  Traning Loss: 57.99702215194702  Train_Reconstruction: 55.06855535507202  Train_KL: 2.9284667372703552  Validation Loss : 57.33172416687012 Val_Reconstruction : 54.43120574951172 Val_KL : 2.900518536567688\n","Epoch: 2579/5000  Traning Loss: 57.83322763442993  Train_Reconstruction: 54.90861177444458  Train_KL: 2.9246160686016083  Validation Loss : 57.199541091918945 Val_Reconstruction : 54.303964614868164 Val_KL : 2.8955763578414917\n","Epoch: 2580/5000  Traning Loss: 58.07477045059204  Train_Reconstruction: 55.144898891448975  Train_KL: 2.929871439933777  Validation Loss : 57.53697967529297 Val_Reconstruction : 54.645469665527344 Val_KL : 2.8915098905563354\n","Epoch: 2581/5000  Traning Loss: 57.96487855911255  Train_Reconstruction: 55.03208684921265  Train_KL: 2.9327912628650665  Validation Loss : 57.35431480407715 Val_Reconstruction : 54.45310974121094 Val_KL : 2.90120530128479\n","Epoch: 2582/5000  Traning Loss: 58.07969570159912  Train_Reconstruction: 55.14180040359497  Train_KL: 2.9378950893878937  Validation Loss : 57.74445152282715 Val_Reconstruction : 54.839256286621094 Val_KL : 2.9051952362060547\n","Epoch: 2583/5000  Traning Loss: 58.18672037124634  Train_Reconstruction: 55.256943225860596  Train_KL: 2.929776430130005  Validation Loss : 57.422285079956055 Val_Reconstruction : 54.526105880737305 Val_KL : 2.896180033683777\n","Epoch: 2584/5000  Traning Loss: 58.003849029541016  Train_Reconstruction: 55.07729530334473  Train_KL: 2.9265539944171906  Validation Loss : 57.17374801635742 Val_Reconstruction : 54.274715423583984 Val_KL : 2.8990334272384644\n","Epoch: 2585/5000  Traning Loss: 57.95101833343506  Train_Reconstruction: 55.02495241165161  Train_KL: 2.926065295934677  Validation Loss : 57.35152626037598 Val_Reconstruction : 54.46562194824219 Val_KL : 2.8859047889709473\n","Epoch: 2586/5000  Traning Loss: 58.45737981796265  Train_Reconstruction: 55.53585386276245  Train_KL: 2.92152538895607  Validation Loss : 58.19404411315918 Val_Reconstruction : 55.30584144592285 Val_KL : 2.888203740119934\n","Epoch: 2587/5000  Traning Loss: 58.33218765258789  Train_Reconstruction: 55.405641078948975  Train_KL: 2.9265461564064026  Validation Loss : 57.88041305541992 Val_Reconstruction : 54.99081039428711 Val_KL : 2.889603018760681\n","Epoch: 2588/5000  Traning Loss: 58.28836250305176  Train_Reconstruction: 55.35579872131348  Train_KL: 2.932564228773117  Validation Loss : 57.59906005859375 Val_Reconstruction : 54.69485282897949 Val_KL : 2.9042056798934937\n","Epoch: 2589/5000  Traning Loss: 58.19156074523926  Train_Reconstruction: 55.25211811065674  Train_KL: 2.9394422471523285  Validation Loss : 57.682186126708984 Val_Reconstruction : 54.78022003173828 Val_KL : 2.901966094970703\n","Epoch: 2590/5000  Traning Loss: 57.86484241485596  Train_Reconstruction: 54.93743324279785  Train_KL: 2.9274089336395264  Validation Loss : 57.442148208618164 Val_Reconstruction : 54.54544258117676 Val_KL : 2.8967050313949585\n","Epoch: 2591/5000  Traning Loss: 57.995702266693115  Train_Reconstruction: 55.06061315536499  Train_KL: 2.935089111328125  Validation Loss : 57.60408973693848 Val_Reconstruction : 54.6968879699707 Val_KL : 2.907201886177063\n","Epoch: 2592/5000  Traning Loss: 58.87323331832886  Train_Reconstruction: 55.94268083572388  Train_KL: 2.930552452802658  Validation Loss : 58.54496955871582 Val_Reconstruction : 55.63631248474121 Val_KL : 2.9086579084396362\n","Epoch: 2593/5000  Traning Loss: 58.67035961151123  Train_Reconstruction: 55.74435377120972  Train_KL: 2.926005780696869  Validation Loss : 58.105451583862305 Val_Reconstruction : 55.212392807006836 Val_KL : 2.8930584192276\n","Epoch: 2594/5000  Traning Loss: 58.091087341308594  Train_Reconstruction: 55.1613073348999  Train_KL: 2.929779499769211  Validation Loss : 57.24598693847656 Val_Reconstruction : 54.34531211853027 Val_KL : 2.900674819946289\n","Epoch: 2595/5000  Traning Loss: 57.88000440597534  Train_Reconstruction: 54.9393310546875  Train_KL: 2.9406725764274597  Validation Loss : 57.24611854553223 Val_Reconstruction : 54.340904235839844 Val_KL : 2.905213236808777\n","Epoch: 2596/5000  Traning Loss: 58.027912616729736  Train_Reconstruction: 55.102362632751465  Train_KL: 2.9255499839782715  Validation Loss : 57.695152282714844 Val_Reconstruction : 54.79628562927246 Val_KL : 2.8988661766052246\n","Epoch: 2597/5000  Traning Loss: 58.106770515441895  Train_Reconstruction: 55.17806673049927  Train_KL: 2.9287032186985016  Validation Loss : 57.36384963989258 Val_Reconstruction : 54.464500427246094 Val_KL : 2.899349570274353\n","Epoch: 2598/5000  Traning Loss: 57.98664140701294  Train_Reconstruction: 55.065476417541504  Train_KL: 2.9211653769016266  Validation Loss : 57.266557693481445 Val_Reconstruction : 54.38271713256836 Val_KL : 2.88383948802948\n","Epoch: 2599/5000  Traning Loss: 58.13914680480957  Train_Reconstruction: 55.216726303100586  Train_KL: 2.9224202036857605  Validation Loss : 57.89622116088867 Val_Reconstruction : 55.012672424316406 Val_KL : 2.8835490942001343\n","Epoch: 2600/5000  Traning Loss: 58.08843946456909  Train_Reconstruction: 55.165709018707275  Train_KL: 2.922730177640915  Validation Loss : 57.24190330505371 Val_Reconstruction : 54.34359550476074 Val_KL : 2.8983075618743896\n","Epoch: 2601/5000  Traning Loss: 58.08318281173706  Train_Reconstruction: 55.153995990753174  Train_KL: 2.929186910390854  Validation Loss : 57.78457260131836 Val_Reconstruction : 54.891218185424805 Val_KL : 2.8933533430099487\n","Epoch: 2602/5000  Traning Loss: 57.98638153076172  Train_Reconstruction: 55.05753231048584  Train_KL: 2.928849548101425  Validation Loss : 57.311635971069336 Val_Reconstruction : 54.41749382019043 Val_KL : 2.89414119720459\n","Epoch: 2603/5000  Traning Loss: 57.91849184036255  Train_Reconstruction: 54.99190711975098  Train_KL: 2.92658531665802  Validation Loss : 57.08416938781738 Val_Reconstruction : 54.18925666809082 Val_KL : 2.8949140310287476\n","Epoch: 2604/5000  Traning Loss: 57.86881732940674  Train_Reconstruction: 54.9365439414978  Train_KL: 2.9322735965251923  Validation Loss : 57.33711242675781 Val_Reconstruction : 54.44350624084473 Val_KL : 2.8936049938201904\n","Epoch: 2605/5000  Traning Loss: 57.9028434753418  Train_Reconstruction: 54.983275413513184  Train_KL: 2.91956827044487  Validation Loss : 57.31220245361328 Val_Reconstruction : 54.428579330444336 Val_KL : 2.8836233615875244\n","Epoch: 2606/5000  Traning Loss: 57.97464179992676  Train_Reconstruction: 55.045395851135254  Train_KL: 2.9292457699775696  Validation Loss : 57.58674430847168 Val_Reconstruction : 54.68238067626953 Val_KL : 2.904362201690674\n","Epoch: 2607/5000  Traning Loss: 57.88575458526611  Train_Reconstruction: 54.94471263885498  Train_KL: 2.941042125225067  Validation Loss : 57.244075775146484 Val_Reconstruction : 54.33333206176758 Val_KL : 2.9107433557510376\n","Epoch: 2608/5000  Traning Loss: 57.73786926269531  Train_Reconstruction: 54.79957818984985  Train_KL: 2.9382918179035187  Validation Loss : 57.03093910217285 Val_Reconstruction : 54.12390327453613 Val_KL : 2.907037138938904\n","Epoch: 2609/5000  Traning Loss: 57.85508155822754  Train_Reconstruction: 54.918392181396484  Train_KL: 2.9366892278194427  Validation Loss : 57.38616180419922 Val_Reconstruction : 54.47879600524902 Val_KL : 2.9073656797409058\n","Epoch: 2610/5000  Traning Loss: 57.94181442260742  Train_Reconstruction: 55.003803730010986  Train_KL: 2.938010513782501  Validation Loss : 57.08043098449707 Val_Reconstruction : 54.175506591796875 Val_KL : 2.904924511909485\n","Epoch: 2611/5000  Traning Loss: 58.30252122879028  Train_Reconstruction: 55.363022327423096  Train_KL: 2.9394985139369965  Validation Loss : 57.506669998168945 Val_Reconstruction : 54.59324836730957 Val_KL : 2.913421630859375\n","Epoch: 2612/5000  Traning Loss: 57.8063268661499  Train_Reconstruction: 54.86895513534546  Train_KL: 2.9373719394207  Validation Loss : 57.431819915771484 Val_Reconstruction : 54.53562355041504 Val_KL : 2.8961960077285767\n","Epoch: 2613/5000  Traning Loss: 57.70527982711792  Train_Reconstruction: 54.781376361846924  Train_KL: 2.9239035844802856  Validation Loss : 57.06653022766113 Val_Reconstruction : 54.16313934326172 Val_KL : 2.9033925533294678\n","Epoch: 2614/5000  Traning Loss: 57.91513681411743  Train_Reconstruction: 54.98039150238037  Train_KL: 2.9347455203533173  Validation Loss : 57.31976318359375 Val_Reconstruction : 54.41004180908203 Val_KL : 2.9097225666046143\n","Epoch: 2615/5000  Traning Loss: 57.85980033874512  Train_Reconstruction: 54.92991828918457  Train_KL: 2.9298813939094543  Validation Loss : 57.1591854095459 Val_Reconstruction : 54.26507568359375 Val_KL : 2.894109606742859\n","Epoch: 2616/5000  Traning Loss: 57.879770278930664  Train_Reconstruction: 54.95487689971924  Train_KL: 2.9248931109905243  Validation Loss : 57.178733825683594 Val_Reconstruction : 54.27103805541992 Val_KL : 2.9076963663101196\n","Epoch: 2617/5000  Traning Loss: 57.92893409729004  Train_Reconstruction: 54.99451780319214  Train_KL: 2.9344159066677094  Validation Loss : 57.386592864990234 Val_Reconstruction : 54.48978805541992 Val_KL : 2.896804690361023\n","Epoch: 2618/5000  Traning Loss: 58.18082237243652  Train_Reconstruction: 55.26409292221069  Train_KL: 2.9167288839817047  Validation Loss : 57.574832916259766 Val_Reconstruction : 54.70361328125 Val_KL : 2.8712196350097656\n","Epoch: 2619/5000  Traning Loss: 58.00456714630127  Train_Reconstruction: 55.09295463562012  Train_KL: 2.911612629890442  Validation Loss : 57.200334548950195 Val_Reconstruction : 54.31434440612793 Val_KL : 2.885990619659424\n","Epoch: 2620/5000  Traning Loss: 57.9326434135437  Train_Reconstruction: 54.99574279785156  Train_KL: 2.9369001388549805  Validation Loss : 57.5768985748291 Val_Reconstruction : 54.66254234313965 Val_KL : 2.9143558740615845\n","Epoch: 2621/5000  Traning Loss: 57.993648052215576  Train_Reconstruction: 55.05247259140015  Train_KL: 2.9411763846874237  Validation Loss : 57.24329948425293 Val_Reconstruction : 54.341196060180664 Val_KL : 2.9021037817001343\n","Epoch: 2622/5000  Traning Loss: 58.13165807723999  Train_Reconstruction: 55.20908498764038  Train_KL: 2.922573447227478  Validation Loss : 57.4128303527832 Val_Reconstruction : 54.52469825744629 Val_KL : 2.8881325721740723\n","Epoch: 2623/5000  Traning Loss: 58.04044532775879  Train_Reconstruction: 55.1105580329895  Train_KL: 2.929886609315872  Validation Loss : 57.44076919555664 Val_Reconstruction : 54.52908897399902 Val_KL : 2.911680221557617\n","Epoch: 2624/5000  Traning Loss: 58.00787591934204  Train_Reconstruction: 55.07101488113403  Train_KL: 2.9368611872196198  Validation Loss : 57.51279830932617 Val_Reconstruction : 54.61307144165039 Val_KL : 2.8997262716293335\n","Epoch: 2625/5000  Traning Loss: 57.92452526092529  Train_Reconstruction: 54.99842309951782  Train_KL: 2.9261025488376617  Validation Loss : 57.17687797546387 Val_Reconstruction : 54.279531478881836 Val_KL : 2.8973459005355835\n","Epoch: 2626/5000  Traning Loss: 57.846713066101074  Train_Reconstruction: 54.911975383758545  Train_KL: 2.934737414121628  Validation Loss : 57.35424041748047 Val_Reconstruction : 54.43869209289551 Val_KL : 2.9155471324920654\n","Epoch: 2627/5000  Traning Loss: 57.78762149810791  Train_Reconstruction: 54.848251819610596  Train_KL: 2.9393699169158936  Validation Loss : 57.05855751037598 Val_Reconstruction : 54.151933670043945 Val_KL : 2.9066243171691895\n","Epoch: 2628/5000  Traning Loss: 57.659783363342285  Train_Reconstruction: 54.724010944366455  Train_KL: 2.9357725381851196  Validation Loss : 57.587310791015625 Val_Reconstruction : 54.674381256103516 Val_KL : 2.9129284620285034\n","Epoch: 2629/5000  Traning Loss: 57.90744352340698  Train_Reconstruction: 54.974003314971924  Train_KL: 2.933440774679184  Validation Loss : 57.55705642700195 Val_Reconstruction : 54.66160011291504 Val_KL : 2.895457625389099\n","Epoch: 2630/5000  Traning Loss: 57.95332670211792  Train_Reconstruction: 55.01380157470703  Train_KL: 2.939525842666626  Validation Loss : 57.55421447753906 Val_Reconstruction : 54.65235710144043 Val_KL : 2.9018574953079224\n","Epoch: 2631/5000  Traning Loss: 58.05716419219971  Train_Reconstruction: 55.12751388549805  Train_KL: 2.9296507835388184  Validation Loss : 57.449079513549805 Val_Reconstruction : 54.54731369018555 Val_KL : 2.901765823364258\n","Epoch: 2632/5000  Traning Loss: 58.05043029785156  Train_Reconstruction: 55.113608837127686  Train_KL: 2.936821848154068  Validation Loss : 57.23705863952637 Val_Reconstruction : 54.33244323730469 Val_KL : 2.904616355895996\n","Epoch: 2633/5000  Traning Loss: 57.978214740753174  Train_Reconstruction: 55.0476770401001  Train_KL: 2.9305381774902344  Validation Loss : 57.415231704711914 Val_Reconstruction : 54.516761779785156 Val_KL : 2.8984702825546265\n","Epoch: 2634/5000  Traning Loss: 57.974727153778076  Train_Reconstruction: 55.0516095161438  Train_KL: 2.92311754822731  Validation Loss : 57.31579399108887 Val_Reconstruction : 54.4304141998291 Val_KL : 2.8853795528411865\n","Epoch: 2635/5000  Traning Loss: 57.969199657440186  Train_Reconstruction: 55.04191255569458  Train_KL: 2.927286833524704  Validation Loss : 57.3228645324707 Val_Reconstruction : 54.41337203979492 Val_KL : 2.9094924926757812\n","Epoch: 2636/5000  Traning Loss: 57.71385717391968  Train_Reconstruction: 54.770498752593994  Train_KL: 2.9433578550815582  Validation Loss : 56.941640853881836 Val_Reconstruction : 54.042564392089844 Val_KL : 2.89907705783844\n","Epoch: 2637/5000  Traning Loss: 57.56700897216797  Train_Reconstruction: 54.6349720954895  Train_KL: 2.932037502527237  Validation Loss : 56.894596099853516 Val_Reconstruction : 53.98132133483887 Val_KL : 2.9132741689682007\n","Epoch: 2638/5000  Traning Loss: 57.68815469741821  Train_Reconstruction: 54.745633125305176  Train_KL: 2.9425210058689117  Validation Loss : 56.94873237609863 Val_Reconstruction : 54.02927780151367 Val_KL : 2.919454336166382\n","Epoch: 2639/5000  Traning Loss: 57.7577919960022  Train_Reconstruction: 54.81505537033081  Train_KL: 2.9427365362644196  Validation Loss : 57.232011795043945 Val_Reconstruction : 54.319271087646484 Val_KL : 2.912740111351013\n","Epoch: 2640/5000  Traning Loss: 58.00101661682129  Train_Reconstruction: 55.074378967285156  Train_KL: 2.926637649536133  Validation Loss : 57.44126510620117 Val_Reconstruction : 54.5377197265625 Val_KL : 2.9035451412200928\n","Epoch: 2641/5000  Traning Loss: 58.139294147491455  Train_Reconstruction: 55.19988965988159  Train_KL: 2.9394039511680603  Validation Loss : 57.393287658691406 Val_Reconstruction : 54.48457908630371 Val_KL : 2.9087074995040894\n","Epoch: 2642/5000  Traning Loss: 57.8933687210083  Train_Reconstruction: 54.95024824142456  Train_KL: 2.943120628595352  Validation Loss : 57.165687561035156 Val_Reconstruction : 54.25174522399902 Val_KL : 2.913941740989685\n","Epoch: 2643/5000  Traning Loss: 58.13676643371582  Train_Reconstruction: 55.20352363586426  Train_KL: 2.9332422018051147  Validation Loss : 57.890804290771484 Val_Reconstruction : 54.98977279663086 Val_KL : 2.90103280544281\n","Epoch: 2644/5000  Traning Loss: 57.90636157989502  Train_Reconstruction: 54.986011028289795  Train_KL: 2.920350670814514  Validation Loss : 57.198646545410156 Val_Reconstruction : 54.306575775146484 Val_KL : 2.892071843147278\n","Epoch: 2645/5000  Traning Loss: 57.95322561264038  Train_Reconstruction: 55.03379154205322  Train_KL: 2.9194337725639343  Validation Loss : 57.17783546447754 Val_Reconstruction : 54.29085350036621 Val_KL : 2.886980891227722\n","Epoch: 2646/5000  Traning Loss: 57.78835582733154  Train_Reconstruction: 54.87377166748047  Train_KL: 2.914584219455719  Validation Loss : 57.04413414001465 Val_Reconstruction : 54.155160903930664 Val_KL : 2.8889729976654053\n","Epoch: 2647/5000  Traning Loss: 57.76720571517944  Train_Reconstruction: 54.84381818771362  Train_KL: 2.923387348651886  Validation Loss : 57.17949867248535 Val_Reconstruction : 54.28664588928223 Val_KL : 2.8928537368774414\n","Epoch: 2648/5000  Traning Loss: 57.857351303100586  Train_Reconstruction: 54.92528963088989  Train_KL: 2.9320626258850098  Validation Loss : 57.1225700378418 Val_Reconstruction : 54.21516799926758 Val_KL : 2.907402276992798\n","Epoch: 2649/5000  Traning Loss: 58.17795276641846  Train_Reconstruction: 55.242942810058594  Train_KL: 2.9350095689296722  Validation Loss : 57.639949798583984 Val_Reconstruction : 54.73678207397461 Val_KL : 2.9031686782836914\n","Epoch: 2650/5000  Traning Loss: 58.755393505096436  Train_Reconstruction: 55.82671880722046  Train_KL: 2.9286747872829437  Validation Loss : 58.325008392333984 Val_Reconstruction : 55.424795150756836 Val_KL : 2.9002143144607544\n","Epoch: 2651/5000  Traning Loss: 58.5777645111084  Train_Reconstruction: 55.64513301849365  Train_KL: 2.9326310753822327  Validation Loss : 57.28301811218262 Val_Reconstruction : 54.380531311035156 Val_KL : 2.902485489845276\n","Epoch: 2652/5000  Traning Loss: 58.01884412765503  Train_Reconstruction: 55.091976165771484  Train_KL: 2.9268675446510315  Validation Loss : 57.27693748474121 Val_Reconstruction : 54.376834869384766 Val_KL : 2.900102376937866\n","Epoch: 2653/5000  Traning Loss: 58.069194316864014  Train_Reconstruction: 55.13583993911743  Train_KL: 2.9333551824092865  Validation Loss : 57.49556922912598 Val_Reconstruction : 54.5885066986084 Val_KL : 2.907063841819763\n","Epoch: 2654/5000  Traning Loss: 57.87765836715698  Train_Reconstruction: 54.939594745635986  Train_KL: 2.938063681125641  Validation Loss : 56.94776916503906 Val_Reconstruction : 54.04763984680176 Val_KL : 2.900127649307251\n","Epoch: 2655/5000  Traning Loss: 57.74913454055786  Train_Reconstruction: 54.8214750289917  Train_KL: 2.9276593029499054  Validation Loss : 57.46873664855957 Val_Reconstruction : 54.57254600524902 Val_KL : 2.8961910009384155\n","Epoch: 2656/5000  Traning Loss: 57.86745548248291  Train_Reconstruction: 54.940146923065186  Train_KL: 2.92730849981308  Validation Loss : 57.13783645629883 Val_Reconstruction : 54.24713134765625 Val_KL : 2.89070463180542\n","Epoch: 2657/5000  Traning Loss: 58.19932746887207  Train_Reconstruction: 55.27473831176758  Train_KL: 2.9245892763137817  Validation Loss : 57.98155212402344 Val_Reconstruction : 55.076406478881836 Val_KL : 2.905144453048706\n","Epoch: 2658/5000  Traning Loss: 58.12515830993652  Train_Reconstruction: 55.19424629211426  Train_KL: 2.9309110045433044  Validation Loss : 57.4421443939209 Val_Reconstruction : 54.5489616394043 Val_KL : 2.8931838274002075\n","Epoch: 2659/5000  Traning Loss: 58.044710636138916  Train_Reconstruction: 55.11635065078735  Train_KL: 2.92835995554924  Validation Loss : 57.26600456237793 Val_Reconstruction : 54.362138748168945 Val_KL : 2.903865694999695\n","Epoch: 2660/5000  Traning Loss: 57.81406879425049  Train_Reconstruction: 54.892249584198  Train_KL: 2.92181932926178  Validation Loss : 57.37412452697754 Val_Reconstruction : 54.4800968170166 Val_KL : 2.894027352333069\n","Epoch: 2661/5000  Traning Loss: 58.001728534698486  Train_Reconstruction: 55.072165966033936  Train_KL: 2.92956280708313  Validation Loss : 57.62916946411133 Val_Reconstruction : 54.72011375427246 Val_KL : 2.909056305885315\n","Epoch: 2662/5000  Traning Loss: 58.14192485809326  Train_Reconstruction: 55.209267139434814  Train_KL: 2.9326573312282562  Validation Loss : 57.47374725341797 Val_Reconstruction : 54.5796012878418 Val_KL : 2.894147038459778\n","Epoch: 2663/5000  Traning Loss: 58.44104194641113  Train_Reconstruction: 55.50838088989258  Train_KL: 2.932660460472107  Validation Loss : 57.85801887512207 Val_Reconstruction : 54.94822883605957 Val_KL : 2.9097917079925537\n","Epoch: 2664/5000  Traning Loss: 57.99005603790283  Train_Reconstruction: 55.05179691314697  Train_KL: 2.9382589757442474  Validation Loss : 57.093828201293945 Val_Reconstruction : 54.19089889526367 Val_KL : 2.9029290676116943\n","Epoch: 2665/5000  Traning Loss: 57.79928636550903  Train_Reconstruction: 54.87576103210449  Train_KL: 2.92352557182312  Validation Loss : 57.20644950866699 Val_Reconstruction : 54.30959129333496 Val_KL : 2.8968591690063477\n","Epoch: 2666/5000  Traning Loss: 57.757872581481934  Train_Reconstruction: 54.82236337661743  Train_KL: 2.9355093240737915  Validation Loss : 57.02084922790527 Val_Reconstruction : 54.111581802368164 Val_KL : 2.909267783164978\n","Epoch: 2667/5000  Traning Loss: 57.81638193130493  Train_Reconstruction: 54.88163375854492  Train_KL: 2.9347489774227142  Validation Loss : 57.414167404174805 Val_Reconstruction : 54.511606216430664 Val_KL : 2.9025609493255615\n","Epoch: 2668/5000  Traning Loss: 57.76071214675903  Train_Reconstruction: 54.82733774185181  Train_KL: 2.9333744943141937  Validation Loss : 57.15215301513672 Val_Reconstruction : 54.24380683898926 Val_KL : 2.90834641456604\n","Epoch: 2669/5000  Traning Loss: 57.7837233543396  Train_Reconstruction: 54.85635423660278  Train_KL: 2.9273692667484283  Validation Loss : 57.15607833862305 Val_Reconstruction : 54.2593936920166 Val_KL : 2.896686315536499\n","Epoch: 2670/5000  Traning Loss: 57.7585244178772  Train_Reconstruction: 54.83308506011963  Train_KL: 2.925440102815628  Validation Loss : 57.10581970214844 Val_Reconstruction : 54.19702339172363 Val_KL : 2.9087960720062256\n","Epoch: 2671/5000  Traning Loss: 57.72232246398926  Train_Reconstruction: 54.78024101257324  Train_KL: 2.9420814514160156  Validation Loss : 57.23328399658203 Val_Reconstruction : 54.320058822631836 Val_KL : 2.913224458694458\n","Epoch: 2672/5000  Traning Loss: 57.881640911102295  Train_Reconstruction: 54.935203552246094  Train_KL: 2.946438044309616  Validation Loss : 57.399635314941406 Val_Reconstruction : 54.49464797973633 Val_KL : 2.904987096786499\n","Epoch: 2673/5000  Traning Loss: 57.97507905960083  Train_Reconstruction: 55.046385288238525  Train_KL: 2.9286936223506927  Validation Loss : 57.44056510925293 Val_Reconstruction : 54.54966163635254 Val_KL : 2.890904426574707\n","Epoch: 2674/5000  Traning Loss: 58.01396036148071  Train_Reconstruction: 55.08421039581299  Train_KL: 2.9297499656677246  Validation Loss : 57.814414978027344 Val_Reconstruction : 54.907331466674805 Val_KL : 2.9070823192596436\n","Epoch: 2675/5000  Traning Loss: 58.11710453033447  Train_Reconstruction: 55.17814588546753  Train_KL: 2.938958376646042  Validation Loss : 57.29147148132324 Val_Reconstruction : 54.38949775695801 Val_KL : 2.901973247528076\n","Epoch: 2676/5000  Traning Loss: 58.301698207855225  Train_Reconstruction: 55.37356424331665  Train_KL: 2.9281339049339294  Validation Loss : 57.552955627441406 Val_Reconstruction : 54.66571807861328 Val_KL : 2.887237310409546\n","Epoch: 2677/5000  Traning Loss: 58.14169454574585  Train_Reconstruction: 55.215497970581055  Train_KL: 2.926195979118347  Validation Loss : 57.24199104309082 Val_Reconstruction : 54.33968544006348 Val_KL : 2.9023061990737915\n","Epoch: 2678/5000  Traning Loss: 57.81523036956787  Train_Reconstruction: 54.88660287857056  Train_KL: 2.9286271035671234  Validation Loss : 57.26003074645996 Val_Reconstruction : 54.36263084411621 Val_KL : 2.8974002599716187\n","Epoch: 2679/5000  Traning Loss: 57.79877233505249  Train_Reconstruction: 54.86534881591797  Train_KL: 2.9334230422973633  Validation Loss : 57.26391410827637 Val_Reconstruction : 54.35568046569824 Val_KL : 2.908232092857361\n","Epoch: 2680/5000  Traning Loss: 58.01188564300537  Train_Reconstruction: 55.08285665512085  Train_KL: 2.9290289282798767  Validation Loss : 57.39301872253418 Val_Reconstruction : 54.495718002319336 Val_KL : 2.8973000049591064\n","Epoch: 2681/5000  Traning Loss: 57.996220111846924  Train_Reconstruction: 55.07463312149048  Train_KL: 2.921587824821472  Validation Loss : 57.20903205871582 Val_Reconstruction : 54.31313705444336 Val_KL : 2.8958948850631714\n","Epoch: 2682/5000  Traning Loss: 58.22535943984985  Train_Reconstruction: 55.294283866882324  Train_KL: 2.9310750663280487  Validation Loss : 57.62307357788086 Val_Reconstruction : 54.726558685302734 Val_KL : 2.8965137004852295\n","Epoch: 2683/5000  Traning Loss: 58.09358310699463  Train_Reconstruction: 55.16118764877319  Train_KL: 2.9323959052562714  Validation Loss : 57.1385383605957 Val_Reconstruction : 54.23567008972168 Val_KL : 2.9028666019439697\n","Epoch: 2684/5000  Traning Loss: 57.755298137664795  Train_Reconstruction: 54.82122850418091  Train_KL: 2.934069871902466  Validation Loss : 57.024728775024414 Val_Reconstruction : 54.12247085571289 Val_KL : 2.9022586345672607\n","Epoch: 2685/5000  Traning Loss: 57.82980537414551  Train_Reconstruction: 54.905736446380615  Train_KL: 2.924068808555603  Validation Loss : 57.28022575378418 Val_Reconstruction : 54.381147384643555 Val_KL : 2.8990767002105713\n","Epoch: 2686/5000  Traning Loss: 58.1384801864624  Train_Reconstruction: 55.196757793426514  Train_KL: 2.9417219161987305  Validation Loss : 57.31459617614746 Val_Reconstruction : 54.40896797180176 Val_KL : 2.905627965927124\n","Epoch: 2687/5000  Traning Loss: 57.81478214263916  Train_Reconstruction: 54.873512744903564  Train_KL: 2.9412689805030823  Validation Loss : 57.17600440979004 Val_Reconstruction : 54.26360321044922 Val_KL : 2.912400722503662\n","Epoch: 2688/5000  Traning Loss: 58.04290819168091  Train_Reconstruction: 55.1085410118103  Train_KL: 2.9343676567077637  Validation Loss : 57.40831184387207 Val_Reconstruction : 54.504079818725586 Val_KL : 2.904232621192932\n","Epoch: 2689/5000  Traning Loss: 57.770501136779785  Train_Reconstruction: 54.84219837188721  Train_KL: 2.9283027350902557  Validation Loss : 57.428998947143555 Val_Reconstruction : 54.52594184875488 Val_KL : 2.903057813644409\n","Epoch: 2690/5000  Traning Loss: 57.87102746963501  Train_Reconstruction: 54.93881034851074  Train_KL: 2.9322175085544586  Validation Loss : 57.77889633178711 Val_Reconstruction : 54.86675834655762 Val_KL : 2.9121387004852295\n","Epoch: 2691/5000  Traning Loss: 57.78554916381836  Train_Reconstruction: 54.85023784637451  Train_KL: 2.9353116750717163  Validation Loss : 57.43315124511719 Val_Reconstruction : 54.53280448913574 Val_KL : 2.900346040725708\n","Epoch: 2692/5000  Traning Loss: 57.87757062911987  Train_Reconstruction: 54.95364713668823  Train_KL: 2.9239233136177063  Validation Loss : 57.19774055480957 Val_Reconstruction : 54.30885314941406 Val_KL : 2.8888877630233765\n","Epoch: 2693/5000  Traning Loss: 57.77301597595215  Train_Reconstruction: 54.847203731536865  Train_KL: 2.9258124828338623  Validation Loss : 57.153757095336914 Val_Reconstruction : 54.25704383850098 Val_KL : 2.896713376045227\n","Epoch: 2694/5000  Traning Loss: 57.8812780380249  Train_Reconstruction: 54.94716548919678  Train_KL: 2.934112846851349  Validation Loss : 57.38158416748047 Val_Reconstruction : 54.48165702819824 Val_KL : 2.899926543235779\n","Epoch: 2695/5000  Traning Loss: 57.775794982910156  Train_Reconstruction: 54.84376859664917  Train_KL: 2.9320269525051117  Validation Loss : 57.08236312866211 Val_Reconstruction : 54.17780303955078 Val_KL : 2.904560923576355\n","Epoch: 2696/5000  Traning Loss: 57.62544059753418  Train_Reconstruction: 54.693974018096924  Train_KL: 2.931466192007065  Validation Loss : 56.90557098388672 Val_Reconstruction : 54.004770278930664 Val_KL : 2.900800347328186\n","Epoch: 2697/5000  Traning Loss: 57.595261096954346  Train_Reconstruction: 54.67064094543457  Train_KL: 2.924619823694229  Validation Loss : 56.903242111206055 Val_Reconstruction : 53.999223709106445 Val_KL : 2.9040184020996094\n","Epoch: 2698/5000  Traning Loss: 57.66190052032471  Train_Reconstruction: 54.72741174697876  Train_KL: 2.934489518404007  Validation Loss : 57.24225616455078 Val_Reconstruction : 54.33201217651367 Val_KL : 2.9102444648742676\n","Epoch: 2699/5000  Traning Loss: 57.71133804321289  Train_Reconstruction: 54.77346611022949  Train_KL: 2.937872350215912  Validation Loss : 57.019880294799805 Val_Reconstruction : 54.114431381225586 Val_KL : 2.905450224876404\n","Epoch: 2700/5000  Traning Loss: 57.61379528045654  Train_Reconstruction: 54.683183670043945  Train_KL: 2.93061164021492  Validation Loss : 57.05988883972168 Val_Reconstruction : 54.15837478637695 Val_KL : 2.9015129804611206\n","Epoch: 2701/5000  Traning Loss: 57.61839008331299  Train_Reconstruction: 54.68870496749878  Train_KL: 2.9296849370002747  Validation Loss : 56.93384552001953 Val_Reconstruction : 54.03135681152344 Val_KL : 2.902490019798279\n","Epoch: 2702/5000  Traning Loss: 57.729512214660645  Train_Reconstruction: 54.794150829315186  Train_KL: 2.935361683368683  Validation Loss : 56.98724174499512 Val_Reconstruction : 54.088430404663086 Val_KL : 2.898813009262085\n","Epoch: 2703/5000  Traning Loss: 57.9315071105957  Train_Reconstruction: 54.99877882003784  Train_KL: 2.9327282309532166  Validation Loss : 57.37284278869629 Val_Reconstruction : 54.462520599365234 Val_KL : 2.910322904586792\n","Epoch: 2704/5000  Traning Loss: 58.06185245513916  Train_Reconstruction: 55.126972675323486  Train_KL: 2.934879571199417  Validation Loss : 57.55027961730957 Val_Reconstruction : 54.661590576171875 Val_KL : 2.888689160346985\n","Epoch: 2705/5000  Traning Loss: 57.879863262176514  Train_Reconstruction: 54.95726156234741  Train_KL: 2.922600895166397  Validation Loss : 57.17526817321777 Val_Reconstruction : 54.28242301940918 Val_KL : 2.892844796180725\n","Epoch: 2706/5000  Traning Loss: 57.86544227600098  Train_Reconstruction: 54.934377670288086  Train_KL: 2.9310644268989563  Validation Loss : 57.2641487121582 Val_Reconstruction : 54.362281799316406 Val_KL : 2.9018670320510864\n","Epoch: 2707/5000  Traning Loss: 58.18880367279053  Train_Reconstruction: 55.25595235824585  Train_KL: 2.932851791381836  Validation Loss : 57.58807182312012 Val_Reconstruction : 54.69856262207031 Val_KL : 2.8895102739334106\n","Epoch: 2708/5000  Traning Loss: 58.15513277053833  Train_Reconstruction: 55.23863744735718  Train_KL: 2.916494995355606  Validation Loss : 57.48389434814453 Val_Reconstruction : 54.60078239440918 Val_KL : 2.8831112384796143\n","Epoch: 2709/5000  Traning Loss: 58.00682592391968  Train_Reconstruction: 55.08436346054077  Train_KL: 2.9224624633789062  Validation Loss : 57.28936195373535 Val_Reconstruction : 54.39678764343262 Val_KL : 2.892574191093445\n","Epoch: 2710/5000  Traning Loss: 57.861711502075195  Train_Reconstruction: 54.93912935256958  Train_KL: 2.9225820899009705  Validation Loss : 57.32229423522949 Val_Reconstruction : 54.433143615722656 Val_KL : 2.8891521692276\n","Epoch: 2711/5000  Traning Loss: 57.85432815551758  Train_Reconstruction: 54.93244791030884  Train_KL: 2.9218799769878387  Validation Loss : 57.07857322692871 Val_Reconstruction : 54.19309043884277 Val_KL : 2.8854845762252808\n","Epoch: 2712/5000  Traning Loss: 58.025113582611084  Train_Reconstruction: 55.10033464431763  Train_KL: 2.924778699874878  Validation Loss : 57.44195556640625 Val_Reconstruction : 54.549556732177734 Val_KL : 2.8924002647399902\n","Epoch: 2713/5000  Traning Loss: 58.520411014556885  Train_Reconstruction: 55.60106706619263  Train_KL: 2.919344276189804  Validation Loss : 57.94832229614258 Val_Reconstruction : 55.05684471130371 Val_KL : 2.8914783000946045\n","Epoch: 2714/5000  Traning Loss: 58.5783953666687  Train_Reconstruction: 55.65714693069458  Train_KL: 2.921248584985733  Validation Loss : 57.90696907043457 Val_Reconstruction : 55.0141658782959 Val_KL : 2.892803192138672\n","Epoch: 2715/5000  Traning Loss: 58.112972259521484  Train_Reconstruction: 55.18179225921631  Train_KL: 2.931179642677307  Validation Loss : 57.08251190185547 Val_Reconstruction : 54.183271408081055 Val_KL : 2.8992397785186768\n","Epoch: 2716/5000  Traning Loss: 57.73525810241699  Train_Reconstruction: 54.80008602142334  Train_KL: 2.935172349214554  Validation Loss : 56.93521308898926 Val_Reconstruction : 54.034799575805664 Val_KL : 2.900412678718567\n","Epoch: 2717/5000  Traning Loss: 57.90344715118408  Train_Reconstruction: 54.97013807296753  Train_KL: 2.9333088099956512  Validation Loss : 57.62143135070801 Val_Reconstruction : 54.71647071838379 Val_KL : 2.904961347579956\n","Epoch: 2718/5000  Traning Loss: 58.15312051773071  Train_Reconstruction: 55.21336650848389  Train_KL: 2.9397536516189575  Validation Loss : 57.804901123046875 Val_Reconstruction : 54.89023208618164 Val_KL : 2.9146692752838135\n","Epoch: 2719/5000  Traning Loss: 58.29179573059082  Train_Reconstruction: 55.35597467422485  Train_KL: 2.9358215630054474  Validation Loss : 57.731258392333984 Val_Reconstruction : 54.82655143737793 Val_KL : 2.9047069549560547\n","Epoch: 2720/5000  Traning Loss: 58.071961879730225  Train_Reconstruction: 55.150251388549805  Train_KL: 2.9217102229595184  Validation Loss : 57.346641540527344 Val_Reconstruction : 54.45404243469238 Val_KL : 2.8925979137420654\n","Epoch: 2721/5000  Traning Loss: 58.091386795043945  Train_Reconstruction: 55.168057918548584  Train_KL: 2.9233283698558807  Validation Loss : 57.61646842956543 Val_Reconstruction : 54.713768005371094 Val_KL : 2.9026994705200195\n","Epoch: 2722/5000  Traning Loss: 58.71722459793091  Train_Reconstruction: 55.78312683105469  Train_KL: 2.9340975284576416  Validation Loss : 58.57069969177246 Val_Reconstruction : 55.6672306060791 Val_KL : 2.9034687280654907\n","Epoch: 2723/5000  Traning Loss: 58.26103067398071  Train_Reconstruction: 55.33625030517578  Train_KL: 2.924780488014221  Validation Loss : 57.004499435424805 Val_Reconstruction : 54.11623191833496 Val_KL : 2.8882668018341064\n","Epoch: 2724/5000  Traning Loss: 57.77309465408325  Train_Reconstruction: 54.836663246154785  Train_KL: 2.93643257021904  Validation Loss : 57.02364921569824 Val_Reconstruction : 54.11038780212402 Val_KL : 2.9132606983184814\n","Epoch: 2725/5000  Traning Loss: 57.91474008560181  Train_Reconstruction: 54.97175407409668  Train_KL: 2.9429860711097717  Validation Loss : 57.35925483703613 Val_Reconstruction : 54.46001625061035 Val_KL : 2.8992385864257812\n","Epoch: 2726/5000  Traning Loss: 57.94923400878906  Train_Reconstruction: 55.0200400352478  Train_KL: 2.929194301366806  Validation Loss : 57.0087776184082 Val_Reconstruction : 54.119192123413086 Val_KL : 2.8895857334136963\n","Epoch: 2727/5000  Traning Loss: 58.03044319152832  Train_Reconstruction: 55.10629844665527  Train_KL: 2.9241443872451782  Validation Loss : 57.73738670349121 Val_Reconstruction : 54.84507751464844 Val_KL : 2.892309308052063\n","Epoch: 2728/5000  Traning Loss: 58.78596115112305  Train_Reconstruction: 55.84808826446533  Train_KL: 2.9378728568553925  Validation Loss : 58.86268615722656 Val_Reconstruction : 55.93959999084473 Val_KL : 2.923086166381836\n","Epoch: 2729/5000  Traning Loss: 58.51590967178345  Train_Reconstruction: 55.568724155426025  Train_KL: 2.947185844182968  Validation Loss : 57.99517250061035 Val_Reconstruction : 55.07678031921387 Val_KL : 2.9183934926986694\n","Epoch: 2730/5000  Traning Loss: 58.24334764480591  Train_Reconstruction: 55.3139328956604  Train_KL: 2.9294146597385406  Validation Loss : 57.595943450927734 Val_Reconstruction : 54.7026481628418 Val_KL : 2.8932961225509644\n","Epoch: 2731/5000  Traning Loss: 58.12043809890747  Train_Reconstruction: 55.19059467315674  Train_KL: 2.929843634366989  Validation Loss : 57.44610595703125 Val_Reconstruction : 54.542625427246094 Val_KL : 2.903482437133789\n","Epoch: 2732/5000  Traning Loss: 57.872984409332275  Train_Reconstruction: 54.9319167137146  Train_KL: 2.9410673081874847  Validation Loss : 57.1810188293457 Val_Reconstruction : 54.2783203125 Val_KL : 2.902698278427124\n","Epoch: 2733/5000  Traning Loss: 57.94709396362305  Train_Reconstruction: 55.022738456726074  Train_KL: 2.92435559630394  Validation Loss : 57.31058883666992 Val_Reconstruction : 54.41911506652832 Val_KL : 2.891473412513733\n","Epoch: 2734/5000  Traning Loss: 57.950334548950195  Train_Reconstruction: 55.03056240081787  Train_KL: 2.9197717905044556  Validation Loss : 57.1810417175293 Val_Reconstruction : 54.28608322143555 Val_KL : 2.8949599266052246\n","Epoch: 2735/5000  Traning Loss: 57.928922176361084  Train_Reconstruction: 55.00074338912964  Train_KL: 2.928177624940872  Validation Loss : 57.0463924407959 Val_Reconstruction : 54.14748573303223 Val_KL : 2.898907780647278\n","Epoch: 2736/5000  Traning Loss: 58.10645914077759  Train_Reconstruction: 55.179272174835205  Train_KL: 2.927186667919159  Validation Loss : 57.32194900512695 Val_Reconstruction : 54.43684196472168 Val_KL : 2.8851062059402466\n","Epoch: 2737/5000  Traning Loss: 57.673020362854004  Train_Reconstruction: 54.74768590927124  Train_KL: 2.9253336787223816  Validation Loss : 56.84159851074219 Val_Reconstruction : 53.94618797302246 Val_KL : 2.8954118490219116\n","Epoch: 2738/5000  Traning Loss: 57.6499924659729  Train_Reconstruction: 54.723403453826904  Train_KL: 2.9265888333320618  Validation Loss : 57.026235580444336 Val_Reconstruction : 54.1245174407959 Val_KL : 2.9017189741134644\n","Epoch: 2739/5000  Traning Loss: 57.62686729431152  Train_Reconstruction: 54.68931770324707  Train_KL: 2.937550723552704  Validation Loss : 57.02868843078613 Val_Reconstruction : 54.12966346740723 Val_KL : 2.899023652076721\n","Epoch: 2740/5000  Traning Loss: 57.60302925109863  Train_Reconstruction: 54.678062438964844  Train_KL: 2.92496657371521  Validation Loss : 57.07238960266113 Val_Reconstruction : 54.18013381958008 Val_KL : 2.892255663871765\n","Epoch: 2741/5000  Traning Loss: 57.73128795623779  Train_Reconstruction: 54.79924964904785  Train_KL: 2.932037830352783  Validation Loss : 57.01926612854004 Val_Reconstruction : 54.102210998535156 Val_KL : 2.9170557260513306\n","Epoch: 2742/5000  Traning Loss: 57.656487464904785  Train_Reconstruction: 54.710206031799316  Train_KL: 2.9462816417217255  Validation Loss : 57.05004692077637 Val_Reconstruction : 54.13594627380371 Val_KL : 2.9141018390655518\n","Epoch: 2743/5000  Traning Loss: 57.665818214416504  Train_Reconstruction: 54.726874351501465  Train_KL: 2.9389439821243286  Validation Loss : 57.354177474975586 Val_Reconstruction : 54.447519302368164 Val_KL : 2.9066580533981323\n","Epoch: 2744/5000  Traning Loss: 57.970215797424316  Train_Reconstruction: 55.03378868103027  Train_KL: 2.9364272952079773  Validation Loss : 57.59029769897461 Val_Reconstruction : 54.69846153259277 Val_KL : 2.891837000846863\n","Epoch: 2745/5000  Traning Loss: 58.35387563705444  Train_Reconstruction: 55.422661781311035  Train_KL: 2.9312136471271515  Validation Loss : 57.78313636779785 Val_Reconstruction : 54.88442420959473 Val_KL : 2.898712158203125\n","Epoch: 2746/5000  Traning Loss: 57.788548946380615  Train_Reconstruction: 54.85316753387451  Train_KL: 2.935381770133972  Validation Loss : 57.12955093383789 Val_Reconstruction : 54.22512245178223 Val_KL : 2.9044277667999268\n","Epoch: 2747/5000  Traning Loss: 57.609217166900635  Train_Reconstruction: 54.68174171447754  Train_KL: 2.9274756610393524  Validation Loss : 56.95778846740723 Val_Reconstruction : 54.06446647644043 Val_KL : 2.893321394920349\n","Epoch: 2748/5000  Traning Loss: 57.729369163513184  Train_Reconstruction: 54.79832077026367  Train_KL: 2.931049406528473  Validation Loss : 57.19388198852539 Val_Reconstruction : 54.28610801696777 Val_KL : 2.907774329185486\n","Epoch: 2749/5000  Traning Loss: 57.75847768783569  Train_Reconstruction: 54.82206439971924  Train_KL: 2.936413824558258  Validation Loss : 56.892934799194336 Val_Reconstruction : 53.98226737976074 Val_KL : 2.9106667041778564\n","Epoch: 2750/5000  Traning Loss: 57.736154556274414  Train_Reconstruction: 54.79232931137085  Train_KL: 2.9438254833221436  Validation Loss : 57.492435455322266 Val_Reconstruction : 54.57572555541992 Val_KL : 2.916709542274475\n","Epoch: 2751/5000  Traning Loss: 58.118895053863525  Train_Reconstruction: 55.17993927001953  Train_KL: 2.938955992460251  Validation Loss : 57.70437240600586 Val_Reconstruction : 54.80230712890625 Val_KL : 2.9020661115646362\n","Epoch: 2752/5000  Traning Loss: 58.27658748626709  Train_Reconstruction: 55.34532690048218  Train_KL: 2.9312606751918793  Validation Loss : 57.72284126281738 Val_Reconstruction : 54.81448554992676 Val_KL : 2.90835440158844\n","Epoch: 2753/5000  Traning Loss: 58.20839595794678  Train_Reconstruction: 55.270405769348145  Train_KL: 2.9379897713661194  Validation Loss : 57.61956787109375 Val_Reconstruction : 54.70286560058594 Val_KL : 2.916701555252075\n","Epoch: 2754/5000  Traning Loss: 57.70070743560791  Train_Reconstruction: 54.767412185668945  Train_KL: 2.933295249938965  Validation Loss : 57.07691764831543 Val_Reconstruction : 54.17159843444824 Val_KL : 2.9053181409835815\n","Epoch: 2755/5000  Traning Loss: 57.72080898284912  Train_Reconstruction: 54.78606081008911  Train_KL: 2.9347484707832336  Validation Loss : 57.29008674621582 Val_Reconstruction : 54.39321708679199 Val_KL : 2.8968690633773804\n","Epoch: 2756/5000  Traning Loss: 57.859403133392334  Train_Reconstruction: 54.926058769226074  Train_KL: 2.9333446323871613  Validation Loss : 57.13812828063965 Val_Reconstruction : 54.237491607666016 Val_KL : 2.9006370306015015\n","Epoch: 2757/5000  Traning Loss: 57.84067726135254  Train_Reconstruction: 54.904536724090576  Train_KL: 2.9361398816108704  Validation Loss : 57.0001106262207 Val_Reconstruction : 54.09784507751465 Val_KL : 2.9022669792175293\n","Epoch: 2758/5000  Traning Loss: 57.55694389343262  Train_Reconstruction: 54.62686204910278  Train_KL: 2.9300823509693146  Validation Loss : 57.09087562561035 Val_Reconstruction : 54.18481254577637 Val_KL : 2.9060643911361694\n","Epoch: 2759/5000  Traning Loss: 57.540950775146484  Train_Reconstruction: 54.60913419723511  Train_KL: 2.931816816329956  Validation Loss : 56.77963829040527 Val_Reconstruction : 53.88478088378906 Val_KL : 2.8948580026626587\n","Epoch: 2760/5000  Traning Loss: 57.580031871795654  Train_Reconstruction: 54.65086221694946  Train_KL: 2.929169327020645  Validation Loss : 56.865861892700195 Val_Reconstruction : 53.968313217163086 Val_KL : 2.897549271583557\n","Epoch: 2761/5000  Traning Loss: 57.64728355407715  Train_Reconstruction: 54.716548919677734  Train_KL: 2.9307345151901245  Validation Loss : 57.111961364746094 Val_Reconstruction : 54.21126174926758 Val_KL : 2.900699257850647\n","Epoch: 2762/5000  Traning Loss: 57.89954948425293  Train_Reconstruction: 54.97262907028198  Train_KL: 2.9269204139709473  Validation Loss : 57.270294189453125 Val_Reconstruction : 54.371084213256836 Val_KL : 2.899211287498474\n","Epoch: 2763/5000  Traning Loss: 58.04551696777344  Train_Reconstruction: 55.117558002471924  Train_KL: 2.9279588758945465  Validation Loss : 57.36219024658203 Val_Reconstruction : 54.46584129333496 Val_KL : 2.8963476419448853\n","Epoch: 2764/5000  Traning Loss: 58.09171772003174  Train_Reconstruction: 55.171700954437256  Train_KL: 2.920016586780548  Validation Loss : 57.34000587463379 Val_Reconstruction : 54.45314598083496 Val_KL : 2.886859655380249\n","Epoch: 2765/5000  Traning Loss: 58.258501052856445  Train_Reconstruction: 55.34006309509277  Train_KL: 2.9184373021125793  Validation Loss : 57.5981388092041 Val_Reconstruction : 54.70597457885742 Val_KL : 2.8921653032302856\n","Epoch: 2766/5000  Traning Loss: 58.16261053085327  Train_Reconstruction: 55.23764085769653  Train_KL: 2.924970030784607  Validation Loss : 57.53577995300293 Val_Reconstruction : 54.63926315307617 Val_KL : 2.896516799926758\n","Epoch: 2767/5000  Traning Loss: 57.86106872558594  Train_Reconstruction: 54.92990779876709  Train_KL: 2.9311604499816895  Validation Loss : 56.91241645812988 Val_Reconstruction : 54.01872253417969 Val_KL : 2.893694758415222\n","Epoch: 2768/5000  Traning Loss: 57.824567794799805  Train_Reconstruction: 54.88968563079834  Train_KL: 2.934882551431656  Validation Loss : 57.449872970581055 Val_Reconstruction : 54.555973052978516 Val_KL : 2.893900156021118\n","Epoch: 2769/5000  Traning Loss: 57.962656021118164  Train_Reconstruction: 55.03126096725464  Train_KL: 2.931395560503006  Validation Loss : 57.27040100097656 Val_Reconstruction : 54.37357521057129 Val_KL : 2.8968262672424316\n","Epoch: 2770/5000  Traning Loss: 57.7566933631897  Train_Reconstruction: 54.82636880874634  Train_KL: 2.9303241968154907  Validation Loss : 57.01870346069336 Val_Reconstruction : 54.118940353393555 Val_KL : 2.899763584136963\n","Epoch: 2771/5000  Traning Loss: 58.081921100616455  Train_Reconstruction: 55.150001525878906  Train_KL: 2.9319196343421936  Validation Loss : 57.4566593170166 Val_Reconstruction : 54.55481719970703 Val_KL : 2.901843547821045\n","Epoch: 2772/5000  Traning Loss: 58.24545907974243  Train_Reconstruction: 55.31413555145264  Train_KL: 2.9313232004642487  Validation Loss : 57.38828086853027 Val_Reconstruction : 54.48801231384277 Val_KL : 2.9002667665481567\n","Epoch: 2773/5000  Traning Loss: 58.00484752655029  Train_Reconstruction: 55.070916175842285  Train_KL: 2.93393138051033  Validation Loss : 57.49235153198242 Val_Reconstruction : 54.58395767211914 Val_KL : 2.9083938598632812\n","Epoch: 2774/5000  Traning Loss: 58.04243850708008  Train_Reconstruction: 55.10481834411621  Train_KL: 2.9376203417778015  Validation Loss : 57.781999588012695 Val_Reconstruction : 54.88505744934082 Val_KL : 2.8969417810440063\n","Epoch: 2775/5000  Traning Loss: 58.192331314086914  Train_Reconstruction: 55.26372814178467  Train_KL: 2.9286036491394043  Validation Loss : 57.619998931884766 Val_Reconstruction : 54.726938247680664 Val_KL : 2.893059492111206\n","Epoch: 2776/5000  Traning Loss: 58.08125352859497  Train_Reconstruction: 55.159568786621094  Train_KL: 2.9216848015785217  Validation Loss : 57.34044075012207 Val_Reconstruction : 54.447509765625 Val_KL : 2.8929301500320435\n","Epoch: 2777/5000  Traning Loss: 58.17943572998047  Train_Reconstruction: 55.248777866363525  Train_KL: 2.9306583404541016  Validation Loss : 57.47185707092285 Val_Reconstruction : 54.571533203125 Val_KL : 2.900322914123535\n","Epoch: 2778/5000  Traning Loss: 58.08679437637329  Train_Reconstruction: 55.156920433044434  Train_KL: 2.929873287677765  Validation Loss : 57.146060943603516 Val_Reconstruction : 54.24632263183594 Val_KL : 2.899737238883972\n","Epoch: 2779/5000  Traning Loss: 57.903987407684326  Train_Reconstruction: 54.96821975708008  Train_KL: 2.9357671439647675  Validation Loss : 57.68576431274414 Val_Reconstruction : 54.784440994262695 Val_KL : 2.9013240337371826\n","Epoch: 2780/5000  Traning Loss: 58.109352111816406  Train_Reconstruction: 55.186469078063965  Train_KL: 2.9228834211826324  Validation Loss : 57.65402603149414 Val_Reconstruction : 54.764822006225586 Val_KL : 2.889203906059265\n","Epoch: 2781/5000  Traning Loss: 58.54800271987915  Train_Reconstruction: 55.6279673576355  Train_KL: 2.920035719871521  Validation Loss : 58.06831169128418 Val_Reconstruction : 55.175228118896484 Val_KL : 2.8930851221084595\n","Epoch: 2782/5000  Traning Loss: 58.388909339904785  Train_Reconstruction: 55.46249866485596  Train_KL: 2.9264101684093475  Validation Loss : 57.67619323730469 Val_Reconstruction : 54.779279708862305 Val_KL : 2.8969147205352783\n","Epoch: 2783/5000  Traning Loss: 58.107500076293945  Train_Reconstruction: 55.192699909210205  Train_KL: 2.914800077676773  Validation Loss : 57.34267807006836 Val_Reconstruction : 54.46746063232422 Val_KL : 2.8752176761627197\n","Epoch: 2784/5000  Traning Loss: 57.80298566818237  Train_Reconstruction: 54.88788652420044  Train_KL: 2.915099710226059  Validation Loss : 57.16606521606445 Val_Reconstruction : 54.27265167236328 Val_KL : 2.8934125900268555\n","Epoch: 2785/5000  Traning Loss: 57.778178691864014  Train_Reconstruction: 54.84380578994751  Train_KL: 2.934372752904892  Validation Loss : 56.96458435058594 Val_Reconstruction : 54.0622501373291 Val_KL : 2.902333974838257\n","Epoch: 2786/5000  Traning Loss: 57.672832012176514  Train_Reconstruction: 54.7382378578186  Train_KL: 2.934593975543976  Validation Loss : 56.91694450378418 Val_Reconstruction : 54.00569152832031 Val_KL : 2.911253333091736\n","Epoch: 2787/5000  Traning Loss: 57.62750434875488  Train_Reconstruction: 54.69393301010132  Train_KL: 2.9335711896419525  Validation Loss : 56.97563552856445 Val_Reconstruction : 54.07741928100586 Val_KL : 2.8982157707214355\n","Epoch: 2788/5000  Traning Loss: 57.65840482711792  Train_Reconstruction: 54.729350566864014  Train_KL: 2.929054766893387  Validation Loss : 57.12627029418945 Val_Reconstruction : 54.22109413146973 Val_KL : 2.9051754474639893\n","Epoch: 2789/5000  Traning Loss: 57.62311410903931  Train_Reconstruction: 54.68517303466797  Train_KL: 2.937940865755081  Validation Loss : 57.067832946777344 Val_Reconstruction : 54.1549129486084 Val_KL : 2.9129196405410767\n","Epoch: 2790/5000  Traning Loss: 57.53032875061035  Train_Reconstruction: 54.597060680389404  Train_KL: 2.9332677721977234  Validation Loss : 56.86044883728027 Val_Reconstruction : 53.95977592468262 Val_KL : 2.9006714820861816\n","Epoch: 2791/5000  Traning Loss: 57.526620864868164  Train_Reconstruction: 54.59713935852051  Train_KL: 2.9294812083244324  Validation Loss : 56.84178924560547 Val_Reconstruction : 53.939008712768555 Val_KL : 2.9027812480926514\n","Epoch: 2792/5000  Traning Loss: 57.886178970336914  Train_Reconstruction: 54.94971799850464  Train_KL: 2.93646103143692  Validation Loss : 57.25720977783203 Val_Reconstruction : 54.357086181640625 Val_KL : 2.900122880935669\n","Epoch: 2793/5000  Traning Loss: 57.85252141952515  Train_Reconstruction: 54.92062473297119  Train_KL: 2.931897073984146  Validation Loss : 57.259010314941406 Val_Reconstruction : 54.373491287231445 Val_KL : 2.8855189085006714\n","Epoch: 2794/5000  Traning Loss: 57.65500259399414  Train_Reconstruction: 54.734214782714844  Train_KL: 2.920788049697876  Validation Loss : 57.1915283203125 Val_Reconstruction : 54.29421424865723 Val_KL : 2.8973132371902466\n","Epoch: 2795/5000  Traning Loss: 57.67162084579468  Train_Reconstruction: 54.737462520599365  Train_KL: 2.934158205986023  Validation Loss : 57.23557662963867 Val_Reconstruction : 54.32313537597656 Val_KL : 2.912441372871399\n","Epoch: 2796/5000  Traning Loss: 57.70866537094116  Train_Reconstruction: 54.78072929382324  Train_KL: 2.9279358983039856  Validation Loss : 57.142900466918945 Val_Reconstruction : 54.250722885131836 Val_KL : 2.8921775817871094\n","Epoch: 2797/5000  Traning Loss: 57.70714855194092  Train_Reconstruction: 54.77828073501587  Train_KL: 2.9288680255413055  Validation Loss : 57.26662826538086 Val_Reconstruction : 54.35345268249512 Val_KL : 2.9131767749786377\n","Epoch: 2798/5000  Traning Loss: 57.80469369888306  Train_Reconstruction: 54.87099313735962  Train_KL: 2.93370059132576  Validation Loss : 57.25699234008789 Val_Reconstruction : 54.36065483093262 Val_KL : 2.8963359594345093\n","Epoch: 2799/5000  Traning Loss: 58.24446201324463  Train_Reconstruction: 55.31849384307861  Train_KL: 2.9259685277938843  Validation Loss : 58.336191177368164 Val_Reconstruction : 55.432960510253906 Val_KL : 2.903231143951416\n","Epoch: 2800/5000  Traning Loss: 58.63143730163574  Train_Reconstruction: 55.700172424316406  Train_KL: 2.9312644600868225  Validation Loss : 58.437246322631836 Val_Reconstruction : 55.52422332763672 Val_KL : 2.913022518157959\n","Epoch: 2801/5000  Traning Loss: 58.215410232543945  Train_Reconstruction: 55.27584743499756  Train_KL: 2.9395631551742554  Validation Loss : 57.579912185668945 Val_Reconstruction : 54.67244911193848 Val_KL : 2.9074625968933105\n","Epoch: 2802/5000  Traning Loss: 58.06039905548096  Train_Reconstruction: 55.13351058959961  Train_KL: 2.926888257265091  Validation Loss : 57.25619316101074 Val_Reconstruction : 54.37051963806152 Val_KL : 2.8856745958328247\n","Epoch: 2803/5000  Traning Loss: 57.859408378601074  Train_Reconstruction: 54.92993783950806  Train_KL: 2.929470181465149  Validation Loss : 57.155216217041016 Val_Reconstruction : 54.24439239501953 Val_KL : 2.910824418067932\n","Epoch: 2804/5000  Traning Loss: 57.7522087097168  Train_Reconstruction: 54.810067653656006  Train_KL: 2.9421405494213104  Validation Loss : 57.20509910583496 Val_Reconstruction : 54.29434585571289 Val_KL : 2.910753607749939\n","Epoch: 2805/5000  Traning Loss: 57.70316123962402  Train_Reconstruction: 54.77384662628174  Train_KL: 2.9293149411678314  Validation Loss : 57.030616760253906 Val_Reconstruction : 54.14070129394531 Val_KL : 2.889914631843567\n","Epoch: 2806/5000  Traning Loss: 57.77573537826538  Train_Reconstruction: 54.848870277404785  Train_KL: 2.926865130662918  Validation Loss : 56.85581016540527 Val_Reconstruction : 53.94670104980469 Val_KL : 2.9091079235076904\n","Epoch: 2807/5000  Traning Loss: 57.89058494567871  Train_Reconstruction: 54.94787311553955  Train_KL: 2.9427120685577393  Validation Loss : 57.7319450378418 Val_Reconstruction : 54.817434310913086 Val_KL : 2.9145106077194214\n","Epoch: 2808/5000  Traning Loss: 58.15630865097046  Train_Reconstruction: 55.227192401885986  Train_KL: 2.929115116596222  Validation Loss : 57.59504508972168 Val_Reconstruction : 54.7081298828125 Val_KL : 2.886914610862732\n","Epoch: 2809/5000  Traning Loss: 58.341612339019775  Train_Reconstruction: 55.41094732284546  Train_KL: 2.9306640923023224  Validation Loss : 57.77258110046387 Val_Reconstruction : 54.85951614379883 Val_KL : 2.913066267967224\n","Epoch: 2810/5000  Traning Loss: 58.27584171295166  Train_Reconstruction: 55.324466705322266  Train_KL: 2.951375126838684  Validation Loss : 58.42688179016113 Val_Reconstruction : 55.50304985046387 Val_KL : 2.9238325357437134\n","Epoch: 2811/5000  Traning Loss: 58.67369031906128  Train_Reconstruction: 55.73291778564453  Train_KL: 2.940772920846939  Validation Loss : 58.118099212646484 Val_Reconstruction : 55.20896339416504 Val_KL : 2.9091347455978394\n","Epoch: 2812/5000  Traning Loss: 58.04414939880371  Train_Reconstruction: 55.09827136993408  Train_KL: 2.945877820253372  Validation Loss : 57.464468002319336 Val_Reconstruction : 54.55032539367676 Val_KL : 2.914143443107605\n","Epoch: 2813/5000  Traning Loss: 57.908623695373535  Train_Reconstruction: 54.966956615448  Train_KL: 2.941667318344116  Validation Loss : 57.68610382080078 Val_Reconstruction : 54.76846694946289 Val_KL : 2.917637586593628\n","Epoch: 2814/5000  Traning Loss: 58.16837739944458  Train_Reconstruction: 55.2357292175293  Train_KL: 2.9326487481594086  Validation Loss : 57.41562271118164 Val_Reconstruction : 54.501895904541016 Val_KL : 2.9137264490127563\n","Epoch: 2815/5000  Traning Loss: 57.85269784927368  Train_Reconstruction: 54.928184509277344  Train_KL: 2.9245137870311737  Validation Loss : 56.94443321228027 Val_Reconstruction : 54.047372817993164 Val_KL : 2.897060751914978\n","Epoch: 2816/5000  Traning Loss: 57.69216871261597  Train_Reconstruction: 54.76134443283081  Train_KL: 2.9308238327503204  Validation Loss : 57.03134727478027 Val_Reconstruction : 54.13150596618652 Val_KL : 2.899841547012329\n","Epoch: 2817/5000  Traning Loss: 57.528122901916504  Train_Reconstruction: 54.59605360031128  Train_KL: 2.9320691525936127  Validation Loss : 56.89750671386719 Val_Reconstruction : 54.01007652282715 Val_KL : 2.8874303102493286\n","Epoch: 2818/5000  Traning Loss: 57.693552017211914  Train_Reconstruction: 54.76561498641968  Train_KL: 2.9279362559318542  Validation Loss : 57.140037536621094 Val_Reconstruction : 54.23904991149902 Val_KL : 2.9009872674942017\n","Epoch: 2819/5000  Traning Loss: 58.139519691467285  Train_Reconstruction: 55.20940971374512  Train_KL: 2.93010938167572  Validation Loss : 57.40138244628906 Val_Reconstruction : 54.498863220214844 Val_KL : 2.9025193452835083\n","Epoch: 2820/5000  Traning Loss: 58.08087968826294  Train_Reconstruction: 55.14591979980469  Train_KL: 2.934959888458252  Validation Loss : 57.59687423706055 Val_Reconstruction : 54.68405723571777 Val_KL : 2.9128174781799316\n","Epoch: 2821/5000  Traning Loss: 57.841736793518066  Train_Reconstruction: 54.90618562698364  Train_KL: 2.935551106929779  Validation Loss : 57.13912010192871 Val_Reconstruction : 54.23128128051758 Val_KL : 2.9078396558761597\n","Epoch: 2822/5000  Traning Loss: 58.164371490478516  Train_Reconstruction: 55.235358238220215  Train_KL: 2.929013043642044  Validation Loss : 57.79959297180176 Val_Reconstruction : 54.90168380737305 Val_KL : 2.897909164428711\n","Epoch: 2823/5000  Traning Loss: 57.94237947463989  Train_Reconstruction: 55.01552629470825  Train_KL: 2.92685329914093  Validation Loss : 57.24003219604492 Val_Reconstruction : 54.337270736694336 Val_KL : 2.9027615785598755\n","Epoch: 2824/5000  Traning Loss: 57.953131675720215  Train_Reconstruction: 55.01175308227539  Train_KL: 2.9413786232471466  Validation Loss : 57.73841857910156 Val_Reconstruction : 54.82192802429199 Val_KL : 2.916492223739624\n","Epoch: 2825/5000  Traning Loss: 58.13532781600952  Train_Reconstruction: 55.204960346221924  Train_KL: 2.9303674399852753  Validation Loss : 57.350900650024414 Val_Reconstruction : 54.45540237426758 Val_KL : 2.895498037338257\n","Epoch: 2826/5000  Traning Loss: 57.90215015411377  Train_Reconstruction: 54.974231243133545  Train_KL: 2.9279186129570007  Validation Loss : 57.35454559326172 Val_Reconstruction : 54.45452690124512 Val_KL : 2.900019407272339\n","Epoch: 2827/5000  Traning Loss: 57.912757396698  Train_Reconstruction: 54.97220849990845  Train_KL: 2.940548986196518  Validation Loss : 57.276832580566406 Val_Reconstruction : 54.36641311645508 Val_KL : 2.9104198217391968\n","Epoch: 2828/5000  Traning Loss: 57.72360038757324  Train_Reconstruction: 54.802388191223145  Train_KL: 2.92121160030365  Validation Loss : 56.95933532714844 Val_Reconstruction : 54.0797233581543 Val_KL : 2.879610061645508\n","Epoch: 2829/5000  Traning Loss: 57.68536043167114  Train_Reconstruction: 54.759881019592285  Train_KL: 2.9254796504974365  Validation Loss : 57.54522895812988 Val_Reconstruction : 54.63359069824219 Val_KL : 2.911637783050537\n","Epoch: 2830/5000  Traning Loss: 57.93266773223877  Train_Reconstruction: 54.997244358062744  Train_KL: 2.9354236125946045  Validation Loss : 57.27984809875488 Val_Reconstruction : 54.38690185546875 Val_KL : 2.8929474353790283\n","Epoch: 2831/5000  Traning Loss: 58.13639974594116  Train_Reconstruction: 55.212565898895264  Train_KL: 2.9238336980342865  Validation Loss : 57.825050354003906 Val_Reconstruction : 54.92722511291504 Val_KL : 2.8978265523910522\n","Epoch: 2832/5000  Traning Loss: 57.961570739746094  Train_Reconstruction: 55.03317451477051  Train_KL: 2.928396314382553  Validation Loss : 57.608070373535156 Val_Reconstruction : 54.70762252807617 Val_KL : 2.900447964668274\n","Epoch: 2833/5000  Traning Loss: 57.86959362030029  Train_Reconstruction: 54.932053089141846  Train_KL: 2.937540113925934  Validation Loss : 57.335859298706055 Val_Reconstruction : 54.42555809020996 Val_KL : 2.910302758216858\n","Epoch: 2834/5000  Traning Loss: 58.10614728927612  Train_Reconstruction: 55.166661739349365  Train_KL: 2.939485251903534  Validation Loss : 57.49058151245117 Val_Reconstruction : 54.58650207519531 Val_KL : 2.9040783643722534\n","Epoch: 2835/5000  Traning Loss: 57.71469497680664  Train_Reconstruction: 54.792011737823486  Train_KL: 2.9226834774017334  Validation Loss : 56.95408821105957 Val_Reconstruction : 54.06282997131348 Val_KL : 2.891258120536804\n","Epoch: 2836/5000  Traning Loss: 57.69592809677124  Train_Reconstruction: 54.767658710479736  Train_KL: 2.928269863128662  Validation Loss : 56.999528884887695 Val_Reconstruction : 54.10411071777344 Val_KL : 2.895419478416443\n","Epoch: 2837/5000  Traning Loss: 58.01268529891968  Train_Reconstruction: 55.08263158798218  Train_KL: 2.930053800344467  Validation Loss : 57.45599555969238 Val_Reconstruction : 54.55465316772461 Val_KL : 2.9013415575027466\n","Epoch: 2838/5000  Traning Loss: 57.705421924591064  Train_Reconstruction: 54.776113986968994  Train_KL: 2.9293084740638733  Validation Loss : 56.9295597076416 Val_Reconstruction : 54.036502838134766 Val_KL : 2.893056273460388\n","Epoch: 2839/5000  Traning Loss: 57.668760776519775  Train_Reconstruction: 54.73933029174805  Train_KL: 2.9294301569461823  Validation Loss : 56.99565124511719 Val_Reconstruction : 54.09232711791992 Val_KL : 2.9033238887786865\n","Epoch: 2840/5000  Traning Loss: 57.58163070678711  Train_Reconstruction: 54.63866996765137  Train_KL: 2.942960947751999  Validation Loss : 57.0806770324707 Val_Reconstruction : 54.16685485839844 Val_KL : 2.9138225317001343\n","Epoch: 2841/5000  Traning Loss: 57.66904878616333  Train_Reconstruction: 54.73957300186157  Train_KL: 2.9294762313365936  Validation Loss : 57.029052734375 Val_Reconstruction : 54.13456344604492 Val_KL : 2.8944884538650513\n","Epoch: 2842/5000  Traning Loss: 57.6061897277832  Train_Reconstruction: 54.67583131790161  Train_KL: 2.9303584694862366  Validation Loss : 56.86598014831543 Val_Reconstruction : 53.97445869445801 Val_KL : 2.891521453857422\n","Epoch: 2843/5000  Traning Loss: 57.53690433502197  Train_Reconstruction: 54.604267597198486  Train_KL: 2.9326372146606445  Validation Loss : 56.94399070739746 Val_Reconstruction : 54.040687561035156 Val_KL : 2.9033043384552\n","Epoch: 2844/5000  Traning Loss: 57.42256736755371  Train_Reconstruction: 54.48078775405884  Train_KL: 2.9417793452739716  Validation Loss : 57.00421333312988 Val_Reconstruction : 54.09619903564453 Val_KL : 2.9080134630203247\n","Epoch: 2845/5000  Traning Loss: 57.57618761062622  Train_Reconstruction: 54.654306411743164  Train_KL: 2.921880751848221  Validation Loss : 56.81240463256836 Val_Reconstruction : 53.92258262634277 Val_KL : 2.8898210525512695\n","Epoch: 2846/5000  Traning Loss: 57.53839635848999  Train_Reconstruction: 54.62578630447388  Train_KL: 2.9126102328300476  Validation Loss : 57.02517509460449 Val_Reconstruction : 54.13754844665527 Val_KL : 2.8876270055770874\n","Epoch: 2847/5000  Traning Loss: 57.735331535339355  Train_Reconstruction: 54.80570697784424  Train_KL: 2.9296248257160187  Validation Loss : 56.87580680847168 Val_Reconstruction : 53.97546577453613 Val_KL : 2.900341272354126\n","Epoch: 2848/5000  Traning Loss: 57.423386096954346  Train_Reconstruction: 54.49749565124512  Train_KL: 2.925890773534775  Validation Loss : 56.812021255493164 Val_Reconstruction : 53.92034912109375 Val_KL : 2.891671657562256\n","Epoch: 2849/5000  Traning Loss: 57.692734241485596  Train_Reconstruction: 54.77601385116577  Train_KL: 2.9167204797267914  Validation Loss : 57.13655662536621 Val_Reconstruction : 54.2503547668457 Val_KL : 2.886202573776245\n","Epoch: 2850/5000  Traning Loss: 57.945037841796875  Train_Reconstruction: 55.021960735321045  Train_KL: 2.9230771958827972  Validation Loss : 57.369802474975586 Val_Reconstruction : 54.47574234008789 Val_KL : 2.894060492515564\n","Epoch: 2851/5000  Traning Loss: 58.05852460861206  Train_Reconstruction: 55.135212421417236  Train_KL: 2.9233122766017914  Validation Loss : 57.75678634643555 Val_Reconstruction : 54.86065101623535 Val_KL : 2.8961344957351685\n","Epoch: 2852/5000  Traning Loss: 57.983917236328125  Train_Reconstruction: 55.056785106658936  Train_KL: 2.9271318912506104  Validation Loss : 57.45670700073242 Val_Reconstruction : 54.552955627441406 Val_KL : 2.90375018119812\n","Epoch: 2853/5000  Traning Loss: 57.886133670806885  Train_Reconstruction: 54.948349952697754  Train_KL: 2.9377842843532562  Validation Loss : 57.39712333679199 Val_Reconstruction : 54.49452018737793 Val_KL : 2.9026020765304565\n","Epoch: 2854/5000  Traning Loss: 57.660658836364746  Train_Reconstruction: 54.73675489425659  Train_KL: 2.9239040315151215  Validation Loss : 57.1342716217041 Val_Reconstruction : 54.24650573730469 Val_KL : 2.887764811515808\n","Epoch: 2855/5000  Traning Loss: 57.455201148986816  Train_Reconstruction: 54.521483421325684  Train_KL: 2.9337174892425537  Validation Loss : 56.75182342529297 Val_Reconstruction : 53.84829521179199 Val_KL : 2.90352725982666\n","Epoch: 2856/5000  Traning Loss: 57.67410135269165  Train_Reconstruction: 54.74030542373657  Train_KL: 2.933796316385269  Validation Loss : 57.109527587890625 Val_Reconstruction : 54.203521728515625 Val_KL : 2.9060051441192627\n","Epoch: 2857/5000  Traning Loss: 58.055936336517334  Train_Reconstruction: 55.12159967422485  Train_KL: 2.9343369007110596  Validation Loss : 57.6683406829834 Val_Reconstruction : 54.75982093811035 Val_KL : 2.908520460128784\n","Epoch: 2858/5000  Traning Loss: 58.00649404525757  Train_Reconstruction: 55.075496673583984  Train_KL: 2.9309973418712616  Validation Loss : 57.347251892089844 Val_Reconstruction : 54.43659782409668 Val_KL : 2.9106552600860596\n","Epoch: 2859/5000  Traning Loss: 57.83120250701904  Train_Reconstruction: 54.8915114402771  Train_KL: 2.9396910071372986  Validation Loss : 57.332271575927734 Val_Reconstruction : 54.424203872680664 Val_KL : 2.9080684185028076\n","Epoch: 2860/5000  Traning Loss: 57.7217812538147  Train_Reconstruction: 54.792800426483154  Train_KL: 2.9289804995059967  Validation Loss : 57.01226234436035 Val_Reconstruction : 54.125680923461914 Val_KL : 2.886581063270569\n","Epoch: 2861/5000  Traning Loss: 57.52710247039795  Train_Reconstruction: 54.598477363586426  Train_KL: 2.928625166416168  Validation Loss : 56.9451904296875 Val_Reconstruction : 54.04695129394531 Val_KL : 2.8982397317886353\n","Epoch: 2862/5000  Traning Loss: 57.42999219894409  Train_Reconstruction: 54.486055850982666  Train_KL: 2.9439362287521362  Validation Loss : 56.93052673339844 Val_Reconstruction : 54.012250900268555 Val_KL : 2.918276309967041\n","Epoch: 2863/5000  Traning Loss: 57.684608936309814  Train_Reconstruction: 54.756887435913086  Train_KL: 2.927721679210663  Validation Loss : 57.371524810791016 Val_Reconstruction : 54.48687934875488 Val_KL : 2.884644627571106\n","Epoch: 2864/5000  Traning Loss: 58.01957130432129  Train_Reconstruction: 55.10036849975586  Train_KL: 2.919202893972397  Validation Loss : 57.230224609375 Val_Reconstruction : 54.342084884643555 Val_KL : 2.888139486312866\n","Epoch: 2865/5000  Traning Loss: 57.74070072174072  Train_Reconstruction: 54.80572462081909  Train_KL: 2.9349760115146637  Validation Loss : 56.85600280761719 Val_Reconstruction : 53.94608116149902 Val_KL : 2.9099223613739014\n","Epoch: 2866/5000  Traning Loss: 57.6395149230957  Train_Reconstruction: 54.6991081237793  Train_KL: 2.9404067397117615  Validation Loss : 57.08183670043945 Val_Reconstruction : 54.181230545043945 Val_KL : 2.9006069898605347\n","Epoch: 2867/5000  Traning Loss: 57.41345834732056  Train_Reconstruction: 54.490172386169434  Train_KL: 2.9232854545116425  Validation Loss : 56.805240631103516 Val_Reconstruction : 53.91900825500488 Val_KL : 2.886231303215027\n","Epoch: 2868/5000  Traning Loss: 57.561081409454346  Train_Reconstruction: 54.62784957885742  Train_KL: 2.933231383562088  Validation Loss : 57.11251449584961 Val_Reconstruction : 54.19780158996582 Val_KL : 2.9147132635116577\n","Epoch: 2869/5000  Traning Loss: 57.705697536468506  Train_Reconstruction: 54.76317644119263  Train_KL: 2.9425208270549774  Validation Loss : 57.07059860229492 Val_Reconstruction : 54.16827201843262 Val_KL : 2.9023267030715942\n","Epoch: 2870/5000  Traning Loss: 57.805739402770996  Train_Reconstruction: 54.869669914245605  Train_KL: 2.9360697269439697  Validation Loss : 57.230133056640625 Val_Reconstruction : 54.32608222961426 Val_KL : 2.9040515422821045\n","Epoch: 2871/5000  Traning Loss: 57.74594688415527  Train_Reconstruction: 54.80780649185181  Train_KL: 2.938140779733658  Validation Loss : 57.274871826171875 Val_Reconstruction : 54.37486457824707 Val_KL : 2.9000078439712524\n","Epoch: 2872/5000  Traning Loss: 57.54648780822754  Train_Reconstruction: 54.61936283111572  Train_KL: 2.927125573158264  Validation Loss : 56.807159423828125 Val_Reconstruction : 53.91088104248047 Val_KL : 2.896277904510498\n","Epoch: 2873/5000  Traning Loss: 57.64338541030884  Train_Reconstruction: 54.71978569030762  Train_KL: 2.9235995411872864  Validation Loss : 56.932430267333984 Val_Reconstruction : 54.04361343383789 Val_KL : 2.888817071914673\n","Epoch: 2874/5000  Traning Loss: 57.503928661346436  Train_Reconstruction: 54.58747434616089  Train_KL: 2.9164546132087708  Validation Loss : 56.94236373901367 Val_Reconstruction : 54.059593200683594 Val_KL : 2.8827710151672363\n","Epoch: 2875/5000  Traning Loss: 57.50811576843262  Train_Reconstruction: 54.57409334182739  Train_KL: 2.934021532535553  Validation Loss : 57.0460319519043 Val_Reconstruction : 54.12822723388672 Val_KL : 2.91780424118042\n","Epoch: 2876/5000  Traning Loss: 57.533578395843506  Train_Reconstruction: 54.59642314910889  Train_KL: 2.9371545910835266  Validation Loss : 56.9301643371582 Val_Reconstruction : 54.03170204162598 Val_KL : 2.898462176322937\n","Epoch: 2877/5000  Traning Loss: 57.68952655792236  Train_Reconstruction: 54.770822525024414  Train_KL: 2.9187046885490417  Validation Loss : 57.062469482421875 Val_Reconstruction : 54.175472259521484 Val_KL : 2.8869963884353638\n","Epoch: 2878/5000  Traning Loss: 57.74625062942505  Train_Reconstruction: 54.818840980529785  Train_KL: 2.9274091720581055  Validation Loss : 56.98242950439453 Val_Reconstruction : 54.077138900756836 Val_KL : 2.905291199684143\n","Epoch: 2879/5000  Traning Loss: 57.73860168457031  Train_Reconstruction: 54.79999017715454  Train_KL: 2.938611537218094  Validation Loss : 57.02294921875 Val_Reconstruction : 54.123979568481445 Val_KL : 2.898969054222107\n","Epoch: 2880/5000  Traning Loss: 57.63095712661743  Train_Reconstruction: 54.70003271102905  Train_KL: 2.93092343211174  Validation Loss : 56.96568298339844 Val_Reconstruction : 54.06517791748047 Val_KL : 2.900504231452942\n","Epoch: 2881/5000  Traning Loss: 57.474772453308105  Train_Reconstruction: 54.543214321136475  Train_KL: 2.931558519601822  Validation Loss : 57.22443199157715 Val_Reconstruction : 54.32318305969238 Val_KL : 2.9012495279312134\n","Epoch: 2882/5000  Traning Loss: 57.98534870147705  Train_Reconstruction: 55.06090497970581  Train_KL: 2.9244433641433716  Validation Loss : 57.479108810424805 Val_Reconstruction : 54.586503982543945 Val_KL : 2.8926035165786743\n","Epoch: 2883/5000  Traning Loss: 57.9437313079834  Train_Reconstruction: 55.01881551742554  Train_KL: 2.9249156415462494  Validation Loss : 57.03379821777344 Val_Reconstruction : 54.12956428527832 Val_KL : 2.9042351245880127\n","Epoch: 2884/5000  Traning Loss: 57.66366195678711  Train_Reconstruction: 54.72358560562134  Train_KL: 2.9400765001773834  Validation Loss : 57.44547080993652 Val_Reconstruction : 54.5315055847168 Val_KL : 2.9139647483825684\n","Epoch: 2885/5000  Traning Loss: 57.83544731140137  Train_Reconstruction: 54.898486614227295  Train_KL: 2.9369605779647827  Validation Loss : 57.74422073364258 Val_Reconstruction : 54.840755462646484 Val_KL : 2.903465986251831\n","Epoch: 2886/5000  Traning Loss: 57.73602485656738  Train_Reconstruction: 54.80838203430176  Train_KL: 2.9276425540447235  Validation Loss : 56.88490867614746 Val_Reconstruction : 53.98921012878418 Val_KL : 2.895698070526123\n","Epoch: 2887/5000  Traning Loss: 57.6423602104187  Train_Reconstruction: 54.71465015411377  Train_KL: 2.9277105629444122  Validation Loss : 57.44254684448242 Val_Reconstruction : 54.54327583312988 Val_KL : 2.899271607398987\n","Epoch: 2888/5000  Traning Loss: 57.82932376861572  Train_Reconstruction: 54.893041133880615  Train_KL: 2.936282992362976  Validation Loss : 57.40146255493164 Val_Reconstruction : 54.5009708404541 Val_KL : 2.900491714477539\n","Epoch: 2889/5000  Traning Loss: 57.78963279724121  Train_Reconstruction: 54.86078453063965  Train_KL: 2.9288477301597595  Validation Loss : 57.11366081237793 Val_Reconstruction : 54.22091484069824 Val_KL : 2.8927453756332397\n","Epoch: 2890/5000  Traning Loss: 57.69297790527344  Train_Reconstruction: 54.76403474807739  Train_KL: 2.9289430379867554  Validation Loss : 57.201730728149414 Val_Reconstruction : 54.29951286315918 Val_KL : 2.902219533920288\n","Epoch: 2891/5000  Traning Loss: 57.41840696334839  Train_Reconstruction: 54.49003887176514  Train_KL: 2.928368419408798  Validation Loss : 56.8368034362793 Val_Reconstruction : 53.9405460357666 Val_KL : 2.896257519721985\n","Epoch: 2892/5000  Traning Loss: 57.317872047424316  Train_Reconstruction: 54.39181423187256  Train_KL: 2.9260578751564026  Validation Loss : 56.73573684692383 Val_Reconstruction : 53.83016395568848 Val_KL : 2.9055733680725098\n","Epoch: 2893/5000  Traning Loss: 57.511013984680176  Train_Reconstruction: 54.57028102874756  Train_KL: 2.9407332837581635  Validation Loss : 57.02981758117676 Val_Reconstruction : 54.113943099975586 Val_KL : 2.9158741235733032\n","Epoch: 2894/5000  Traning Loss: 57.735108852386475  Train_Reconstruction: 54.79306697845459  Train_KL: 2.9420410096645355  Validation Loss : 57.21755790710449 Val_Reconstruction : 54.31234931945801 Val_KL : 2.905208110809326\n","Epoch: 2895/5000  Traning Loss: 57.53012943267822  Train_Reconstruction: 54.605310916900635  Train_KL: 2.924818992614746  Validation Loss : 57.03164482116699 Val_Reconstruction : 54.13282775878906 Val_KL : 2.8988181352615356\n","Epoch: 2896/5000  Traning Loss: 57.695744037628174  Train_Reconstruction: 54.76128625869751  Train_KL: 2.9344578087329865  Validation Loss : 57.08489418029785 Val_Reconstruction : 54.174617767333984 Val_KL : 2.9102758169174194\n","Epoch: 2897/5000  Traning Loss: 57.74902629852295  Train_Reconstruction: 54.809558391571045  Train_KL: 2.93946835398674  Validation Loss : 57.092227935791016 Val_Reconstruction : 54.193246841430664 Val_KL : 2.898980498313904\n","Epoch: 2898/5000  Traning Loss: 57.73356342315674  Train_Reconstruction: 54.803617000579834  Train_KL: 2.929946482181549  Validation Loss : 57.45781898498535 Val_Reconstruction : 54.57309150695801 Val_KL : 2.884727358818054\n","Epoch: 2899/5000  Traning Loss: 57.6273078918457  Train_Reconstruction: 54.70075035095215  Train_KL: 2.92655810713768  Validation Loss : 56.98213577270508 Val_Reconstruction : 54.084110260009766 Val_KL : 2.8980259895324707\n","Epoch: 2900/5000  Traning Loss: 57.76476240158081  Train_Reconstruction: 54.83066415786743  Train_KL: 2.9340983629226685  Validation Loss : 57.48688507080078 Val_Reconstruction : 54.589515686035156 Val_KL : 2.8973686695098877\n","Epoch: 2901/5000  Traning Loss: 57.80416774749756  Train_Reconstruction: 54.872933864593506  Train_KL: 2.931233763694763  Validation Loss : 57.13018608093262 Val_Reconstruction : 54.2237663269043 Val_KL : 2.9064196348190308\n","Epoch: 2902/5000  Traning Loss: 57.742995262145996  Train_Reconstruction: 54.80514430999756  Train_KL: 2.9378510415554047  Validation Loss : 57.239227294921875 Val_Reconstruction : 54.3342170715332 Val_KL : 2.905011534690857\n","Epoch: 2903/5000  Traning Loss: 57.56261205673218  Train_Reconstruction: 54.62351131439209  Train_KL: 2.939100444316864  Validation Loss : 56.77033042907715 Val_Reconstruction : 53.86868095397949 Val_KL : 2.901650071144104\n","Epoch: 2904/5000  Traning Loss: 57.46189546585083  Train_Reconstruction: 54.53096055984497  Train_KL: 2.9309350848197937  Validation Loss : 57.17321586608887 Val_Reconstruction : 54.27646064758301 Val_KL : 2.896753668785095\n","Epoch: 2905/5000  Traning Loss: 57.80999565124512  Train_Reconstruction: 54.88772249221802  Train_KL: 2.922273337841034  Validation Loss : 57.63283348083496 Val_Reconstruction : 54.731306076049805 Val_KL : 2.901528239250183\n","Epoch: 2906/5000  Traning Loss: 57.862181663513184  Train_Reconstruction: 54.93052864074707  Train_KL: 2.93165323138237  Validation Loss : 57.15606498718262 Val_Reconstruction : 54.24186897277832 Val_KL : 2.9141945838928223\n","Epoch: 2907/5000  Traning Loss: 57.79389238357544  Train_Reconstruction: 54.85567092895508  Train_KL: 2.938221573829651  Validation Loss : 57.235477447509766 Val_Reconstruction : 54.32135200500488 Val_KL : 2.914124369621277\n","Epoch: 2908/5000  Traning Loss: 57.82234859466553  Train_Reconstruction: 54.89423227310181  Train_KL: 2.928115636110306  Validation Loss : 57.37190628051758 Val_Reconstruction : 54.48039627075195 Val_KL : 2.8915103673934937\n","Epoch: 2909/5000  Traning Loss: 57.63206624984741  Train_Reconstruction: 54.706027030944824  Train_KL: 2.926038980484009  Validation Loss : 56.99261474609375 Val_Reconstruction : 54.09121322631836 Val_KL : 2.9014018774032593\n","Epoch: 2910/5000  Traning Loss: 57.468069553375244  Train_Reconstruction: 54.54263257980347  Train_KL: 2.925436705350876  Validation Loss : 56.73606491088867 Val_Reconstruction : 53.849931716918945 Val_KL : 2.8861321210861206\n","Epoch: 2911/5000  Traning Loss: 57.53115463256836  Train_Reconstruction: 54.60598707199097  Train_KL: 2.9251676201820374  Validation Loss : 56.96982955932617 Val_Reconstruction : 54.075252532958984 Val_KL : 2.894576668739319\n","Epoch: 2912/5000  Traning Loss: 58.13593339920044  Train_Reconstruction: 55.20645713806152  Train_KL: 2.929476171731949  Validation Loss : 57.36132621765137 Val_Reconstruction : 54.46405029296875 Val_KL : 2.897275924682617\n","Epoch: 2913/5000  Traning Loss: 58.343119621276855  Train_Reconstruction: 55.39971923828125  Train_KL: 2.9434006810188293  Validation Loss : 57.30290985107422 Val_Reconstruction : 54.40048027038574 Val_KL : 2.902428388595581\n","Epoch: 2914/5000  Traning Loss: 57.99564981460571  Train_Reconstruction: 55.06200695037842  Train_KL: 2.933643162250519  Validation Loss : 57.36527442932129 Val_Reconstruction : 54.461727142333984 Val_KL : 2.903547525405884\n","Epoch: 2915/5000  Traning Loss: 57.9403657913208  Train_Reconstruction: 55.00692892074585  Train_KL: 2.9334369599819183  Validation Loss : 57.81441879272461 Val_Reconstruction : 54.906951904296875 Val_KL : 2.9074671268463135\n","Epoch: 2916/5000  Traning Loss: 57.670021533966064  Train_Reconstruction: 54.74125146865845  Train_KL: 2.9287691712379456  Validation Loss : 57.21675491333008 Val_Reconstruction : 54.32186508178711 Val_KL : 2.8948894739151\n","Epoch: 2917/5000  Traning Loss: 57.60380697250366  Train_Reconstruction: 54.670528411865234  Train_KL: 2.933278113603592  Validation Loss : 57.395498275756836 Val_Reconstruction : 54.493093490600586 Val_KL : 2.902405023574829\n","Epoch: 2918/5000  Traning Loss: 57.66477632522583  Train_Reconstruction: 54.73595952987671  Train_KL: 2.9288167655467987  Validation Loss : 56.90836524963379 Val_Reconstruction : 54.01181983947754 Val_KL : 2.896544098854065\n","Epoch: 2919/5000  Traning Loss: 57.57259654998779  Train_Reconstruction: 54.633065700531006  Train_KL: 2.9395302832126617  Validation Loss : 56.91648483276367 Val_Reconstruction : 54.00493240356445 Val_KL : 2.911552309989929\n","Epoch: 2920/5000  Traning Loss: 57.50671863555908  Train_Reconstruction: 54.56602239608765  Train_KL: 2.940695285797119  Validation Loss : 56.79853630065918 Val_Reconstruction : 53.89304161071777 Val_KL : 2.905495285987854\n","Epoch: 2921/5000  Traning Loss: 57.35956287384033  Train_Reconstruction: 54.435752868652344  Train_KL: 2.923810303211212  Validation Loss : 56.71351432800293 Val_Reconstruction : 53.82309341430664 Val_KL : 2.890420436859131\n","Epoch: 2922/5000  Traning Loss: 57.47104549407959  Train_Reconstruction: 54.54109811782837  Train_KL: 2.9299470484256744  Validation Loss : 56.980743408203125 Val_Reconstruction : 54.081207275390625 Val_KL : 2.8995355367660522\n","Epoch: 2923/5000  Traning Loss: 57.88436841964722  Train_Reconstruction: 54.94596529006958  Train_KL: 2.938403606414795  Validation Loss : 57.17303657531738 Val_Reconstruction : 54.26807403564453 Val_KL : 2.904961347579956\n","Epoch: 2924/5000  Traning Loss: 57.573633670806885  Train_Reconstruction: 54.63404321670532  Train_KL: 2.9395910501480103  Validation Loss : 57.132049560546875 Val_Reconstruction : 54.2204647064209 Val_KL : 2.911583662033081\n","Epoch: 2925/5000  Traning Loss: 57.644336223602295  Train_Reconstruction: 54.71031188964844  Train_KL: 2.9340245127677917  Validation Loss : 57.179399490356445 Val_Reconstruction : 54.28426551818848 Val_KL : 2.8951336145401\n","Epoch: 2926/5000  Traning Loss: 58.214755058288574  Train_Reconstruction: 55.27426624298096  Train_KL: 2.9404889941215515  Validation Loss : 57.5128288269043 Val_Reconstruction : 54.60128211975098 Val_KL : 2.911547064781189\n","Epoch: 2927/5000  Traning Loss: 58.183910846710205  Train_Reconstruction: 55.24369764328003  Train_KL: 2.9402128159999847  Validation Loss : 57.14604187011719 Val_Reconstruction : 54.245697021484375 Val_KL : 2.900344729423523\n","Epoch: 2928/5000  Traning Loss: 57.74449348449707  Train_Reconstruction: 54.81906270980835  Train_KL: 2.925430804491043  Validation Loss : 57.22851753234863 Val_Reconstruction : 54.3349723815918 Val_KL : 2.893545150756836\n","Epoch: 2929/5000  Traning Loss: 57.829174518585205  Train_Reconstruction: 54.89723348617554  Train_KL: 2.931940793991089  Validation Loss : 57.19750213623047 Val_Reconstruction : 54.289987564086914 Val_KL : 2.9075140953063965\n","Epoch: 2930/5000  Traning Loss: 57.82658338546753  Train_Reconstruction: 54.89635133743286  Train_KL: 2.930231988430023  Validation Loss : 56.98813438415527 Val_Reconstruction : 54.09248161315918 Val_KL : 2.8956528902053833\n","Epoch: 2931/5000  Traning Loss: 57.5223913192749  Train_Reconstruction: 54.60040473937988  Train_KL: 2.9219864308834076  Validation Loss : 56.821359634399414 Val_Reconstruction : 53.93153381347656 Val_KL : 2.889825940132141\n","Epoch: 2932/5000  Traning Loss: 57.31795692443848  Train_Reconstruction: 54.396084785461426  Train_KL: 2.92187237739563  Validation Loss : 56.781829833984375 Val_Reconstruction : 53.88815689086914 Val_KL : 2.893672466278076\n","Epoch: 2933/5000  Traning Loss: 57.200013637542725  Train_Reconstruction: 54.27199602127075  Train_KL: 2.9280175864696503  Validation Loss : 56.58048629760742 Val_Reconstruction : 53.67596435546875 Val_KL : 2.9045217037200928\n","Epoch: 2934/5000  Traning Loss: 57.31930589675903  Train_Reconstruction: 54.378371715545654  Train_KL: 2.9409345984458923  Validation Loss : 56.7759952545166 Val_Reconstruction : 53.873226165771484 Val_KL : 2.9027695655822754\n","Epoch: 2935/5000  Traning Loss: 57.38239765167236  Train_Reconstruction: 54.44932985305786  Train_KL: 2.9330676198005676  Validation Loss : 56.692644119262695 Val_Reconstruction : 53.78727912902832 Val_KL : 2.9053646326065063\n","Epoch: 2936/5000  Traning Loss: 57.33323287963867  Train_Reconstruction: 54.39732265472412  Train_KL: 2.935910552740097  Validation Loss : 56.69250679016113 Val_Reconstruction : 53.780935287475586 Val_KL : 2.9115729331970215\n","Epoch: 2937/5000  Traning Loss: 57.38743448257446  Train_Reconstruction: 54.452965259552  Train_KL: 2.9344694316387177  Validation Loss : 56.83822059631348 Val_Reconstruction : 53.93855857849121 Val_KL : 2.8996609449386597\n","Epoch: 2938/5000  Traning Loss: 57.49920988082886  Train_Reconstruction: 54.56539535522461  Train_KL: 2.9338139295578003  Validation Loss : 57.21081733703613 Val_Reconstruction : 54.3056583404541 Val_KL : 2.9051589965820312\n","Epoch: 2939/5000  Traning Loss: 57.616586685180664  Train_Reconstruction: 54.68757390975952  Train_KL: 2.9290135204792023  Validation Loss : 56.771230697631836 Val_Reconstruction : 53.88157081604004 Val_KL : 2.8896591663360596\n","Epoch: 2940/5000  Traning Loss: 57.314764976501465  Train_Reconstruction: 54.39039850234985  Train_KL: 2.9243663251399994  Validation Loss : 56.73648262023926 Val_Reconstruction : 53.833669662475586 Val_KL : 2.9028128385543823\n","Epoch: 2941/5000  Traning Loss: 57.3400559425354  Train_Reconstruction: 54.41251468658447  Train_KL: 2.9275414645671844  Validation Loss : 56.74818420410156 Val_Reconstruction : 53.84858512878418 Val_KL : 2.899597406387329\n","Epoch: 2942/5000  Traning Loss: 57.7802357673645  Train_Reconstruction: 54.84969139099121  Train_KL: 2.930544435977936  Validation Loss : 57.56707954406738 Val_Reconstruction : 54.6702880859375 Val_KL : 2.8967922925949097\n","Epoch: 2943/5000  Traning Loss: 58.31018686294556  Train_Reconstruction: 55.379108905792236  Train_KL: 2.931077480316162  Validation Loss : 58.12982749938965 Val_Reconstruction : 55.22997856140137 Val_KL : 2.8998488187789917\n","Epoch: 2944/5000  Traning Loss: 58.20225715637207  Train_Reconstruction: 55.26801538467407  Train_KL: 2.934242069721222  Validation Loss : 58.300453186035156 Val_Reconstruction : 55.40434646606445 Val_KL : 2.8961069583892822\n","Epoch: 2945/5000  Traning Loss: 57.911348819732666  Train_Reconstruction: 54.97443723678589  Train_KL: 2.936911553144455  Validation Loss : 57.35492134094238 Val_Reconstruction : 54.44881248474121 Val_KL : 2.906108021736145\n","Epoch: 2946/5000  Traning Loss: 57.89830541610718  Train_Reconstruction: 54.96710205078125  Train_KL: 2.9312031865119934  Validation Loss : 57.69875144958496 Val_Reconstruction : 54.79572868347168 Val_KL : 2.903022050857544\n","Epoch: 2947/5000  Traning Loss: 57.953707695007324  Train_Reconstruction: 55.02155351638794  Train_KL: 2.932154268026352  Validation Loss : 57.27689170837402 Val_Reconstruction : 54.369876861572266 Val_KL : 2.9070154428482056\n","Epoch: 2948/5000  Traning Loss: 57.7448410987854  Train_Reconstruction: 54.81541728973389  Train_KL: 2.9294234216213226  Validation Loss : 57.058197021484375 Val_Reconstruction : 54.160221099853516 Val_KL : 2.897976040840149\n","Epoch: 2949/5000  Traning Loss: 57.470693588256836  Train_Reconstruction: 54.544774532318115  Train_KL: 2.9259192943573  Validation Loss : 56.90012741088867 Val_Reconstruction : 53.99984359741211 Val_KL : 2.900283932685852\n","Epoch: 2950/5000  Traning Loss: 57.42103290557861  Train_Reconstruction: 54.485190868377686  Train_KL: 2.9358421862125397  Validation Loss : 56.82937240600586 Val_Reconstruction : 53.93054008483887 Val_KL : 2.8988314867019653\n","Epoch: 2951/5000  Traning Loss: 57.616445541381836  Train_Reconstruction: 54.67154264450073  Train_KL: 2.9449023008346558  Validation Loss : 57.02999687194824 Val_Reconstruction : 54.111040115356445 Val_KL : 2.9189563989639282\n","Epoch: 2952/5000  Traning Loss: 57.77180624008179  Train_Reconstruction: 54.83932399749756  Train_KL: 2.932481437921524  Validation Loss : 57.60938835144043 Val_Reconstruction : 54.7143611907959 Val_KL : 2.8950268030166626\n","Epoch: 2953/5000  Traning Loss: 57.69225454330444  Train_Reconstruction: 54.778879165649414  Train_KL: 2.9133757948875427  Validation Loss : 57.214216232299805 Val_Reconstruction : 54.3350772857666 Val_KL : 2.8791385889053345\n","Epoch: 2954/5000  Traning Loss: 57.61964225769043  Train_Reconstruction: 54.691946029663086  Train_KL: 2.927695482969284  Validation Loss : 56.873273849487305 Val_Reconstruction : 53.966047286987305 Val_KL : 2.9072272777557373\n","Epoch: 2955/5000  Traning Loss: 57.49889898300171  Train_Reconstruction: 54.55951690673828  Train_KL: 2.9393820762634277  Validation Loss : 56.80318069458008 Val_Reconstruction : 53.89988136291504 Val_KL : 2.9032986164093018\n","Epoch: 2956/5000  Traning Loss: 57.656453132629395  Train_Reconstruction: 54.7269082069397  Train_KL: 2.929545372724533  Validation Loss : 56.92520332336426 Val_Reconstruction : 54.03237533569336 Val_KL : 2.8928277492523193\n","Epoch: 2957/5000  Traning Loss: 57.64946699142456  Train_Reconstruction: 54.725425243377686  Train_KL: 2.924041360616684  Validation Loss : 57.12748146057129 Val_Reconstruction : 54.23692512512207 Val_KL : 2.8905580043792725\n","Epoch: 2958/5000  Traning Loss: 57.464394092559814  Train_Reconstruction: 54.535032749176025  Train_KL: 2.929361492395401  Validation Loss : 56.81243133544922 Val_Reconstruction : 53.92263221740723 Val_KL : 2.889798879623413\n","Epoch: 2959/5000  Traning Loss: 57.300174713134766  Train_Reconstruction: 54.37670135498047  Train_KL: 2.923472911119461  Validation Loss : 56.80853843688965 Val_Reconstruction : 53.9072151184082 Val_KL : 2.901322603225708\n","Epoch: 2960/5000  Traning Loss: 57.49659061431885  Train_Reconstruction: 54.55747127532959  Train_KL: 2.9391190111637115  Validation Loss : 56.98838424682617 Val_Reconstruction : 54.078779220581055 Val_KL : 2.909605622291565\n","Epoch: 2961/5000  Traning Loss: 57.77440547943115  Train_Reconstruction: 54.83590221405029  Train_KL: 2.938503473997116  Validation Loss : 57.22011184692383 Val_Reconstruction : 54.309078216552734 Val_KL : 2.911033034324646\n","Epoch: 2962/5000  Traning Loss: 57.84967041015625  Train_Reconstruction: 54.910892486572266  Train_KL: 2.938777595758438  Validation Loss : 57.340248107910156 Val_Reconstruction : 54.435773849487305 Val_KL : 2.9044740200042725\n","Epoch: 2963/5000  Traning Loss: 57.46344566345215  Train_Reconstruction: 54.52421474456787  Train_KL: 2.939230650663376  Validation Loss : 56.91105270385742 Val_Reconstruction : 54.01280212402344 Val_KL : 2.8982510566711426\n","Epoch: 2964/5000  Traning Loss: 57.35061454772949  Train_Reconstruction: 54.41729974746704  Train_KL: 2.9333148300647736  Validation Loss : 56.79200553894043 Val_Reconstruction : 53.88632392883301 Val_KL : 2.9056822061538696\n","Epoch: 2965/5000  Traning Loss: 57.44884395599365  Train_Reconstruction: 54.510746479034424  Train_KL: 2.9380976259708405  Validation Loss : 56.83231735229492 Val_Reconstruction : 53.93057823181152 Val_KL : 2.90173876285553\n","Epoch: 2966/5000  Traning Loss: 57.758840560913086  Train_Reconstruction: 54.82986307144165  Train_KL: 2.9289772510528564  Validation Loss : 57.28394317626953 Val_Reconstruction : 54.39279747009277 Val_KL : 2.8911463022232056\n","Epoch: 2967/5000  Traning Loss: 57.74312973022461  Train_Reconstruction: 54.81641626358032  Train_KL: 2.9267137944698334  Validation Loss : 57.51633834838867 Val_Reconstruction : 54.62541198730469 Val_KL : 2.8909263610839844\n","Epoch: 2968/5000  Traning Loss: 57.71141242980957  Train_Reconstruction: 54.78239917755127  Train_KL: 2.92901349067688  Validation Loss : 57.18426704406738 Val_Reconstruction : 54.28224182128906 Val_KL : 2.9020239114761353\n","Epoch: 2969/5000  Traning Loss: 57.84664821624756  Train_Reconstruction: 54.90815877914429  Train_KL: 2.9384898245334625  Validation Loss : 57.121328353881836 Val_Reconstruction : 54.215200424194336 Val_KL : 2.9061291217803955\n","Epoch: 2970/5000  Traning Loss: 57.57887077331543  Train_Reconstruction: 54.64808893203735  Train_KL: 2.930781900882721  Validation Loss : 56.87603759765625 Val_Reconstruction : 53.972116470336914 Val_KL : 2.903921365737915\n","Epoch: 2971/5000  Traning Loss: 57.624372482299805  Train_Reconstruction: 54.69271898269653  Train_KL: 2.93165385723114  Validation Loss : 57.20149803161621 Val_Reconstruction : 54.29524803161621 Val_KL : 2.906250834465027\n","Epoch: 2972/5000  Traning Loss: 57.677202224731445  Train_Reconstruction: 54.740915298461914  Train_KL: 2.9362871944904327  Validation Loss : 57.13052558898926 Val_Reconstruction : 54.23438835144043 Val_KL : 2.896135926246643\n","Epoch: 2973/5000  Traning Loss: 58.04575967788696  Train_Reconstruction: 55.111456871032715  Train_KL: 2.9343030154705048  Validation Loss : 57.49893379211426 Val_Reconstruction : 54.60776710510254 Val_KL : 2.8911666870117188\n","Epoch: 2974/5000  Traning Loss: 57.984130859375  Train_Reconstruction: 55.04929828643799  Train_KL: 2.9348317980766296  Validation Loss : 57.241092681884766 Val_Reconstruction : 54.336456298828125 Val_KL : 2.904637575149536\n","Epoch: 2975/5000  Traning Loss: 57.97985363006592  Train_Reconstruction: 55.03966665267944  Train_KL: 2.9401867389678955  Validation Loss : 57.30673027038574 Val_Reconstruction : 54.40007019042969 Val_KL : 2.9066601991653442\n","Epoch: 2976/5000  Traning Loss: 57.86672878265381  Train_Reconstruction: 54.934000968933105  Train_KL: 2.9327276945114136  Validation Loss : 57.238630294799805 Val_Reconstruction : 54.334163665771484 Val_KL : 2.9044668674468994\n","Epoch: 2977/5000  Traning Loss: 57.75187873840332  Train_Reconstruction: 54.81621599197388  Train_KL: 2.935662269592285  Validation Loss : 56.91093826293945 Val_Reconstruction : 54.00364685058594 Val_KL : 2.9072916507720947\n","Epoch: 2978/5000  Traning Loss: 57.474491119384766  Train_Reconstruction: 54.54217052459717  Train_KL: 2.9323200285434723  Validation Loss : 56.80773162841797 Val_Reconstruction : 53.911142349243164 Val_KL : 2.8965893983840942\n","Epoch: 2979/5000  Traning Loss: 57.59682559967041  Train_Reconstruction: 54.66617822647095  Train_KL: 2.9306474328041077  Validation Loss : 57.02154541015625 Val_Reconstruction : 54.11898612976074 Val_KL : 2.9025580883026123\n","Epoch: 2980/5000  Traning Loss: 57.85387897491455  Train_Reconstruction: 54.92164134979248  Train_KL: 2.932237982749939  Validation Loss : 57.34738540649414 Val_Reconstruction : 54.44083213806152 Val_KL : 2.906553864479065\n","Epoch: 2981/5000  Traning Loss: 57.79683017730713  Train_Reconstruction: 54.86733388900757  Train_KL: 2.9294967651367188  Validation Loss : 57.20407485961914 Val_Reconstruction : 54.317909240722656 Val_KL : 2.886166214942932\n","Epoch: 2982/5000  Traning Loss: 57.70565986633301  Train_Reconstruction: 54.780272006988525  Train_KL: 2.925387442111969  Validation Loss : 56.98560333251953 Val_Reconstruction : 54.08590126037598 Val_KL : 2.8997015953063965\n","Epoch: 2983/5000  Traning Loss: 57.575379371643066  Train_Reconstruction: 54.643027782440186  Train_KL: 2.932351917028427  Validation Loss : 56.89913558959961 Val_Reconstruction : 53.99812889099121 Val_KL : 2.9010061025619507\n","Epoch: 2984/5000  Traning Loss: 57.47483253479004  Train_Reconstruction: 54.54606342315674  Train_KL: 2.9287687838077545  Validation Loss : 56.84228324890137 Val_Reconstruction : 53.95184898376465 Val_KL : 2.8904356956481934\n","Epoch: 2985/5000  Traning Loss: 57.3346905708313  Train_Reconstruction: 54.413633823394775  Train_KL: 2.921057164669037  Validation Loss : 56.6919059753418 Val_Reconstruction : 53.80194854736328 Val_KL : 2.88995623588562\n","Epoch: 2986/5000  Traning Loss: 57.421573638916016  Train_Reconstruction: 54.49335718154907  Train_KL: 2.9282160103321075  Validation Loss : 56.99475288391113 Val_Reconstruction : 54.09264373779297 Val_KL : 2.902108669281006\n","Epoch: 2987/5000  Traning Loss: 57.87957048416138  Train_Reconstruction: 54.94786787033081  Train_KL: 2.9317023754119873  Validation Loss : 57.41729164123535 Val_Reconstruction : 54.5209903717041 Val_KL : 2.896300435066223\n","Epoch: 2988/5000  Traning Loss: 57.771865367889404  Train_Reconstruction: 54.851430892944336  Train_KL: 2.920434832572937  Validation Loss : 57.24769401550293 Val_Reconstruction : 54.358314514160156 Val_KL : 2.889379620552063\n","Epoch: 2989/5000  Traning Loss: 58.19439935684204  Train_Reconstruction: 55.26130771636963  Train_KL: 2.933091402053833  Validation Loss : 57.35477828979492 Val_Reconstruction : 54.45286750793457 Val_KL : 2.9019103050231934\n","Epoch: 2990/5000  Traning Loss: 57.638153076171875  Train_Reconstruction: 54.711440563201904  Train_KL: 2.9267121851444244  Validation Loss : 56.94936180114746 Val_Reconstruction : 54.053810119628906 Val_KL : 2.8955516815185547\n","Epoch: 2991/5000  Traning Loss: 57.59305810928345  Train_Reconstruction: 54.6654109954834  Train_KL: 2.927647143602371  Validation Loss : 56.94943428039551 Val_Reconstruction : 54.048866271972656 Val_KL : 2.9005684852600098\n","Epoch: 2992/5000  Traning Loss: 57.53380537033081  Train_Reconstruction: 54.60209608078003  Train_KL: 2.931709259748459  Validation Loss : 56.938602447509766 Val_Reconstruction : 54.04396629333496 Val_KL : 2.8946361541748047\n","Epoch: 2993/5000  Traning Loss: 57.476017475128174  Train_Reconstruction: 54.54291915893555  Train_KL: 2.9330986738204956  Validation Loss : 56.649173736572266 Val_Reconstruction : 53.74764060974121 Val_KL : 2.9015331268310547\n","Epoch: 2994/5000  Traning Loss: 57.27681636810303  Train_Reconstruction: 54.33542585372925  Train_KL: 2.9413905441761017  Validation Loss : 56.743818283081055 Val_Reconstruction : 53.830209732055664 Val_KL : 2.913609504699707\n","Epoch: 2995/5000  Traning Loss: 57.31137418746948  Train_Reconstruction: 54.37964916229248  Train_KL: 2.9317255914211273  Validation Loss : 56.75164031982422 Val_Reconstruction : 53.851057052612305 Val_KL : 2.90058171749115\n","Epoch: 2996/5000  Traning Loss: 57.41213417053223  Train_Reconstruction: 54.481478214263916  Train_KL: 2.930655390024185  Validation Loss : 56.667314529418945 Val_Reconstruction : 53.76664352416992 Val_KL : 2.9006710052490234\n","Epoch: 2997/5000  Traning Loss: 57.4493932723999  Train_Reconstruction: 54.5141167640686  Train_KL: 2.9352767169475555  Validation Loss : 57.1779842376709 Val_Reconstruction : 54.26503372192383 Val_KL : 2.9129514694213867\n","Epoch: 2998/5000  Traning Loss: 57.569846630096436  Train_Reconstruction: 54.63304805755615  Train_KL: 2.9367989897727966  Validation Loss : 56.91144371032715 Val_Reconstruction : 54.01083755493164 Val_KL : 2.9006069898605347\n","Epoch: 2999/5000  Traning Loss: 57.64523649215698  Train_Reconstruction: 54.71185350418091  Train_KL: 2.9333830773830414  Validation Loss : 57.04193687438965 Val_Reconstruction : 54.142276763916016 Val_KL : 2.899659514427185\n","Epoch: 3000/5000  Traning Loss: 57.68489122390747  Train_Reconstruction: 54.75342035293579  Train_KL: 2.931469738483429  Validation Loss : 57.642818450927734 Val_Reconstruction : 54.73757362365723 Val_KL : 2.9052459001541138\n","Epoch: 3001/5000  Traning Loss: 57.670859813690186  Train_Reconstruction: 54.75001049041748  Train_KL: 2.9208498001098633  Validation Loss : 57.172882080078125 Val_Reconstruction : 54.27826690673828 Val_KL : 2.894615411758423\n","Epoch: 3002/5000  Traning Loss: 57.87454128265381  Train_Reconstruction: 54.943963050842285  Train_KL: 2.9305778443813324  Validation Loss : 57.367719650268555 Val_Reconstruction : 54.4724063873291 Val_KL : 2.895312190055847\n","Epoch: 3003/5000  Traning Loss: 57.92273283004761  Train_Reconstruction: 54.99419164657593  Train_KL: 2.9285404086112976  Validation Loss : 57.09385681152344 Val_Reconstruction : 54.18919563293457 Val_KL : 2.9046614170074463\n","Epoch: 3004/5000  Traning Loss: 57.64291429519653  Train_Reconstruction: 54.71479272842407  Train_KL: 2.928121030330658  Validation Loss : 57.19565963745117 Val_Reconstruction : 54.29777145385742 Val_KL : 2.897889256477356\n","EarlyStopping counter: 1 out of 100\n","Epoch: 3005/5000  Traning Loss: 57.81827926635742  Train_Reconstruction: 54.89153432846069  Train_KL: 2.926745444536209  Validation Loss : 57.367624282836914 Val_Reconstruction : 54.47601127624512 Val_KL : 2.8916133642196655\n","EarlyStopping counter: 2 out of 100\n","Epoch: 3006/5000  Traning Loss: 57.67491912841797  Train_Reconstruction: 54.742177963256836  Train_KL: 2.932741194963455  Validation Loss : 57.10128211975098 Val_Reconstruction : 54.197181701660156 Val_KL : 2.9040998220443726\n","EarlyStopping counter: 3 out of 100\n","Epoch: 3007/5000  Traning Loss: 57.922876834869385  Train_Reconstruction: 54.98979473114014  Train_KL: 2.9330820441246033  Validation Loss : 56.893306732177734 Val_Reconstruction : 53.99422264099121 Val_KL : 2.899083971977234\n","Epoch: 3008/5000  Traning Loss: 57.64824867248535  Train_Reconstruction: 54.71142578125  Train_KL: 2.936823397874832  Validation Loss : 57.05616760253906 Val_Reconstruction : 54.149362564086914 Val_KL : 2.906805634498596\n","EarlyStopping counter: 1 out of 100\n","Epoch: 3009/5000  Traning Loss: 57.52522325515747  Train_Reconstruction: 54.583500385284424  Train_KL: 2.941723197698593  Validation Loss : 56.919132232666016 Val_Reconstruction : 54.00775909423828 Val_KL : 2.911372661590576\n","EarlyStopping counter: 2 out of 100\n","Epoch: 3010/5000  Traning Loss: 57.45565176010132  Train_Reconstruction: 54.52066421508789  Train_KL: 2.934987097978592  Validation Loss : 57.00576400756836 Val_Reconstruction : 54.1077995300293 Val_KL : 2.8979647159576416\n","EarlyStopping counter: 3 out of 100\n","Epoch: 3011/5000  Traning Loss: 57.5997200012207  Train_Reconstruction: 54.67806816101074  Train_KL: 2.9216519594192505  Validation Loss : 57.00394248962402 Val_Reconstruction : 54.11249542236328 Val_KL : 2.8914486169815063\n","EarlyStopping counter: 4 out of 100\n","Epoch: 3012/5000  Traning Loss: 57.3588547706604  Train_Reconstruction: 54.43815851211548  Train_KL: 2.920696198940277  Validation Loss : 56.578975677490234 Val_Reconstruction : 53.692182540893555 Val_KL : 2.8867928981781006\n","Epoch: 3013/5000  Traning Loss: 57.31024742126465  Train_Reconstruction: 54.374680519104004  Train_KL: 2.9355663657188416  Validation Loss : 56.81018257141113 Val_Reconstruction : 53.906843185424805 Val_KL : 2.9033381938934326\n","EarlyStopping counter: 1 out of 100\n","Epoch: 3014/5000  Traning Loss: 57.48312187194824  Train_Reconstruction: 54.54696273803711  Train_KL: 2.9361590445041656  Validation Loss : 56.93265151977539 Val_Reconstruction : 54.038124084472656 Val_KL : 2.894527792930603\n","EarlyStopping counter: 2 out of 100\n","Epoch: 3015/5000  Traning Loss: 57.55638027191162  Train_Reconstruction: 54.63383626937866  Train_KL: 2.922544091939926  Validation Loss : 56.77279853820801 Val_Reconstruction : 53.8842716217041 Val_KL : 2.8885276317596436\n","EarlyStopping counter: 3 out of 100\n","Epoch: 3016/5000  Traning Loss: 57.49216651916504  Train_Reconstruction: 54.55287265777588  Train_KL: 2.9392941892147064  Validation Loss : 57.099422454833984 Val_Reconstruction : 54.18076705932617 Val_KL : 2.9186551570892334\n","EarlyStopping counter: 4 out of 100\n","Epoch: 3017/5000  Traning Loss: 57.794395446777344  Train_Reconstruction: 54.85090732574463  Train_KL: 2.9434878826141357  Validation Loss : 57.40658760070801 Val_Reconstruction : 54.50546455383301 Val_KL : 2.9011226892471313\n","EarlyStopping counter: 5 out of 100\n","Epoch: 3018/5000  Traning Loss: 58.14443063735962  Train_Reconstruction: 55.21223735809326  Train_KL: 2.9321937263011932  Validation Loss : 57.68472671508789 Val_Reconstruction : 54.770164489746094 Val_KL : 2.9145628213882446\n","EarlyStopping counter: 6 out of 100\n","Epoch: 3019/5000  Traning Loss: 58.157169342041016  Train_Reconstruction: 55.217153549194336  Train_KL: 2.94001567363739  Validation Loss : 57.72340393066406 Val_Reconstruction : 54.82006072998047 Val_KL : 2.9033429622650146\n","EarlyStopping counter: 7 out of 100\n","Epoch: 3020/5000  Traning Loss: 57.94926357269287  Train_Reconstruction: 55.021785259246826  Train_KL: 2.927478104829788  Validation Loss : 57.12527275085449 Val_Reconstruction : 54.23369216918945 Val_KL : 2.8915817737579346\n","EarlyStopping counter: 8 out of 100\n","Epoch: 3021/5000  Traning Loss: 57.41484880447388  Train_Reconstruction: 54.47256326675415  Train_KL: 2.942285895347595  Validation Loss : 57.00492858886719 Val_Reconstruction : 54.083566665649414 Val_KL : 2.921362280845642\n","EarlyStopping counter: 9 out of 100\n","Epoch: 3022/5000  Traning Loss: 57.48747205734253  Train_Reconstruction: 54.542434215545654  Train_KL: 2.9450372755527496  Validation Loss : 56.729257583618164 Val_Reconstruction : 53.826904296875 Val_KL : 2.9023526906967163\n","EarlyStopping counter: 10 out of 100\n","Epoch: 3023/5000  Traning Loss: 57.55344295501709  Train_Reconstruction: 54.622591972351074  Train_KL: 2.9308506846427917  Validation Loss : 57.111440658569336 Val_Reconstruction : 54.216712951660156 Val_KL : 2.8947283029556274\n","EarlyStopping counter: 11 out of 100\n","Epoch: 3024/5000  Traning Loss: 57.86633491516113  Train_Reconstruction: 54.92838954925537  Train_KL: 2.9379447400569916  Validation Loss : 57.678287506103516 Val_Reconstruction : 54.77035713195801 Val_KL : 2.907931327819824\n","EarlyStopping counter: 12 out of 100\n","Epoch: 3025/5000  Traning Loss: 57.627017974853516  Train_Reconstruction: 54.688143253326416  Train_KL: 2.938874363899231  Validation Loss : 57.09336471557617 Val_Reconstruction : 54.18249320983887 Val_KL : 2.910870671272278\n","EarlyStopping counter: 13 out of 100\n","Epoch: 3026/5000  Traning Loss: 57.528008460998535  Train_Reconstruction: 54.59123992919922  Train_KL: 2.9367676973342896  Validation Loss : 56.99477195739746 Val_Reconstruction : 54.08486366271973 Val_KL : 2.9099085330963135\n","EarlyStopping counter: 14 out of 100\n","Epoch: 3027/5000  Traning Loss: 57.43754291534424  Train_Reconstruction: 54.49858808517456  Train_KL: 2.9389542043209076  Validation Loss : 56.847129821777344 Val_Reconstruction : 53.945735931396484 Val_KL : 2.901395320892334\n","EarlyStopping counter: 15 out of 100\n","Epoch: 3028/5000  Traning Loss: 57.74780893325806  Train_Reconstruction: 54.81936693191528  Train_KL: 2.9284425377845764  Validation Loss : 57.634389877319336 Val_Reconstruction : 54.737478256225586 Val_KL : 2.8969112634658813\n","EarlyStopping counter: 16 out of 100\n","Epoch: 3029/5000  Traning Loss: 58.219637393951416  Train_Reconstruction: 55.27444887161255  Train_KL: 2.9451883733272552  Validation Loss : 57.128793716430664 Val_Reconstruction : 54.21268272399902 Val_KL : 2.916110873222351\n","EarlyStopping counter: 17 out of 100\n","Epoch: 3030/5000  Traning Loss: 57.81828212738037  Train_Reconstruction: 54.87437915802002  Train_KL: 2.9439029693603516  Validation Loss : 56.75947570800781 Val_Reconstruction : 53.85553169250488 Val_KL : 2.903945207595825\n","EarlyStopping counter: 18 out of 100\n","Epoch: 3031/5000  Traning Loss: 57.29831409454346  Train_Reconstruction: 54.36442565917969  Train_KL: 2.933889240026474  Validation Loss : 56.69930458068848 Val_Reconstruction : 53.79950523376465 Val_KL : 2.8997976779937744\n","EarlyStopping counter: 19 out of 100\n","Epoch: 3032/5000  Traning Loss: 57.56528043746948  Train_Reconstruction: 54.630825996398926  Train_KL: 2.9344547986984253  Validation Loss : 57.469749450683594 Val_Reconstruction : 54.57627296447754 Val_KL : 2.8934766054153442\n","EarlyStopping counter: 20 out of 100\n","Epoch: 3033/5000  Traning Loss: 57.899752616882324  Train_Reconstruction: 54.964781761169434  Train_KL: 2.934971362352371  Validation Loss : 57.88152885437012 Val_Reconstruction : 54.97941589355469 Val_KL : 2.902111768722534\n","EarlyStopping counter: 21 out of 100\n","Epoch: 3034/5000  Traning Loss: 58.12483215332031  Train_Reconstruction: 55.19031238555908  Train_KL: 2.934520035982132  Validation Loss : 58.74300003051758 Val_Reconstruction : 55.84624099731445 Val_KL : 2.896759033203125\n","EarlyStopping counter: 22 out of 100\n","Epoch: 3035/5000  Traning Loss: 58.68518352508545  Train_Reconstruction: 55.751450061798096  Train_KL: 2.9337343871593475  Validation Loss : 57.67869758605957 Val_Reconstruction : 54.777774810791016 Val_KL : 2.9009218215942383\n","EarlyStopping counter: 23 out of 100\n","Epoch: 3036/5000  Traning Loss: 57.62667798995972  Train_Reconstruction: 54.69764232635498  Train_KL: 2.929035872220993  Validation Loss : 56.79350471496582 Val_Reconstruction : 53.897199630737305 Val_KL : 2.8963056802749634\n","EarlyStopping counter: 24 out of 100\n","Epoch: 3037/5000  Traning Loss: 57.63614368438721  Train_Reconstruction: 54.70753288269043  Train_KL: 2.928610682487488  Validation Loss : 56.797550201416016 Val_Reconstruction : 53.88774299621582 Val_KL : 2.9098068475723267\n","EarlyStopping counter: 25 out of 100\n","Epoch: 3038/5000  Traning Loss: 57.86808443069458  Train_Reconstruction: 54.94290494918823  Train_KL: 2.9251794517040253  Validation Loss : 57.33262825012207 Val_Reconstruction : 54.431236267089844 Val_KL : 2.901392102241516\n","EarlyStopping counter: 26 out of 100\n","Epoch: 3039/5000  Traning Loss: 57.80674934387207  Train_Reconstruction: 54.870447635650635  Train_KL: 2.9363020062446594  Validation Loss : 57.01106262207031 Val_Reconstruction : 54.10530662536621 Val_KL : 2.9057557582855225\n","EarlyStopping counter: 27 out of 100\n","Epoch: 3040/5000  Traning Loss: 57.743290424346924  Train_Reconstruction: 54.800424575805664  Train_KL: 2.9428660571575165  Validation Loss : 56.91729927062988 Val_Reconstruction : 54.00543212890625 Val_KL : 2.911866545677185\n","EarlyStopping counter: 28 out of 100\n","Epoch: 3041/5000  Traning Loss: 57.75240898132324  Train_Reconstruction: 54.814754486083984  Train_KL: 2.937654972076416  Validation Loss : 56.97051429748535 Val_Reconstruction : 54.065757751464844 Val_KL : 2.9047558307647705\n","EarlyStopping counter: 29 out of 100\n","Epoch: 3042/5000  Traning Loss: 58.305851459503174  Train_Reconstruction: 55.380226612091064  Train_KL: 2.9256251752376556  Validation Loss : 58.07889747619629 Val_Reconstruction : 55.1755256652832 Val_KL : 2.9033702611923218\n","EarlyStopping counter: 30 out of 100\n","Epoch: 3043/5000  Traning Loss: 58.0622444152832  Train_Reconstruction: 55.12764596939087  Train_KL: 2.9345978796482086  Validation Loss : 57.738203048706055 Val_Reconstruction : 54.84074592590332 Val_KL : 2.897456645965576\n","EarlyStopping counter: 31 out of 100\n","Epoch: 3044/5000  Traning Loss: 57.67629337310791  Train_Reconstruction: 54.75292730331421  Train_KL: 2.9233653247356415  Validation Loss : 56.916303634643555 Val_Reconstruction : 54.027366638183594 Val_KL : 2.8889365196228027\n","EarlyStopping counter: 32 out of 100\n","Epoch: 3045/5000  Traning Loss: 57.78796625137329  Train_Reconstruction: 54.85964107513428  Train_KL: 2.9283250868320465  Validation Loss : 57.14348220825195 Val_Reconstruction : 54.24196243286133 Val_KL : 2.901519775390625\n","EarlyStopping counter: 33 out of 100\n","Epoch: 3046/5000  Traning Loss: 57.39546823501587  Train_Reconstruction: 54.45840787887573  Train_KL: 2.937060624361038  Validation Loss : 56.83138465881348 Val_Reconstruction : 53.91630935668945 Val_KL : 2.9150747060775757\n","EarlyStopping counter: 34 out of 100\n","Epoch: 3047/5000  Traning Loss: 57.40210723876953  Train_Reconstruction: 54.4621467590332  Train_KL: 2.9399605095386505  Validation Loss : 57.03660583496094 Val_Reconstruction : 54.1389045715332 Val_KL : 2.897700071334839\n","EarlyStopping counter: 35 out of 100\n","Epoch: 3048/5000  Traning Loss: 57.46102523803711  Train_Reconstruction: 54.53283405303955  Train_KL: 2.9281914830207825  Validation Loss : 57.219228744506836 Val_Reconstruction : 54.33198928833008 Val_KL : 2.887239098548889\n","EarlyStopping counter: 36 out of 100\n","Epoch: 3049/5000  Traning Loss: 57.681729316711426  Train_Reconstruction: 54.75079822540283  Train_KL: 2.9309310615062714  Validation Loss : 57.0904598236084 Val_Reconstruction : 54.18034744262695 Val_KL : 2.9101113080978394\n","EarlyStopping counter: 37 out of 100\n","Epoch: 3050/5000  Traning Loss: 57.85791349411011  Train_Reconstruction: 54.92659139633179  Train_KL: 2.931322067975998  Validation Loss : 57.0627384185791 Val_Reconstruction : 54.15896797180176 Val_KL : 2.903770685195923\n","EarlyStopping counter: 38 out of 100\n","Epoch: 3051/5000  Traning Loss: 57.624305725097656  Train_Reconstruction: 54.687721252441406  Train_KL: 2.9365846812725067  Validation Loss : 57.20065689086914 Val_Reconstruction : 54.291921615600586 Val_KL : 2.9087350368499756\n","EarlyStopping counter: 39 out of 100\n","Epoch: 3052/5000  Traning Loss: 57.55390787124634  Train_Reconstruction: 54.6221284866333  Train_KL: 2.9317798018455505  Validation Loss : 56.72964286804199 Val_Reconstruction : 53.83102226257324 Val_KL : 2.8986212015151978\n","EarlyStopping counter: 40 out of 100\n","Epoch: 3053/5000  Traning Loss: 57.35051393508911  Train_Reconstruction: 54.41836738586426  Train_KL: 2.9321464896202087  Validation Loss : 56.97574424743652 Val_Reconstruction : 54.06280708312988 Val_KL : 2.912936210632324\n","EarlyStopping counter: 41 out of 100\n","Epoch: 3054/5000  Traning Loss: 57.77761173248291  Train_Reconstruction: 54.839091777801514  Train_KL: 2.938520908355713  Validation Loss : 57.84519004821777 Val_Reconstruction : 54.93466567993164 Val_KL : 2.910524845123291\n","EarlyStopping counter: 42 out of 100\n","Epoch: 3055/5000  Traning Loss: 58.05482864379883  Train_Reconstruction: 55.11834669113159  Train_KL: 2.9364828765392303  Validation Loss : 57.46760559082031 Val_Reconstruction : 54.56484603881836 Val_KL : 2.9027600288391113\n","EarlyStopping counter: 43 out of 100\n","Epoch: 3056/5000  Traning Loss: 57.854764461517334  Train_Reconstruction: 54.92866277694702  Train_KL: 2.926101803779602  Validation Loss : 56.94088554382324 Val_Reconstruction : 54.04317092895508 Val_KL : 2.8977150917053223\n","EarlyStopping counter: 44 out of 100\n","Epoch: 3057/5000  Traning Loss: 57.52863645553589  Train_Reconstruction: 54.59320402145386  Train_KL: 2.9354320764541626  Validation Loss : 57.169517517089844 Val_Reconstruction : 54.26206588745117 Val_KL : 2.9074530601501465\n","EarlyStopping counter: 45 out of 100\n","Epoch: 3058/5000  Traning Loss: 57.57408332824707  Train_Reconstruction: 54.64681577682495  Train_KL: 2.9272676706314087  Validation Loss : 57.32657432556152 Val_Reconstruction : 54.430931091308594 Val_KL : 2.8956427574157715\n","EarlyStopping counter: 46 out of 100\n","Epoch: 3059/5000  Traning Loss: 57.454164028167725  Train_Reconstruction: 54.51346397399902  Train_KL: 2.940700024366379  Validation Loss : 56.772504806518555 Val_Reconstruction : 53.85798454284668 Val_KL : 2.914520740509033\n","EarlyStopping counter: 47 out of 100\n","Epoch: 3060/5000  Traning Loss: 57.4569354057312  Train_Reconstruction: 54.51952934265137  Train_KL: 2.937406539916992  Validation Loss : 56.90059471130371 Val_Reconstruction : 53.99070930480957 Val_KL : 2.9098851680755615\n","EarlyStopping counter: 48 out of 100\n","Epoch: 3061/5000  Traning Loss: 57.43923234939575  Train_Reconstruction: 54.512364864349365  Train_KL: 2.926867187023163  Validation Loss : 56.873313903808594 Val_Reconstruction : 53.98490524291992 Val_KL : 2.8884090185165405\n","EarlyStopping counter: 49 out of 100\n","Epoch: 3062/5000  Traning Loss: 57.40833902359009  Train_Reconstruction: 54.487120151519775  Train_KL: 2.9212184846401215  Validation Loss : 57.17853355407715 Val_Reconstruction : 54.28721046447754 Val_KL : 2.8913230895996094\n","EarlyStopping counter: 50 out of 100\n","Epoch: 3063/5000  Traning Loss: 57.70237112045288  Train_Reconstruction: 54.76502847671509  Train_KL: 2.9373428225517273  Validation Loss : 56.913421630859375 Val_Reconstruction : 54.006792068481445 Val_KL : 2.9066288471221924\n","EarlyStopping counter: 51 out of 100\n","Epoch: 3064/5000  Traning Loss: 57.4550199508667  Train_Reconstruction: 54.52247858047485  Train_KL: 2.9325409829616547  Validation Loss : 56.885887145996094 Val_Reconstruction : 53.98282241821289 Val_KL : 2.9030638933181763\n","EarlyStopping counter: 52 out of 100\n","Epoch: 3065/5000  Traning Loss: 57.46383571624756  Train_Reconstruction: 54.53923988342285  Train_KL: 2.9245957136154175  Validation Loss : 56.90790939331055 Val_Reconstruction : 54.01352310180664 Val_KL : 2.8943856954574585\n","EarlyStopping counter: 53 out of 100\n","Epoch: 3066/5000  Traning Loss: 57.679527759552  Train_Reconstruction: 54.75367307662964  Train_KL: 2.9258545637130737  Validation Loss : 57.05526924133301 Val_Reconstruction : 54.165618896484375 Val_KL : 2.8896509408950806\n","EarlyStopping counter: 54 out of 100\n","Epoch: 3067/5000  Traning Loss: 57.95970439910889  Train_Reconstruction: 55.029385566711426  Train_KL: 2.9303189516067505  Validation Loss : 57.33401679992676 Val_Reconstruction : 54.435306549072266 Val_KL : 2.8987114429473877\n","EarlyStopping counter: 55 out of 100\n","Epoch: 3068/5000  Traning Loss: 58.22005271911621  Train_Reconstruction: 55.2843599319458  Train_KL: 2.935692608356476  Validation Loss : 58.089195251464844 Val_Reconstruction : 55.183584213256836 Val_KL : 2.9056122303009033\n","EarlyStopping counter: 56 out of 100\n","Epoch: 3069/5000  Traning Loss: 58.248934745788574  Train_Reconstruction: 55.331782817840576  Train_KL: 2.9171516597270966  Validation Loss : 57.4987907409668 Val_Reconstruction : 54.61396598815918 Val_KL : 2.884825110435486\n","EarlyStopping counter: 57 out of 100\n","Epoch: 3070/5000  Traning Loss: 57.9526104927063  Train_Reconstruction: 55.03482103347778  Train_KL: 2.917789399623871  Validation Loss : 57.44213104248047 Val_Reconstruction : 54.55642890930176 Val_KL : 2.8857028484344482\n","EarlyStopping counter: 58 out of 100\n","Epoch: 3071/5000  Traning Loss: 57.66386127471924  Train_Reconstruction: 54.74075222015381  Train_KL: 2.9231095016002655  Validation Loss : 57.0193977355957 Val_Reconstruction : 54.11674690246582 Val_KL : 2.9026522636413574\n","EarlyStopping counter: 59 out of 100\n","Epoch: 3072/5000  Traning Loss: 57.43333721160889  Train_Reconstruction: 54.50377178192139  Train_KL: 2.92956480383873  Validation Loss : 56.900821685791016 Val_Reconstruction : 53.99495506286621 Val_KL : 2.9058663845062256\n","EarlyStopping counter: 60 out of 100\n","Epoch: 3073/5000  Traning Loss: 57.34337568283081  Train_Reconstruction: 54.41536855697632  Train_KL: 2.9280066192150116  Validation Loss : 56.70762634277344 Val_Reconstruction : 53.81323051452637 Val_KL : 2.8943971395492554\n","EarlyStopping counter: 61 out of 100\n","Epoch: 3074/5000  Traning Loss: 57.37716293334961  Train_Reconstruction: 54.45110607147217  Train_KL: 2.9260565638542175  Validation Loss : 57.10095977783203 Val_Reconstruction : 54.208824157714844 Val_KL : 2.892135977745056\n","EarlyStopping counter: 62 out of 100\n","Epoch: 3075/5000  Traning Loss: 57.69317388534546  Train_Reconstruction: 54.76405572891235  Train_KL: 2.9291183948516846  Validation Loss : 57.618804931640625 Val_Reconstruction : 54.73175048828125 Val_KL : 2.887054920196533\n","EarlyStopping counter: 63 out of 100\n","Epoch: 3076/5000  Traning Loss: 58.05615854263306  Train_Reconstruction: 55.13394355773926  Train_KL: 2.922214835882187  Validation Loss : 57.23252868652344 Val_Reconstruction : 54.34894371032715 Val_KL : 2.8835866451263428\n","EarlyStopping counter: 64 out of 100\n","Epoch: 3077/5000  Traning Loss: 58.28110122680664  Train_Reconstruction: 55.35629081726074  Train_KL: 2.924810141324997  Validation Loss : 58.17665481567383 Val_Reconstruction : 55.28084945678711 Val_KL : 2.895806670188904\n","EarlyStopping counter: 65 out of 100\n","Epoch: 3078/5000  Traning Loss: 57.88653039932251  Train_Reconstruction: 54.95513582229614  Train_KL: 2.9313946962356567  Validation Loss : 57.115882873535156 Val_Reconstruction : 54.21841812133789 Val_KL : 2.8974642753601074\n","EarlyStopping counter: 66 out of 100\n","Epoch: 3079/5000  Traning Loss: 57.63707780838013  Train_Reconstruction: 54.70908737182617  Train_KL: 2.927990287542343  Validation Loss : 57.03633689880371 Val_Reconstruction : 54.140493392944336 Val_KL : 2.895844578742981\n","EarlyStopping counter: 67 out of 100\n","Epoch: 3080/5000  Traning Loss: 57.49956226348877  Train_Reconstruction: 54.57191514968872  Train_KL: 2.927647829055786  Validation Loss : 57.06539535522461 Val_Reconstruction : 54.17287063598633 Val_KL : 2.8925259113311768\n","EarlyStopping counter: 68 out of 100\n","Epoch: 3081/5000  Traning Loss: 57.628806591033936  Train_Reconstruction: 54.701483726501465  Train_KL: 2.927323043346405  Validation Loss : 57.29365539550781 Val_Reconstruction : 54.39367866516113 Val_KL : 2.89997661113739\n","EarlyStopping counter: 69 out of 100\n","Epoch: 3082/5000  Traning Loss: 57.818156242370605  Train_Reconstruction: 54.89166498184204  Train_KL: 2.9264911115169525  Validation Loss : 57.2810115814209 Val_Reconstruction : 54.37753105163574 Val_KL : 2.9034814834594727\n","EarlyStopping counter: 70 out of 100\n","Epoch: 3083/5000  Traning Loss: 57.442362785339355  Train_Reconstruction: 54.51775503158569  Train_KL: 2.9246076941490173  Validation Loss : 56.788808822631836 Val_Reconstruction : 53.894121170043945 Val_KL : 2.8946889638900757\n","EarlyStopping counter: 71 out of 100\n","Epoch: 3084/5000  Traning Loss: 57.65054798126221  Train_Reconstruction: 54.73045063018799  Train_KL: 2.92009773850441  Validation Loss : 57.21049690246582 Val_Reconstruction : 54.32134819030762 Val_KL : 2.88914954662323\n","EarlyStopping counter: 72 out of 100\n","Epoch: 3085/5000  Traning Loss: 57.761290073394775  Train_Reconstruction: 54.836599349975586  Train_KL: 2.9246908724308014  Validation Loss : 57.02529525756836 Val_Reconstruction : 54.12497901916504 Val_KL : 2.9003156423568726\n","EarlyStopping counter: 73 out of 100\n","Epoch: 3086/5000  Traning Loss: 57.65916872024536  Train_Reconstruction: 54.71992492675781  Train_KL: 2.93924418091774  Validation Loss : 57.590484619140625 Val_Reconstruction : 54.694007873535156 Val_KL : 2.8964760303497314\n","EarlyStopping counter: 74 out of 100\n","Epoch: 3087/5000  Traning Loss: 57.590096950531006  Train_Reconstruction: 54.667269706726074  Train_KL: 2.9228269457817078  Validation Loss : 57.21344184875488 Val_Reconstruction : 54.32460403442383 Val_KL : 2.888838052749634\n","EarlyStopping counter: 75 out of 100\n","Epoch: 3088/5000  Traning Loss: 57.515482902526855  Train_Reconstruction: 54.59016036987305  Train_KL: 2.9253224432468414  Validation Loss : 56.5996150970459 Val_Reconstruction : 53.70286750793457 Val_KL : 2.8967485427856445\n","EarlyStopping counter: 76 out of 100\n","Epoch: 3089/5000  Traning Loss: 57.22493839263916  Train_Reconstruction: 54.29890203475952  Train_KL: 2.9260361790657043  Validation Loss : 56.494964599609375 Val_Reconstruction : 53.59647750854492 Val_KL : 2.8984867334365845\n","Epoch: 3090/5000  Traning Loss: 57.47547960281372  Train_Reconstruction: 54.53915548324585  Train_KL: 2.936323791742325  Validation Loss : 56.95526123046875 Val_Reconstruction : 54.04289436340332 Val_KL : 2.9123674631118774\n","EarlyStopping counter: 1 out of 100\n","Epoch: 3091/5000  Traning Loss: 57.83938932418823  Train_Reconstruction: 54.89852333068848  Train_KL: 2.9408656656742096  Validation Loss : 57.40015411376953 Val_Reconstruction : 54.491716384887695 Val_KL : 2.9084383249282837\n","EarlyStopping counter: 2 out of 100\n","Epoch: 3092/5000  Traning Loss: 57.93968105316162  Train_Reconstruction: 55.00245380401611  Train_KL: 2.937226802110672  Validation Loss : 57.64323425292969 Val_Reconstruction : 54.730905532836914 Val_KL : 2.9123278856277466\n","EarlyStopping counter: 3 out of 100\n","Epoch: 3093/5000  Traning Loss: 57.59013509750366  Train_Reconstruction: 54.659066677093506  Train_KL: 2.9310686588287354  Validation Loss : 56.87291145324707 Val_Reconstruction : 53.97705841064453 Val_KL : 2.8958547115325928\n","EarlyStopping counter: 4 out of 100\n","Epoch: 3094/5000  Traning Loss: 57.42498588562012  Train_Reconstruction: 54.4861216545105  Train_KL: 2.938863903284073  Validation Loss : 56.528635025024414 Val_Reconstruction : 53.62030029296875 Val_KL : 2.908333897590637\n","EarlyStopping counter: 5 out of 100\n","Epoch: 3095/5000  Traning Loss: 57.29323959350586  Train_Reconstruction: 54.357887744903564  Train_KL: 2.93535178899765  Validation Loss : 56.71495246887207 Val_Reconstruction : 53.82028770446777 Val_KL : 2.8946638107299805\n","EarlyStopping counter: 6 out of 100\n","Epoch: 3096/5000  Traning Loss: 57.44911813735962  Train_Reconstruction: 54.53395128250122  Train_KL: 2.9151660203933716  Validation Loss : 56.989437103271484 Val_Reconstruction : 54.106977462768555 Val_KL : 2.882460594177246\n","EarlyStopping counter: 7 out of 100\n","Epoch: 3097/5000  Traning Loss: 57.690688133239746  Train_Reconstruction: 54.771697998046875  Train_KL: 2.9189896285533905  Validation Loss : 57.2034854888916 Val_Reconstruction : 54.30716514587402 Val_KL : 2.8963197469711304\n","EarlyStopping counter: 8 out of 100\n","Epoch: 3098/5000  Traning Loss: 57.68944549560547  Train_Reconstruction: 54.756149768829346  Train_KL: 2.933295875787735  Validation Loss : 56.98584747314453 Val_Reconstruction : 54.07790946960449 Val_KL : 2.9079384803771973\n","EarlyStopping counter: 9 out of 100\n","Epoch: 3099/5000  Traning Loss: 57.75258016586304  Train_Reconstruction: 54.82132911682129  Train_KL: 2.93125119805336  Validation Loss : 57.12917900085449 Val_Reconstruction : 54.223440170288086 Val_KL : 2.9057390689849854\n","EarlyStopping counter: 10 out of 100\n","Epoch: 3100/5000  Traning Loss: 57.646265506744385  Train_Reconstruction: 54.71866798400879  Train_KL: 2.9275977313518524  Validation Loss : 57.02129936218262 Val_Reconstruction : 54.12580108642578 Val_KL : 2.895497679710388\n","EarlyStopping counter: 11 out of 100\n","Epoch: 3101/5000  Traning Loss: 57.55875253677368  Train_Reconstruction: 54.63569116592407  Train_KL: 2.923061430454254  Validation Loss : 57.151275634765625 Val_Reconstruction : 54.24896240234375 Val_KL : 2.9023135900497437\n","EarlyStopping counter: 12 out of 100\n","Epoch: 3102/5000  Traning Loss: 57.53497934341431  Train_Reconstruction: 54.60034227371216  Train_KL: 2.934636414051056  Validation Loss : 57.05122756958008 Val_Reconstruction : 54.14409255981445 Val_KL : 2.9071362018585205\n","EarlyStopping counter: 13 out of 100\n","Epoch: 3103/5000  Traning Loss: 57.580580711364746  Train_Reconstruction: 54.652369022369385  Train_KL: 2.9282117187976837  Validation Loss : 57.08473587036133 Val_Reconstruction : 54.19027328491211 Val_KL : 2.894462823867798\n","EarlyStopping counter: 14 out of 100\n","Epoch: 3104/5000  Traning Loss: 57.48356914520264  Train_Reconstruction: 54.56009006500244  Train_KL: 2.9234790205955505  Validation Loss : 56.85909080505371 Val_Reconstruction : 53.96569061279297 Val_KL : 2.8934000730514526\n","EarlyStopping counter: 15 out of 100\n","Epoch: 3105/5000  Traning Loss: 57.51401376724243  Train_Reconstruction: 54.57440662384033  Train_KL: 2.9396072030067444  Validation Loss : 56.77079963684082 Val_Reconstruction : 53.854257583618164 Val_KL : 2.916542172431946\n","EarlyStopping counter: 16 out of 100\n","Epoch: 3106/5000  Traning Loss: 57.48707151412964  Train_Reconstruction: 54.54956579208374  Train_KL: 2.9375056326389313  Validation Loss : 57.02579307556152 Val_Reconstruction : 54.12449264526367 Val_KL : 2.9013004302978516\n","EarlyStopping counter: 17 out of 100\n","Epoch: 3107/5000  Traning Loss: 57.671753883361816  Train_Reconstruction: 54.73776054382324  Train_KL: 2.9339930415153503  Validation Loss : 57.781511306762695 Val_Reconstruction : 54.88004112243652 Val_KL : 2.9014699459075928\n","EarlyStopping counter: 18 out of 100\n","Epoch: 3108/5000  Traning Loss: 57.98031282424927  Train_Reconstruction: 55.04478073120117  Train_KL: 2.9355325400829315  Validation Loss : 57.75661659240723 Val_Reconstruction : 54.85355758666992 Val_KL : 2.9030598402023315\n","EarlyStopping counter: 19 out of 100\n","Epoch: 3109/5000  Traning Loss: 57.79083824157715  Train_Reconstruction: 54.8700737953186  Train_KL: 2.920764595270157  Validation Loss : 57.090301513671875 Val_Reconstruction : 54.195016860961914 Val_KL : 2.8952850103378296\n","EarlyStopping counter: 20 out of 100\n","Epoch: 3110/5000  Traning Loss: 57.4429817199707  Train_Reconstruction: 54.51037359237671  Train_KL: 2.932608485221863  Validation Loss : 56.69036102294922 Val_Reconstruction : 53.76795196533203 Val_KL : 2.922408699989319\n","EarlyStopping counter: 21 out of 100\n","Epoch: 3111/5000  Traning Loss: 57.26786136627197  Train_Reconstruction: 54.324734687805176  Train_KL: 2.943126827478409  Validation Loss : 57.03866958618164 Val_Reconstruction : 54.1197624206543 Val_KL : 2.9189083576202393\n","EarlyStopping counter: 22 out of 100\n","Epoch: 3112/5000  Traning Loss: 57.431405544281006  Train_Reconstruction: 54.4971227645874  Train_KL: 2.934282511472702  Validation Loss : 56.993717193603516 Val_Reconstruction : 54.08677101135254 Val_KL : 2.906946301460266\n","EarlyStopping counter: 23 out of 100\n","Epoch: 3113/5000  Traning Loss: 57.747612953186035  Train_Reconstruction: 54.81040811538696  Train_KL: 2.9372046291828156  Validation Loss : 57.62813186645508 Val_Reconstruction : 54.719722747802734 Val_KL : 2.9084099531173706\n","EarlyStopping counter: 24 out of 100\n","Epoch: 3114/5000  Traning Loss: 58.15119552612305  Train_Reconstruction: 55.22049903869629  Train_KL: 2.9306963980197906  Validation Loss : 57.2990779876709 Val_Reconstruction : 54.41291427612305 Val_KL : 2.8861629962921143\n","EarlyStopping counter: 25 out of 100\n","Epoch: 3115/5000  Traning Loss: 57.97458267211914  Train_Reconstruction: 55.04568862915039  Train_KL: 2.928894519805908  Validation Loss : 56.818620681762695 Val_Reconstruction : 53.92543029785156 Val_KL : 2.8931920528411865\n","EarlyStopping counter: 26 out of 100\n","Epoch: 3116/5000  Traning Loss: 57.257943630218506  Train_Reconstruction: 54.32770586013794  Train_KL: 2.9302376210689545  Validation Loss : 56.53286170959473 Val_Reconstruction : 53.645469665527344 Val_KL : 2.887391209602356\n","EarlyStopping counter: 27 out of 100\n","Epoch: 3117/5000  Traning Loss: 57.08659267425537  Train_Reconstruction: 54.165616512298584  Train_KL: 2.9209757447242737  Validation Loss : 56.5945930480957 Val_Reconstruction : 53.69809150695801 Val_KL : 2.8965015411376953\n","EarlyStopping counter: 28 out of 100\n","Epoch: 3118/5000  Traning Loss: 57.25865840911865  Train_Reconstruction: 54.314011573791504  Train_KL: 2.9446468353271484  Validation Loss : 56.86329650878906 Val_Reconstruction : 53.94381904602051 Val_KL : 2.919476866722107\n","EarlyStopping counter: 29 out of 100\n","Epoch: 3119/5000  Traning Loss: 57.38355350494385  Train_Reconstruction: 54.44976091384888  Train_KL: 2.9337920248508453  Validation Loss : 56.860595703125 Val_Reconstruction : 53.957130432128906 Val_KL : 2.903465151786804\n","EarlyStopping counter: 30 out of 100\n","Epoch: 3120/5000  Traning Loss: 57.41629600524902  Train_Reconstruction: 54.48556041717529  Train_KL: 2.9307354986667633  Validation Loss : 56.84222412109375 Val_Reconstruction : 53.943342208862305 Val_KL : 2.898882031440735\n","EarlyStopping counter: 31 out of 100\n","Epoch: 3121/5000  Traning Loss: 57.43946838378906  Train_Reconstruction: 54.50606298446655  Train_KL: 2.9334056675434113  Validation Loss : 57.097557067871094 Val_Reconstruction : 54.192087173461914 Val_KL : 2.905469298362732\n","EarlyStopping counter: 32 out of 100\n","Epoch: 3122/5000  Traning Loss: 57.6530179977417  Train_Reconstruction: 54.718520164489746  Train_KL: 2.9344972670078278  Validation Loss : 57.40130424499512 Val_Reconstruction : 54.48976135253906 Val_KL : 2.911543846130371\n","EarlyStopping counter: 33 out of 100\n","Epoch: 3123/5000  Traning Loss: 57.55179834365845  Train_Reconstruction: 54.61202955245972  Train_KL: 2.9397684931755066  Validation Loss : 56.80034637451172 Val_Reconstruction : 53.89239501953125 Val_KL : 2.907951235771179\n","EarlyStopping counter: 34 out of 100\n","Epoch: 3124/5000  Traning Loss: 57.416436195373535  Train_Reconstruction: 54.48713397979736  Train_KL: 2.9293017387390137  Validation Loss : 56.8248348236084 Val_Reconstruction : 53.929189682006836 Val_KL : 2.8956451416015625\n","EarlyStopping counter: 35 out of 100\n","Epoch: 3125/5000  Traning Loss: 57.456702709198  Train_Reconstruction: 54.5325288772583  Train_KL: 2.9241733253002167  Validation Loss : 57.019290924072266 Val_Reconstruction : 54.1165885925293 Val_KL : 2.902701735496521\n","EarlyStopping counter: 36 out of 100\n","Epoch: 3126/5000  Traning Loss: 57.51098585128784  Train_Reconstruction: 54.57036066055298  Train_KL: 2.9406251907348633  Validation Loss : 56.50935173034668 Val_Reconstruction : 53.59900665283203 Val_KL : 2.9103455543518066\n","EarlyStopping counter: 37 out of 100\n","Epoch: 3127/5000  Traning Loss: 57.32040452957153  Train_Reconstruction: 54.37396001815796  Train_KL: 2.9464451372623444  Validation Loss : 56.79559516906738 Val_Reconstruction : 53.879404067993164 Val_KL : 2.916192054748535\n","EarlyStopping counter: 38 out of 100\n","Epoch: 3128/5000  Traning Loss: 57.72040271759033  Train_Reconstruction: 54.79145431518555  Train_KL: 2.9289485812187195  Validation Loss : 57.76528549194336 Val_Reconstruction : 54.870405197143555 Val_KL : 2.8948804140090942\n","EarlyStopping counter: 39 out of 100\n","Epoch: 3129/5000  Traning Loss: 57.61148643493652  Train_Reconstruction: 54.68684768676758  Train_KL: 2.924638420343399  Validation Loss : 56.87922668457031 Val_Reconstruction : 53.989484786987305 Val_KL : 2.8897414207458496\n","EarlyStopping counter: 40 out of 100\n","Epoch: 3130/5000  Traning Loss: 57.448118686676025  Train_Reconstruction: 54.51241207122803  Train_KL: 2.9357067346572876  Validation Loss : 56.77370071411133 Val_Reconstruction : 53.86372947692871 Val_KL : 2.909972667694092\n","EarlyStopping counter: 41 out of 100\n","Epoch: 3131/5000  Traning Loss: 57.5981969833374  Train_Reconstruction: 54.656805992126465  Train_KL: 2.941390961408615  Validation Loss : 57.09249496459961 Val_Reconstruction : 54.198110580444336 Val_KL : 2.894383668899536\n","EarlyStopping counter: 42 out of 100\n","Epoch: 3132/5000  Traning Loss: 57.766600608825684  Train_Reconstruction: 54.83554792404175  Train_KL: 2.931052267551422  Validation Loss : 57.33974075317383 Val_Reconstruction : 54.437238693237305 Val_KL : 2.9025025367736816\n","EarlyStopping counter: 43 out of 100\n","Epoch: 3133/5000  Traning Loss: 57.45478963851929  Train_Reconstruction: 54.52190113067627  Train_KL: 2.932888835668564  Validation Loss : 56.82822799682617 Val_Reconstruction : 53.93577575683594 Val_KL : 2.8924530744552612\n","EarlyStopping counter: 44 out of 100\n","Epoch: 3134/5000  Traning Loss: 57.59589862823486  Train_Reconstruction: 54.66085720062256  Train_KL: 2.9350412786006927  Validation Loss : 57.33888816833496 Val_Reconstruction : 54.437387466430664 Val_KL : 2.901501774787903\n","EarlyStopping counter: 45 out of 100\n","Epoch: 3135/5000  Traning Loss: 58.20716428756714  Train_Reconstruction: 55.2742133140564  Train_KL: 2.9329505562782288  Validation Loss : 57.78223419189453 Val_Reconstruction : 54.88311958312988 Val_KL : 2.899115204811096\n","EarlyStopping counter: 46 out of 100\n","Epoch: 3136/5000  Traning Loss: 57.72897958755493  Train_Reconstruction: 54.80357837677002  Train_KL: 2.9254013299942017  Validation Loss : 56.88322067260742 Val_Reconstruction : 53.984832763671875 Val_KL : 2.8983887434005737\n","EarlyStopping counter: 47 out of 100\n","Epoch: 3137/5000  Traning Loss: 57.46808338165283  Train_Reconstruction: 54.5431809425354  Train_KL: 2.924902230501175  Validation Loss : 56.99469566345215 Val_Reconstruction : 54.10744094848633 Val_KL : 2.8872554302215576\n","EarlyStopping counter: 48 out of 100\n","Epoch: 3138/5000  Traning Loss: 57.50311279296875  Train_Reconstruction: 54.57808828353882  Train_KL: 2.925024449825287  Validation Loss : 57.13729667663574 Val_Reconstruction : 54.23111534118652 Val_KL : 2.9061821699142456\n","EarlyStopping counter: 49 out of 100\n","Epoch: 3139/5000  Traning Loss: 57.53691339492798  Train_Reconstruction: 54.61632299423218  Train_KL: 2.9205906093120575  Validation Loss : 57.085248947143555 Val_Reconstruction : 54.195255279541016 Val_KL : 2.889993906021118\n","EarlyStopping counter: 50 out of 100\n","Epoch: 3140/5000  Traning Loss: 57.73439979553223  Train_Reconstruction: 54.80031394958496  Train_KL: 2.934086114168167  Validation Loss : 57.29600715637207 Val_Reconstruction : 54.381975173950195 Val_KL : 2.9140321016311646\n","EarlyStopping counter: 51 out of 100\n","Epoch: 3141/5000  Traning Loss: 57.716097354888916  Train_Reconstruction: 54.786781787872314  Train_KL: 2.9293148815631866  Validation Loss : 57.03073692321777 Val_Reconstruction : 54.131011962890625 Val_KL : 2.899725556373596\n","EarlyStopping counter: 52 out of 100\n","Epoch: 3142/5000  Traning Loss: 57.61295461654663  Train_Reconstruction: 54.683735370635986  Train_KL: 2.9292196929454803  Validation Loss : 56.8979606628418 Val_Reconstruction : 54.00306701660156 Val_KL : 2.894894242286682\n","EarlyStopping counter: 53 out of 100\n","Epoch: 3143/5000  Traning Loss: 57.423434257507324  Train_Reconstruction: 54.49707794189453  Train_KL: 2.9263568818569183  Validation Loss : 57.3350772857666 Val_Reconstruction : 54.445743560791016 Val_KL : 2.88933265209198\n","EarlyStopping counter: 54 out of 100\n","Epoch: 3144/5000  Traning Loss: 57.7965784072876  Train_Reconstruction: 54.86759042739868  Train_KL: 2.92898827791214  Validation Loss : 56.88311195373535 Val_Reconstruction : 53.980255126953125 Val_KL : 2.902856469154358\n","EarlyStopping counter: 55 out of 100\n","Epoch: 3145/5000  Traning Loss: 57.39462614059448  Train_Reconstruction: 54.46901082992554  Train_KL: 2.925615280866623  Validation Loss : 56.64254379272461 Val_Reconstruction : 53.747188568115234 Val_KL : 2.8953561782836914\n","EarlyStopping counter: 56 out of 100\n","Epoch: 3146/5000  Traning Loss: 57.20047664642334  Train_Reconstruction: 54.26931667327881  Train_KL: 2.9311600029468536  Validation Loss : 56.58332443237305 Val_Reconstruction : 53.67320442199707 Val_KL : 2.910119414329529\n","EarlyStopping counter: 57 out of 100\n","Epoch: 3147/5000  Traning Loss: 57.37010860443115  Train_Reconstruction: 54.43625354766846  Train_KL: 2.9338550567626953  Validation Loss : 56.907705307006836 Val_Reconstruction : 54.01148223876953 Val_KL : 2.8962243795394897\n","EarlyStopping counter: 58 out of 100\n","Epoch: 3148/5000  Traning Loss: 57.456223011016846  Train_Reconstruction: 54.51971435546875  Train_KL: 2.9365087151527405  Validation Loss : 56.7552604675293 Val_Reconstruction : 53.84831619262695 Val_KL : 2.9069435596466064\n","EarlyStopping counter: 59 out of 100\n","Epoch: 3149/5000  Traning Loss: 57.28265190124512  Train_Reconstruction: 54.351189613342285  Train_KL: 2.931462198495865  Validation Loss : 56.840280532836914 Val_Reconstruction : 53.94000434875488 Val_KL : 2.9002766609191895\n","EarlyStopping counter: 60 out of 100\n","Epoch: 3150/5000  Traning Loss: 57.16975545883179  Train_Reconstruction: 54.23266649246216  Train_KL: 2.937088966369629  Validation Loss : 56.591575622558594 Val_Reconstruction : 53.6822509765625 Val_KL : 2.909324884414673\n","EarlyStopping counter: 61 out of 100\n","Epoch: 3151/5000  Traning Loss: 57.143330097198486  Train_Reconstruction: 54.20488214492798  Train_KL: 2.9384484589099884  Validation Loss : 56.78507423400879 Val_Reconstruction : 53.878173828125 Val_KL : 2.9069018363952637\n","EarlyStopping counter: 62 out of 100\n","Epoch: 3152/5000  Traning Loss: 57.1809344291687  Train_Reconstruction: 54.24138259887695  Train_KL: 2.9395513832569122  Validation Loss : 56.67491912841797 Val_Reconstruction : 53.762765884399414 Val_KL : 2.9121532440185547\n","EarlyStopping counter: 63 out of 100\n","Epoch: 3153/5000  Traning Loss: 57.38773727416992  Train_Reconstruction: 54.45025873184204  Train_KL: 2.93747878074646  Validation Loss : 56.62114334106445 Val_Reconstruction : 53.7126579284668 Val_KL : 2.9084856510162354\n","EarlyStopping counter: 64 out of 100\n","Epoch: 3154/5000  Traning Loss: 57.6650710105896  Train_Reconstruction: 54.73242425918579  Train_KL: 2.9326466619968414  Validation Loss : 57.06647300720215 Val_Reconstruction : 54.16559028625488 Val_KL : 2.900883197784424\n","EarlyStopping counter: 65 out of 100\n","Epoch: 3155/5000  Traning Loss: 57.46102476119995  Train_Reconstruction: 54.52979135513306  Train_KL: 2.931233197450638  Validation Loss : 56.778865814208984 Val_Reconstruction : 53.8760929107666 Val_KL : 2.902774214744568\n","EarlyStopping counter: 66 out of 100\n","Epoch: 3156/5000  Traning Loss: 57.32835388183594  Train_Reconstruction: 54.394816875457764  Train_KL: 2.933537572622299  Validation Loss : 56.83311080932617 Val_Reconstruction : 53.93358039855957 Val_KL : 2.8995312452316284\n","EarlyStopping counter: 67 out of 100\n","Epoch: 3157/5000  Traning Loss: 57.277599811553955  Train_Reconstruction: 54.34799909591675  Train_KL: 2.9296011924743652  Validation Loss : 56.95746994018555 Val_Reconstruction : 54.064666748046875 Val_KL : 2.8928027153015137\n","EarlyStopping counter: 68 out of 100\n","Epoch: 3158/5000  Traning Loss: 57.36817407608032  Train_Reconstruction: 54.44245433807373  Train_KL: 2.9257198572158813  Validation Loss : 56.8553409576416 Val_Reconstruction : 53.949310302734375 Val_KL : 2.9060295820236206\n","EarlyStopping counter: 69 out of 100\n","Epoch: 3159/5000  Traning Loss: 57.32569694519043  Train_Reconstruction: 54.387810707092285  Train_KL: 2.937886029481888  Validation Loss : 56.794076919555664 Val_Reconstruction : 53.87869453430176 Val_KL : 2.915382742881775\n","EarlyStopping counter: 70 out of 100\n","Epoch: 3160/5000  Traning Loss: 57.338422775268555  Train_Reconstruction: 54.40118217468262  Train_KL: 2.9372402131557465  Validation Loss : 56.981109619140625 Val_Reconstruction : 54.07561111450195 Val_KL : 2.9054986238479614\n","EarlyStopping counter: 71 out of 100\n","Epoch: 3161/5000  Traning Loss: 57.439565658569336  Train_Reconstruction: 54.51126670837402  Train_KL: 2.9282985031604767  Validation Loss : 56.97221755981445 Val_Reconstruction : 54.07402801513672 Val_KL : 2.8981891870498657\n","EarlyStopping counter: 72 out of 100\n","Epoch: 3162/5000  Traning Loss: 57.53484392166138  Train_Reconstruction: 54.607786655426025  Train_KL: 2.927057296037674  Validation Loss : 56.954938888549805 Val_Reconstruction : 54.05508041381836 Val_KL : 2.899858236312866\n","EarlyStopping counter: 73 out of 100\n","Epoch: 3163/5000  Traning Loss: 57.42742872238159  Train_Reconstruction: 54.49594259262085  Train_KL: 2.9314863979816437  Validation Loss : 56.88312339782715 Val_Reconstruction : 53.98526191711426 Val_KL : 2.8978623151779175\n","EarlyStopping counter: 74 out of 100\n","Epoch: 3164/5000  Traning Loss: 57.47437858581543  Train_Reconstruction: 54.54664993286133  Train_KL: 2.9277289509773254  Validation Loss : 56.720991134643555 Val_Reconstruction : 53.82131767272949 Val_KL : 2.8996737003326416\n","EarlyStopping counter: 75 out of 100\n","Epoch: 3165/5000  Traning Loss: 57.55014610290527  Train_Reconstruction: 54.61738300323486  Train_KL: 2.9327626824378967  Validation Loss : 57.22796821594238 Val_Reconstruction : 54.32914924621582 Val_KL : 2.8988183736801147\n","EarlyStopping counter: 76 out of 100\n","Epoch: 3166/5000  Traning Loss: 57.330739974975586  Train_Reconstruction: 54.397486209869385  Train_KL: 2.9332537055015564  Validation Loss : 56.70190620422363 Val_Reconstruction : 53.80160140991211 Val_KL : 2.900304436683655\n","EarlyStopping counter: 77 out of 100\n","Epoch: 3167/5000  Traning Loss: 57.2640585899353  Train_Reconstruction: 54.33512878417969  Train_KL: 2.9289295971393585  Validation Loss : 56.54891014099121 Val_Reconstruction : 53.64627265930176 Val_KL : 2.902637243270874\n","EarlyStopping counter: 78 out of 100\n","Epoch: 3168/5000  Traning Loss: 57.25611877441406  Train_Reconstruction: 54.32390594482422  Train_KL: 2.9322123527526855  Validation Loss : 56.702491760253906 Val_Reconstruction : 53.79171562194824 Val_KL : 2.910774827003479\n","EarlyStopping counter: 79 out of 100\n","Epoch: 3169/5000  Traning Loss: 57.06189489364624  Train_Reconstruction: 54.12260961532593  Train_KL: 2.939285635948181  Validation Loss : 56.43817710876465 Val_Reconstruction : 53.530317306518555 Val_KL : 2.9078586101531982\n","Epoch: 3170/5000  Traning Loss: 57.22318172454834  Train_Reconstruction: 54.296982765197754  Train_KL: 2.926199108362198  Validation Loss : 56.631826400756836 Val_Reconstruction : 53.73069953918457 Val_KL : 2.9011266231536865\n","EarlyStopping counter: 1 out of 100\n","Epoch: 3171/5000  Traning Loss: 57.37215995788574  Train_Reconstruction: 54.43637657165527  Train_KL: 2.9357842803001404  Validation Loss : 56.740291595458984 Val_Reconstruction : 53.83222198486328 Val_KL : 2.908069610595703\n","EarlyStopping counter: 2 out of 100\n","Epoch: 3172/5000  Traning Loss: 57.39715909957886  Train_Reconstruction: 54.46216821670532  Train_KL: 2.934991329908371  Validation Loss : 56.78360366821289 Val_Reconstruction : 53.88076210021973 Val_KL : 2.9028416872024536\n","EarlyStopping counter: 3 out of 100\n","Epoch: 3173/5000  Traning Loss: 57.490028858184814  Train_Reconstruction: 54.557255268096924  Train_KL: 2.9327734112739563  Validation Loss : 56.955360412597656 Val_Reconstruction : 54.05073547363281 Val_KL : 2.904625177383423\n","EarlyStopping counter: 4 out of 100\n","Epoch: 3174/5000  Traning Loss: 57.76685619354248  Train_Reconstruction: 54.84140729904175  Train_KL: 2.9254490733146667  Validation Loss : 57.4122200012207 Val_Reconstruction : 54.527029037475586 Val_KL : 2.8851895332336426\n","EarlyStopping counter: 5 out of 100\n","Epoch: 3175/5000  Traning Loss: 57.66180229187012  Train_Reconstruction: 54.73769426345825  Train_KL: 2.924107939004898  Validation Loss : 57.07051658630371 Val_Reconstruction : 54.17765998840332 Val_KL : 2.8928565979003906\n","EarlyStopping counter: 6 out of 100\n","Epoch: 3176/5000  Traning Loss: 57.81121253967285  Train_Reconstruction: 54.87880182266235  Train_KL: 2.9324104487895966  Validation Loss : 57.60176086425781 Val_Reconstruction : 54.69599914550781 Val_KL : 2.9057615995407104\n","EarlyStopping counter: 7 out of 100\n","Epoch: 3177/5000  Traning Loss: 58.09792613983154  Train_Reconstruction: 55.16376447677612  Train_KL: 2.9341614842414856  Validation Loss : 57.274009704589844 Val_Reconstruction : 54.37155342102051 Val_KL : 2.902456521987915\n","EarlyStopping counter: 8 out of 100\n","Epoch: 3178/5000  Traning Loss: 58.41665267944336  Train_Reconstruction: 55.47931480407715  Train_KL: 2.937337964773178  Validation Loss : 57.87228584289551 Val_Reconstruction : 54.958866119384766 Val_KL : 2.913419246673584\n","EarlyStopping counter: 9 out of 100\n","Epoch: 3179/5000  Traning Loss: 58.22147274017334  Train_Reconstruction: 55.28834915161133  Train_KL: 2.9331235587596893  Validation Loss : 57.61938285827637 Val_Reconstruction : 54.723154067993164 Val_KL : 2.896228075027466\n","EarlyStopping counter: 10 out of 100\n","Epoch: 3180/5000  Traning Loss: 57.78482913970947  Train_Reconstruction: 54.863914012908936  Train_KL: 2.9209153950214386  Validation Loss : 57.52326583862305 Val_Reconstruction : 54.63774490356445 Val_KL : 2.8855193853378296\n","EarlyStopping counter: 11 out of 100\n","Epoch: 3181/5000  Traning Loss: 57.356067180633545  Train_Reconstruction: 54.42762041091919  Train_KL: 2.928446501493454  Validation Loss : 56.55404853820801 Val_Reconstruction : 53.64935111999512 Val_KL : 2.9046971797943115\n","EarlyStopping counter: 12 out of 100\n","Epoch: 3182/5000  Traning Loss: 57.439022064208984  Train_Reconstruction: 54.50214433670044  Train_KL: 2.9368776082992554  Validation Loss : 57.043771743774414 Val_Reconstruction : 54.14495086669922 Val_KL : 2.898820400238037\n","EarlyStopping counter: 13 out of 100\n","Epoch: 3183/5000  Traning Loss: 57.59056377410889  Train_Reconstruction: 54.65884065628052  Train_KL: 2.931722491979599  Validation Loss : 57.07315254211426 Val_Reconstruction : 54.1812744140625 Val_KL : 2.8918780088424683\n","EarlyStopping counter: 14 out of 100\n","Epoch: 3184/5000  Traning Loss: 57.50503873825073  Train_Reconstruction: 54.58900213241577  Train_KL: 2.9160366356372833  Validation Loss : 56.945905685424805 Val_Reconstruction : 54.06202697753906 Val_KL : 2.8838785886764526\n","EarlyStopping counter: 15 out of 100\n","Epoch: 3185/5000  Traning Loss: 57.35142469406128  Train_Reconstruction: 54.42175769805908  Train_KL: 2.9296663105487823  Validation Loss : 56.635929107666016 Val_Reconstruction : 53.728689193725586 Val_KL : 2.9072386026382446\n","EarlyStopping counter: 16 out of 100\n","Epoch: 3186/5000  Traning Loss: 57.453449726104736  Train_Reconstruction: 54.51361608505249  Train_KL: 2.9398339688777924  Validation Loss : 57.276512145996094 Val_Reconstruction : 54.367244720458984 Val_KL : 2.909268856048584\n","EarlyStopping counter: 17 out of 100\n","Epoch: 3187/5000  Traning Loss: 57.41689682006836  Train_Reconstruction: 54.488431453704834  Train_KL: 2.9284651279449463  Validation Loss : 56.8855094909668 Val_Reconstruction : 53.98394775390625 Val_KL : 2.90156090259552\n","EarlyStopping counter: 18 out of 100\n","Epoch: 3188/5000  Traning Loss: 57.39534091949463  Train_Reconstruction: 54.45627164840698  Train_KL: 2.9390688240528107  Validation Loss : 56.802703857421875 Val_Reconstruction : 53.889930725097656 Val_KL : 2.91277277469635\n","EarlyStopping counter: 19 out of 100\n","Epoch: 3189/5000  Traning Loss: 57.3363471031189  Train_Reconstruction: 54.39596080780029  Train_KL: 2.94038662314415  Validation Loss : 56.65078353881836 Val_Reconstruction : 53.74645805358887 Val_KL : 2.9043257236480713\n","EarlyStopping counter: 20 out of 100\n","Epoch: 3190/5000  Traning Loss: 57.35962915420532  Train_Reconstruction: 54.42410945892334  Train_KL: 2.9355197846889496  Validation Loss : 56.90913772583008 Val_Reconstruction : 54.00300216674805 Val_KL : 2.9061359167099\n","EarlyStopping counter: 21 out of 100\n","Epoch: 3191/5000  Traning Loss: 57.39748573303223  Train_Reconstruction: 54.4601674079895  Train_KL: 2.9373177886009216  Validation Loss : 56.936994552612305 Val_Reconstruction : 54.039602279663086 Val_KL : 2.8973926305770874\n","EarlyStopping counter: 22 out of 100\n","Epoch: 3192/5000  Traning Loss: 57.339059829711914  Train_Reconstruction: 54.40993070602417  Train_KL: 2.9291290044784546  Validation Loss : 56.73618698120117 Val_Reconstruction : 53.83040428161621 Val_KL : 2.9057819843292236\n","EarlyStopping counter: 23 out of 100\n","Epoch: 3193/5000  Traning Loss: 57.40713930130005  Train_Reconstruction: 54.46024703979492  Train_KL: 2.946892589330673  Validation Loss : 57.004804611206055 Val_Reconstruction : 54.09248733520508 Val_KL : 2.9123178720474243\n","EarlyStopping counter: 24 out of 100\n","Epoch: 3194/5000  Traning Loss: 57.55710315704346  Train_Reconstruction: 54.61822557449341  Train_KL: 2.9388774931430817  Validation Loss : 57.12188720703125 Val_Reconstruction : 54.2105770111084 Val_KL : 2.9113107919692993\n","EarlyStopping counter: 25 out of 100\n","Epoch: 3195/5000  Traning Loss: 57.567649364471436  Train_Reconstruction: 54.6337571144104  Train_KL: 2.9338934123516083  Validation Loss : 57.089229583740234 Val_Reconstruction : 54.17536735534668 Val_KL : 2.913861632347107\n","EarlyStopping counter: 26 out of 100\n","Epoch: 3196/5000  Traning Loss: 57.569127559661865  Train_Reconstruction: 54.62605619430542  Train_KL: 2.9430715441703796  Validation Loss : 56.92427062988281 Val_Reconstruction : 54.0107307434082 Val_KL : 2.91353976726532\n","EarlyStopping counter: 27 out of 100\n","Epoch: 3197/5000  Traning Loss: 57.326037883758545  Train_Reconstruction: 54.383347034454346  Train_KL: 2.9426909387111664  Validation Loss : 56.76276969909668 Val_Reconstruction : 53.85959243774414 Val_KL : 2.9031760692596436\n","EarlyStopping counter: 28 out of 100\n","Epoch: 3198/5000  Traning Loss: 57.246378898620605  Train_Reconstruction: 54.30311584472656  Train_KL: 2.9432632327079773  Validation Loss : 56.5306339263916 Val_Reconstruction : 53.60503005981445 Val_KL : 2.925604462623596\n","EarlyStopping counter: 29 out of 100\n","Epoch: 3199/5000  Traning Loss: 57.1435284614563  Train_Reconstruction: 54.19674873352051  Train_KL: 2.9467792212963104  Validation Loss : 56.72563171386719 Val_Reconstruction : 53.80659866333008 Val_KL : 2.9190328121185303\n","EarlyStopping counter: 30 out of 100\n","Epoch: 3200/5000  Traning Loss: 57.518420696258545  Train_Reconstruction: 54.58001184463501  Train_KL: 2.9384091794490814  Validation Loss : 56.83804130554199 Val_Reconstruction : 53.940765380859375 Val_KL : 2.897275686264038\n","EarlyStopping counter: 31 out of 100\n","Epoch: 3201/5000  Traning Loss: 57.6625018119812  Train_Reconstruction: 54.73690319061279  Train_KL: 2.9255984127521515  Validation Loss : 56.828433990478516 Val_Reconstruction : 53.93639945983887 Val_KL : 2.892033338546753\n","EarlyStopping counter: 32 out of 100\n","Epoch: 3202/5000  Traning Loss: 57.641982078552246  Train_Reconstruction: 54.70700120925903  Train_KL: 2.9349806904792786  Validation Loss : 57.18267631530762 Val_Reconstruction : 54.279422760009766 Val_KL : 2.903252959251404\n","EarlyStopping counter: 33 out of 100\n","Epoch: 3203/5000  Traning Loss: 57.322744846343994  Train_Reconstruction: 54.39305925369263  Train_KL: 2.9296859800815582  Validation Loss : 56.66033744812012 Val_Reconstruction : 53.766273498535156 Val_KL : 2.894065260887146\n","EarlyStopping counter: 34 out of 100\n","Epoch: 3204/5000  Traning Loss: 57.59956932067871  Train_Reconstruction: 54.67252063751221  Train_KL: 2.9270492792129517  Validation Loss : 57.09024620056152 Val_Reconstruction : 54.19904327392578 Val_KL : 2.891202926635742\n","EarlyStopping counter: 35 out of 100\n","Epoch: 3205/5000  Traning Loss: 57.88862752914429  Train_Reconstruction: 54.95505619049072  Train_KL: 2.933571517467499  Validation Loss : 57.348981857299805 Val_Reconstruction : 54.44045829772949 Val_KL : 2.9085237979888916\n","EarlyStopping counter: 36 out of 100\n","Epoch: 3206/5000  Traning Loss: 58.40109300613403  Train_Reconstruction: 55.46132707595825  Train_KL: 2.93976628780365  Validation Loss : 57.36169242858887 Val_Reconstruction : 54.46767616271973 Val_KL : 2.8940166234970093\n","EarlyStopping counter: 37 out of 100\n","Epoch: 3207/5000  Traning Loss: 57.58470678329468  Train_Reconstruction: 54.65502309799194  Train_KL: 2.929683595895767  Validation Loss : 56.62801742553711 Val_Reconstruction : 53.73962211608887 Val_KL : 2.8883942365646362\n","EarlyStopping counter: 38 out of 100\n","Epoch: 3208/5000  Traning Loss: 57.25408601760864  Train_Reconstruction: 54.322211265563965  Train_KL: 2.9318751394748688  Validation Loss : 56.73990058898926 Val_Reconstruction : 53.82896614074707 Val_KL : 2.9109342098236084\n","EarlyStopping counter: 39 out of 100\n","Epoch: 3209/5000  Traning Loss: 57.52613019943237  Train_Reconstruction: 54.57813739776611  Train_KL: 2.9479925334453583  Validation Loss : 56.86419486999512 Val_Reconstruction : 53.96223449707031 Val_KL : 2.9019603729248047\n","EarlyStopping counter: 40 out of 100\n","Epoch: 3210/5000  Traning Loss: 57.52192974090576  Train_Reconstruction: 54.60344076156616  Train_KL: 2.9184884130954742  Validation Loss : 57.015289306640625 Val_Reconstruction : 54.12438774108887 Val_KL : 2.890902042388916\n","EarlyStopping counter: 41 out of 100\n","Epoch: 3211/5000  Traning Loss: 57.86858558654785  Train_Reconstruction: 54.94004678726196  Train_KL: 2.9285387992858887  Validation Loss : 57.04952621459961 Val_Reconstruction : 54.14693641662598 Val_KL : 2.9025899171829224\n","EarlyStopping counter: 42 out of 100\n","Epoch: 3212/5000  Traning Loss: 57.6022891998291  Train_Reconstruction: 54.675368785858154  Train_KL: 2.926920086145401  Validation Loss : 56.75528144836426 Val_Reconstruction : 53.85734176635742 Val_KL : 2.897940158843994\n","EarlyStopping counter: 43 out of 100\n","Epoch: 3213/5000  Traning Loss: 57.454819679260254  Train_Reconstruction: 54.52051544189453  Train_KL: 2.9343042075634003  Validation Loss : 56.80442237854004 Val_Reconstruction : 53.89736557006836 Val_KL : 2.9070571660995483\n","EarlyStopping counter: 44 out of 100\n","Epoch: 3214/5000  Traning Loss: 57.577919006347656  Train_Reconstruction: 54.64237070083618  Train_KL: 2.9355487525463104  Validation Loss : 57.05385398864746 Val_Reconstruction : 54.15605354309082 Val_KL : 2.8977999687194824\n","EarlyStopping counter: 45 out of 100\n","Epoch: 3215/5000  Traning Loss: 57.34686613082886  Train_Reconstruction: 54.417025089263916  Train_KL: 2.9298407435417175  Validation Loss : 56.85465049743652 Val_Reconstruction : 53.95528602600098 Val_KL : 2.899364709854126\n","EarlyStopping counter: 46 out of 100\n","Epoch: 3216/5000  Traning Loss: 57.426684856414795  Train_Reconstruction: 54.493019580841064  Train_KL: 2.9336644113063812  Validation Loss : 56.94094467163086 Val_Reconstruction : 54.03619194030762 Val_KL : 2.9047526121139526\n","EarlyStopping counter: 47 out of 100\n","Epoch: 3217/5000  Traning Loss: 57.51780319213867  Train_Reconstruction: 54.595951557159424  Train_KL: 2.921852082014084  Validation Loss : 57.01368522644043 Val_Reconstruction : 54.13315773010254 Val_KL : 2.8805261850357056\n","EarlyStopping counter: 48 out of 100\n","Epoch: 3218/5000  Traning Loss: 57.558507442474365  Train_Reconstruction: 54.63484764099121  Train_KL: 2.9236598312854767  Validation Loss : 57.008832931518555 Val_Reconstruction : 54.114397048950195 Val_KL : 2.8944344520568848\n","EarlyStopping counter: 49 out of 100\n","Epoch: 3219/5000  Traning Loss: 57.58027219772339  Train_Reconstruction: 54.64679670333862  Train_KL: 2.9334757924079895  Validation Loss : 57.23288154602051 Val_Reconstruction : 54.338623046875 Val_KL : 2.894259810447693\n","EarlyStopping counter: 50 out of 100\n","Epoch: 3220/5000  Traning Loss: 57.71491098403931  Train_Reconstruction: 54.79554748535156  Train_KL: 2.9193632304668427  Validation Loss : 57.496402740478516 Val_Reconstruction : 54.61441612243652 Val_KL : 2.881986141204834\n","EarlyStopping counter: 51 out of 100\n","Epoch: 3221/5000  Traning Loss: 57.6289176940918  Train_Reconstruction: 54.697805404663086  Train_KL: 2.931111991405487  Validation Loss : 57.29719352722168 Val_Reconstruction : 54.39307403564453 Val_KL : 2.9041194915771484\n","EarlyStopping counter: 52 out of 100\n","Epoch: 3222/5000  Traning Loss: 57.7887978553772  Train_Reconstruction: 54.85678291320801  Train_KL: 2.9320158660411835  Validation Loss : 57.173234939575195 Val_Reconstruction : 54.27273941040039 Val_KL : 2.900494337081909\n","EarlyStopping counter: 53 out of 100\n","Epoch: 3223/5000  Traning Loss: 57.81106996536255  Train_Reconstruction: 54.886966705322266  Train_KL: 2.9241042137145996  Validation Loss : 57.12093734741211 Val_Reconstruction : 54.22649383544922 Val_KL : 2.8944443464279175\n","EarlyStopping counter: 54 out of 100\n","Epoch: 3224/5000  Traning Loss: 57.34262657165527  Train_Reconstruction: 54.41200256347656  Train_KL: 2.9306240379810333  Validation Loss : 56.664472579956055 Val_Reconstruction : 53.76737022399902 Val_KL : 2.8971015214920044\n","EarlyStopping counter: 55 out of 100\n","Epoch: 3225/5000  Traning Loss: 57.34543037414551  Train_Reconstruction: 54.422136306762695  Train_KL: 2.9232943058013916  Validation Loss : 56.80670166015625 Val_Reconstruction : 53.910444259643555 Val_KL : 2.896258592605591\n","EarlyStopping counter: 56 out of 100\n","Epoch: 3226/5000  Traning Loss: 57.24824666976929  Train_Reconstruction: 54.31787729263306  Train_KL: 2.9303697645664215  Validation Loss : 56.61006164550781 Val_Reconstruction : 53.70648384094238 Val_KL : 2.9035768508911133\n","EarlyStopping counter: 57 out of 100\n","Epoch: 3227/5000  Traning Loss: 57.11394691467285  Train_Reconstruction: 54.17157602310181  Train_KL: 2.9423709213733673  Validation Loss : 56.62322998046875 Val_Reconstruction : 53.714189529418945 Val_KL : 2.909039855003357\n","EarlyStopping counter: 58 out of 100\n","Epoch: 3228/5000  Traning Loss: 57.12332248687744  Train_Reconstruction: 54.1826114654541  Train_KL: 2.9407113194465637  Validation Loss : 56.58382606506348 Val_Reconstruction : 53.67407417297363 Val_KL : 2.9097533226013184\n","EarlyStopping counter: 59 out of 100\n","Epoch: 3229/5000  Traning Loss: 57.23719882965088  Train_Reconstruction: 54.306822299957275  Train_KL: 2.9303765892982483  Validation Loss : 56.60596466064453 Val_Reconstruction : 53.71100616455078 Val_KL : 2.894958019256592\n","EarlyStopping counter: 60 out of 100\n","Epoch: 3230/5000  Traning Loss: 57.32889413833618  Train_Reconstruction: 54.39625310897827  Train_KL: 2.9326406717300415  Validation Loss : 56.57058334350586 Val_Reconstruction : 53.66811752319336 Val_KL : 2.9024657011032104\n","EarlyStopping counter: 61 out of 100\n","Epoch: 3231/5000  Traning Loss: 57.58478355407715  Train_Reconstruction: 54.6458477973938  Train_KL: 2.938935339450836  Validation Loss : 57.22220993041992 Val_Reconstruction : 54.31134223937988 Val_KL : 2.910866618156433\n","EarlyStopping counter: 62 out of 100\n","Epoch: 3232/5000  Traning Loss: 57.886014461517334  Train_Reconstruction: 54.95125865936279  Train_KL: 2.934755027294159  Validation Loss : 57.55514335632324 Val_Reconstruction : 54.644704818725586 Val_KL : 2.9104392528533936\n","EarlyStopping counter: 63 out of 100\n","Epoch: 3233/5000  Traning Loss: 57.66857957839966  Train_Reconstruction: 54.73652124404907  Train_KL: 2.9320579171180725  Validation Loss : 57.00328063964844 Val_Reconstruction : 54.10088539123535 Val_KL : 2.9023948907852173\n","EarlyStopping counter: 64 out of 100\n","Epoch: 3234/5000  Traning Loss: 57.320964336395264  Train_Reconstruction: 54.39290189743042  Train_KL: 2.9280620217323303  Validation Loss : 56.8520450592041 Val_Reconstruction : 53.953895568847656 Val_KL : 2.8981505632400513\n","EarlyStopping counter: 65 out of 100\n","Epoch: 3235/5000  Traning Loss: 57.45931148529053  Train_Reconstruction: 54.53268527984619  Train_KL: 2.9266264140605927  Validation Loss : 56.779747009277344 Val_Reconstruction : 53.875 Val_KL : 2.9047482013702393\n","EarlyStopping counter: 66 out of 100\n","Epoch: 3236/5000  Traning Loss: 57.44174909591675  Train_Reconstruction: 54.51543617248535  Train_KL: 2.9263127744197845  Validation Loss : 56.68764305114746 Val_Reconstruction : 53.80078887939453 Val_KL : 2.8868541717529297\n","EarlyStopping counter: 67 out of 100\n","Epoch: 3237/5000  Traning Loss: 57.39292287826538  Train_Reconstruction: 54.46638631820679  Train_KL: 2.926536440849304  Validation Loss : 56.67262649536133 Val_Reconstruction : 53.77311706542969 Val_KL : 2.8995083570480347\n","EarlyStopping counter: 68 out of 100\n","Epoch: 3238/5000  Traning Loss: 57.20319414138794  Train_Reconstruction: 54.26193189620972  Train_KL: 2.9412614703178406  Validation Loss : 56.58237075805664 Val_Reconstruction : 53.66889953613281 Val_KL : 2.9134719371795654\n","EarlyStopping counter: 69 out of 100\n","Epoch: 3239/5000  Traning Loss: 57.24814748764038  Train_Reconstruction: 54.31132984161377  Train_KL: 2.9368175864219666  Validation Loss : 56.57733345031738 Val_Reconstruction : 53.678951263427734 Val_KL : 2.898382782936096\n","EarlyStopping counter: 70 out of 100\n","Epoch: 3240/5000  Traning Loss: 57.263386726379395  Train_Reconstruction: 54.33716106414795  Train_KL: 2.9262249171733856  Validation Loss : 56.591543197631836 Val_Reconstruction : 53.69249725341797 Val_KL : 2.89904522895813\n","EarlyStopping counter: 71 out of 100\n","Epoch: 3241/5000  Traning Loss: 57.36585807800293  Train_Reconstruction: 54.42461585998535  Train_KL: 2.9412421584129333  Validation Loss : 56.77059745788574 Val_Reconstruction : 53.85606575012207 Val_KL : 2.9145305156707764\n","EarlyStopping counter: 72 out of 100\n","Epoch: 3242/5000  Traning Loss: 57.33744955062866  Train_Reconstruction: 54.40055465698242  Train_KL: 2.936894655227661  Validation Loss : 56.731101989746094 Val_Reconstruction : 53.83035659790039 Val_KL : 2.9007461071014404\n","EarlyStopping counter: 73 out of 100\n","Epoch: 3243/5000  Traning Loss: 57.818167209625244  Train_Reconstruction: 54.890231132507324  Train_KL: 2.9279359579086304  Validation Loss : 57.298688888549805 Val_Reconstruction : 54.40392303466797 Val_KL : 2.894765615463257\n","EarlyStopping counter: 74 out of 100\n","Epoch: 3244/5000  Traning Loss: 57.60625457763672  Train_Reconstruction: 54.670777797698975  Train_KL: 2.935476154088974  Validation Loss : 56.933712005615234 Val_Reconstruction : 54.03958702087402 Val_KL : 2.894125461578369\n","EarlyStopping counter: 75 out of 100\n","Epoch: 3245/5000  Traning Loss: 57.41277265548706  Train_Reconstruction: 54.48164081573486  Train_KL: 2.9311316311359406  Validation Loss : 56.733699798583984 Val_Reconstruction : 53.837114334106445 Val_KL : 2.8965871334075928\n","EarlyStopping counter: 76 out of 100\n","Epoch: 3246/5000  Traning Loss: 57.45552110671997  Train_Reconstruction: 54.52687311172485  Train_KL: 2.92864790558815  Validation Loss : 57.24824523925781 Val_Reconstruction : 54.34487724304199 Val_KL : 2.9033674001693726\n","EarlyStopping counter: 77 out of 100\n","Epoch: 3247/5000  Traning Loss: 57.46824073791504  Train_Reconstruction: 54.52433156967163  Train_KL: 2.9439097344875336  Validation Loss : 56.75931167602539 Val_Reconstruction : 53.85229301452637 Val_KL : 2.907020092010498\n","EarlyStopping counter: 78 out of 100\n","Epoch: 3248/5000  Traning Loss: 57.353776931762695  Train_Reconstruction: 54.41835880279541  Train_KL: 2.9354186356067657  Validation Loss : 56.83993339538574 Val_Reconstruction : 53.9441032409668 Val_KL : 2.89583158493042\n","EarlyStopping counter: 79 out of 100\n","Epoch: 3249/5000  Traning Loss: 57.23922395706177  Train_Reconstruction: 54.3209171295166  Train_KL: 2.918306976556778  Validation Loss : 56.8817081451416 Val_Reconstruction : 53.97842216491699 Val_KL : 2.903286576271057\n","EarlyStopping counter: 80 out of 100\n","Epoch: 3250/5000  Traning Loss: 57.33838891983032  Train_Reconstruction: 54.39824914932251  Train_KL: 2.940140038728714  Validation Loss : 57.15270233154297 Val_Reconstruction : 54.241065979003906 Val_KL : 2.9116368293762207\n","EarlyStopping counter: 81 out of 100\n","Epoch: 3251/5000  Traning Loss: 57.65219068527222  Train_Reconstruction: 54.72447729110718  Train_KL: 2.927713245153427  Validation Loss : 56.76060676574707 Val_Reconstruction : 53.88214302062988 Val_KL : 2.878463387489319\n","EarlyStopping counter: 82 out of 100\n","Epoch: 3252/5000  Traning Loss: 57.169968605041504  Train_Reconstruction: 54.25168561935425  Train_KL: 2.9182829558849335  Validation Loss : 56.71542739868164 Val_Reconstruction : 53.82448959350586 Val_KL : 2.890939235687256\n","EarlyStopping counter: 83 out of 100\n","Epoch: 3253/5000  Traning Loss: 57.062482833862305  Train_Reconstruction: 54.13354301452637  Train_KL: 2.928939640522003  Validation Loss : 56.500253677368164 Val_Reconstruction : 53.595252990722656 Val_KL : 2.9050008058547974\n","EarlyStopping counter: 84 out of 100\n","Epoch: 3254/5000  Traning Loss: 57.1988787651062  Train_Reconstruction: 54.257094860076904  Train_KL: 2.941784143447876  Validation Loss : 57.06080627441406 Val_Reconstruction : 54.14384078979492 Val_KL : 2.9169644117355347\n","EarlyStopping counter: 85 out of 100\n","Epoch: 3255/5000  Traning Loss: 57.57401132583618  Train_Reconstruction: 54.63411235809326  Train_KL: 2.9398986399173737  Validation Loss : 57.038896560668945 Val_Reconstruction : 54.13628959655762 Val_KL : 2.90260648727417\n","EarlyStopping counter: 86 out of 100\n","Epoch: 3256/5000  Traning Loss: 57.42106485366821  Train_Reconstruction: 54.49050235748291  Train_KL: 2.9305621683597565  Validation Loss : 56.69030570983887 Val_Reconstruction : 53.79236602783203 Val_KL : 2.897939682006836\n","EarlyStopping counter: 87 out of 100\n","Epoch: 3257/5000  Traning Loss: 57.1707878112793  Train_Reconstruction: 54.236243724823  Train_KL: 2.9345436692237854  Validation Loss : 56.76517295837402 Val_Reconstruction : 53.86260414123535 Val_KL : 2.9025702476501465\n","EarlyStopping counter: 88 out of 100\n","Epoch: 3258/5000  Traning Loss: 57.624104499816895  Train_Reconstruction: 54.69295597076416  Train_KL: 2.931148499250412  Validation Loss : 57.25290489196777 Val_Reconstruction : 54.35698127746582 Val_KL : 2.895922303199768\n","EarlyStopping counter: 89 out of 100\n","Epoch: 3259/5000  Traning Loss: 57.38572454452515  Train_Reconstruction: 54.454718589782715  Train_KL: 2.9310057759284973  Validation Loss : 56.822349548339844 Val_Reconstruction : 53.92909622192383 Val_KL : 2.893254041671753\n","EarlyStopping counter: 90 out of 100\n","Epoch: 3260/5000  Traning Loss: 57.188929080963135  Train_Reconstruction: 54.25905990600586  Train_KL: 2.92986923456192  Validation Loss : 56.55459976196289 Val_Reconstruction : 53.650474548339844 Val_KL : 2.904125690460205\n","EarlyStopping counter: 91 out of 100\n","Epoch: 3261/5000  Traning Loss: 57.4180645942688  Train_Reconstruction: 54.47639179229736  Train_KL: 2.9416727423667908  Validation Loss : 57.134023666381836 Val_Reconstruction : 54.220252990722656 Val_KL : 2.9137712717056274\n","EarlyStopping counter: 92 out of 100\n","Epoch: 3262/5000  Traning Loss: 57.51820945739746  Train_Reconstruction: 54.57861566543579  Train_KL: 2.939593404531479  Validation Loss : 56.948974609375 Val_Reconstruction : 54.04060935974121 Val_KL : 2.9083651304244995\n","EarlyStopping counter: 93 out of 100\n","Epoch: 3263/5000  Traning Loss: 57.39950466156006  Train_Reconstruction: 54.46489667892456  Train_KL: 2.9346081018447876  Validation Loss : 56.73161697387695 Val_Reconstruction : 53.83243751525879 Val_KL : 2.8991780281066895\n","EarlyStopping counter: 94 out of 100\n","Epoch: 3264/5000  Traning Loss: 57.501845359802246  Train_Reconstruction: 54.56160640716553  Train_KL: 2.9402392506599426  Validation Loss : 57.04611015319824 Val_Reconstruction : 54.11947441101074 Val_KL : 2.9266358613967896\n","EarlyStopping counter: 95 out of 100\n","Epoch: 3265/5000  Traning Loss: 57.25959300994873  Train_Reconstruction: 54.314136028289795  Train_KL: 2.945457845926285  Validation Loss : 56.597150802612305 Val_Reconstruction : 53.69284439086914 Val_KL : 2.904305934906006\n","EarlyStopping counter: 96 out of 100\n","Epoch: 3266/5000  Traning Loss: 57.6448392868042  Train_Reconstruction: 54.70641803741455  Train_KL: 2.9384214878082275  Validation Loss : 57.426198959350586 Val_Reconstruction : 54.52537155151367 Val_KL : 2.90082848072052\n","EarlyStopping counter: 97 out of 100\n","Epoch: 3267/5000  Traning Loss: 57.63504695892334  Train_Reconstruction: 54.7009711265564  Train_KL: 2.9340760111808777  Validation Loss : 56.94485282897949 Val_Reconstruction : 54.046852111816406 Val_KL : 2.8980010747909546\n","EarlyStopping counter: 98 out of 100\n","Epoch: 3268/5000  Traning Loss: 57.645116329193115  Train_Reconstruction: 54.709723472595215  Train_KL: 2.935393512248993  Validation Loss : 56.9108943939209 Val_Reconstruction : 54.005489349365234 Val_KL : 2.9054034948349\n","EarlyStopping counter: 99 out of 100\n","Epoch: 3269/5000  Traning Loss: 57.21844291687012  Train_Reconstruction: 54.28828144073486  Train_KL: 2.9301619827747345  Validation Loss : 56.75136184692383 Val_Reconstruction : 53.86372756958008 Val_KL : 2.8876343965530396\n","EarlyStopping counter: 100 out of 100\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","optimizer1 = torch.optim.Adam\n","optimizer2 = torch.optim.Adadelta\n","\n","batch_size = 1600\n","print(data_input.shape)\n","\n","\n","epoch = 0\n","\n","model = VAE_MSE(z_dim =8 , device = device).to(device)\n","epoch = train(model=model, optimizer=optimizer1, epochs=5000, batch_size = batch_size,\\\n","          lr=0.001, data_input=data_input, loss_fig_name=\"MSE_z8_earlyfrom3000\")\n","     \n","path = \"/home/igari/igari/VAE_本へ/checkpoint.pt\"\n","model = VAE_MSE(z_dim = 8, device = 'cpu')\n","model.load_state_dict(torch.load(path, map_location=torch.device('cpu')))\n","     \n","path = \"learned_model/epoch5000_z8_earlyfrom3000_weight.pth\"\n","torch.save(model.to('cpu').state_dict(), path)"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["path = \"/home/igari/igari/VAE_本へ/checkpoint.pt\"\n","model = VAE_MSE(z_dim = 4, device = 'cpu')\n","model.load_state_dict(torch.load(path, map_location=torch.device('cpu')))\n","     \n","path = \"learned_model/epoch30000_beta100_earlyfrom2000_Adam_weight.pth\"\n","torch.save(model.to('cpu').state_dict(), path)"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":630,"status":"ok","timestamp":1676625126825,"user":{"displayName":"猪狩高","userId":"00742343053661794311"},"user_tz":-540},"id":"y1ol8Jhp98s2","outputId":"478fe491-1357-4573-be47-9ed2303d981c"},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["#学習済みモデルの読み込み\n","path = \"learned_model/epoch30000_beta100_earlyfrom2000_Adam_weight.pth\"\n","model = VAE_MSE(z_dim = 4, device = 'cpu')\n","model.load_state_dict(torch.load(path, map_location=torch.device('cpu')))\n","#model = torch.load(\"/content/drive/MyDrive/VAE/model_weight.pth\")"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1            [-1, 8, 24, 32]             224\n","       BatchNorm2d-2            [-1, 8, 24, 32]              16\n","              Mish-3            [-1, 8, 24, 32]               0\n","            Conv2d-4           [-1, 16, 12, 16]           1,168\n","       BatchNorm2d-5           [-1, 16, 12, 16]              32\n","              Mish-6           [-1, 16, 12, 16]               0\n","            Conv2d-7             [-1, 32, 5, 7]           4,640\n","       BatchNorm2d-8             [-1, 32, 5, 7]              64\n","              Mish-9             [-1, 32, 5, 7]               0\n","           Linear-10                  [-1, 200]         224,200\n","             Mish-11                  [-1, 200]               0\n","      BatchNorm1d-12                  [-1, 200]             400\n","           Linear-13                    [-1, 5]           1,005\n","           Linear-14                  [-1, 150]         168,150\n","             Mish-15                  [-1, 150]               0\n","      BatchNorm1d-16                  [-1, 150]             300\n","           Linear-17                    [-1, 5]             755\n","      VAE_Encoder-18         [[-1, 5], [-1, 5]]               0\n","           Linear-19                  [-1, 100]             600\n","           Linear-20                  [-1, 384]          38,784\n","  ConvTranspose2d-21             [-1, 16, 6, 8]           8,208\n","      BatchNorm2d-22             [-1, 16, 6, 8]              32\n","             Mish-23             [-1, 16, 6, 8]               0\n","  ConvTranspose2d-24            [-1, 8, 12, 16]           2,056\n","      BatchNorm2d-25            [-1, 8, 12, 16]              16\n","             Mish-26            [-1, 8, 12, 16]               0\n","  ConvTranspose2d-27            [-1, 3, 24, 32]             387\n","      VAE_Decoder-28            [-1, 3, 24, 32]               0\n","================================================================\n","Total params: 451,037\n","Trainable params: 451,037\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.01\n","Forward/backward pass size (MB): 0.34\n","Params size (MB): 1.72\n","Estimated Total Size (MB): 2.07\n","----------------------------------------------------------------\n"]}],"source":["net = VAE_MSE(z_dim=5, device='cpu').to('cpu')\n","summary(net, (3, 24, 32))"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":472},"executionInfo":{"elapsed":1438,"status":"ok","timestamp":1676625130649,"user":{"displayName":"猪狩高","userId":"00742343053661794311"},"user_tz":-540},"id":"SH3yGWGl98s3","outputId":"3973a5fe-d646-4737-dbc3-9c8d9ac67ab3"},"outputs":[{"name":"stdout","output_type":"stream","text":["input_image\n","(10, 3, 24, 32)\n","0.0\n","1.0\n","renconst_image\n","(10, 3, 24, 32)\n","0.014716987\n","0.9999982\n"]},{"name":"stderr","output_type":"stream","text":["/home/igari/.local/share/virtualenvs/VAE_本へ-eDH28Mmx/lib/python3.6/site-packages/ipykernel_launcher.py:33: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 10 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["generate_xy_Image(model, data_input, fig_name=\"MSEepoch2000_z2_10e5_halfscale_weight.pth\")"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":170497,"status":"ok","timestamp":1676625623592,"user":{"displayName":"猪狩高","userId":"00742343053661794311"},"user_tz":-540},"id":"fdR6PaFEwngI","outputId":"125e917a-f2f0-48af-97fe-28c67cb9ec60"},"outputs":[],"source":["#data(50, 160, 2) -> return latent(160, 2)\n","def cal_step_meanlatent(data):\n","    z_mean_latent = torch.zeros(data.shape[1], 2)\n","    data = torch.from_numpy(data)\n","\n","    for i in range(data.shape[1]):\n","        zs_step_mean = torch.mean(data[:, i, :], axis = 0).reshape([-1, 2])           \n","        z_mean_latent[i] = zs_step_mean\n","\n","    return z_mean_latent#(160, 2)      \n","\n","\n","def cal_task_latent(model, image_data):\n","    device = 'cpu'\n","    \n","    image_data = image_data.reshape(image_data.shape[0]*image_data.shape[1], 3, 24, 32)\n","    model.eval()\n","    image_data = torch.from_numpy(image_data).float()\n","    image_data = image_data.to(device)\n","    with torch.no_grad():\n","        z_latent, _ = model.forward(image_data)#(16000, 2)\n","\n","    z_latent_pca, explain_variance_ratio = pca(z_latent, n_components=2)\n","    \n","    left_task_z_pca = z_latent_pca[:int(z_latent_pca.shape[0]/2), :]#(8000, 2)\n","    right_task_z_pca = z_latent_pca[int(z_latent_pca.shape[0]/2) :, :]#(8000, 2)\n","    left_task_z_pca = left_task_z_pca.reshape(50, 160, 2)\n","    right_task_z_pca = right_task_z_pca.reshape(50, 160, 2)\n","    print(\"z_left_latent_pca\")\n","\n","    left_task_mean_latent = cal_step_meanlatent(left_task_z_pca)\n","    right_task_mean_latent = cal_step_meanlatent(right_task_z_pca)\n","    \n","    return left_task_mean_latent, right_task_mean_latent, explain_variance_ratio\n","\n","def pca(z, n_components):\n","    pca = PCA(n_components).fit(z)\n","    return pca.fit_transform(z), pca.explained_variance_ratio_\n","\n","\n","#散布図にカラーマップ、ラベルをつけたりする\n","# https://python-academia.com/matplotlib-scatter/\n","#scatterとplotの二つを同時に適用できる\n","def visualize_2task_pca(left_pca,right_pca, explain_variance_ratio, fig_name):\n","    labels = [str(10*num) for num in range(1,int(left_pca.shape[0]/10)+1)]\n","    \n","    fig = plt.figure()\n","    ax = fig.add_subplot()\n","    ax.scatter(left_pca[:, 0], left_pca[:, 1])\n","    ax.scatter(right_pca[:, 0], right_pca[:, 1])\n","    ax.plot(left_pca[:, 0], left_pca[:, 1])\n","    ax.plot(right_pca[:, 0], right_pca[:, 1])\n","    ax.set_xlabel(\"first_pricipal_component(variance_ratio : {})\".format(explain_variance_ratio[0]))\n","    ax.set_ylabel(\"second_pricipal_component(variance_ratio : {})\".format(explain_variance_ratio[1]))\n","    ax.legend(['left_task', 'right_task'])\n","    ax.set_title('pca')\n","\n","    for i, label in enumerate(labels):\n","        ax.text(left_pca[10*i, 0], left_pca[10*i, 1],label)\n","        ax.text(right_pca[10*i, 0], right_pca[10*i, 1],label)\n","    plt.savefig(fig_name + \".png\")\n","\n","def visualize_2task_time_horizontal(task1_pca1, task_pca2, fig_name):\n","    fig = plt.figure()\n","    plt.subplots_adjust(hspace=0.6)\n","\n","    ax1 = fig.add_subplot(2, 1, 1)\n","    ax1.set_title(\"first_pricipal_component\")\n","    ax1.set_xlim(0, 160)\n","    # ax1.scatter(task1_pca1[:, 0])\n","    # ax1.scatter(task_pca2[:, 0])\n","    ax1.plot(task1_pca1[:, 0])\n","    ax1.plot(task_pca2[:, 0])\n","    ax1.legend(['task1', 'task2'])\n","\n","    ax2 = fig.add_subplot(2, 1, 2)\n","    ax2.set_title(\"second_pricipal_component\")\n","    ax2.set_xlim(0, 160)\n","    # ax2.scatter(task1_pca1[:, 1])\n","    # ax2.scatter(task_pca2[:, 1])\n","    ax2.plot(task1_pca1[:, 1])\n","    ax2.plot(task_pca2[:, 1])\n","    ax2.legend(['task1', 'task2'])\n","\n","    plt.savefig(fig_name + \"time_step.png\")\n","\n"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"elapsed":761,"status":"ok","timestamp":1676625652926,"user":{"displayName":"猪狩高","userId":"00742343053661794311"},"user_tz":-540},"id":"B1b4oDFAjDnJ","outputId":"5df968d4-5dd8-4a4f-82e1-848ba84145f2"},"outputs":[{"name":"stdout","output_type":"stream","text":["z_left_latent_pca\n","z_left : torch.Size([160, 2])\n","z_right : torch.Size([160, 2])\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["#それぞれのタスクの潜在表現を出力(160ステップ)\n","\n","z_left_latent_pca, z_right_latent_pca, explain_variabce_ratio = cal_task_latent(model, data_narray.reshape(100,160, 3, 24, 32))\n","print(\"z_left : {}\".format(z_left_latent_pca.shape))\n","print(\"z_right : {}\".format(z_right_latent_pca.shape))\n","\n","\n","visualize_2task_pca(z_left_latent_pca,z_right_latent_pca,explain_variabce_ratio,\"z_latent_z5\")"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["visualize_2task_time_horizontal(z_left_latent_pca, z_right_latent_pca, \"z_latent_z5\")"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"0ec090aa773ade7e0f8ec67d2e33208bf1095523e07eda0b6930f8923d806c02"}}},"nbformat":4,"nbformat_minor":0}
